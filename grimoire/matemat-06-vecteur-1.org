
#+STARTUP: showall

#+TITLE: Eclats de vers : Matemat 06 : Vecteurs - 1
#+AUTHOR: chimay
#+EMAIL: or du val chez gé courriel commercial
#+LANGUAGE: fr
#+LINK_HOME: file:../index.html
#+LINK_UP: file:index.html
#+HTML_HEAD: <link rel="stylesheet" type="text/css" href="../style/defaut.css" />

#+OPTIONS: H:6
#+OPTIONS: toc:nil

#+TAGS: noexport(n)

[[file:index.org][Index des Grimoires]]

#+INCLUDE: "../include/navigan-1.org"

#+TOC: headlines 1

#+INCLUDE: "../include/commandes-tex.org"

* Espaces vectoriels

#+TOC: headlines 1 local

\label{chap:vecteur}


** Dépendances

  - Chapitre \ref{chap:algebre} : Les structures algébriques
  - Chapitre \ref{chap:somme} : Les sommes


** Introduction

Soit un corps $\corps$, un ensemble quelconque $A$ et $n \in \setN$. Le but des espaces vectoriels est de fournir un cadre général aux $n$-tuples de $\corps^n$ et aux fonctions de $\corps^A$. Nous avons vu la correspondance $\corps^n \leftrightarrow \corps^A$ dans le cas particulier où $A$ possède un nombre fini d'éléments. Mais le lien entre les deux types d'objets ne s'arrête pas là : la comparaison de deux fonctions se base sur le même principe (étendu) que la comparaison de deux $n$-tuples. Nous avons également défini des produits mixtes $\cdot : \corps \times \corps^n \to \corps^n$ et $\cdot : \corps \times \corps^A \to \corps^A$ semblables. Les matrices représentant des applications linéaires, nous pouvons également les ajouter dans la liste. Si $u,v \in E$, avec $E \in \{ \corps^n , \corps^A , \matrice(\corps,m,n) \}$, on a :

#+BEGIN_CENTER
\(
\label{eq:mixte}
(\alpha \cdot \beta) \cdot u = \alpha \cdot (\beta \cdot u) \\
(\alpha + \beta) \cdot x = \alpha \cdot x + \beta \cdot x \\
\alpha \cdot (u + v) = \alpha \cdot u + \alpha \cdot v \\
1 \cdot u = 1
\)
#+END_CENTER

pour tout $\alpha, \beta \in \corps$. De plus l'addition induite sur $E$ par l'addition de $\corps$ transforme $E$ en groupe commutatif. On ne peut toutefois pas parler de corps pour $E$, car la multiplication matricielle n'est pas une multiplication induite, et est non commutative.


*** Attention

Ne pas confondre les additions définies sur $E$ et $\corps$, ni la multiplication de $\corps$ avec la multiplication mixte, ni le neutre de $E$ avec celui de $\corps$. Lorsqu'il y a un risque d'ambiguité, on parle du vecteur nul $0 \in E$ et du scalaire nul $0 \in \corps$.


** Définition

Soit un groupe commutatif pour l'addition $E$, ainsi qu'un corps $\corps$. Si il existe une opération de multiplication mixte $\cdot : \corps \times E \mapsto E$ vérifiant les propriétés \ref{eq:mixte} ci-dessus, on dit que $E$ est un espace vectoriel sur $\corps$. On nomme alors « vecteurs » les éléments de $E$ et « scalaires » les éléments de $\corps$.


*** Notation

On note aussi :

#+BEGIN_CENTER
\(
x - y = x + (-1) \cdot y \\
x \cdot \alpha = \alpha \cdot x \\
\alpha \cdot \beta \cdot x = (\alpha \cdot \beta) \cdot x \\
\alpha x = \alpha \cdot x \\
\frac{x}{\alpha} = \alpha^{-1} \cdot x
\)
#+END_CENTER

Lorsque $\alpha$ a un inverse dans $\corps$, on a même les « fractions » :

$$\frac{x}{\alpha} = \unsur{\alpha} \cdot x$$


*** Corollaires

Les propriétés de la multiplication mixte nous montrent directement que :

#+BEGIN_CENTER
\(
0 \cdot u = (1 - 1) \cdot u = u - u = 0 \\
\alpha \cdot 0 = \alpha \cdot (u - u) = \alpha - \alpha = 0
\)
#+END_CENTER


*** Remarque

Le corps $\corps$ est souvent $\setR$ ou $\setC$.


** Sous-espace

On dit que $F \subseteq E$ est un sous-espace vectoriel de $E$ si $0 \in F$ et si :

$$z = \alpha \cdot x + \beta \cdot y$$

appartient à $F$ quels que soient les vecteurs $x,y \in F$ et les scalaires $\alpha,\beta \in \corps$.

On vérifie par exemple que $E$ est un sous-espace vectoriel de lui-même.


** Espace engendré

L'espace engendré par les vecteurs $e_1,e_2,...,e_n \in E$ est l'ensemble des combinaisons linéaires formées à partir des $e_i$ :

$$\combilin{e_1,...,e_n} = \left\{ \sum_{i=1}^{n} \alpha_i \cdot e_i : \alpha_i \in \corps \right\}$$

On vérifie que $\combilin{e_1,...,e_n}$ est un sous-espace vectoriel de $E$.


*** Remarque

Les espaces vectoriels ne pouvant pas s'exprimer comme ci-dessus sont dit de dimension infinie.


** Indépendance linéaire

On dit qu'une série de vecteurs $e_1,...,e_n$ est linéairement
indépendante si pour toute suite de scalaires $\alpha_i$, la condition :

$$\sum_{i=1}^{n} \alpha_i \cdot e_i = 0$$

implique que tous les scalaires soient nuls :

$$\alpha_i = 0$$

pour tout $i \in \{1,2,...,n\}$.


** Coordonnées

Soit les vecteurs linéairement indépendants $(e_1,...,e_n)$ et
$x \in \combilin{e_1,...,e_n}$. On peut trouver une suite de scalaire $\alpha_i$ tels que :

$$x = \sum_{i = 1}^n \alpha_i \cdot e_i$$

Supposons que l'on ait également :

$$x = \sum_{i=1}^n \beta_i \cdot e_i$$

pour une autre suite de scalaires $\beta_i$. En soustrayant les deux
équations, on obtient :

$$\sum_{i=1}^n (\alpha_i - \beta_i) \cdot e_i = 0$$

L'indépendance linéaire des $e_i$ implique alors que $\alpha_i - \beta_i = 0$, c'est-à-dire :

$$\alpha_i = \beta_i$$

pour tout $i \in \{1,2,...,n\}$.

On a donc unicité des coefficients scalaire de la combinaison linéaire. On dit que les $\alpha_i$ sont les coordonnées de $x$ par rapport aux $(e_1,...,e_n)$.


*** Base

Par contre, l'existence de telles coordonnées n'est pas garantie pour tout $x \in E$. Ce ne sera le cas que si :

$$E \subseteq \combilin{e_1,...,e_n}$$

On dit alors que $(e_1,...,e_n)$ forme une base de $E$.


*** Dimension finie

On dit qu'un espace vectoriel $E$ est de dimension finie s'il posséde au moins une base de la forme $(e_1,...,e_n)$, où $n \in \setN$ est fini. Dans le cas où $E$ {\em ne possède pas} une telle base, il est dit de dimension infinie.


*** Equivalence

On voit qu'étant donné une base de $E$, il y a équivalence entre un vecteur $x \in E$ et un élément $(x_1,x_2,...,x_n) \in \corps^n$ formé par ses coordonnées.

Nous noterons donc également (et abusivement) $x = (x_1,x_2,...,x_n)$, mais attention : il ne faut jamais perdre de vue que les $x_i$ dépendent de la base utilisée. Le vecteur $x$ est lui invariant sous changement de base.


** Absence de redondance

Soit $e_1,...,e_n \in E$ une suite de vecteurs linéairement indépendants.
Soit $i \in \{ 1,2,...,n \}$ et :

$$J(i) = \setZ(0,n) \setminus \{ i \}$$

Supposons que le vecteur $e_i$ soit une combinaison des autres vecteurs :

$$e_i = \sum_{ j \in J(i) } \alpha_j \cdot e_j$$

On a donc :

$$e_i - \sum_{ j \in J(i) } \alpha_j \cdot e_j = 0$$

L'hypothèse d'indépendance linéaire voudrait que tous les $\alpha_j$ et le $\alpha_i$ soient nuls. Ce qui n'est manifestement pas le cas puisque $\alpha_i = 1 \ne 0$ !

Aucun des vecteurs de la suite n'est donc combinaison des autres. On dit qu'aucun vecteur n'est redondant dans la suite.

** Base canonique sur $\corps^n$

Soit $\corps \in \{ \setR, \setC \}$. On note $\canonique_i$ l'élément de $\corps^n$ ayant un $1$ en $i^{ème}$ position et des $0$ partout ailleurs. On a donc :

\begin{eqnarray*}
\canonique_1 &=& (1,0,...,0) \\
\canonique_2 &=& (0,1,0,...,0) \\
&\vdots& \\
\canonique_n &=& (0,...,0,1)
\end{eqnarray*}

On a alors, pour tout $x = (x_1,...,x_n) \in \corps^n$ :

$$x = \sum_{i = 1}^n x_i \cdot \canonique_i$$


** Représentation matricielle

On représente généralement les vecteurs de $\corps^n$ par des vecteurs lignes ou colonnes. On parle alors de « vecteurs matriciels ». Le $i^{ème}$ vecteur de la base canonique est défini par le vecteur colonne :

#+BEGIN_CENTER
\(
\canonique_i = ( \indicatrice_{ij} )_j =
\begin{Matrix}{c}
\vdots \\ 0 \\ 1 \\ 0 \\ \vdots
\end{Matrix}
\)
#+END_CENTER

soit :

#+BEGIN_CENTER
\(
\canonique_1 = [1 \ 0 \ \hdots \ 0]^T \\
\canonique_2 = [0 \ 1 \ \hdots \ 0]^T \\
\vdots \\
\canonique_n = [0 \ \hdots \ 0 \ 1]^T
\)
#+END_CENTER


* Norme

#+TOC: headlines 1 local

\label{chap:norme}


** Dépendances

  - Chapitre \ref{chap:distance} : Les distances
  - Chapitre \ref{chap:vecteur} : Les espaces vectoriels


** Norme

Soit $S$ un corps muni d'un ordre $\le$ et $E$ un espace vectoriel sur $\corps$. Une norme $\norme{.} : E \to \setR$ est intuitivement la « grandeur » d'un vecteur $x \in E$. Cette grandeur correspond à la distance séparant le vecteur nul de $x$ :

$$\norme{x} \equiv \distance(x,0)$$

Soit $x,y,z \in E$.

Comme $\distance(x,0) \ge 0$, on impose par analogie que :

$$\norme{x} \ge 0$$

De plus, le seul vecteur $x$ de $E$ vérifiant :

$$\norme{x} = 0$$

implique que notre distance équivalente $\distance(x,0) = 0$ soit nulle, ce qui n'est possible que si $x = 0$. Lorsqu'on fait référence à ces conditions, on dit que la norme est strictement définie positive.

Considérons à présent l'inégalité triangulaire :

$$\distance(0 , x + y) \le \distance(0,x) + \distance(x , x + y)$$

Comment faire correspondre $\distance(x , x + y)$ à une norme ? En demandant simplement que notre distance particulière soit invariante sous translation de $x$ :

$$\distance(x , x + y) = \distance(x - x , x + y - x) = \distance(0,y) \equiv \norme{y}$$

On impose donc l'inégalité triangulaire :

$$\norme{x + y} \le \norme{x} + \norme{y}$$

Par ailleurs, lorsqu'on allonge ou réduit un vecteur $x$ d'un facteur $\alpha \in \corps$, la distance parcourue sur $x$ devra être allongée ou réduite par la valeur absolue de $\alpha$ :

$$\norme{\alpha \cdot x} = \abs{\alpha} \cdot \norme{x}$$


** Borne inférieure

On déduit de la définition que :

$$\norme{-x} = \norme{(-1) \cdot x} = \abs{-1} \cdot \norme{x} = \norme{x}$$

En posant $z = x + y$, on a :

$$\norme{z} \le \norme{x} + \norme{y} = \norme{x}  + \norme{z-x}$$

c'est-à-dire :

$$\norme{z - x} \ge \norme{z} - \norme{x}$$

Mais comme $\norme{z-x} = \norme{(-1) \cdot (z-x)} = \norme{x-z}$, la propriété vaut également en interchangeant $z$ et $x$, et on obtient :

$$\norme{z - x} \ge \max\{ \norme{z} - \norme{x}, \norme{x} -\norme{z} \}$$


** Distance associée

On peut associer une distance $d$ à une norme $\norme{.}$ en posant :

$$\distance(x,y) = \norme{x - y}$$

En effet :

  - $\distance(x,x) = \norme{x - x} = \norme{0} = 0$
  - si $\distance(x,y) = 0 = \norme{x-y}$, on a forcément $x - y = 0$ et donc $x = y$
  - $\distance(x,y) = \norme{x-y} = \norme{y-x} = \distance(y,x)$
  - $\distance(x,y) + \distance(y,z) = \norme{x-y} + \norme{y-z} \ge \norme{x-y+y-z} = \norme{x-z} = \distance(x,z)$


** Normalisation

On peut toujours normaliser un vecteur $w \ne 0$ pour obtenir un vecteur $u$ de norme $1$. Comme $\norme{w} \ne 0$, on peut écrire :

$$u = \frac{w}{\norme{w}}$$

On a alors :

$$\norme{u} = \norme{ \frac{w}{\norme{w}} } = \unsur{\norme{w}} \cdot \norme{w} = 1$$


* Espaces de Banach

#+TOC: headlines 1 local

\label{chap:banach}


** Définition

On dit qu'un espace vectoriel $X$ est un espace de Banach si il est complet pour la distance issue de la norme $\distance(x,y) = \norme{x - y}$. Dans la suite, nous considérons un espace de Banach $X$ sur $\setR$ ou $\setC$.


** Application contractante

On dit qu'une application $A : X \mapsto X$ est contractante s'il existe un $c \in \intervallesemiouvertdroite{0}{1} \subseteq \setR$ tel que :

$$\distance\big( A(u) , A(v) \big) \le c \cdot \distance(u,v)$$

pour tout $u,v\in X$.


** Suite de Cauchy

Soit une application contractante $A : X \mapsto X$ et $u_0 \in X$. On définit la suite $u_0,u_1,u_2,...$ par :

$$u_n = A(u_{n - 1}) = ... = A^n(u_0)$$

pour tout $n \in \setN$. On a alors :

$$\distance( u_{n + 1} , u_n ) \le c \cdot \distance( u_n , u_{n - 1} ) \le ... \le c^n \cdot \distance( u_1 , u_0 )$$

Soit $m,n \in \setN$. Les propriétés des distances nous permettent d'écrire :

$$\distance( u_{n + m} , u_n ) \le \sum_{i = 0}^{m - 1} \distance( u_{n + i + 1} , u_{n + i} )$$

Mais comme $\distance( u_{n + i + 1} , u_{n + i} ) \le c^{n + i} \cdot \distance( u_1 , u_0 )$, on a :

\begin{eqnarray*}
\distance( u_{n + m} , u_n ) &\le& \sum_{i = 0}^{m - 1} c^{n + i} \cdot \distance( u_1 , u_0 ) \\
&\le& c^n \cdot \distance( u_1 , u_0 ) \cdot \sum_{i = 0}^{m - 1} c^i \\
&\le& c^n \cdot \frac{1 - c^m}{1-c}
\end{eqnarray*}

Finalement, comme $1 - c^m \le 1$ quel que soit $m \in \setN$ on obtient une expression qui ne dépend pas de $m$ :

$$\distance( u_{n + m} , u_n ) \le \frac{c^n}{1 - c} \cdot \distance( u_1 , u_0 )$$

Les éléments de la suite sont donc de plus en plus proche l'un de l'autre
lorsque $n$ augmente. Soit $\epsilon \strictsuperieur 0$. Comme la suite $c^n$ converge vers $0$ lorsque $n \to \infty$, on peut toujours trouver $N$ tel que :

$$c^N \le \frac{\epsilon \cdot (1 - c)}{\distance( u_1 , u_0 )}$$

Il suffit donc de choisir $i,j \in \setN$ tels que $i,j \ge N$ pour avoir :

$$\distance( u_i , u_j ) = \distance( u_j , u_i ) \le \epsilon$$

On en conclut que la suite des $u_n$ est de Cauchy.


** Point fixe

Comme $X$ est complet, notre suite $u_n$ étant de Cauchy converge vers une certaine limite :

$$p = \lim_{n \to \infty} u_n$$

appartenant à $X$. Analysons le comportement de $A(p)$. On a la borne supérieure :

$$\distance\big( A(p) , p \big) \le \distance\big( A(p) , A(u_n) \big) + \distance\big( A(u_n) , u_n \big) + \distance\big( u_n , p \big)$$

On sait déjà que $\distance( u_n , p )$ converge vers $0$ par définition de $p$. On sait aussi que $\distance\big( A(u_n) , u_n \big) = \distance( u_{n + 1} , u_n ) \to 0$. On a également :

$$\distance\big( A(p) , A(u_n) \big) \le c \cdot \distance\big( p , u_n \big)$$

On en conclut que la suite $A(u_n)$ converge vers $A(p)$ :

$$A(p) = \lim_{n \to \infty} A(u_n)$$

Les trois termes de la borne supérieure convergeant chacun vers $0$, cette borne est aussi petite que l'on veut lorsque $n$ est assez grand. On a donc $\distance\big( A(p) , p \big) = 0$ et :

$$A(p) = p$$

L'élément $p \in X$ est un point fixe de $A$.


** Unicité

Si $p_1$ et $p_2$ sont deux points fixes, on a :

#+BEGIN_CENTER
\(
A(p_1) = p_1 \\
A(p_2) = p_2
\)
#+END_CENTER

et :

$$\distance( p_1 , p_2 ) \le c \cdot \distance\big( A(p_1) , A(p_2) \big) \le c \cdot \distance\left( p_1 , p_2 \right)$$

Comme $c \strictinferieur 1$, ce n'est possible que si $\distance(p_1,p_2) = 0$, c'est-à-dire :

$$p_1 = p_2$$

Le point fixe de $A$ est unique.


** Vitesse de convergence

Nous avons donc montré que la suite des $u_n$ converge vers l'unique point fixe $p$ de $A$, et ce quel que soit $u_0$. On a même la propriété suivante nous donnant une borne supérieure pour le taux de convergence :

$$\distance(u_n,p) \le \distance\big(A(u_{n - 1}),A(p)\big) \le c \cdot \distance(u_{n - 1},p) \le ... \le c^n \cdot \distance(u_0,p)$$
