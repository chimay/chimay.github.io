
#+STARTUP: showall

#+TITLE: Eclats de vers : Matemat 06 : Vecteurs - 2
#+AUTHOR: chimay
#+EMAIL: or du val chez gé courriel commercial
#+LANGUAGE: fr
#+LINK_HOME: file:../index.html
#+LINK_UP: file:index.html
#+HTML_HEAD: <link rel="stylesheet" type="text/css" href="../style/defaut.css" />

#+OPTIONS: H:6
#+OPTIONS: toc:nil

#+TAGS: noexport(n)

[[file:index.org][Index des Grimoires]]

#+INCLUDE: "../include/navigan-1.org"

#+TOC: headlines 1

#+INCLUDE: "../include/commandes-tex.org"

* Continuité

#+TOC: headlines 1 local

\label{chap:limite}


** Dépendances

  - Chapitre \ref{chap:limite} : Les limites
  - Chapitre \ref{chap:reel} : Les réels


** Fonctions continues

Une fonction $f : D \to F$ est dite continue en $a$ si :

$$\lim_{ \substack{ x \to a \\ x \in D } } f(x) = f(a)$$

Soit $A \subseteq D$. On dit qu'une fonction est continue sur $A$ si :

$$\lim_{ \substack{ x \to a \\ x \in A } } f(x) = f(a)$$

pour tout $a \in A$. On note $\continue(A,F)$ l'ensemble des fonctions $f : A \mapsto F$ continues sur $A$.


*** Remarque

Si $F$ est muni d'une norme, les limites s'évaluent au sens de la distance découlant de la norme.


** Espace vectoriel

Si $\corps$ est un corps, on vérifie que $\continue(A,\corps)$ est un espace vectoriel sur $\corps$. En effet, la fonction nulle $0$ est continue. Si $\alpha,\beta \in \corps$ et si $f,g \in \continue(A,\corps)$, on a :

\begin{align}
\lim_{x \to a} (\alpha \cdot f(x) + \beta \cdot g(x)) &= \alpha \cdot \lim_{x \to a} f(x) + \beta \cdot \lim_{x \to a} g(x) \\
&= \alpha \cdot f(a) + \beta \cdot g(a)
\end{align}

pour tout $a \in A$. On en conclut que $\alpha \cdot f + \beta \cdot g$ est également continue.


** Norme des fonctions continues

Si l'ensemble $F$ est muni d'une norme, on peut définir la norme $\norme{.}_\continue$ d'une fonction continue $u$ par :

$$\norme{u}_\continue = \sup \big\{ \norme{u(x)} : x \in A \big\}$$


*** Notation

On note aussi :

$$\norme{u}_\infty = \norme{u}_\continue$$


*** Convergence uniforme

Cette norme est surtout utilisée lorsqu'il s'agit de mesurer l'écart entre deux fonctions $f,g : A \to B$, en particulier lorsque $g$ représente une approximation de $f$. Dans ce cas, l'écart $e = f - g$ représente l'erreur la plus élevée de l'estimation :

$$\norme{e}_\infty = \norme{f - g}_\infty = \sup \{ \norme{f(x) - g(x)} : x \in A \}$$

Lorsque cette norme particulière de l'erreur tend vers zéro, on parle de convergence uniforme.


** Théorème des valeurs intermédiaires

Les fonctions continues possèdent l'importante propriété suivante.

\begin{theoreme}

$$ $$


  - Soit $f \in \continue(I,\setR)$ où $I = [a,b]$ est un intervalle inclus dans $\setR$. On suppose que :

$$f(a) \strictinferieur f(b)$$

Soit le réel $\varphi$ vérifiant $f(a) \strictinferieur \varphi \strictinferieur f(b)$. On peut alors trouver un $c \in \intervalleouvert{a}{b}$ tel que $f(c) = \varphi$.

  - Soit $g \in \continue(I,\setR)$. On suppose que :

$$g(a) \strictsuperieur g(b)$$

Soit le réel $\varphi$ vérifiant $g(a) \strictsuperieur \varphi \strictsuperieur g(b)$. On peut alors trouver un $c \in \intervalleouvert{a}{b}$ tel que $g(c) = \varphi$.


\end{theoreme}

\begin{demonstration}

Nous allons démontrer ce résultat par l'absurde.


  - Considérons le cas où $f(a) \strictinferieur \varphi \strictinferieur f(b)$. On définit les ensembles :

#+BEGIN_CENTER
\(
A^+ = \{ x \in I : f(x) \strictsuperieur \varphi \} \\
A^- = \{ x \in I : f(x) \strictinferieur \varphi \}
\)
#+END_CENTER

Si aucun $c \in I$ ne vérifie $f(c) = \varphi$, on doit avoir clairement $A^+ \cup A^- = I$.

Nous définissons $\alpha = \sup A^-$. Comme $A^-\subseteq I$, on à clairement
$\alpha \in I$. Si $\alpha \in A^-$, alors par continuité de $f$ en $\alpha$,
on peut trouver $\delta \strictsuperieur 0$ tel que :

$$\abs{ f(\alpha + \delta) - f(\alpha) } \le \epsilon = \unsur{2}(\varphi - f(\alpha))$$

On a alors clairement $f(\alpha + \delta) \strictinferieur \varphi$ et $\alpha \strictinferieur \alpha + \delta \in A^-$ ce qui contredit l'hypothèse de suprémum pour $\alpha$.

On doit donc avoir $\alpha \notin A^-$. Mais alors $\alpha \in I \setminus A^- = A^+$. Donc $f(\alpha) \strictsuperieur \varphi$. Par continuité de $f$ en $\alpha$, on peut trouver $\delta \strictsuperieur 0$ tel que :

$$\abs{ f(\alpha) - f(x) } \le \epsilon = \unsur{2}(f(\alpha)-\varphi)$$

pour tout $x \in \intervalleouvert{\alpha - \delta}{\alpha}$. On a alors clairement $f(x) \strictsuperieur \varphi$ pour tout $x \in \intervalleouvert{\alpha - \delta}{\alpha}$.

Soit $\beta \in \intervalleouvert{\alpha - \delta}{\alpha}$. Par définition de $\alpha$, on ne peut pas avoir $\beta \ge A^-$, donc il existe $\gamma \in \intervalleouvert{\beta}{\alpha} \subseteq \intervalleouvert{\alpha - \delta}{\alpha}$ tel que $\gamma \in A^-$. On a donc $f(\gamma) \strictinferieur \varphi$ ce qui contredit la propriété ci-dessus en $x = \gamma$.

  - Considérons à présent le cas $g(a) \strictsuperieur \varphi \strictsuperieur g(b)$. On pose :

$$f = -g$$

On a :

$$f(a) \strictinferieur -\varphi \strictinferieur f(b)$$

On peut donc trouver un $c \in \intervalleouvert{a}{b}$ tel que :

$$f(c) = -\varphi$$

On en conclut que :

$$g(c) = -f(c) = \varphi$$


\end{demonstration}


*** Généralisation

Soit le réel $\varphi$ tel que $f(a) \le \varphi \le f(b)$. Si $\varphi \in \{f(a),f(b)\}$, il suffit de prendre $c \in \{a,b\}$ pour avoir $f(a) \le f(c) \le f(b)$. Sinon, on applique le théorème des valeurs intermédiaires et on trouve un $c$ vérifiant $f(a) \strictinferieur f(c) \strictinferieur f(b)$. Mais dans tous les cas, on pourra trouver un $c \in [a,b]$ tel que $f(a) \le f(c) \le f(b)$. De même si $f(a) \strictsuperieur f(b)$.


** Théorème de la bijection

\begin{theoreme}

Soit $f \in \continue(I,J)$ où $I = [a,b]$ est un intervalle inclus dans $\setR$ et où $J = f(I)$. Si $f$ est strictement croissante (ou décroissante), alors $f$ est inversible et :

$$f(I) = [\alpha,\beta]$$

avec :

#+BEGIN_CENTER
\(
\alpha = \min \{ f(a), f(b) \} \\
\beta = \max \{ f(a), f(b) \}
\)
#+END_CENTER

\end{theoreme}

\begin{demonstration}

Comme $f : I \mapsto J = f(I)$ est strictement croissante ou décroissante, on a vu dans le chapitre traitant des bijections que $f$ est inversible. Choisissons un réel $\varphi \in [\alpha,\beta]$. Le théorème des valeurs intermédiaires nous dit qu'on peut trouver un réel $c \in [a,b]$ tel que $f(c) = \varphi$. On en conclut que $\varphi \in f(I)$. Cette relation étant vérifiée pour tout $\varphi \in [\alpha,\beta]$, on a :

$$[\alpha,\beta] \subseteq f(I)$$

Soit $x \in I$. Comme $f$ est croissante ou décroissante, on doit avoir :

$$f(a) \le f(x) \le f(b)$$

ou :

$$f(a) \ge f(x) \ge f(b)$$

On a donc également :

$$f(I) \subseteq [\alpha,\beta]$$

L'inclusion étant réciproque, on a :

$$f(I) = [\alpha,\beta]$$

\end{demonstration}


** Continuité uniforme

\label{sec:continuite_uniforme}

On dit qu'une fonction $f$ est uniformément continue sur $A$, si pour toute précision $\epsilon \strictsuperieur 0$, on peut trouver un $\delta \strictsuperieur 0$ tel que :

$$\abs{f(s) - f(t)} \le \epsilon$$

pour tout $s,t \in A$ vérifiant $\abs{s - t} \le \delta$.


*** Continuité simple

Si $f$ est uniformément continue sur $A$, on a clairement :

$$\lim_{s \to t} \abs{f(s) - f(t)} = 0$$

et donc :

$$\lim_{s \to t} f(s) = f(t)$$

pour tout $t \in A$. Toute fonction uniformément continue est donc continue.


** Polynômes

Soit $n \in \setN$ et $\alpha,\beta \in \setR$ avec $\alpha \le \beta$. Nous allons analyser la continuité du monôme $\mu : [\alpha,\beta] \mapsto \setR$ défini par :

$$\mu : x \mapsto x^n$$

pour tout $x \in [\alpha,\beta]$. Reprenant les résultats de la section \ref{sec:factorisation_progression_geometrique}, nous avons :

$$s^n - t^n = (s - t) \sum_{i = 0}^{n - 1} s^{n - 1 - i} \cdot t^i$$

Quelque soient $s,t \in [\alpha,\beta]$, il est clair que

$$\abs{s}, \abs{t} \le M = \max \{ \abs{\alpha} , \abs{\beta} \}$$

On a donc :

$$\abs{s^n - t^n} \le \abs{s - t} \cdot n \cdot M^{n - 1}$$

Fixons à présent $\epsilon \strictsuperieur 0$. Il suffit de prendre :

$$\abs{s - t} \le \delta \le \frac{\epsilon}{ n \cdot M^{n - 1} }$$

pour avoir :

$$\abs{s^n - t^n} \le \delta \cdot n \cdot M^n \le \epsilon$$

Comme le choix de $\delta$ ne dépend ni de $s$ ni de $t$, le monôme $\mu$ est uniformément continu sur $[\alpha,\beta]$.

Pour généraliser aux polynômes de la forme :

$$p(x) = \sum_{i = 0}^n a_i \cdot x^i$$

on part de :

$$p(s) - p(t) = \sum_{i = 0}^n a_i \cdot (s^i - t^i)$$

On a donc :

$$\abs{p(s) - p(t)} \le \sum_{i = 0}^n \abs{a_i} \cdot \abs{s^i - t^i}$$

Mais comme on peut trouver des $\delta_k$ tels que :

$$\abs{s^k - t^k} \le \frac{\epsilon}{\sum_j \abs{a_j}}$$

il suffit de choisir $\delta = \min \{ \delta_0, \delta_1, ..., \delta_n \}$ pour avoir :

$$\abs{p(s) - p(t)} \le \epsilon \cdot \frac{ \sum_i \abs{a_i} }{ \sum_j \abs{a_j} } = \epsilon$$

Cette généralisation montre aussi que toute combinaison linéaire de fonctions uniformément continues est uniformément continue.


*** Continuité simple

Qu'en est-il de la continuité sur $\setR$ ? Choisissons $a \in \setR$ et considérons l'intervalle $I = [a - 1, a + 1]$. Le polynôme est uniformément continu sur cette intervalle, et $a \in \interieur I$. Plus précisément, $\distance(a,\setR \setminus I) = 1 \strictsuperieur 0$. Donc, si $\abs{x - a} \le 1$, on a forcément $x \in I$. On peut donc se servir de la continuité uniforme sur $I$ pour trouver un $\delta \in \intervalleouvert{0}{1}$ tel que $\abs{p(x) - p(a)} \le \epsilon$ lorsque $\abs{x - a} \le \delta$. Les polynômes sont donc continus en tout point de $\setR$, et par conséquent continus sur $\setR$.


** Uniformité

Nous allons à présent montrer que toute fonction continue sur un intervalle de la forme $[\alpha,\beta]$ y est uniformément continue.

\begin{theoreme}

Soit la fonction $f \in \continue([\alpha,\beta],\setR)$. Etant donné un $\epsilon \strictsuperieur 0$ et un $a \in [\alpha,\beta]$, on note $\Delta(a,\epsilon)$ l'ensemble des écarts strictement positifs offrant la précision demandée. Pour tout $\delta \in \Delta(a,\epsilon)$, on aura donc $\delta \strictsuperieur 0$ et :

$$\abs{f(a + h) - f(a)} \le \epsilon$$

pourvu que $h \in \setR$ vérifie :

#+BEGIN_CENTER
\(
\abs{h} \le \delta \\
a + h \in [\alpha,\beta]
\)
#+END_CENTER

On note les supremums de cette famille d'ensemble par :

$$\sigma(a,\epsilon) = \sup \Delta(a,\epsilon)$$

Nous allons voir que l'intersection de ces ensembles est non vide, même après avoir parcouru tout l'intervalle :

$$\Gamma(\epsilon) = \bigcap_{a \in [\alpha,\beta]} \Delta(a,\epsilon) \ne \emptyset$$

et que l'infimum des supremums est strictement positif :

$$I(\epsilon) = \inf \{ \sigma(a,\epsilon) : a \in [\alpha,\beta] \} \strictsuperieur 0$$

Etant donné un $\epsilon \strictsuperieur 0$, on peut donc trouver un $\delta  \in \Gamma(\epsilon)$ tel que :

$$\abs{f(x + h) - f(x)} \le \epsilon$$

pour tout $x \in [\alpha,\beta]$ et pour tout $h$ vérifiant :

#+BEGIN_CENTER
\(
\abs{h} \le \delta \\
x + h \in [\alpha,\beta]
\)
#+END_CENTER

Posant $s = x + h$ et $t = x$, cela revient à dire que :

$$\abs{f(s) - f(t)} \le \epsilon$$

pour tout $s,t \in [\alpha,\beta]$ vérifiant $\abs{s - t} \le \delta$. La fonction $f$ est donc uniformément continue sur $[\alpha,\beta]$.

\end{theoreme}


*** Remarques


  - La continuité de $f$ nous garantit que ces écarts strictement positifs existent bien, c'est à dire que :

$$\Delta(a,\epsilon) \ne \emptyset$$

quelles que soient les valeurs de $\epsilon \strictsuperieur 0$ et de $a \in [\alpha,\beta]$.

  - Par ailleurs, si :

$$0 \strictinferieur \gamma \le \delta \in \Delta(a,\epsilon)$$

tous les réels présentant un écart inférieur à $\gamma$ (par rapport à $a$) auront a fortiori un écart inférieur à $\delta$ et satisferont donc la précision $\epsilon$ :

$$f\big( [a - \gamma, a + \gamma] \big) \subseteq [f(a) - \epsilon, f(a) + \epsilon]$$

Par conséquent, $\gamma$ appartient à $\Delta(a,\epsilon)$ et :

$$]0,\delta] \subseteq \Delta(a,\epsilon)$$

pour tout $\delta \in \Delta(a,\epsilon)$.

  -  Si $x \in \intervalleouvert{0}{\sigma(a,\epsilon)}$, on a :

$$\psi = \sigma(a,\epsilon) - x \strictsuperieur 0$$

Comme le supremum est dans l'adhérence, la distance à son ensemble est nulle et on peut trouver un $\delta \in \Delta(a,\epsilon)$ tel que :

$$\abs{\sigma(a,\epsilon) - \delta} \le \psi$$

On a donc :

$$\sigma(a,\epsilon) - \delta \le \sigma(a,\epsilon) - x$$

et :

$$x \le \delta$$

On a alors :

$$x \in \ ]0,\delta] \subseteq \Delta(a,\epsilon)$$

et donc $x \in \Delta(a,\epsilon)$. Cette relation étant vérifiée pour tout $x \in \intervalleouvert{0}{\sigma(a,\epsilon)}$, on a :

$$\intervalleouvert{0}{\sigma(a,\epsilon)} \subseteq \Delta(a,\epsilon)$$

  - Par définition du supremum, il ne peut avoir d'élément de $\Delta(a,\epsilon)$ supérieur à $\sigma(a,\epsilon)$, et on a également :

$$\Delta(a,\epsilon) \subseteq \intervallesemiouvertgauche{0}{\sigma(a,\epsilon)}$$

  - Les propositions sur l'intersection non vide et l'infimum strictement positif sont équivalentes. En effet, si $I(\epsilon) \strictsuperieur 0$, on a :

$$\sigma(a,\epsilon) \ge I(\epsilon) \strictsuperieur 0$$

pour tout $a$. On a donc :

$$\emptyset \ne \ ]0,I(\epsilon)[ \ \subseteq \bigcap_{a \in [\alpha,\beta]} (0,\sigma(a,\epsilon)) \subseteq \bigcap_{a \in [\alpha,\beta]} \Delta(a,\epsilon)$$

D'un autre coté, si l'intersection est non nulle, soit :

$$\delta \in \bigcap_{a \in [\alpha,\beta]} \Delta(a,\epsilon)$$

Comme $\delta$ appartient à $\Delta(a,\epsilon)$ pour tout $a \in [\alpha,\beta]$, on a :

$$\sigma(a,\epsilon) \ge \delta \strictsuperieur 0$$

par définition du supremum localisé en $a$. Il suffit alors de passer à l'infimum sur $a$ pour obtenir :

$$I(\epsilon) = \inf_{a \in [\alpha,\beta]} \sigma(a,\epsilon) \ge \delta \strictsuperieur 0$$

Nous nous attelerons ici à démontrer que $I(\epsilon) \strictsuperieur 0$.


\begin{demonstration}

Soit $\epsilon \strictsuperieur 0$. Considérons la suite d'infimums intermédiaires :

$$D(x) = \inf \{ \sigma(\xi,\epsilon) : \xi \in [\alpha,x] \}$$

où $x \in [\alpha,\beta]$. Nous considérons l'ensemble $\Psi$ des éléments tels que cet infimum soit non nul :

$$\Psi = \{ x \in [\alpha,\beta] : D(x) \strictsuperieur 0 \}$$

On a $D(\alpha) = \inf\{ \sigma(\alpha,\epsilon) \} = \sigma(\alpha,\epsilon) \strictsuperieur 0$. Donc $\alpha \in \Psi$. On a aussi $\Psi \subseteq [\alpha,\beta] \le \beta$. L'ensemble $\Psi$ est non vide et majoré. Il admet donc un supremum :

$$S = \sup \Psi \le \beta$$

  - Si $x \in \Psi$ et $a \in [\alpha,x]$, on a :

$$[\alpha,a] \subseteq [\alpha,x]$$

Les propriétés de l'infimum pour l'inclusion nous donnent alors :

$$D(a) \ge D(x) \strictsuperieur 0$$

On en conclut que :

$$[\alpha,x] \subseteq \Psi$$

pour tout $x \in \Psi$.

  - Soit $a \in [\alpha,\beta]$. Nous allons construire une zone autour de $a$ où l'infimum est strictement positif. Choisissons $\delta(a) \in \Delta(a,\epsilon/2)$ et posons :

$$\gamma(a) =  \unsur{2} \delta(a) \strictsuperieur 0$$

Considérons à présent l'ensemble :

$$U(a) = [a - \gamma(a), a + \gamma(a)] \cap [\alpha,\beta]$$

Pour tout $b \in U(a)$ et $x \in [b - \gamma(a), b + \gamma(a)] \cap [\alpha,\beta]$, on a alors :

\begin{align}
\abs{b - a} &\le \gamma(a) \strictinferieur \delta(a) \\
\abs{x - b} &\le \gamma(a) \\
\abs{x - a} &\le \abs{x - b} + \abs{b - a} \le 2 \gamma(a) \le \delta(a)
\end{align}

et :

#+BEGIN_CENTER
\(
\abs{f(b) - f(a)} \le \frac{\epsilon}{2} \\ \\
\abs{f(x) - f(a)} \le \frac{\epsilon}{2}
\)
#+END_CENTER

On en déduit la borne supérieure :

\begin{align}
\abs{f(x) - f(b)} &\le \abs{f(x) - f(a)} + \abs{f(a) - f(b)} \\
&\le \frac{\epsilon}{2} + \frac{\epsilon}{2} = \epsilon
\end{align}

On a donc :

$$\sigma(b,\epsilon) \ge \gamma(a) \strictsuperieur 0$$

pour tout $b \in U(a)$. En prenant l'infimum, il vient :

$$\inf_{b \in U(a)} \sigma(b,\epsilon) \ge \gamma(a) \strictsuperieur 0 $$

  - Supposons que $S = \alpha$ et posons :

$$\theta = \min \{ \gamma(\alpha) , \beta - \alpha \} \strictsuperieur 0$$

On a alors $U(S) = U(\alpha) = [\alpha, \alpha + \theta]$ et :

$$D(\alpha + \theta) = \inf_{b \in U(\alpha)} \sigma(b,\epsilon) \ge \gamma(\alpha) \strictsuperieur 0 $$

On en conclut que $\alpha + \theta \in \Psi$ avec $S \strictinferieur \alpha + \theta$, ce qui contredit $S = \sup \Psi$.

  - Supposons à présent que $\alpha \strictinferieur S \strictinferieur \beta$ et posons :

#+BEGIN_CENTER
\(
\eta = \min \{ \gamma(S) , S - \alpha \} \strictsuperieur 0 \\
\theta = \min \{ \gamma(S) , \beta - S \} \strictsuperieur 0
\)
#+END_CENTER

On a alors $U(S) = [S - \eta, S + \theta]$. Voyons comment se comporte $\sigma(b,\epsilon)$ lorsquer $b$ voyage dans $[\alpha, S + \theta]$. Le supremum étant dans l'adhérence, on peut trouver $\psi \in \Psi$ tel que :

$$\abs{S - \psi} = S - \psi \le \eta$$

Si $b \in [\alpha,\psi]$, on a :

$$\sigma(b,\epsilon) \ge D(\psi) \strictsuperieur 0 $$

par définition de $\Psi$. Mais si $b \in [\psi, S + \theta]$, on a :

$$S - \eta \le \psi \le b \le S + \theta$$

Donc, $b \in [S - \eta, S + \theta] = U(S)$ et :

$$\sigma(b,\epsilon) \ge \gamma(S) \strictsuperieur 0$$

Il suffit donc de poser :

$$\varpi = \min \{ \gamma(S) , D(\psi) \} \strictsuperieur 0$$

pour avoir $\sigma(b,\epsilon) \ge \varpi$ sur $[\alpha, S + \theta]$. On en déduit que l'infimum est strictement positif :

$$D(S + \theta) \ge \varpi \strictsuperieur 0 $$

C'est à dire $S + \theta \in \Psi$ avec $S \strictinferieur S + \theta$, ce qui contredit la définition du supremum. On doit donc avoir $S = \sup \Psi = \beta$.

  - Posons :

$$\theta = \min \{ \gamma(\beta) , \beta - \alpha \} \strictsuperieur 0$$

On a alors $U(S) = U(\beta) = [S - \theta, S] = [\beta - \theta, \beta]$. Comme le supremum est dans l'adhérence, on peut trouver un $\psi \in \Psi$ tel que :

$$\abs{S - \psi} = S - \psi \le \theta$$

Si $b \in [\alpha,\psi]$, on a :

$$\sigma(b,\epsilon) \ge D(\psi) \strictsuperieur 0 $$

par définition de $\Psi$. Mais si $b \in [\psi, S] = [\psi, \beta]$, on a :

$$S - \theta \le \psi \le b \le \beta$$

Donc, $b \in [S - \theta, S] = U(S)$ et :

$$\sigma(b,\epsilon) \ge \gamma(S) = \gamma(\beta) \strictsuperieur 0$$

Il suffit donc de poser :

$$\varpi = \min \{ \gamma(\beta) , D(\psi) \} \strictsuperieur 0$$

pour avoir $\sigma(b,\epsilon) \ge \varpi$ sur $[\alpha, \beta]$. On en déduit que l'infimum est strictement positif :

$$D(\beta) \ge \varpi \strictsuperieur 0 $$

C'est à dire $\beta \in \Psi$ et :

$$\Psi = [\alpha,\beta]$$

L'infimum $D(x)$ est donc strictement positif sur tout l'intervalle $[\alpha,\beta]$ et on a :

#+BEGIN_CENTER
\(
I(\epsilon) = D(\beta) \strictsuperieur 0 \\
\intervalleouvert{0}{I(\epsilon)} \subseteq \Gamma(\epsilon)
\)
#+END_CENTER


\end{demonstration}


*** Remarque

Le théorème {\em n'est pas} applicable aux intervalles ouverts ou semi-ouverts.


** Variations bornées

On déduit de l'uniforme continuité des fonctions continues sur les intervalles que les fonctions continues y sont bornées. En effet, soit $\epsilon \strictsuperieur$ et $\delta \strictsuperieur 0$ tel que :

$$\abs{f(b) - f(a)} \le \epsilon$$

pour tout $a,b \in [\alpha,\beta]$ tels que $\abs{a - b} \le \delta$.
Choisissons $N \in \setN$ tel que :

$$\frac{\beta - \alpha}{N} \le \delta$$

Choisissons à présent $x,y \in [\alpha,\beta]$ et posons :

$$h = \frac{y - x}{N}$$

On a alors :

$$\abs{h} = \abs{\frac{y - x}{N}} \le \frac{\beta - \alpha}{N} \le \delta$$

Posons $x_i = x + i \cdot h$ pour $i = 0,1,2,...,N$. On a alors $x_0 = x$ et $x_N = y$. La variation est bornée par :

$$\abs{f(x) - f(y)} \le \sum_{i = 0}^N \abs{f(x_i) - f(x_{i - 1})} \le (N + 1) \cdot \epsilon$$


*** Norme

Comme $N$ ne dépend ni de $x$ ni de $y$, on en conclut que les variations de $f$ sont bornées sur l'intervalle. On a aussi :

$$\abs{f(x)} \le \abs{f(x) - f(\alpha)} + \abs{f(\alpha)} \le M$$

où $M = (N + 1) \cdot \epsilon + \abs{f(\alpha)}$ ne dépend pas du choix de $x$, ce qui prouve que $f$ est bornée sur l'intervalle. En passant au supremum, on en déduit que la norme est finie et que :

$$\norme{f}_\infty \le M$$


** Extrema

Soit $f \in \continue([a,b],\setR)$. Comme $f$ est bornée, on peut poser :

$$I = \inf \{ f(x) : x \in [a,b] \} = \inf f([a,b])$$

Comme la distance de l'infimum à l'ensemble est nulle, on peut construire une suite $\{x_1,x_2,...\}$ convergente vers $\lambda \in [a,b]$ :

$$\lambda = \lim_{n \to \infty} x_n$$

et telle que :

$$f(x_n) - I \le \unsur{2^n}$$

On voit que :

$$\lim_{n \to \infty} f(x_n) = I$$

Mais par continuité de $f$, on a aussi :

$$\lim_{n \to \infty} f(x_n) = f(\lambda)$$

On en conclut que :

$$f(\lambda) = I$$

On peut donc trouver un réel dans l'intervalle qui minimise la fonction. L'infimum appartient à l'image $f([a,b])$. Il est donc également un minimum et on a :

$$f(\lambda) = \inf f([a,b]) = \min f([a,b])$$

On construit de même un $\sigma \in [a,b]$ qui atteint le supremum :

$$f(\sigma) = \sup f([a,b]) = \max f([a,b])$$
