
#+STARTUP: showall

#+TITLE: Eclats de vers : Matemat 06 : Vecteurs - 3
#+AUTHOR: chimay
#+EMAIL: or du val chez gé courriel commercial
#+LANGUAGE: fr
#+LINK_HOME: file:../index.html
#+LINK_UP: file:index.html
#+HTML_HEAD: <link rel="stylesheet" type="text/css" href="../style/defaut.css" />

#+OPTIONS: H:6
#+OPTIONS: toc:nil

#+TAGS: noexport(n)

[[file:index.org][Index des Grimoires]]

#+INCLUDE: "../include/navigan-1.org"

#+TOC: headlines 1

#+INCLUDE: "../include/commandes-tex.org"

* Applications linéaires

#+TOC: headlines 1 local

\label{chap:lineaire}


** Dépendances

  - Chapitre \ref{chap:fonction} : Les fonctions


** Définition

Soit les espaces vectoriels $E$ et $F$ sur $\corps$ et la fonction $f : E \mapsto F$. On dit que $f$ est linéaire si, pour tout $x,y \in E$ et $\alpha, \beta \in \corps$, on a :

$$f(\alpha \cdot x + \beta \cdot y) = \alpha \cdot f(x) + \beta \cdot f(y)$$

On note $\lineaire(E,F)$ l'ensemble des fonctions linéaires de $E$ vers $F$.


** Identité

L'application identité est clairement linéaire.


** Inverse

Soit $u = f(x)$ et $v = f(y)$. Si l'application inverse existe, on a $x = f^{-1}(u)$ et $y = f^{-1}(v)$. En composant à gauche par $f^{-1}$ la définition de la linéarité, on obtient :

$$\alpha \cdot f^{-1}(u) + \beta \cdot f^{-1}(v) = f^{-1}(\alpha \cdot u + \beta \cdot v)$$

ce qui montre que l'inverse est également linéaire.


** Valeur au vecteur nul

Choisissons un $x \in E$. on voit que :

$$f(0) = f(0 \cdot x) = 0 \cdot f(x) = 0$$

La valeur d'une application linéaire s'annule au vecteur nul.


** Norme des applications linéaires

La norme d'une application linéaire est définie comme étant l'extension maximale qu'elle produit :

$$\norme{f} = \sup \left\{ \frac{ \norme{f(x)} }{ \norme{x} } : x \in E, \ x \ne 0 \right\}$$

On a donc :

$$\norme{f(x)} \le \norme{f} \cdot \norme{x}$$

pour tout $x \in E \setminus \{ 0 \}$.


*** Vérification

Nous allons vérifier qu'il s'agit bien d'une norme. On a $\norme{f} \ge 0$ par positivité de la norme sur $E$ et $F$. La condition $\norme{f} = 0$ implique $\norme{f(x)} = 0$ et donc $f(x) = 0$ pour tout $x \ne 0$. Comme $f$ est linéaire, on a aussi $f(0) = 0$ et $f = 0$.

Si $f,g$ sont linéaires, on a :

$$\norme{f(x) + g(x)} \le \norme{f(x)} + \norme{g(x)} \le \norme{f} \cdot \norme{x} + \norme{g} \cdot \norme{x} = (\norme{f} + \norme{g}) \cdot \norme{x}$$

pour tout $x \ne 0$. En divisant par $\norme{x}$ et en passant au supremum, on obtient :

$$\norme{f + g} \le \norme{f} + \norme{g}$$

Enfin, si $\alpha \in \corps$, on a :

$$\frac{ \norme{\alpha \cdot f(x)} }{ \norme{x} } = \abs{\alpha} \cdot \frac{ \norme{f(x)} }{ \norme{x} }$$

En passant au supremum, on obtient :

$$\norme{\alpha \cdot f} = \abs{\alpha} \cdot \norme{f}$$


*** Notation

Lorsqu'il est nécessaire de différentier la norme au sens des applications linéaires d'autres types de normes utilisées, on note :

$$\norme{f}_\lineaire = \norme{f}$$


*** Définition alternative

Soit $N \in \corps$, avec $N \strictsuperieur 0$ et :

$$B = \{ u \in E : \norme{u} = N \}$$

Soit $x \in E$ avec $x \ne 0$ et :

$$\lambda = \frac{ \norme{x} }{N}$$

Définissons :

$$u = \frac{x}{\lambda}$$

On voit que :

$$\norme{u} = \norme{\unsur{\lambda} \cdot x} = \unsur{\lambda} \cdot \norme{x} = \frac{N}{ \norme{x} } \cdot \norme{x} = N$$

On a donc $u \in B$. Le rapport des normes s'écrit :

$$\frac{ \norme{f(x)} }{ \norme{x} } = \frac{ \norme{f(x)} }{ N \cdot \lambda } = \unsur{N} \norme{ \frac{f(x)}{ \lambda } } = \unsur{ \norme{u} } \cdot \norme{ f\left( \frac{x}{ \lambda } \right) } = \frac{ \norme{f(u)} }{ \norme{u} }$$

On en conclut que :

$$\frac{ \norme{f(x)} }{ \norme{x} } = \frac{ \norme{f(u)} }{ \norme{u} } \le \sup \Big\{ \frac{ \norme{f(v)} }{ \norme{v} } : v \in B \Big\}$$

Comme ce doit être valable quelque soit $x \ne 0$, on obtient :

$$\norme{f} \le \sup \Big\{ \frac{ \norme{f(v)} }{ \norme{v} } : v \in B \Big\}$$

en passant au supremum sur $x$.

Choisissons à présent $u \in B$. On a alors :

$$\frac{ \norme{f(u)} }{ \norme{u} } \le \norme{f}$$

En passant au supremum sur $u$, on obtient :

$$\sup \Big\{ \frac{ \norme{f(v)} }{ \norme{v} } : v \in B \Big\} \le \norme{f}$$

On en conclut que les deux supremums sont égaux :

$$\sup \left\{ \frac{ \norme{f(v)} }{ \norme{v} } : v \in B \right\} = \norme{f}$$


*** Norme unitaire

Une conséquence importante du résultat ci-dessus est le cas particulier $N = 1$. On a alors :

$$\norme{f} = \sup \left\{ \norme{f(v)} : v \in E, \ \norme{v} = 1 \right\}$$


** Norme d'une composée

Soit $f : E \mapsto F$ et $g : F \mapsto G$ deux applications linéaires de normes finies. Si $x \in E$ avec $x \ne 0$ on a $f(x) \in F$ et :

$$\norme{g \circ f(x)} \le \norme{g} \cdot \norme{f(x)} \le \norme{g} \cdot \norme{f} \cdot \norme{x}$$

En divisant par $\norme{x} \ne 0$ :

$$\frac{ \norme{g \circ f(x)} }{ \norme{x} } \le \norme{g} \cdot \norme{f}$$

et en passant au supremum sur $x \ne 0$, on en conclut que :

$$\norme{g \circ f} \le \norme{g} \cdot \norme{f}$$


** Norme d'une puissance

On a clairement :

$$\norme{f^n} = \norme{f \circ ... \circ f} \le \norme{f}^n$$


** Continuité

Nous allons montrer que, pour tout $f \in \lineaire(A,B)$, on a l'équivalence entre l'hypothèse d'une norme de $f$ finie et l'hypothèse de $f$ continue.

Si la norme est finie, on a :

$$\norme{f(x) - f(a)} = \norme{f(x - a)} \le \norme{f} \cdot \norme{x-a}$$

qui tend bien vers $0$ lorsque $x$ tend vers $a$. Inversément, si $f$ est continue, on peut trouver un $\delta \strictsuperieur 0$ tel que :

$$\norme{f(x) - f(0)} \le 1$$

pour tout $x$ vérifiant $\distance(x,0) = \norme{x} \le \delta$. Posons $B = \{ x \in A : \norme{x} = \delta \}$. On a alors :

$$\sup_{x \in B} \frac{\norme{f(x)}}{\norme{x}} = \unsur{\delta} \sup_{x \in B} \norme{f(x) - f(0)} \le \unsur{\delta}$$

La norme est donc finie :

$$\norme{f} = \sup_{x \in B} \frac{\norme{f(x)}}{\norme{x}} \le \unsur{\delta} \strictinferieur +\infty$$


** $n$-linéarité

On dit que la fonction $f : E_1 \times ... \times E_n \mapsto F$ est $n$-linéaire si elle est linéaire par rapport à chacune des composantes de son argument, les autres composantes restant inchangées :

$$f(...,\alpha x + \beta y,...) = \alpha \cdot f(...,x,...) + \beta \cdot f(...,y,...)$$

pour tout $\alpha,\beta \in \corps$ et $x,y \in E$. On note $\lineaire_n(E_1,...,E_n,F)$ l'ensemble des fonctions $n$-linéaires de $E_1 \times ... \times E_n$ vers $F$.


*** Norme

La norme est définie dans ce cas par :

$$\norme{f} = \sup \left\{ \frac{ \norme{f(x_1,...,x_n)} }{ \prod_{i = 1}^n \norme{x_i} } : (x_1,...,x_n) \in E_1 \times ... \times E_n \right\}$$

Si cette norme est finie, on a :

$$\norme{f(x_1,...,x_n)} \le \norme{f} \cdot \prod_{i = 1}^n \norme{x_i}$$

pour tout $(x_1,...,x_n) \in E_1 \times ... \times E_n$.


*** Bilinéarité

On dit aussi des fonctions $2$-linéaires qu'elles sont bilinéaires. La norme d'une fonction $f : E_1 \times E_2 \mapsto F$ bilinéaire est définie par :

$$\norme{f} = \sup \left\{ \frac{ \norme{f(u,v)} }{ \norme{u} \cdot \norme{v} } : (u,v) \in E_1 \times E_2 \right\}$$

Si cette norme est finie, on a :

$$\norme{f(u,v)} \le \norme{f} \cdot \norme{u} \cdot \norme{v}$$

pour tout $(u,v) \in E_1 \times E_2$.


** Représentation matricielle

Soit une application linéaire $\mathcal{A} : E \to F$. Choisissons $x \in E$ et posons :

$$y = \mathcal{A}(x)$$

Si on dispose d'une base $(e_1,...,e_n)$ de $E$ et d'une base $(f_1,...,f_m)$ de $F$, on a :

#+BEGIN_CENTER
\(
x = \sum_{i = 1}^n x_i \cdot e_i \\
y = \sum_{i = 1}^m y_i \cdot f_i
\)
#+END_CENTER

pour certains $x_i,y_i \in \corps$. La linéarité de $\mathcal{A}$ implique que :

$$y = \sum_{j = 1}^n \mathcal{A}(e_j) \cdot x_j$$

Si les $a_{ij} \in \corps$ sont les coordonnées de $\mathcal{A}(e_j)$ dans la base des $f_i$, on a :

$$\mathcal{A}(e_j) = \sum_{i = 1}^m a_{ij} \cdot f_i$$

En substituant cette expression, on obtient :

$$y = \sum_{i = 1}^m f_i \sum_{j = 1}^n a_{ij} \cdot x_j$$

La $i^{ème}$ coordonnée de $y$ est donc donnée par :

$$y_i = \sum_{j = 1}^n a_{ij} \cdot x_j$$

On définit la matrice $A \in \matrice(\corps,m,n)$ associée à $\mathcal{A}$ en posant :

$$A = ( a_{ij} )_{i,j}$$

Nous définissons ensuite le produit d'une matrice avec le « vecteur » $x$ équivalent de $\corps^n$ :

$$A \cdot x = \left( \sum_j a_{ij} \cdot x_j  \right)_i$$

de telle sorte que :

$$A \cdot x = \mathcal{A}(x)$$


*** Norme

La norme d'une matrice est la norme de l'application linéaire associée, c'est-à-dire :

$$\norme{A}_2 = \sup \left\{ \frac{ \norme{A \cdot x} }{ \norme{x} } : x \in \corps^n, \ x \ne 0 \right\}$$

Soit :

$$M = \max_{i,j} \abs{\composante_{ij} A}$$

On a alors :

$$\norme{A \cdot x} \le M \cdot m \cdot n \cdot \max_i x_i \le M \cdot m \cdot n \cdot \norme{x}$$

ce qui montre que :

$$\norme{A} \le M \cdot m \cdot n \strictinferieur \infty$$

La norme d'une matrice finie ($m,n \strictinferieur \infty$) existe toujours.


*** Image

L'image d'une matrice est l'image de l'application linéaire associée, c'est-à-dire :

$$\image A = \{ A \cdot x : x \in \corps^n \}$$

Si $c_i = \colonne_i A$, on a :

$$A = [ c_1 \ c_2 \ ... \ c_n ]$$

On voit que :

$$A \cdot x = \sum_i c_i \cdot x_i$$

autrement dit l'image de $A$ est l'espace vectoriel engendré par ses colonnes :

$$\image A = \combilin{c_1,c_2,...,c_n}$$


*** Noyau

Le noyau d'une matrice est le noyau de l'application linéaire associée, c'est-à-dire :

$$\noyau A = \{ x \in \corps^n : A \cdot x = 0 \}$$


** Produit matriciel

Soit à présent les matrices $A \in \matrice(\corps,m,n)$ et $B \in \matrice(\corps,n,p)$ données par :

#+BEGIN_CENTER
\(
A = ( a_{ij} )_{i,j} \\
B = ( b_{ij} )_{i,j}
\)
#+END_CENTER

où les $a_{ij},b_{ij} \in K$. Soit les applications linéaires $\mathcal{B} : \corps^p \to \corps^n$ et $\mathcal{A} : \corps^n \to \corps^m$ définies par :

#+BEGIN_CENTER
\(
\mathcal{B}(x) = B \cdot x \\
\mathcal{A}(y) = A \cdot y
\)
#+END_CENTER

pour tout $x \in \corps^p$, $y \in \corps^n$. Choisissons $z \in \corps^m$ et relions $x,y,z$ par :

#+BEGIN_CENTER
\(
y = \mathcal{B}(x) \\
z = \mathcal{A}(y) = \big( \mathcal{A} \circ \mathcal{B} \big)(x)
\)
#+END_CENTER

Examinons les composantes de $z$ en fonction de celles de $x$ :

$$z_i = \sum_k a_{ik} \cdot y_k = \sum_k a_{ik} \sum_j b_{kj} \cdot x_j = \sum_{k,j} a_{ik} \cdot b_{kj} \cdot x_j$$

On en déduit que la composée $\mathcal{C} = \mathcal{A} \circ \mathcal{B}$ est représentée par la matrice $C \in \matrice(\corps,m,p)$ de composantes :

$$\composante_{ij} C = c_{ij} = \sum_k a_{ik} \cdot b_{kj}$$

Il suffit donc de définir le produit matriciel $A \cdot B$ par :

$$A \cdot B = \left(\sum_{k=1}^n a_{ik} \cdot b_{kj}\right)_{i,j}$$

pour avoir :

$$(A \cdot B) \cdot x = \big( \mathcal{A} \circ \mathcal{B} \big)(x)$$

Le produit matriciel représente donc une composée d'applications linéaires. Pour que ce produit soit bien défini, il est nécessaire que le nombre
de colonnes $n$ de $A$ et le nombre de lignes de $B$ soient identiques.

On voit également que le produit matrice - vecteur défini précédemment en est un cas particulier lorsque $p = 1$.


*** Notation

En pratique, on laisse souvent tomber le ``$\cdot$'' et on note $A B$
au lieu de $A \cdot B$ lorsqu'il est évident que $A$ et $B$ sont deux
matrices différentes.


*** Taille

Le produit d'une matrice de taille $(m,n)$ par une matrice de taille $(n,p)$ est une matrice de taille $(m,p)$.


*** Lignes et colonnes

Si $x_i^T = \ligne_i(A)$ et $y_j = \colonne_j(B)$, on voit que :

$$\composante_{ij} (A \cdot B) = x_i^T \cdot y_j$$


*** Associativité

Soit les matrices $A \in \matrice(\corps,m,n)$, $B \in \matrice(\corps,n,p)$ et $C \in \matrice(\corps,p,q)$ données par :

#+BEGIN_CENTER
\(
A = ( a_{ij} )_{i,j} \\
B = ( b_{ij} )_{i,j} \\
C = ( c_{ij} )_{i,j}
\)
#+END_CENTER

où les $a_{ij},b_{ij},c_{ij} \in K$. La relation :

$$A \cdot (B \cdot C) = \left( \sum_{k,l} a_{ik} \cdot b_{kl} \cdot c_{lj} \right)_{i,j} = (A \cdot B) \cdot C$$

nous montre que la multiplication entre matrices est associative. On définit :

$$A \cdot B \cdot C = A \cdot (B \cdot C) = (A \cdot B) \cdot C$$


*** Distributivité

On a aussi les propriétés de distribution :

#+BEGIN_CENTER
\(
A \cdot (B + C) = A \cdot B + A \cdot C \\
(B + C) \cdot D = B \cdot D + C \cdot D
\)
#+END_CENTER

où $A \in \matrice(\corps,m,n)$, $B,C \in \matrice(\corps,n,p)$ et $D \in \matrice(\corps,p,q)$.


*** Non commutativité

Par contre, on peut trouver des matrices $A$ et $B$ telles que :

$$A \cdot B \ne B \cdot A$$

La multiplication matricielle n'est donc en général pas commutative. D'ailleurs, pour que ces deux produits existent simultanément, il faut que $A$ et $B$ soient toutes deux carrées, ce qui n'est pas forcément le cas.


*** Commutateur

La matrice associée au commutateur :

$$[\mathcal{A},\mathcal{B}] = \mathcal{A} \circ \mathcal{B} - \mathcal{B} \circ \mathcal{A}$$

est donnée par le commutateur équivalent :

$$[A,B] = A \cdot B - B \cdot A$$


*** Transposée

On vérifie que :

$$(A \cdot B)^T = B^T \cdot A^T$$


** Blocs

En utilisant l'associativité de l'addition, on peut facilement vérifier que la formule de multiplication reste valable lorsqu'on considère des blocs de matrices au lieu des éléments, à condition de respecter l'ordre de multiplication. Un exemple fréquemment utilisé :

#+BEGIN_CENTER
\(
\begin{Matrix}{cc}
A_{11} & A_{12} \\ A_{21} & A_{22}
\end{Matrix}
\cdot
\begin{Matrix}{cc}
B_{11} & B_{12} \\ B_{21} & B_{22}
\end{Matrix}
=
\begin{Matrix}{cc}
A_{11} \cdot B_{11} +  A_{12} \cdot B_{21} & A_{11} \cdot B_{12} +  A_{12} \cdot B_{22} \\
A_{21} \cdot B_{11} +  A_{22} \cdot B_{21} & A_{21} \cdot B_{12} +  A_{22} \cdot B_{22}
\end{Matrix}
\)
#+END_CENTER


*** Bloc-diagonale

Un cas particulier important :

#+BEGIN_CENTER
\(
\begin{Matrix}{cc}
A_1 & 0 \\ 0 & A_2
\end{Matrix}
\cdot
\begin{Matrix}{cc}
B_1 & 0 \\ 0 & B_2
\end{Matrix}
=
\begin{Matrix}{cc}
A_1 \cdot B_1 & 0 \\
0 & A_2 \cdot B_2
\end{Matrix}
\)
#+END_CENTER


** Matrice identité

La matrice identité $I \in \matrice(\corps,n,n)$ correspond à la fonction $\identité$. On a donc :

$$I \cdot x = x$$

pour tout $x \in \corps^n$. Si $(\canonique_1,...\canonique_n)$ est la base canonique de $\corps^n$, on a donc :

$$I \cdot \canonique_i = \canonique_i$$

ce qui entraîne directement :

$$I = ( \indicatrice_{ij} )_{i,j}$$

On remarque que :

$$I = [\canonique_1 \ \hdots \ \canonique_n]$$


*** Neutre

Comme la fonction identité est neutre pour la composition, la matrice unité correspondante $I \in \matrice(\corps,m,n)$ doit être neutre pour la multiplication avec toutes les matrices de dimensions compatibles. Soit $A \in \matrice(\corps,m,n)$ et $B \in \matrice(\corps,n,p)$. On vérifie que l'on a bien :

#+BEGIN_CENTER
\(
A \cdot I = A \\
I \cdot B = B
\)
#+END_CENTER


*** Notation

On note aussi $I_n$ pour préciser que $I$ est de taille $(n,n)$.


** Inverse

Lorsqu'elle existe, la matrice inverse de $A$, notée $A^{-1}$, reflète l'application linéaire inverse sous-jacente. Elle est donc l'unique matrice telle que :

$$A^{-1} \cdot A = A \cdot A^{-1} = I$$


** Inverse d'un produit

Soit $A$ et $B$ deux matrices inversibles. Les relations $C \cdot (A \cdot B) = I$ et $(A \cdot B) \cdot D = I$ nous donnent :

$$C = D = B^{-1} \cdot A^{-1}$$

et donc :

$$(A \cdot B)^{-1} = B^{-1} \cdot A^{-1}$$


*** Inverse à gauche et à droite

On dit que $L$ est un inverse à gauche de $A$ si $L \cdot A = I$. On dit que $R$ est un inverse à droite de $A$ si $A \cdot R = I$.


** Puissance

Il est possible de multiplier une matrice carrée $A$ avec elle-même.
On peut donc définir l'exposant par :

#+BEGIN_CENTER
\(
A^0 = I \\
A^k = A \cdot A^{k-1}
\)
#+END_CENTER


*** Négative

Si l'inverse $A^{-1}$ existe, on définit également :

$$A^{-k} = (A^{-1})^k$$


** Polynômes matriciels

Ici, $\corps$ n'est plus un corps mais l'anneau des matrices $X$ de taille $(N,N)$. On dit que $p : \matrice(\corps,N,N) \mapsto \matrice(\corps,N,N)$ est un polynôme matriciel si il existe $a_0,...,a_n \in \corps$ tels que :

$$p(X) = \sum_{i = 0}^n a_i \cdot X^i$$

pour tout $X \in \matrice(\corps,N,N)$.


* Géométrie

#+TOC: headlines 1 local


** Courbe

Une courbe sur un espace vectoriel $E$ (par exemple $\setR^n$) est de la forme :

#+BEGIN_CENTER
\(
\Lambda = \{ \lambda(s) : s \in [\alpha,\beta] \}
\)
#+END_CENTER

où $\lambda : [\alpha,\beta] \mapsto E$ est une fonction continue et où $\alpha,\beta \in \setR$ vérifient $\alpha \le \beta$.


** Segment

Les segments sont une généralisation des intervalles. Un segment de $u \in E$ vers $v \in E$ est un cas particulier de courbe où $\lambda : [0,1] \subseteq \setR \mapsto E$ est une fonction linéaire définie par :

#+BEGIN_CENTER
\(
\lambda(s) = u + s \cdot (v - u)
\)
#+END_CENTER

pour tout $s \in [0,1] \subseteq \setR$. On voit que $\lambda(0) = u$ et que $\lambda(1) = v$. On note aussi :

#+BEGIN_CENTER
\(\relax
[u,v] = \lambda([0,1]) = \{ u + s \cdot (v - u) : s \in [0,1] \} \subseteq E
\)
#+END_CENTER


*** Alternative

On dispose aussi d'une définition alternative. On utilise :

#+BEGIN_CENTER
\(
L = \{ (s,t) \in \setR^2 : (s,t) \ge 0 \text{ et } s + t = 1 \}
\)
#+END_CENTER

et la fonction $\sigma : L \mapsto E$ définie par :

#+BEGIN_CENTER
\(\relax
\sigma(s,t) = s \cdot u + t \cdot v
\)
#+END_CENTER

On a alors $[u,v] = \sigma(L)$. On voit aussi que $\sigma(1,0) = u$ et $\sigma(0,1) = v$.


** Enveloppe convexe

Soit $A \subseteq E$ et la collection des segments reliant deux points quelconques de $A$ :

#+BEGIN_CENTER
\(
\mathcal{S} = \{ [u,v] : u,v \in A \}
\)
#+END_CENTER

L'enveloppe convexe de $A$ est l'union de tous ces segments :

#+BEGIN_CENTER
\(
\convexe(A) = \bigcup \mathcal{S}
\)
#+END_CENTER

Pour tout $u,v \in A$ et $(s,t) \in \setR^2$ tels que $s,t \ge 0$ et $s + t = 1$, on a donc :

#+BEGIN_CENTER
\(
s \cdot u + t \cdot v \in \convexe(A)
\)
#+END_CENTER


*** Inclusion

Il suffit de considérer le choix $(s,t) = (1,0)$ pour voir que tout $u \in A$ appartient à $\convexe(A)$. On a donc $A \subseteq \convexe(A)$.


*** Ensemble convexe

On dit qu'un ensemble $C \subseteq E$ est convexe si $\convexe(C) = C$.


** Surface

Une surface de $E$ est de la forme :

#+BEGIN_CENTER
\(
\Phi = \{ \varphi(s,t) : (s,t) \in [a,b] \times [c,d] \}
\)
#+END_CENTER

où $\varphi : [a,b] \times [c,d] \mapsto E$ est une fonction continue et où $a,b,c,d \in \setR$ vérifient $a \le b$ et $c \le d$.


* Formes linéaires

#+TOC: headlines 1 local

\label{chap:forme}


** Dépendances

  - Chapitre \ref{chap:relation} : Les fonctions
  - Chapitre \ref{chap:lineaire} : Les fonctions linéaires


** Définition

Soit un espace vectoriel $E$ sur $\corps$. Une forme linéaire est une fonction linéaire continue
$\varphi : E \mapsto \corps$.


** Espace dual

L'espace dual $E^\dual$ de $E$ est l'ensemble des formes linéaires sur $E$ , autrement dit
l'ensemble des fonctions linéaires continues de $E$ vers $\corps$ :

$$E^\dual = \{ \varphi \in \lineaire(E,\corps) : \norme{\varphi}_\lineaire \strictinferieur +\infty \}$$

Il s'agit d'un espace vectoriel pour les opérations d'addition et de multiplication mixte définies sur les fonctions.


** Notation

Pour toute forme $\varphi \in E^\dual$ et tout vecteur $v \in E$, on note :

$$\forme{\varphi}{v} = \varphi(v)$$

ce qui définit implicitement la fonction $\forme{}{} : E^\dual \times E \mapsto \corps$.


** Linéarité

Soit $\varphi,\psi \in E^\dual$, $u,v \in E$ et $\alpha,\beta \in S$. Comme $\varphi$ est linéaire, on a :

$$\forme{\varphi}{\alpha \cdot u + \beta \cdot v}  = \alpha \cdot \forme{\varphi}{u} + \beta \cdot \forme{\varphi}{v}$$

Symétriquement, la définition des opérations sur les fonctions nous donne également :

$$\forme{\alpha \cdot \varphi + \beta \cdot \psi}{u}  = \alpha \cdot \forme{\varphi}{u} + \beta \cdot \forme{\psi}{u}$$

L'application $\forme{}{}$ est donc bilinéaire.


** Biorthonormalité

On dit que les suites $(\Phi_1,...,\Phi_m)$ de $E^\dual$ et
$(e_1,...,e_n)$ de $E$ sont biorthonormées si :

$$\forme{\Phi_i}{e_j} = \indicatrice_{ij}$$

pour tout $(i,j) \in \setZ(0,m) \times \setZ(0,n)$. De telles suites permettent d'évaluer facilement les coefficients des développements en série du type :

$$\varphi = \sum_{i = 1}^m \alpha_i \cdot \Phi_i$$

où $\alpha_1,...,\alpha_m \in S$. En effet, il suffit d'évaluer :

$$\varphi(e_j) = \forme{\varphi}{e_j} = \sum_{i = 1}^m \alpha_i \cdot \forme{\Phi_i}{e_j} = \sum_{i = 1}^m \alpha_i \cdot \indicatrice_{ij} = \alpha_j$$

pour obtenir les valeurs des $\alpha_j$.

Réciproquement, si :

$$u = \sum_{i = 1}^n \beta_i \cdot e_i$$

avec $\beta_1,...,\beta_n \in S$, on a :

$$\Phi_j(u) = \forme{\Phi_j}{u} = \sum_{i = 1}^n \beta_i \cdot \forme{\Phi_j}{e_i} = \sum_{i = 1}^n \beta_i \cdot \indicatrice_{ij} = \beta_j$$

ce qui nous donne les valeurs des $\beta_j$.

Forts de ces résultats, il est aisé d'évaluer :

$$\forme{\varphi}{u} = \sum_{i,j} \alpha_i \cdot \forme{\Phi_i}{u_j} \cdot \beta_j = \sum_{i,j} \alpha_i \cdot \indicatrice_{ij} \cdot \beta_j = \sum_i \alpha_i \cdot \beta_i$$

On a donc en définitive :

$$\forme{\varphi}{u} = \sum_i \forme{\varphi}{e_i} \cdot \forme{\Phi_i}{u}$$


** Similitude

On dit que deux fonctions $u,v \in E$ sont identique au sens des distributions si :

$$\forme{\varphi}{u} = \forme{\varphi}{v}$$

pour tout $\varphi \in E^\dual$.

Symétriquement, les deux formes $\varphi,\psi \in E^\dual$ sont identiques par définition si et seulement si :

$$\forme{\varphi}{u} = \forme{\psi}{u}$$

pour tout $u \in E$.


** Espace bidual

On définit l'espace bidual de $E$, noté $E^{\dual \dual}$, par :

$$E^{\dual \dual} = (E^\dual)^\dual$$

On associe à chaque élément $u \in E$ un élément $\hat{u} \in E^{\dual \dual}$ par la condition :

$$\hat{u}(\varphi) = \varphi(u)$$

qui doit être vérifiée pour tout $\varphi \in E^\dual$. On a donc :

$$\forme{\hat{u}}{\varphi} = \forme{\varphi}{u}$$


** Application duale

Soit les espaces vectoriels $E$ et $F$ sur $\corps$ et une fonction $A : E \mapsto F$. Le dual de $A$ au sens des formes, s'il existe, est l'unique fonction $A^\dual : F^\dual \mapsto E^\dual$ telle que :

$$\forme{ A^\dual(\varphi) }{u} = \forme{\varphi}{ A(u) }$$

pour tout $u \in E$ et $\varphi \in F^\dual$.


** Formes bilinéaires

Soit les espaces vectoriels $E$ et $F$ sur $\corps$. Une forme bilinéaire est une fonction bilinéaire continue $\vartheta : F \times E \mapsto \corps$. On utilise une notation analogue à celle des formes :

$$\biforme{x}{\vartheta}{u} = \vartheta(x,u)$$

pour tout $x \in F$ et $u \in E$. On voit que :

#+BEGIN_CENTER
\(
\biforme{\alpha \cdot x + \beta \cdot y}{\vartheta}{u} = \alpha \cdot \biforme{x}{\vartheta}{u} + \beta \cdot \biforme{y}{\vartheta}{u} \\
\biforme{x}{\vartheta}{\alpha \cdot u + \beta \cdot v} = \alpha \cdot \biforme{x}{\vartheta}{u} + \beta \cdot \biforme{x}{\vartheta}{v}
\)
#+END_CENTER

pour tout $\alpha,\beta \in \corps$, $u,v \in E$ et $x,y \in F$.


** Formes quadratiques

Soit la forme bilinéaire $ \vartheta : E \times E \mapsto \corps$. Une forme quadratique $\mathcal{Q} : E \mapsto \corps$ est une fonction de la forme :

$$\mathcal{Q}(x) = \biforme{x}{\vartheta}{x}$$


** Représentation matricielle

On peut représenter toute forme linéaire $\varphi \in \lineaire(\corps^n,\corps)$ par un vecteur matriciel $\hat{\varphi} \in \corps^n$. Etant donné la base canonique $(e_1,...,e_n)$ de $\corps^n$, il suffit de poser :

$$\hat{\varphi}_i = \forme{\varphi}{e_i}$$

pour avoir :

$$\forme{\varphi}{u} = \hat{\varphi}^T \cdot u$$

pour tout $u \in \corps^n$.


*** Formes bilinéaires

On peut représenter toute forme bilinéaire $\vartheta \in \lineaire(\corps^m \times \corps^n,\corps)$ par une matrice $\Theta \in \matrice(K,m,n)$. Etant donné les bases canoniques $(f_1,...,f_m)$ de $\corps^m$ et $(e_1,...,e_n)$ de $\corps^n$, il suffit de poser :

$$\composante_{ij} \Theta = \biforme{f_i}{\vartheta}{e_j}$$

pour avoir :

$$\biforme{v}{\vartheta}{u} = v^T \cdot \Theta \cdot u$$

pour tout $u \in \corps^n$ et tout $v \in \corps^m$.
