<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN"
"http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" lang="fr" xml:lang="fr">
<head>
<!-- 2025-10-19 dim 10:53 -->
<meta http-equiv="Content-Type" content="text/html;charset=utf-8" />
<meta name="viewport" content="width=device-width, initial-scale=1" />
<title>Eclats de vers : Matemat 06 : Vecteurs</title>
<meta name="author" content="chimay" />
<meta name="generator" content="Org Mode" />
<style type="text/css">
  .title  { text-align: center;
             margin-bottom: .2em; }
  .subtitle { text-align: center;
              font-size: medium;
              font-weight: bold;
              margin-top:0; }
  .todo   { font-family: monospace; color: red; }
  .done   { font-family: monospace; color: green; }
  .priority { font-family: monospace; color: orange; }
  .tag    { background-color: #eee; font-family: monospace;
            padding: 2px; font-size: 80%; font-weight: normal; }
  .timestamp { color: #bebebe; }
  .timestamp-kwd { color: #5f9ea0; }
  .org-right  { margin-left: auto; margin-right: 0px;  text-align: right; }
  .org-left   { margin-left: 0px;  margin-right: auto; text-align: left; }
  .org-center { margin-left: auto; margin-right: auto; text-align: center; }
  .underline { text-decoration: underline; }
  #postamble p, #preamble p { font-size: 90%; margin: .2em; }
  p.verse { margin-left: 3%; }
  pre {
    border: 1px solid #e6e6e6;
    border-radius: 3px;
    background-color: #f2f2f2;
    padding: 8pt;
    font-family: monospace;
    overflow: auto;
    margin: 1.2em;
  }
  pre.src {
    position: relative;
    overflow: auto;
  }
  pre.src:before {
    display: none;
    position: absolute;
    top: -8px;
    right: 12px;
    padding: 3px;
    color: #555;
    background-color: #f2f2f299;
  }
  pre.src:hover:before { display: inline; margin-top: 14px;}
  /* Languages per Org manual */
  pre.src-asymptote:before { content: 'Asymptote'; }
  pre.src-awk:before { content: 'Awk'; }
  pre.src-authinfo::before { content: 'Authinfo'; }
  pre.src-C:before { content: 'C'; }
  /* pre.src-C++ doesn't work in CSS */
  pre.src-clojure:before { content: 'Clojure'; }
  pre.src-css:before { content: 'CSS'; }
  pre.src-D:before { content: 'D'; }
  pre.src-ditaa:before { content: 'ditaa'; }
  pre.src-dot:before { content: 'Graphviz'; }
  pre.src-calc:before { content: 'Emacs Calc'; }
  pre.src-emacs-lisp:before { content: 'Emacs Lisp'; }
  pre.src-fortran:before { content: 'Fortran'; }
  pre.src-gnuplot:before { content: 'gnuplot'; }
  pre.src-haskell:before { content: 'Haskell'; }
  pre.src-hledger:before { content: 'hledger'; }
  pre.src-java:before { content: 'Java'; }
  pre.src-js:before { content: 'Javascript'; }
  pre.src-latex:before { content: 'LaTeX'; }
  pre.src-ledger:before { content: 'Ledger'; }
  pre.src-lisp:before { content: 'Lisp'; }
  pre.src-lilypond:before { content: 'Lilypond'; }
  pre.src-lua:before { content: 'Lua'; }
  pre.src-matlab:before { content: 'MATLAB'; }
  pre.src-mscgen:before { content: 'Mscgen'; }
  pre.src-ocaml:before { content: 'Objective Caml'; }
  pre.src-octave:before { content: 'Octave'; }
  pre.src-org:before { content: 'Org mode'; }
  pre.src-oz:before { content: 'OZ'; }
  pre.src-plantuml:before { content: 'Plantuml'; }
  pre.src-processing:before { content: 'Processing.js'; }
  pre.src-python:before { content: 'Python'; }
  pre.src-R:before { content: 'R'; }
  pre.src-ruby:before { content: 'Ruby'; }
  pre.src-sass:before { content: 'Sass'; }
  pre.src-scheme:before { content: 'Scheme'; }
  pre.src-screen:before { content: 'Gnu Screen'; }
  pre.src-sed:before { content: 'Sed'; }
  pre.src-sh:before { content: 'shell'; }
  pre.src-sql:before { content: 'SQL'; }
  pre.src-sqlite:before { content: 'SQLite'; }
  /* additional languages in org.el's org-babel-load-languages alist */
  pre.src-forth:before { content: 'Forth'; }
  pre.src-io:before { content: 'IO'; }
  pre.src-J:before { content: 'J'; }
  pre.src-makefile:before { content: 'Makefile'; }
  pre.src-maxima:before { content: 'Maxima'; }
  pre.src-perl:before { content: 'Perl'; }
  pre.src-picolisp:before { content: 'Pico Lisp'; }
  pre.src-scala:before { content: 'Scala'; }
  pre.src-shell:before { content: 'Shell Script'; }
  pre.src-ebnf2ps:before { content: 'ebfn2ps'; }
  /* additional language identifiers per "defun org-babel-execute"
       in ob-*.el */
  pre.src-cpp:before  { content: 'C++'; }
  pre.src-abc:before  { content: 'ABC'; }
  pre.src-coq:before  { content: 'Coq'; }
  pre.src-groovy:before  { content: 'Groovy'; }
  /* additional language identifiers from org-babel-shell-names in
     ob-shell.el: ob-shell is the only babel language using a lambda to put
     the execution function name together. */
  pre.src-bash:before  { content: 'bash'; }
  pre.src-csh:before  { content: 'csh'; }
  pre.src-ash:before  { content: 'ash'; }
  pre.src-dash:before  { content: 'dash'; }
  pre.src-ksh:before  { content: 'ksh'; }
  pre.src-mksh:before  { content: 'mksh'; }
  pre.src-posh:before  { content: 'posh'; }
  /* Additional Emacs modes also supported by the LaTeX listings package */
  pre.src-ada:before { content: 'Ada'; }
  pre.src-asm:before { content: 'Assembler'; }
  pre.src-caml:before { content: 'Caml'; }
  pre.src-delphi:before { content: 'Delphi'; }
  pre.src-html:before { content: 'HTML'; }
  pre.src-idl:before { content: 'IDL'; }
  pre.src-mercury:before { content: 'Mercury'; }
  pre.src-metapost:before { content: 'MetaPost'; }
  pre.src-modula-2:before { content: 'Modula-2'; }
  pre.src-pascal:before { content: 'Pascal'; }
  pre.src-ps:before { content: 'PostScript'; }
  pre.src-prolog:before { content: 'Prolog'; }
  pre.src-simula:before { content: 'Simula'; }
  pre.src-tcl:before { content: 'tcl'; }
  pre.src-tex:before { content: 'TeX'; }
  pre.src-plain-tex:before { content: 'Plain TeX'; }
  pre.src-verilog:before { content: 'Verilog'; }
  pre.src-vhdl:before { content: 'VHDL'; }
  pre.src-xml:before { content: 'XML'; }
  pre.src-nxml:before { content: 'XML'; }
  /* add a generic configuration mode; LaTeX export needs an additional
     (add-to-list 'org-latex-listings-langs '(conf " ")) in .emacs */
  pre.src-conf:before { content: 'Configuration File'; }

  table { border-collapse:collapse; }
  caption.t-above { caption-side: top; }
  caption.t-bottom { caption-side: bottom; }
  td, th { vertical-align:top;  }
  th.org-right  { text-align: center;  }
  th.org-left   { text-align: center;   }
  th.org-center { text-align: center; }
  td.org-right  { text-align: right;  }
  td.org-left   { text-align: left;   }
  td.org-center { text-align: center; }
  dt { font-weight: bold; }
  .footpara { display: inline; }
  .footdef  { margin-bottom: 1em; }
  .figure { padding: 1em; }
  .figure p { text-align: center; }
  .equation-container {
    display: table;
    text-align: center;
    width: 100%;
  }
  .equation {
    vertical-align: middle;
  }
  .equation-label {
    display: table-cell;
    text-align: right;
    vertical-align: middle;
  }
  .inlinetask {
    padding: 10px;
    border: 2px solid gray;
    margin: 10px;
    background: #ffffcc;
  }
  #org-div-home-and-up
   { text-align: right; font-size: 70%; white-space: nowrap; }
  textarea { overflow-x: auto; }
  .linenr { font-size: smaller }
  .code-highlighted { background-color: #ffff00; }
  .org-info-js_info-navigation { border-style: none; }
  #org-info-js_console-label
    { font-size: 10px; font-weight: bold; white-space: nowrap; }
  .org-info-js_search-highlight
    { background-color: #ffff00; color: #000000; font-weight: bold; }
  .org-svg { }
</style>
<link rel="stylesheet" type="text/css" href="../style/defaut.css" />
<script>
  window.MathJax = {
    tex: {
      ams: {
        multlineWidth: '85%'
      },
      tags: 'ams',
      tagSide: 'right',
      tagIndent: '.8em'
    },
    chtml: {
      scale: 1.0,
      displayAlign: 'center',
      displayIndent: '0em'
    },
    svg: {
      scale: 1.0,
      displayAlign: 'center',
      displayIndent: '0em'
    },
    output: {
      font: 'mathjax-modern',
      displayOverflow: 'overflow'
    }
  };
</script>

<script
  id="MathJax-script"
  async
  src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js">
</script>
</head>
<body>
<div id="content" class="content">
<h1 class="title">Eclats de vers : Matemat 06 : Vecteurs</h1>
<p>
<a href="index.html">Index des Grimoires</a>
</p>

<p>
<a href="../index.html">Retour à l’accueil</a>
</p>

<p>
\(
\newcommand{\parentheses}[1]{\left(#1\right)}
\newcommand{\crochets}[1]{\left[#1\right]}
\newcommand{\accolades}[1]{\left\{#1\right\}}
\newcommand{\ensemble}[1]{\left\{#1\right\}}
\newcommand{\identite}{\mathrm{Id}}
\newcommand{\indicatrice}{\boldsymbol{\delta}}
\newcommand{\dirac}{\delta}
\newcommand{\moinsun}{{-1}}
\newcommand{\inverse}{\ddagger}
\newcommand{\pinverse}{\dagger}
\newcommand{\topologie}{\mathfrak{T}}
\newcommand{\ferme}{\mathfrak{F}}
\DeclareMathOperator*{\img}{\mathbf{i}}
\newcommand{\binome}[2]{
\left\{ \begin{array}{c}
#1 \\
#2 \\
\end{array} \right\}
}
\newcommand{\canonique}{\mathfrak{c}}
\newcommand{\tenseuridentite}{\boldsymbol{\mathcal{I}}}
\newcommand{\permutation}{\boldsymbol{\epsilon}}
\newcommand{\matriceZero}{\mathfrak{0}}
\newcommand{\matriceUn}{\mathfrak{1}}
\newcommand{\christoffel}[2]{
\left\{ \begin{array}{c}
#1 \\
#2 \\
\end{array} \right\}
}
\newcommand{\lagrangien}{\mathfrak{L}}
\newcommand{\sousens}{\mathfrak{P}}
\newcommand{\partition}{\mathrm{Partition}}
\newcommand{\tribu}{\mathrm{Tribu}}
\newcommand{\topologies}{\mathrm{Topo}}
\newcommand{\setB}{\mathbb{B}}
\newcommand{\setN}{\mathbb{N}}
\newcommand{\setZ}{\mathbb{Z}}
\newcommand{\setQ}{\mathbb{Q}}
\newcommand{\setR}{\mathbb{R}}
\newcommand{\setC}{\mathbb{C}}
\newcommand{\corps}{\mathbb{K}}
\newcommand{\boule}{\mathfrak{B}}
\newcommand{\intervalleouvert}[2]{\relax \ ] #1 , #2 [ \ \relax}
\newcommand{\intervallesemiouvertgauche}[2]{\relax \ ] #1 , #2 ]}
\newcommand{\intervallesemiouvertdroite}[2]{[ #1 , #2 [ \ \relax}
\newcommand{\fonction}{\mathbb{F}}
\newcommand{\bijection}{\mathrm{Bij}}
\newcommand{\polynome}{\mathrm{Poly}}
\newcommand{\lineaire}{\mathrm{Lin}}
\newcommand{\continue}{\mathrm{Cont}}
\newcommand{\homeomorphisme}{\mathrm{Hom}}
\newcommand{\etagee}{\mathrm{Etagee}}
\newcommand{\lebesgue}{\mathrm{Leb}}
\newcommand{\lipschitz}{\mathrm{Lip}}
\newcommand{\suitek}{\mathrm{Suite}}
\newcommand{\matrice}{\mathbb{M}}
\newcommand{\krylov}{\mathrm{Krylov}}
\newcommand{\tenseur}{\mathbb{T}}
\newcommand{\essentiel}{\mathfrak{E}}
\newcommand{\relation}{\mathrm{Rel}}
\DeclareMathOperator*{\strictinferieur}{\ < \ }
\DeclareMathOperator*{\strictsuperieur}{\ > \ }
\DeclareMathOperator*{\ensinferieur}{\eqslantless}
\DeclareMathOperator*{\enssuperieur}{\eqslantgtr}
\DeclareMathOperator*{\esssuperieur}{\gtrsim}
\DeclareMathOperator*{\essinferieur}{\lesssim}
\newcommand{\essegal}{\eqsim}
\newcommand{\union}{\ \cup \ }
\newcommand{\intersection}{\ \cap \ }
\newcommand{\opera}{\divideontimes}
\newcommand{\autreaddition}{\boxplus}
\newcommand{\autremultiplication}{\circledast}
\newcommand{\commutateur}[2]{\left[ #1 , #2 \right]}
\newcommand{\convolution}{\circledcirc}
\newcommand{\correlation}{\ \natural \ }
\newcommand{\diventiere}{\div}
\newcommand{\modulo}{\bmod}
\DeclareMathOperator*{\pgcd}{pgcd}
\DeclareMathOperator*{\ppcm}{ppcm}
\newcommand{\produitscalaire}[2]{\left\langle #1 \vert #2 \right\rangle}
\newcommand{\scalaire}[2]{\left\langle #1 \| #2 \right\rangle}
\newcommand{\braket}[3]{\left\langle #1 \vert #2 \\vert 3 \right\rangle}
\newcommand{\orthogonal}{\bot}
\newcommand{\forme}[2]{\left\langle #1 , #2 \right\rangle}
\newcommand{\biforme}[3]{\left\langle #1 , #2 , #3 \right\rangle}
\newcommand{\contraction}[3]{\left\langle #1 \odot #3 \right\rangle_{#2}}
\newcommand{\dblecont}[5]{\left\langle #1 \vert #3 \vert #5 \right\rangle_{#2,#4}}
\DeclareMathOperator*{\major}{major}
\DeclareMathOperator*{\minor}{minor}
\DeclareMathOperator*{\maxim}{maxim}
\DeclareMathOperator*{\minim}{minim}
\DeclareMathOperator*{\argument}{arg}
\DeclareMathOperator*{\argmin}{arg\ min}
\DeclareMathOperator*{\argmax}{arg\ max}
\DeclareMathOperator*{\supessentiel}{ess\ sup}
\DeclareMathOperator*{\infessentiel}{ess\ inf}
\newcommand{\dual}{\star}
\newcommand{\distance}{\mathfrak{dist}}
\newcommand{\norme}[1]{\left\| #1 \right\|}
\newcommand{\normetrois}[1]{\left|\left\| #1 \right\|\right|}
\DeclareMathOperator*{\adh}{adh}
\DeclareMathOperator*{\interieur}{int}
\newcommand{\frontiere}{\partial}
\DeclareMathOperator*{\image}{im}
\DeclareMathOperator*{\domaine}{dom}
\DeclareMathOperator*{\noyau}{ker}
\DeclareMathOperator*{\support}{supp}
\DeclareMathOperator*{\signe}{sign}
\newcommand{\abs}[1]{\left| #1 \right|}
\newcommand{\unsur}[1]{\frac{1}{#1}}
\newcommand{\arrondisup}[1]{\lceil #1 \rceil}
\newcommand{\arrondiinf}[1]{\lfloor #1 \rfloor}
\DeclareMathOperator*{\conjugue}{conj}
\newcommand{\conjaccent}[1]{\overline{#1}}
\DeclareMathOperator*{\division}{division}
\newcommand{\difference}{\boldsymbol{\Delta}}
\newcommand{\differentielle}[2]{\mathfrak{D}^{#1}_{#2}}
\newcommand{\OD}[2]{\frac{d #1}{d #2}}
\newcommand{\OOD}[2]{\frac{d^2 #1}{d #2^2}}
\newcommand{\NOD}[3]{\frac{d^{#3} #1}{d #2^{#3}}}
\newcommand{\deriveepartielle}[2]{\frac{\partial #1}{\partial #2}}
\newcommand{\PD}[2]{\frac{\partial #1}{\partial #2}}
\newcommand{\dblederiveepartielle}[2]{\frac{\partial^2 #1}{\partial #2 \partial #2}}
\newcommand{\dfdxdy}[3]{\frac{\partial^2 #1}{\partial #2 \partial #3}}
\newcommand{\dfdxdx}[2]{\frac{\partial^2 #1}{\partial #2^2}}
\newcommand{\gradient}{\mathbf{\nabla}}
\newcommand{\combilin}[1]{\mathrm{span}\{ #1 \}}
\DeclareMathOperator*{\trace}{tr}
\newcommand{\proba}{\mathbb{P}}
\newcommand{\probaof}[1]{\mathbb{P}\left[#1\right]}
\newcommand{\esperof}[1]{\mathbb{E}\left[#1\right]}
\newcommand{\cov}[2]{\mathrm{cov} \left( #1 , #2 \right) }
\newcommand{\var}[1]{\mathrm{var} \left( #1 \right) }
\newcommand{\rand}{\mathrm{rand}}
\newcommand{\variation}[1]{\left\langle #1 \right\rangle}
\DeclareMathOperator*{\composante}{comp}
\DeclareMathOperator*{\bloc}{bloc}
\DeclareMathOperator*{\ligne}{ligne}
\DeclareMathOperator*{\colonne}{colonne}
\DeclareMathOperator*{\diagonale}{diag}
\newcommand{\matelementaire}{\mathrm{Elem}}
\DeclareMathOperator*{\matpermutation}{permut}
\newcommand{\matunitaire}{\mathrm{Unitaire}}
\newcommand{\gaussjordan}{\mathrm{GaussJordan}}
\newcommand{\householder}{\mathrm{Householder}}
\DeclareMathOperator*{\rang}{rang}
\newcommand{\schur}{\mathrm{Schur}}
\newcommand{\singuliere}{\mathrm{DVS}}
\newcommand{\convexe}{\mathrm{Convexe}}
\newcommand{\petito}[1]{o\left(#1\right)}
\newcommand{\grando}[1]{O\left(#1\right)}
\)
</p>

<div id="table-of-contents" role="doc-toc">
<h2>Table des matières</h2>
<div id="text-table-of-contents" role="doc-toc">
<ul>
<li><a href="#org6f5e7f4">1. Espaces vectoriels</a></li>
<li><a href="#orgad1333f">2. Norme</a></li>
<li><a href="#orgff6be45">3. Espaces de Banach</a></li>
<li><a href="#orge894568">4. Continuité</a></li>
<li><a href="#orgc547a82">5. Applications linéaires</a></li>
<li><a href="#org252bd7c">6. Géométrie</a></li>
<li><a href="#orgae61dd2">7. Formes linéaires</a></li>
<li><a href="#orgbfee45f">8. Produit scalaire</a></li>
<li><a href="#org18f2a1b">9. Norme dérivée du produit scalaire</a></li>
<li><a href="#orgc77f955">10. Applications adjointes</a></li>
<li><a href="#org9e17826">11. Tenseurs</a></li>
<li><a href="#orgd6d1377">12. Produit extérieur</a></li>
<li><a href="#org68b0c0f">13. Matrices élémentaires</a></li>
<li><a href="#org85034f8">14. Systèmes linéaires et inverses</a></li>
<li><a href="#org3ed17ed">15. Matrices unitaires</a></li>
</ul>
</div>
</div>

<p>
\(
\newcommand{\parentheses}[1]{\left(#1\right)}
\newcommand{\crochets}[1]{\left[#1\right]}
\newcommand{\accolades}[1]{\left\{#1\right\}}
\newcommand{\ensemble}[1]{\left\{#1\right\}}
\newcommand{\identite}{\mathrm{Id}}
\newcommand{\indicatrice}{\boldsymbol{\delta}}
\newcommand{\dirac}{\delta}
\newcommand{\moinsun}{{-1}}
\newcommand{\inverse}{\ddagger}
\newcommand{\pinverse}{\dagger}
\newcommand{\topologie}{\mathfrak{T}}
\newcommand{\ferme}{\mathfrak{F}}
\DeclareMathOperator*{\img}{\mathbf{i}}
\newcommand{\binome}[2]{
\left\{ \begin{array}{c}
#1 \\
#2 \\
\end{array} \right\}
}
\newcommand{\canonique}{\mathfrak{c}}
\newcommand{\tenseuridentite}{\boldsymbol{\mathcal{I}}}
\newcommand{\permutation}{\boldsymbol{\epsilon}}
\newcommand{\matriceZero}{\mathfrak{0}}
\newcommand{\matriceUn}{\mathfrak{1}}
\newcommand{\christoffel}[2]{
\left\{ \begin{array}{c}
#1 \\
#2 \\
\end{array} \right\}
}
\newcommand{\lagrangien}{\mathfrak{L}}
\newcommand{\sousens}{\mathfrak{P}}
\newcommand{\partition}{\mathrm{Partition}}
\newcommand{\tribu}{\mathrm{Tribu}}
\newcommand{\topologies}{\mathrm{Topo}}
\newcommand{\setB}{\mathbb{B}}
\newcommand{\setN}{\mathbb{N}}
\newcommand{\setZ}{\mathbb{Z}}
\newcommand{\setQ}{\mathbb{Q}}
\newcommand{\setR}{\mathbb{R}}
\newcommand{\setC}{\mathbb{C}}
\newcommand{\corps}{\mathbb{K}}
\newcommand{\boule}{\mathfrak{B}}
\newcommand{\intervalleouvert}[2]{\relax \ ] #1 , #2 [ \ \relax}
\newcommand{\intervallesemiouvertgauche}[2]{\relax \ ] #1 , #2 ]}
\newcommand{\intervallesemiouvertdroite}[2]{[ #1 , #2 [ \ \relax}
\newcommand{\fonction}{\mathbb{F}}
\newcommand{\bijection}{\mathrm{Bij}}
\newcommand{\polynome}{\mathrm{Poly}}
\newcommand{\lineaire}{\mathrm{Lin}}
\newcommand{\continue}{\mathrm{Cont}}
\newcommand{\homeomorphisme}{\mathrm{Hom}}
\newcommand{\etagee}{\mathrm{Etagee}}
\newcommand{\lebesgue}{\mathrm{Leb}}
\newcommand{\lipschitz}{\mathrm{Lip}}
\newcommand{\suitek}{\mathrm{Suite}}
\newcommand{\matrice}{\mathbb{M}}
\newcommand{\krylov}{\mathrm{Krylov}}
\newcommand{\tenseur}{\mathbb{T}}
\newcommand{\essentiel}{\mathfrak{E}}
\newcommand{\relation}{\mathrm{Rel}}
\DeclareMathOperator*{\strictinferieur}{\ < \ }
\DeclareMathOperator*{\strictsuperieur}{\ > \ }
\DeclareMathOperator*{\ensinferieur}{\eqslantless}
\DeclareMathOperator*{\enssuperieur}{\eqslantgtr}
\DeclareMathOperator*{\esssuperieur}{\gtrsim}
\DeclareMathOperator*{\essinferieur}{\lesssim}
\newcommand{\essegal}{\eqsim}
\newcommand{\union}{\ \cup \ }
\newcommand{\intersection}{\ \cap \ }
\newcommand{\opera}{\divideontimes}
\newcommand{\autreaddition}{\boxplus}
\newcommand{\autremultiplication}{\circledast}
\newcommand{\commutateur}[2]{\left[ #1 , #2 \right]}
\newcommand{\convolution}{\circledcirc}
\newcommand{\correlation}{\ \natural \ }
\newcommand{\diventiere}{\div}
\newcommand{\modulo}{\bmod}
\DeclareMathOperator*{\pgcd}{pgcd}
\DeclareMathOperator*{\ppcm}{ppcm}
\newcommand{\produitscalaire}[2]{\left\langle #1 \vert #2 \right\rangle}
\newcommand{\scalaire}[2]{\left\langle #1 \| #2 \right\rangle}
\newcommand{\braket}[3]{\left\langle #1 \vert #2 \\vert 3 \right\rangle}
\newcommand{\orthogonal}{\bot}
\newcommand{\forme}[2]{\left\langle #1 , #2 \right\rangle}
\newcommand{\biforme}[3]{\left\langle #1 , #2 , #3 \right\rangle}
\newcommand{\contraction}[3]{\left\langle #1 \odot #3 \right\rangle_{#2}}
\newcommand{\dblecont}[5]{\left\langle #1 \vert #3 \vert #5 \right\rangle_{#2,#4}}
\DeclareMathOperator*{\major}{major}
\DeclareMathOperator*{\minor}{minor}
\DeclareMathOperator*{\maxim}{maxim}
\DeclareMathOperator*{\minim}{minim}
\DeclareMathOperator*{\argument}{arg}
\DeclareMathOperator*{\argmin}{arg\ min}
\DeclareMathOperator*{\argmax}{arg\ max}
\DeclareMathOperator*{\supessentiel}{ess\ sup}
\DeclareMathOperator*{\infessentiel}{ess\ inf}
\newcommand{\dual}{\star}
\newcommand{\distance}{\mathfrak{dist}}
\newcommand{\norme}[1]{\left\| #1 \right\|}
\newcommand{\normetrois}[1]{\left|\left\| #1 \right\|\right|}
\DeclareMathOperator*{\adh}{adh}
\DeclareMathOperator*{\interieur}{int}
\newcommand{\frontiere}{\partial}
\DeclareMathOperator*{\image}{im}
\DeclareMathOperator*{\domaine}{dom}
\DeclareMathOperator*{\noyau}{ker}
\DeclareMathOperator*{\support}{supp}
\DeclareMathOperator*{\signe}{sign}
\newcommand{\abs}[1]{\left| #1 \right|}
\newcommand{\unsur}[1]{\frac{1}{#1}}
\newcommand{\arrondisup}[1]{\lceil #1 \rceil}
\newcommand{\arrondiinf}[1]{\lfloor #1 \rfloor}
\DeclareMathOperator*{\conjugue}{conj}
\newcommand{\conjaccent}[1]{\overline{#1}}
\DeclareMathOperator*{\division}{division}
\newcommand{\difference}{\boldsymbol{\Delta}}
\newcommand{\differentielle}[2]{\mathfrak{D}^{#1}_{#2}}
\newcommand{\OD}[2]{\frac{d #1}{d #2}}
\newcommand{\OOD}[2]{\frac{d^2 #1}{d #2^2}}
\newcommand{\NOD}[3]{\frac{d^{#3} #1}{d #2^{#3}}}
\newcommand{\deriveepartielle}[2]{\frac{\partial #1}{\partial #2}}
\newcommand{\PD}[2]{\frac{\partial #1}{\partial #2}}
\newcommand{\dblederiveepartielle}[2]{\frac{\partial^2 #1}{\partial #2 \partial #2}}
\newcommand{\dfdxdy}[3]{\frac{\partial^2 #1}{\partial #2 \partial #3}}
\newcommand{\dfdxdx}[2]{\frac{\partial^2 #1}{\partial #2^2}}
\newcommand{\gradient}{\mathbf{\nabla}}
\newcommand{\combilin}[1]{\mathrm{span}\{ #1 \}}
\DeclareMathOperator*{\trace}{tr}
\newcommand{\proba}{\mathbb{P}}
\newcommand{\probaof}[1]{\mathbb{P}\left[#1\right]}
\newcommand{\esperof}[1]{\mathbb{E}\left[#1\right]}
\newcommand{\cov}[2]{\mathrm{cov} \left( #1 , #2 \right) }
\newcommand{\var}[1]{\mathrm{var} \left( #1 \right) }
\newcommand{\rand}{\mathrm{rand}}
\newcommand{\variation}[1]{\left\langle #1 \right\rangle}
\DeclareMathOperator*{\composante}{comp}
\DeclareMathOperator*{\bloc}{bloc}
\DeclareMathOperator*{\ligne}{ligne}
\DeclareMathOperator*{\colonne}{colonne}
\DeclareMathOperator*{\diagonale}{diag}
\newcommand{\matelementaire}{\mathrm{Elem}}
\DeclareMathOperator*{\matpermutation}{permut}
\newcommand{\matunitaire}{\mathrm{Unitaire}}
\newcommand{\gaussjordan}{\mathrm{GaussJordan}}
\newcommand{\householder}{\mathrm{Householder}}
\DeclareMathOperator*{\rang}{rang}
\newcommand{\schur}{\mathrm{Schur}}
\newcommand{\singuliere}{\mathrm{DVS}}
\newcommand{\convexe}{\mathrm{Convexe}}
\newcommand{\petito}[1]{o\left(#1\right)}
\newcommand{\grando}[1]{O\left(#1\right)}
\)
</p>
<div id="outline-container-org6f5e7f4" class="outline-2">
<h2 id="org6f5e7f4"><span class="section-number-2">1.</span> Espaces vectoriels</h2>
<div class="outline-text-2" id="text-1">
<div id="text-table-of-contents-1" role="doc-toc">
<ul>
<li><a href="#orgdfacecf">1.1. Dépendances</a></li>
<li><a href="#orgc77c4a4">1.2. Introduction</a></li>
<li><a href="#orgd498a91">1.3. Définition</a></li>
<li><a href="#orgf3a762d">1.4. Sous-espace</a></li>
<li><a href="#org18e56b6">1.5. Espace engendré</a></li>
<li><a href="#orge3723e1">1.6. Indépendance linéaire</a></li>
<li><a href="#orgb102346">1.7. Coordonnées</a></li>
<li><a href="#orgeff3a91">1.8. Absence de redondance</a></li>
<li><a href="#org33c814b">1.9. Base canonique sur \(\corps^n\)</a></li>
<li><a href="#orge8ecf8e">1.10. Représentation matricielle</a></li>
</ul>
</div>

<p>
\label{chap:vecteur}
</p>
</div>
<div id="outline-container-orgdfacecf" class="outline-3">
<h3 id="orgdfacecf"><span class="section-number-3">1.1.</span> Dépendances</h3>
<div class="outline-text-3" id="text-1-1">
<ul class="org-ul">
<li>Chapitre \ref{chap:algebre} : Les structures algébriques</li>
<li>Chapitre \ref{chap:somme} : Les sommes</li>
</ul>
</div>
</div>
<div id="outline-container-orgc77c4a4" class="outline-3">
<h3 id="orgc77c4a4"><span class="section-number-3">1.2.</span> Introduction</h3>
<div class="outline-text-3" id="text-1-2">
<p>
Soit un corps \(\corps\), un ensemble quelconque \(A\) et \(n \in \setN\). Le but des espaces vectoriels est de fournir un cadre général aux $n$-tuples de \(\corps^n\) et aux fonctions de \(\corps^A\). Nous avons vu la correspondance \(\corps^n \leftrightarrow \corps^A\) dans le cas particulier où \(A\) possède un nombre fini d'éléments. Mais le lien entre les deux types d'objets ne s'arrête pas là : la comparaison de deux fonctions se base sur le même principe (étendu) que la comparaison de deux $n$-tuples. Nous avons également défini des produits mixtes \(\cdot : \corps \times \corps^n \to \corps^n\) et \(\cdot : \corps \times \corps^A \to \corps^A\) semblables. Les matrices représentant des applications linéaires, nous pouvons également les ajouter dans la liste. Si \(u,v \in E\), avec \(E \in \{ \corps^n , \corps^A , \matrice(\corps,m,n) \}\), on a :
</p>

<div class="org-center">
<p>
\(
\label{eq:mixte}
(\alpha \cdot \beta) \cdot u = \alpha \cdot (\beta \cdot u) \)
</p>

<p>
\(
(\alpha + \beta) \cdot x = \alpha \cdot x + \beta \cdot x \)
</p>

<p>
\(
\alpha \cdot (u + v) = \alpha \cdot u + \alpha \cdot v \)
</p>

<p>
\(
1 \cdot u = 1
\)
</p>
</div>

<p>
pour tout \(\alpha, \beta \in \corps\). De plus l'addition induite sur \(E\) par l'addition de \(\corps\) transforme \(E\) en groupe commutatif. On ne peut toutefois pas parler de corps pour \(E\), car la multiplication matricielle n'est pas une multiplication induite, et est non commutative.
</p>
</div>
<div id="outline-container-org49e6ef1" class="outline-4">
<h4 id="org49e6ef1"><span class="section-number-4">1.2.1.</span> Attention</h4>
<div class="outline-text-4" id="text-1-2-1">
<p>
Ne pas confondre les additions définies sur \(E\) et \(\corps\), ni la multiplication de \(\corps\) avec la multiplication mixte, ni le neutre de \(E\) avec celui de \(\corps\). Lorsqu'il y a un risque d'ambiguité, on parle du vecteur nul \(0 \in E\) et du scalaire nul \(0 \in \corps\).
</p>
</div>
</div>
</div>
<div id="outline-container-orgd498a91" class="outline-3">
<h3 id="orgd498a91"><span class="section-number-3">1.3.</span> Définition</h3>
<div class="outline-text-3" id="text-1-3">
<p>
Soit un groupe commutatif pour l'addition \(E\), ainsi qu'un corps \(\corps\). Si il existe une opération de multiplication mixte \(\cdot : \corps \times E \mapsto E\) vérifiant les propriétés \ref{eq:mixte} ci-dessus, on dit que \(E\) est un espace vectoriel sur \(\corps\). On nomme alors « vecteurs » les éléments de \(E\) et « scalaires » les éléments de \(\corps\).
</p>
</div>
<div id="outline-container-org62dff24" class="outline-4">
<h4 id="org62dff24"><span class="section-number-4">1.3.1.</span> Notation</h4>
<div class="outline-text-4" id="text-1-3-1">
<p>
On note aussi :
</p>

<div class="org-center">
<p>
\(
x - y = x + (-1) \cdot y \)
</p>

<p>
\(
x \cdot \alpha = \alpha \cdot x \)
</p>

<p>
\(
\alpha \cdot \beta \cdot x = (\alpha \cdot \beta) \cdot x \)
</p>

<p>
\(
\alpha x = \alpha \cdot x \)
</p>

<p>
\(
\frac{x}{\alpha} = \alpha^{-1} \cdot x
\)
</p>
</div>

<p>
Lorsque \(\alpha\) a un inverse dans \(\corps\), on a même les « fractions » :
</p>

<p>
\[\frac{x}{\alpha} = \unsur{\alpha} \cdot x\]
</p>
</div>
</div>
<div id="outline-container-orgf62d91b" class="outline-4">
<h4 id="orgf62d91b"><span class="section-number-4">1.3.2.</span> Corollaires</h4>
<div class="outline-text-4" id="text-1-3-2">
<p>
Les propriétés de la multiplication mixte nous montrent directement que :
</p>

<div class="org-center">
<p>
\(
0 \cdot u = (1 - 1) \cdot u = u - u = 0 \)
</p>

<p>
\(
\alpha \cdot 0 = \alpha \cdot (u - u) = \alpha - \alpha = 0
\)
</p>
</div>
</div>
</div>
<div id="outline-container-org0422e0f" class="outline-4">
<h4 id="org0422e0f"><span class="section-number-4">1.3.3.</span> Remarque</h4>
<div class="outline-text-4" id="text-1-3-3">
<p>
Le corps \(\corps\) est souvent \(\setR\) ou \(\setC\).
</p>
</div>
</div>
</div>
<div id="outline-container-orgf3a762d" class="outline-3">
<h3 id="orgf3a762d"><span class="section-number-3">1.4.</span> Sous-espace</h3>
<div class="outline-text-3" id="text-1-4">
<p>
On dit que \(F \subseteq E\) est un sous-espace vectoriel de \(E\) si \(0 \in F\) et si :
</p>

<p>
\[z = \alpha \cdot x + \beta \cdot y\]
</p>

<p>
appartient à \(F\) quels que soient les vecteurs \(x,y \in F\) et les scalaires \(\alpha,\beta \in \corps\).
</p>

<p>
On vérifie par exemple que \(E\) est un sous-espace vectoriel de lui-même.
</p>
</div>
</div>
<div id="outline-container-org18e56b6" class="outline-3">
<h3 id="org18e56b6"><span class="section-number-3">1.5.</span> Espace engendré</h3>
<div class="outline-text-3" id="text-1-5">
<p>
L'espace engendré par les vecteurs \(e_1,e_2,...,e_n \in E\) est l'ensemble des combinaisons linéaires formées à partir des \(e_i\) :
</p>

<p>
\[\combilin{e_1,...,e_n} = \left\{ \sum_{i=1}^{n} \alpha_i \cdot e_i : \alpha_i \in \corps \right\}\]
</p>

<p>
On vérifie que \(\combilin{e_1,...,e_n}\) est un sous-espace vectoriel de \(E\).
</p>
</div>
<div id="outline-container-org299255f" class="outline-4">
<h4 id="org299255f"><span class="section-number-4">1.5.1.</span> Remarque</h4>
<div class="outline-text-4" id="text-1-5-1">
<p>
Les espaces vectoriels ne pouvant pas s'exprimer comme ci-dessus sont dit de dimension infinie.
</p>
</div>
</div>
</div>
<div id="outline-container-orge3723e1" class="outline-3">
<h3 id="orge3723e1"><span class="section-number-3">1.6.</span> Indépendance linéaire</h3>
<div class="outline-text-3" id="text-1-6">
<p>
On dit qu'une série de vecteurs \(e_1,...,e_n\) est linéairement
indépendante si pour toute suite de scalaires \(\alpha_i\), la condition :
</p>

<p>
\[\sum_{i=1}^{n} \alpha_i \cdot e_i = 0\]
</p>

<p>
implique que tous les scalaires soient nuls :
</p>

<p>
\[\alpha_i = 0\]
</p>

<p>
pour tout \(i \in \{1,2,...,n\}\).
</p>
</div>
</div>
<div id="outline-container-orgb102346" class="outline-3">
<h3 id="orgb102346"><span class="section-number-3">1.7.</span> Coordonnées</h3>
<div class="outline-text-3" id="text-1-7">
<p>
Soit les vecteurs linéairement indépendants \((e_1,...,e_n)\) et
\(x \in \combilin{e_1,...,e_n}\). On peut trouver une suite de scalaire \(\alpha_i\) tels que :
</p>

<p>
\[x = \sum_{i = 1}^n \alpha_i \cdot e_i\]
</p>

<p>
Supposons que l'on ait également :
</p>

<p>
\[x = \sum_{i=1}^n \beta_i \cdot e_i\]
</p>

<p>
pour une autre suite de scalaires \(\beta_i\). En soustrayant les deux
équations, on obtient :
</p>

<p>
\[\sum_{i=1}^n (\alpha_i - \beta_i) \cdot e_i = 0\]
</p>

<p>
L'indépendance linéaire des \(e_i\) implique alors que \(\alpha_i - \beta_i = 0\), c'est-à-dire :
</p>

<p>
\[\alpha_i = \beta_i\]
</p>

<p>
pour tout \(i \in \{1,2,...,n\}\).
</p>

<p>
On a donc unicité des coefficients scalaire de la combinaison linéaire. On dit que les \(\alpha_i\) sont les coordonnées de \(x\) par rapport aux \((e_1,...,e_n)\).
</p>
</div>
<div id="outline-container-orga3b05b8" class="outline-4">
<h4 id="orga3b05b8"><span class="section-number-4">1.7.1.</span> Base</h4>
<div class="outline-text-4" id="text-1-7-1">
<p>
Par contre, l'existence de telles coordonnées n'est pas garantie pour tout \(x \in E\). Ce ne sera le cas que si :
</p>

<p>
\[E \subseteq \combilin{e_1,...,e_n}\]
</p>

<p>
On dit alors que \((e_1,...,e_n)\) forme une base de \(E\).
</p>
</div>
</div>
<div id="outline-container-orgf3499fa" class="outline-4">
<h4 id="orgf3499fa"><span class="section-number-4">1.7.2.</span> Dimension finie</h4>
<div class="outline-text-4" id="text-1-7-2">
<p>
On dit qu'un espace vectoriel \(E\) est de dimension finie s'il posséde au moins une base de la forme \((e_1,...,e_n)\), où \(n \in \setN\) est fini. Dans le cas où \(E\) {\em ne possède pas} une telle base, il est dit de dimension infinie.
</p>
</div>
</div>
<div id="outline-container-org448ed3d" class="outline-4">
<h4 id="org448ed3d"><span class="section-number-4">1.7.3.</span> Equivalence</h4>
<div class="outline-text-4" id="text-1-7-3">
<p>
On voit qu'étant donné une base de \(E\), il y a équivalence entre un vecteur \(x \in E\) et un élément \((x_1,x_2,...,x_n) \in \corps^n\) formé par ses coordonnées.
</p>

<p>
Nous noterons donc également (et abusivement) \(x = (x_1,x_2,...,x_n)\), mais attention : il ne faut jamais perdre de vue que les \(x_i\) dépendent de la base utilisée. Le vecteur \(x\) est lui invariant sous changement de base.
</p>
</div>
</div>
</div>
<div id="outline-container-orgeff3a91" class="outline-3">
<h3 id="orgeff3a91"><span class="section-number-3">1.8.</span> Absence de redondance</h3>
<div class="outline-text-3" id="text-1-8">
<p>
Soit \(e_1,...,e_n \in E\) une suite de vecteurs linéairement indépendants.
Soit \(i \in \{ 1,2,...,n \}\) et :
</p>

<p>
\[J(i) = \setZ(0,n) \setminus \{ i \}\]
</p>

<p>
Supposons que le vecteur \(e_i\) soit une combinaison des autres vecteurs :
</p>

<p>
\[e_i = \sum_{ j \in J(i) } \alpha_j \cdot e_j\]
</p>

<p>
On a donc :
</p>

<p>
\[e_i - \sum_{ j \in J(i) } \alpha_j \cdot e_j = 0\]
</p>

<p>
L'hypothèse d'indépendance linéaire voudrait que tous les \(\alpha_j\) et le \(\alpha_i\) soient nuls. Ce qui n'est manifestement pas le cas puisque \(\alpha_i = 1 \ne 0\) !
</p>

<p>
Aucun des vecteurs de la suite n'est donc combinaison des autres. On dit qu'aucun vecteur n'est redondant dans la suite.
</p>
</div>
</div>
<div id="outline-container-org33c814b" class="outline-3">
<h3 id="org33c814b"><span class="section-number-3">1.9.</span> Base canonique sur \(\corps^n\)</h3>
<div class="outline-text-3" id="text-1-9">
<p>
Soit \(\corps \in \{ \setR, \setC \}\). On note \(\canonique_i\) l'élément de \(\corps^n\) ayant un \(1\) en \(i^{ème}\) position et des \(0\) partout ailleurs. On a donc :
</p>

\begin{align}
\canonique_1 &= (1,0,...,0) \)

\(
\canonique_2 &= (0,1,0,...,0) \)

\(
&\vdots& \)

\(
\canonique_n &= (0,...,0,1)
\end{align}

<p>
On a alors, pour tout \(x = (x_1,...,x_n) \in \corps^n\) :
</p>

<p>
\[x = \sum_{i = 1}^n x_i \cdot \canonique_i\]
</p>
</div>
</div>
<div id="outline-container-orge8ecf8e" class="outline-3">
<h3 id="orge8ecf8e"><span class="section-number-3">1.10.</span> Représentation matricielle</h3>
<div class="outline-text-3" id="text-1-10">
<p>
On représente généralement les vecteurs de \(\corps^n\) par des vecteurs lignes ou colonnes. On parle alors de « vecteurs matriciels ». Le \(i^{ème}\) vecteur de la base canonique est défini par le vecteur colonne :
</p>

<div class="org-center">
<p>
\(
\canonique<sub>i</sub> = ( \indicatrice<sub>ij</sub> )<sub>j</sub> =
</p>
\begin{Matrix}{c}
\vdots \\ 0 \\ 1 \\ 0 \\ \vdots
\end{Matrix}
<p>
\)
</p>
</div>

<p>
soit :
</p>

<div class="org-center">
<p>
\(
\canonique_1 = [1 \ 0 \ \hdots \ 0]^T \)
</p>

<p>
\(
\canonique_2 = [0 \ 1 \ \hdots \ 0]^T \)
</p>

<p>
\(
\vdots \)
</p>

<p>
\(
\canonique_n = [0 \ \hdots \ 0 \ 1]^T
\)
</p>
</div>
</div>
</div>
</div>
<div id="outline-container-orgad1333f" class="outline-2">
<h2 id="orgad1333f"><span class="section-number-2">2.</span> Norme</h2>
<div class="outline-text-2" id="text-2">
<div id="text-table-of-contents-2" role="doc-toc">
<ul>
<li><a href="#org8281b4c">2.1. Dépendances</a></li>
<li><a href="#org52ec985">2.2. Norme</a></li>
<li><a href="#org6ab5588">2.3. Borne inférieure</a></li>
<li><a href="#org55ecfb5">2.4. Distance associée</a></li>
<li><a href="#orgbaa009a">2.5. Normalisation</a></li>
</ul>
</div>

<p>
\label{chap:norme}
</p>
</div>
<div id="outline-container-org8281b4c" class="outline-3">
<h3 id="org8281b4c"><span class="section-number-3">2.1.</span> Dépendances</h3>
<div class="outline-text-3" id="text-2-1">
<ul class="org-ul">
<li>Chapitre \ref{chap:distance} : Les distances</li>
<li>Chapitre \ref{chap:vecteur} : Les espaces vectoriels</li>
</ul>
</div>
</div>
<div id="outline-container-org52ec985" class="outline-3">
<h3 id="org52ec985"><span class="section-number-3">2.2.</span> Norme</h3>
<div class="outline-text-3" id="text-2-2">
<p>
Soit \(S\) un corps muni d'un ordre \(\le\) et \(E\) un espace vectoriel sur \(\corps\). Une norme \(\norme{.} : E \to \setR\) est intuitivement la « grandeur » d'un vecteur \(x \in E\). Cette grandeur correspond à la distance séparant le vecteur nul de \(x\) :
</p>

<p>
\[\norme{x} \equiv \distance(x,0)\]
</p>

<p>
Soit \(x,y,z \in E\).
</p>

<p>
Comme \(\distance(x,0) \ge 0\), on impose par analogie que :
</p>

<p>
\[\norme{x} \ge 0\]
</p>

<p>
De plus, le seul vecteur \(x\) de \(E\) vérifiant :
</p>

<p>
\[\norme{x} = 0\]
</p>

<p>
implique que notre distance équivalente \(\distance(x,0) = 0\) soit nulle, ce qui n'est possible que si \(x = 0\). Lorsqu'on fait référence à ces conditions, on dit que la norme est strictement définie positive.
</p>

<p>
Considérons à présent l'inégalité triangulaire :
</p>

<p>
\[\distance(0 , x + y) \le \distance(0,x) + \distance(x , x + y)\]
</p>

<p>
Comment faire correspondre \(\distance(x , x + y)\) à une norme ? En demandant simplement que notre distance particulière soit invariante sous translation de \(x\) :
</p>

<p>
\[\distance(x , x + y) = \distance(x - x , x + y - x) = \distance(0,y) \equiv \norme{y}\]
</p>

<p>
On impose donc l'inégalité triangulaire :
</p>

<p>
\[\norme{x + y} \le \norme{x} + \norme{y}\]
</p>

<p>
Par ailleurs, lorsqu'on allonge ou réduit un vecteur \(x\) d'un facteur \(\alpha \in \corps\), la distance parcourue sur \(x\) devra être allongée ou réduite par la valeur absolue de \(\alpha\) :
</p>

<p>
\[\norme{\alpha \cdot x} = \abs{\alpha} \cdot \norme{x}\]
</p>
</div>
</div>
<div id="outline-container-org6ab5588" class="outline-3">
<h3 id="org6ab5588"><span class="section-number-3">2.3.</span> Borne inférieure</h3>
<div class="outline-text-3" id="text-2-3">
<p>
On déduit de la définition que :
</p>

<p>
\[\norme{-x} = \norme{(-1) \cdot x} = \abs{-1} \cdot \norme{x} = \norme{x}\]
</p>

<p>
En posant \(z = x + y\), on a :
</p>

<p>
\[\norme{z} \le \norme{x} + \norme{y} = \norme{x}  + \norme{z-x}\]
</p>

<p>
c'est-à-dire :
</p>

<p>
\[\norme{z - x} \ge \norme{z} - \norme{x}\]
</p>

<p>
Mais comme \(\norme{z-x} = \norme{(-1) \cdot (z-x)} = \norme{x-z}\), la propriété vaut également en interchangeant \(z\) et \(x\), et on obtient :
</p>

<p>
\[\norme{z - x} \ge \max\{ \norme{z} - \norme{x}, \norme{x} -\norme{z} \}\]
</p>
</div>
</div>
<div id="outline-container-org55ecfb5" class="outline-3">
<h3 id="org55ecfb5"><span class="section-number-3">2.4.</span> Distance associée</h3>
<div class="outline-text-3" id="text-2-4">
<p>
On peut associer une distance \(d\) à une norme \(\norme{.}\) en posant :
</p>

<p>
\[\distance(x,y) = \norme{x - y}\]
</p>

<p>
En effet :
</p>

<ul class="org-ul">
<li>\(\distance(x,x) = \norme{x - x} = \norme{0} = 0\)</li>
<li>si \(\distance(x,y) = 0 = \norme{x-y}\), on a forcément \(x - y = 0\) et donc \(x = y\)</li>
<li>\(\distance(x,y) = \norme{x-y} = \norme{y-x} = \distance(y,x)\)</li>
<li>\(\distance(x,y) + \distance(y,z) = \norme{x-y} + \norme{y-z} \ge \norme{x-y+y-z} = \norme{x-z} = \distance(x,z)\)</li>
</ul>
</div>
</div>
<div id="outline-container-orgbaa009a" class="outline-3">
<h3 id="orgbaa009a"><span class="section-number-3">2.5.</span> Normalisation</h3>
<div class="outline-text-3" id="text-2-5">
<p>
On peut toujours normaliser un vecteur \(w \ne 0\) pour obtenir un vecteur \(u\) de norme \(1\). Comme \(\norme{w} \ne 0\), on peut écrire :
</p>

<p>
\[u = \frac{w}{\norme{w}}\]
</p>

<p>
On a alors :
</p>

<p>
\[\norme{u} = \norme{ \frac{w}{\norme{w}} } = \unsur{\norme{w}} \cdot \norme{w} = 1\]
</p>
</div>
</div>
</div>
<div id="outline-container-orgff6be45" class="outline-2">
<h2 id="orgff6be45"><span class="section-number-2">3.</span> Espaces de Banach</h2>
<div class="outline-text-2" id="text-3">
<div id="text-table-of-contents-3" role="doc-toc">
<ul>
<li><a href="#orgeddf4de">3.1. Définition</a></li>
<li><a href="#org9297ab9">3.2. Application contractante</a></li>
<li><a href="#orgf57aad8">3.3. Suite de Cauchy</a></li>
<li><a href="#orgb5219aa">3.4. Point fixe</a></li>
<li><a href="#org5d883cc">3.5. Unicité</a></li>
<li><a href="#org5243c7d">3.6. Vitesse de convergence</a></li>
</ul>
</div>

<p>
\label{chap:banach}
</p>
</div>
<div id="outline-container-orgeddf4de" class="outline-3">
<h3 id="orgeddf4de"><span class="section-number-3">3.1.</span> Définition</h3>
<div class="outline-text-3" id="text-3-1">
<p>
On dit qu'un espace vectoriel \(X\) est un espace de Banach si il est complet pour la distance issue de la norme \(\distance(x,y) = \norme{x - y}\). Dans la suite, nous considérons un espace de Banach \(X\) sur \(\setR\) ou \(\setC\).
</p>
</div>
</div>
<div id="outline-container-org9297ab9" class="outline-3">
<h3 id="org9297ab9"><span class="section-number-3">3.2.</span> Application contractante</h3>
<div class="outline-text-3" id="text-3-2">
<p>
On dit qu'une application \(A : X \mapsto X\) est contractante s'il existe un \(c \in \intervallesemiouvertdroite{0}{1} \subseteq \setR\) tel que :
</p>

<p>
\[\distance\big( A(u) , A(v) \big) \le c \cdot \distance(u,v)\]
</p>

<p>
pour tout \(u,v\in X\).
</p>
</div>
</div>
<div id="outline-container-orgf57aad8" class="outline-3">
<h3 id="orgf57aad8"><span class="section-number-3">3.3.</span> Suite de Cauchy</h3>
<div class="outline-text-3" id="text-3-3">
<p>
Soit une application contractante \(A : X \mapsto X\) et \(u_0 \in X\). On définit la suite $u<sub>0,u</sub><sub>1,u</sub><sub>2</sub>,&#x2026;$ par :
</p>

<p>
\[u_n = A(u_{n - 1}) = ... = A^n(u_0)\]
</p>

<p>
pour tout \(n \in \setN\). On a alors :
</p>

<p>
\[\distance( u_{n + 1} , u_n ) \le c \cdot \distance( u_n , u_{n - 1} ) \le ... \le c^n \cdot \distance( u_1 , u_0 )\]
</p>

<p>
Soit \(m,n \in \setN\). Les propriétés des distances nous permettent d'écrire :
</p>

<p>
\[\distance( u_{n + m} , u_n ) \le \sum_{i = 0}^{m - 1} \distance( u_{n + i + 1} , u_{n + i} )\]
</p>

<p>
Mais comme \(\distance( u_{n + i + 1} , u_{n + i} ) \le c^{n + i} \cdot \distance( u_1 , u_0 )\), on a :
</p>

\begin{align}
\distance( u_{n + m} , u_n ) &\le \sum_{i = 0}^{m - 1} c^{n + i} \cdot \distance( u_1 , u_0 ) \)

\(
&\le c^n \cdot \distance( u_1 , u_0 ) \cdot \sum_{i = 0}^{m - 1} c^i \)

\(
&\le c^n \cdot \frac{1 - c^m}{1-c}
\end{align}

<p>
Finalement, comme \(1 - c^m \le 1\) quel que soit \(m \in \setN\) on obtient une expression qui ne dépend pas de \(m\) :
</p>

<p>
\[\distance( u_{n + m} , u_n ) \le \frac{c^n}{1 - c} \cdot \distance( u_1 , u_0 )\]
</p>

<p>
Les éléments de la suite sont donc de plus en plus proche l'un de l'autre
lorsque \(n\) augmente. Soit \(\epsilon \strictsuperieur 0\). Comme la suite \(c^n\) converge vers \(0\) lorsque \(n \to \infty\), on peut toujours trouver \(N\) tel que :
</p>

<p>
\[c^N \le \frac{\epsilon \cdot (1 - c)}{\distance( u_1 , u_0 )}\]
</p>

<p>
Il suffit donc de choisir \(i,j \in \setN\) tels que \(i,j \ge N\) pour avoir :
</p>

<p>
\[\distance( u_i , u_j ) = \distance( u_j , u_i ) \le \epsilon\]
</p>

<p>
On en conclut que la suite des \(u_n\) est de Cauchy.
</p>
</div>
</div>
<div id="outline-container-orgb5219aa" class="outline-3">
<h3 id="orgb5219aa"><span class="section-number-3">3.4.</span> Point fixe</h3>
<div class="outline-text-3" id="text-3-4">
<p>
Comme \(X\) est complet, notre suite \(u_n\) étant de Cauchy converge vers une certaine limite :
</p>

<p>
\[p = \lim_{n \to \infty} u_n\]
</p>

<p>
appartenant à \(X\). Analysons le comportement de \(A(p)\). On a la borne supérieure :
</p>

<p>
\[\distance\big( A(p) , p \big) \le \distance\big( A(p) , A(u_n) \big) + \distance\big( A(u_n) , u_n \big) + \distance\big( u_n , p \big)\]
</p>

<p>
On sait déjà que \(\distance( u_n , p )\) converge vers \(0\) par définition de \(p\). On sait aussi que \(\distance\big( A(u_n) , u_n \big) = \distance( u_{n + 1} , u_n ) \to 0\). On a également :
</p>

<p>
\[\distance\big( A(p) , A(u_n) \big) \le c \cdot \distance\big( p , u_n \big)\]
</p>

<p>
On en conclut que la suite \(A(u_n)\) converge vers \(A(p)\) :
</p>

<p>
\[A(p) = \lim_{n \to \infty} A(u_n)\]
</p>

<p>
Les trois termes de la borne supérieure convergeant chacun vers \(0\), cette borne est aussi petite que l'on veut lorsque \(n\) est assez grand. On a donc \(\distance\big( A(p) , p \big) = 0\) et :
</p>

<p>
\[A(p) = p\]
</p>

<p>
L'élément \(p \in X\) est un point fixe de \(A\).
</p>
</div>
</div>
<div id="outline-container-org5d883cc" class="outline-3">
<h3 id="org5d883cc"><span class="section-number-3">3.5.</span> Unicité</h3>
<div class="outline-text-3" id="text-3-5">
<p>
Si \(p_1\) et \(p_2\) sont deux points fixes, on a :
</p>

<div class="org-center">
<p>
\(
A(p_1) = p_1 \)
</p>

<p>
\(
A(p_2) = p_2
\)
</p>
</div>

<p>
et :
</p>

<p>
\[\distance( p_1 , p_2 ) \le c \cdot \distance\big( A(p_1) , A(p_2) \big) \le c \cdot \distance\left( p_1 , p_2 \right)\]
</p>

<p>
Comme \(c \strictinferieur 1\), ce n'est possible que si \(\distance(p_1,p_2) = 0\), c'est-à-dire :
</p>

<p>
\[p_1 = p_2\]
</p>

<p>
Le point fixe de \(A\) est unique.
</p>
</div>
</div>
<div id="outline-container-org5243c7d" class="outline-3">
<h3 id="org5243c7d"><span class="section-number-3">3.6.</span> Vitesse de convergence</h3>
<div class="outline-text-3" id="text-3-6">
<p>
Nous avons donc montré que la suite des \(u_n\) converge vers l'unique point fixe \(p\) de \(A\), et ce quel que soit \(u_0\). On a même la propriété suivante nous donnant une borne supérieure pour le taux de convergence :
</p>

<p>
\[\distance(u_n,p) \le \distance\big(A(u_{n - 1}),A(p)\big) \le c \cdot \distance(u_{n - 1},p) \le ... \le c^n \cdot \distance(u_0,p)\]
</p>
</div>
</div>
</div>
<div id="outline-container-orge894568" class="outline-2">
<h2 id="orge894568"><span class="section-number-2">4.</span> Continuité</h2>
<div class="outline-text-2" id="text-4">
<div id="text-table-of-contents-4" role="doc-toc">
<ul>
<li><a href="#org1a7cc5b">4.1. Dépendances</a></li>
<li><a href="#org5097f33">4.2. Fonctions continues</a></li>
<li><a href="#orgcd51d4c">4.3. Espace vectoriel</a></li>
<li><a href="#org647138e">4.4. Norme des fonctions continues</a></li>
<li><a href="#orgc083c3d">4.5. Théorème des valeurs intermédiaires</a></li>
<li><a href="#org306e398">4.6. Théorème de la bijection</a></li>
<li><a href="#org02373f0">4.7. Continuité uniforme</a></li>
<li><a href="#org84d602d">4.8. Polynômes</a></li>
<li><a href="#orgb65789b">4.9. Uniformité</a></li>
<li><a href="#orgb736e6f">4.10. Variations bornées</a></li>
<li><a href="#orgba320ce">4.11. Extrema</a></li>
</ul>
</div>

<p>
\label{chap:limite}
</p>
</div>
<div id="outline-container-org1a7cc5b" class="outline-3">
<h3 id="org1a7cc5b"><span class="section-number-3">4.1.</span> Dépendances</h3>
<div class="outline-text-3" id="text-4-1">
<ul class="org-ul">
<li>Chapitre \ref{chap:limite} : Les limites</li>
<li>Chapitre \ref{chap:reel} : Les réels</li>
</ul>
</div>
</div>
<div id="outline-container-org5097f33" class="outline-3">
<h3 id="org5097f33"><span class="section-number-3">4.2.</span> Fonctions continues</h3>
<div class="outline-text-3" id="text-4-2">
<p>
Une fonction \(f : D \to F\) est dite continue en \(a\) si :
</p>

<p>
\[\lim_{ \substack{ x \to a \\ x \in D } } f(x) = f(a)\]
</p>

<p>
Soit \(A \subseteq D\). On dit qu'une fonction est continue sur \(A\) si :
</p>

<p>
\[\lim_{ \substack{ x \to a \\ x \in A } } f(x) = f(a)\]
</p>

<p>
pour tout \(a \in A\). On note \(\continue(A,F)\) l'ensemble des fonctions \(f : A \mapsto F\) continues sur \(A\).
</p>
</div>
<div id="outline-container-orgc0f598a" class="outline-4">
<h4 id="orgc0f598a"><span class="section-number-4">4.2.1.</span> Remarque</h4>
<div class="outline-text-4" id="text-4-2-1">
<p>
Si \(F\) est muni d'une norme, les limites s'évaluent au sens de la distance découlant de la norme.
</p>
</div>
</div>
</div>
<div id="outline-container-orgcd51d4c" class="outline-3">
<h3 id="orgcd51d4c"><span class="section-number-3">4.3.</span> Espace vectoriel</h3>
<div class="outline-text-3" id="text-4-3">
<p>
Si \(\corps\) est un corps, on vérifie que \(\continue(A,\corps)\) est un espace vectoriel sur \(\corps\). En effet, la fonction nulle \(0\) est continue. Si \(\alpha,\beta \in \corps\) et si \(f,g \in \continue(A,\corps)\), on a :
</p>

\begin{align}
\lim_{x \to a} (\alpha \cdot f(x) + \beta \cdot g(x)) &= \alpha \cdot \lim_{x \to a} f(x) + \beta \cdot \lim_{x \to a} g(x) \)

\(
&= \alpha \cdot f(a) + \beta \cdot g(a)
\end{align}

<p>
pour tout \(a \in A\). On en conclut que \(\alpha \cdot f + \beta \cdot g\) est également continue.
</p>
</div>
</div>
<div id="outline-container-org647138e" class="outline-3">
<h3 id="org647138e"><span class="section-number-3">4.4.</span> Norme des fonctions continues</h3>
<div class="outline-text-3" id="text-4-4">
<p>
Si l'ensemble \(F\) est muni d'une norme, on peut définir la norme \(\norme{.}_\continue\) d'une fonction continue \(u\) par :
</p>

<p>
\[\norme{u}_\continue = \sup \big\{ \norme{u(x)} : x \in A \big\}\]
</p>
</div>
<div id="outline-container-orgbee1403" class="outline-4">
<h4 id="orgbee1403"><span class="section-number-4">4.4.1.</span> Notation</h4>
<div class="outline-text-4" id="text-4-4-1">
<p>
On note aussi :
</p>

<p>
\[\norme{u}_\infty = \norme{u}_\continue\]
</p>
</div>
</div>
<div id="outline-container-org3248a7f" class="outline-4">
<h4 id="org3248a7f"><span class="section-number-4">4.4.2.</span> Convergence uniforme</h4>
<div class="outline-text-4" id="text-4-4-2">
<p>
Cette norme est surtout utilisée lorsqu'il s'agit de mesurer l'écart entre deux fonctions \(f,g : A \to B\), en particulier lorsque \(g\) représente une approximation de \(f\). Dans ce cas, l'écart \(e = f - g\) représente l'erreur la plus élevée de l'estimation :
</p>

<p>
\[\norme{e}_\infty = \norme{f - g}_\infty = \sup \{ \norme{f(x) - g(x)} : x \in A \}\]
</p>

<p>
Lorsque cette norme particulière de l'erreur tend vers zéro, on parle de convergence uniforme.
</p>
</div>
</div>
</div>
<div id="outline-container-orgc083c3d" class="outline-3">
<h3 id="orgc083c3d"><span class="section-number-3">4.5.</span> Théorème des valeurs intermédiaires</h3>
<div class="outline-text-3" id="text-4-5">
<p>
Les fonctions continues possèdent l'importante propriété suivante.
</p>

\begin{theoreme}

$$ $$


  - Soit $f \in \continue(I,\setR)$ où $I = [a,b]$ est un intervalle inclus dans $\setR$. On suppose que :

$$f(a) \strictinferieur f(b)$$

Soit le réel $\varphi$ vérifiant $f(a) \strictinferieur \varphi \strictinferieur f(b)$. On peut alors trouver un $c \in \intervalleouvert{a}{b}$ tel que $f(c) = \varphi$.

  - Soit $g \in \continue(I,\setR)$. On suppose que :

$$g(a) \strictsuperieur g(b)$$

Soit le réel $\varphi$ vérifiant $g(a) \strictsuperieur \varphi \strictsuperieur g(b)$. On peut alors trouver un $c \in \intervalleouvert{a}{b}$ tel que $g(c) = \varphi$.


\end{theoreme}

\begin{demonstration}

Nous allons démontrer ce résultat par l'absurde.


  - Considérons le cas où $f(a) \strictinferieur \varphi \strictinferieur f(b)$. On définit les ensembles :

#+BEGIN_CENTER
\(
A^+ = \{ x \in I : f(x) \strictsuperieur \varphi \} \)

\(
A^- = \{ x \in I : f(x) \strictinferieur \varphi \}
\)
#+END_CENTER

Si aucun $c \in I$ ne vérifie $f(c) = \varphi$, on doit avoir clairement $A^+ \cup A^- = I$.

Nous définissons $\alpha = \sup A^-$. Comme $A^-\subseteq I$, on à clairement
$\alpha \in I$. Si $\alpha \in A^-$, alors par continuité de $f$ en $\alpha$,
on peut trouver $\delta \strictsuperieur 0$ tel que :

$$\abs{ f(\alpha + \delta) - f(\alpha) } \le \epsilon = \unsur{2}(\varphi - f(\alpha))$$

On a alors clairement $f(\alpha + \delta) \strictinferieur \varphi$ et $\alpha \strictinferieur \alpha + \delta \in A^-$ ce qui contredit l'hypothèse de suprémum pour $\alpha$.

On doit donc avoir $\alpha \notin A^-$. Mais alors $\alpha \in I \setminus A^- = A^+$. Donc $f(\alpha) \strictsuperieur \varphi$. Par continuité de $f$ en $\alpha$, on peut trouver $\delta \strictsuperieur 0$ tel que :

$$\abs{ f(\alpha) - f(x) } \le \epsilon = \unsur{2}(f(\alpha)-\varphi)$$

pour tout $x \in \intervalleouvert{\alpha - \delta}{\alpha}$. On a alors clairement $f(x) \strictsuperieur \varphi$ pour tout $x \in \intervalleouvert{\alpha - \delta}{\alpha}$.

Soit $\beta \in \intervalleouvert{\alpha - \delta}{\alpha}$. Par définition de $\alpha$, on ne peut pas avoir $\beta \ge A^-$, donc il existe $\gamma \in \intervalleouvert{\beta}{\alpha} \subseteq \intervalleouvert{\alpha - \delta}{\alpha}$ tel que $\gamma \in A^-$. On a donc $f(\gamma) \strictinferieur \varphi$ ce qui contredit la propriété ci-dessus en $x = \gamma$.

  - Considérons à présent le cas $g(a) \strictsuperieur \varphi \strictsuperieur g(b)$. On pose :

$$f = -g$$

On a :

$$f(a) \strictinferieur -\varphi \strictinferieur f(b)$$

On peut donc trouver un $c \in \intervalleouvert{a}{b}$ tel que :

$$f(c) = -\varphi$$

On en conclut que :

$$g(c) = -f(c) = \varphi$$


\end{demonstration}
</div>
<div id="outline-container-org5096767" class="outline-4">
<h4 id="org5096767"><span class="section-number-4">4.5.1.</span> Généralisation</h4>
<div class="outline-text-4" id="text-4-5-1">
<p>
Soit le réel \(\varphi\) tel que \(f(a) \le \varphi \le f(b)\). Si \(\varphi \in \{f(a),f(b)\}\), il suffit de prendre \(c \in \{a,b\}\) pour avoir \(f(a) \le f(c) \le f(b)\). Sinon, on applique le théorème des valeurs intermédiaires et on trouve un \(c\) vérifiant \(f(a) \strictinferieur f(c) \strictinferieur f(b)\). Mais dans tous les cas, on pourra trouver un \(c \in [a,b]\) tel que \(f(a) \le f(c) \le f(b)\). De même si \(f(a) \strictsuperieur f(b)\).
</p>
</div>
</div>
</div>
<div id="outline-container-org306e398" class="outline-3">
<h3 id="org306e398"><span class="section-number-3">4.6.</span> Théorème de la bijection</h3>
<div class="outline-text-3" id="text-4-6">
\begin{theoreme}

Soit $f \in \continue(I,J)$ où $I = [a,b]$ est un intervalle inclus dans $\setR$ et où $J = f(I)$. Si $f$ est strictement croissante (ou décroissante), alors $f$ est inversible et :

$$f(I) = [\alpha,\beta]$$

avec :

#+BEGIN_CENTER
\(
\alpha = \min \{ f(a), f(b) \} \)

\(
\beta = \max \{ f(a), f(b) \}
\)
#+END_CENTER

\end{theoreme}

\begin{demonstration}

Comme $f : I \mapsto J = f(I)$ est strictement croissante ou décroissante, on a vu dans le chapitre traitant des bijections que $f$ est inversible. Choisissons un réel $\varphi \in [\alpha,\beta]$. Le théorème des valeurs intermédiaires nous dit qu'on peut trouver un réel $c \in [a,b]$ tel que $f(c) = \varphi$. On en conclut que $\varphi \in f(I)$. Cette relation étant vérifiée pour tout $\varphi \in [\alpha,\beta]$, on a :

$$[\alpha,\beta] \subseteq f(I)$$

Soit $x \in I$. Comme $f$ est croissante ou décroissante, on doit avoir :

$$f(a) \le f(x) \le f(b)$$

ou :

$$f(a) \ge f(x) \ge f(b)$$

On a donc également :

$$f(I) \subseteq [\alpha,\beta]$$

L'inclusion étant réciproque, on a :

$$f(I) = [\alpha,\beta]$$

\end{demonstration}
</div>
</div>
<div id="outline-container-org02373f0" class="outline-3">
<h3 id="org02373f0"><span class="section-number-3">4.7.</span> Continuité uniforme</h3>
<div class="outline-text-3" id="text-4-7">
<p>
\label{sec:continuite_uniforme}
</p>

<p>
On dit qu'une fonction \(f\) est uniformément continue sur \(A\), si pour toute précision \(\epsilon \strictsuperieur 0\), on peut trouver un \(\delta \strictsuperieur 0\) tel que :
</p>

<p>
\[\abs{f(s) - f(t)} \le \epsilon\]
</p>

<p>
pour tout \(s,t \in A\) vérifiant \(\abs{s - t} \le \delta\).
</p>
</div>
<div id="outline-container-org0bd1994" class="outline-4">
<h4 id="org0bd1994"><span class="section-number-4">4.7.1.</span> Continuité simple</h4>
<div class="outline-text-4" id="text-4-7-1">
<p>
Si \(f\) est uniformément continue sur \(A\), on a clairement :
</p>

<p>
\[\lim_{s \to t} \abs{f(s) - f(t)} = 0\]
</p>

<p>
et donc :
</p>

<p>
\[\lim_{s \to t} f(s) = f(t)\]
</p>

<p>
pour tout \(t \in A\). Toute fonction uniformément continue est donc continue.
</p>
</div>
</div>
</div>
<div id="outline-container-org84d602d" class="outline-3">
<h3 id="org84d602d"><span class="section-number-3">4.8.</span> Polynômes</h3>
<div class="outline-text-3" id="text-4-8">
<p>
Soit \(n \in \setN\) et \(\alpha,\beta \in \setR\) avec \(\alpha \le \beta\). Nous allons analyser la continuité du monôme \(\mu : [\alpha,\beta] \mapsto \setR\) défini par :
</p>

<p>
\[\mu : x \mapsto x^n\]
</p>

<p>
pour tout \(x \in [\alpha,\beta]\). Reprenant les résultats de la section \ref{sec:factorisation_progression_geometrique}, nous avons :
</p>

<p>
\[s^n - t^n = (s - t) \sum_{i = 0}^{n - 1} s^{n - 1 - i} \cdot t^i\]
</p>

<p>
Quelque soient \(s,t \in [\alpha,\beta]\), il est clair que
</p>

<p>
\[\abs{s}, \abs{t} \le M = \max \{ \abs{\alpha} , \abs{\beta} \}\]
</p>

<p>
On a donc :
</p>

<p>
\[\abs{s^n - t^n} \le \abs{s - t} \cdot n \cdot M^{n - 1}\]
</p>

<p>
Fixons à présent \(\epsilon \strictsuperieur 0\). Il suffit de prendre :
</p>

<p>
\[\abs{s - t} \le \delta \le \frac{\epsilon}{ n \cdot M^{n - 1} }\]
</p>

<p>
pour avoir :
</p>

<p>
\[\abs{s^n - t^n} \le \delta \cdot n \cdot M^n \le \epsilon\]
</p>

<p>
Comme le choix de \(\delta\) ne dépend ni de \(s\) ni de \(t\), le monôme \(\mu\) est uniformément continu sur \([\alpha,\beta]\).
</p>

<p>
Pour généraliser aux polynômes de la forme :
</p>

<p>
\[p(x) = \sum_{i = 0}^n a_i \cdot x^i\]
</p>

<p>
on part de :
</p>

<p>
\[p(s) - p(t) = \sum_{i = 0}^n a_i \cdot (s^i - t^i)\]
</p>

<p>
On a donc :
</p>

<p>
\[\abs{p(s) - p(t)} \le \sum_{i = 0}^n \abs{a_i} \cdot \abs{s^i - t^i}\]
</p>

<p>
Mais comme on peut trouver des \(\delta_k\) tels que :
</p>

<p>
\[\abs{s^k - t^k} \le \frac{\epsilon}{\sum_j \abs{a_j}}\]
</p>

<p>
il suffit de choisir \(\delta = \min \{ \delta_0, \delta_1, ..., \delta_n \}\) pour avoir :
</p>

<p>
\[\abs{p(s) - p(t)} \le \epsilon \cdot \frac{ \sum_i \abs{a_i} }{ \sum_j \abs{a_j} } = \epsilon\]
</p>

<p>
Cette généralisation montre aussi que toute combinaison linéaire de fonctions uniformément continues est uniformément continue.
</p>
</div>
<div id="outline-container-org57fdde5" class="outline-4">
<h4 id="org57fdde5"><span class="section-number-4">4.8.1.</span> Continuité simple</h4>
<div class="outline-text-4" id="text-4-8-1">
<p>
Qu'en est-il de la continuité sur \(\setR\) ? Choisissons \(a \in \setR\) et considérons l'intervalle \(I = [a - 1, a + 1]\). Le polynôme est uniformément continu sur cette intervalle, et \(a \in \interieur I\). Plus précisément, \(\distance(a,\setR \setminus I) = 1 \strictsuperieur 0\). Donc, si \(\abs{x - a} \le 1\), on a forcément \(x \in I\). On peut donc se servir de la continuité uniforme sur \(I\) pour trouver un \(\delta \in \intervalleouvert{0}{1}\) tel que \(\abs{p(x) - p(a)} \le \epsilon\) lorsque \(\abs{x - a} \le \delta\). Les polynômes sont donc continus en tout point de \(\setR\), et par conséquent continus sur \(\setR\).
</p>
</div>
</div>
</div>
<div id="outline-container-orgb65789b" class="outline-3">
<h3 id="orgb65789b"><span class="section-number-3">4.9.</span> Uniformité</h3>
<div class="outline-text-3" id="text-4-9">
<p>
Nous allons à présent montrer que toute fonction continue sur un intervalle de la forme \([\alpha,\beta]\) y est uniformément continue.
</p>

\begin{theoreme}

Soit la fonction $f \in \continue([\alpha,\beta],\setR)$. Etant donné un $\epsilon \strictsuperieur 0$ et un $a \in [\alpha,\beta]$, on note $\Delta(a,\epsilon)$ l'ensemble des écarts strictement positifs offrant la précision demandée. Pour tout $\delta \in \Delta(a,\epsilon)$, on aura donc $\delta \strictsuperieur 0$ et :

$$\abs{f(a + h) - f(a)} \le \epsilon$$

pourvu que $h \in \setR$ vérifie :

#+BEGIN_CENTER
\(
\abs{h} \le \delta \)

\(
a + h \in [\alpha,\beta]
\)
#+END_CENTER

On note les supremums de cette famille d'ensemble par :

$$\sigma(a,\epsilon) = \sup \Delta(a,\epsilon)$$

Nous allons voir que l'intersection de ces ensembles est non vide, même après avoir parcouru tout l'intervalle :

$$\Gamma(\epsilon) = \bigcap_{a \in [\alpha,\beta]} \Delta(a,\epsilon) \ne \emptyset$$

et que l'infimum des supremums est strictement positif :

$$I(\epsilon) = \inf \{ \sigma(a,\epsilon) : a \in [\alpha,\beta] \} \strictsuperieur 0$$

Etant donné un $\epsilon \strictsuperieur 0$, on peut donc trouver un $\delta  \in \Gamma(\epsilon)$ tel que :

$$\abs{f(x + h) - f(x)} \le \epsilon$$

pour tout $x \in [\alpha,\beta]$ et pour tout $h$ vérifiant :

#+BEGIN_CENTER
\(
\abs{h} \le \delta \)

\(
x + h \in [\alpha,\beta]
\)
#+END_CENTER

Posant $s = x + h$ et $t = x$, cela revient à dire que :

$$\abs{f(s) - f(t)} \le \epsilon$$

pour tout $s,t \in [\alpha,\beta]$ vérifiant $\abs{s - t} \le \delta$. La fonction $f$ est donc uniformément continue sur $[\alpha,\beta]$.

\end{theoreme}
</div>
<div id="outline-container-org4e2849b" class="outline-4">
<h4 id="org4e2849b"><span class="section-number-4">4.9.1.</span> Remarques</h4>
<div class="outline-text-4" id="text-4-9-1">
<ul class="org-ul">
<li>La continuité de \(f\) nous garantit que ces écarts strictement positifs existent bien, c'est à dire que :</li>
</ul>

<p>
\[\Delta(a,\epsilon) \ne \emptyset\]
</p>

<p>
quelles que soient les valeurs de \(\epsilon \strictsuperieur 0\) et de \(a \in [\alpha,\beta]\).
</p>

<ul class="org-ul">
<li>Par ailleurs, si :</li>
</ul>

<p>
\[0 \strictinferieur \gamma \le \delta \in \Delta(a,\epsilon)\]
</p>

<p>
tous les réels présentant un écart inférieur à \(\gamma\) (par rapport à \(a\)) auront a fortiori un écart inférieur à \(\delta\) et satisferont donc la précision \(\epsilon\) :
</p>

<p>
\[f\big( [a - \gamma, a + \gamma] \big) \subseteq [f(a) - \epsilon, f(a) + \epsilon]\]
</p>

<p>
Par conséquent, \(\gamma\) appartient à \(\Delta(a,\epsilon)\) et :
</p>

<p>
\[]0,\delta] \subseteq \Delta(a,\epsilon)\]
</p>

<p>
pour tout \(\delta \in \Delta(a,\epsilon)\).
</p>

<ul class="org-ul">
<li>Si \(x \in \intervalleouvert{0}{\sigma(a,\epsilon)}\), on a :</li>
</ul>

<p>
\[\psi = \sigma(a,\epsilon) - x \strictsuperieur 0\]
</p>

<p>
Comme le supremum est dans l'adhérence, la distance à son ensemble est nulle et on peut trouver un \(\delta \in \Delta(a,\epsilon)\) tel que :
</p>

<p>
\[\abs{\sigma(a,\epsilon) - \delta} \le \psi\]
</p>

<p>
On a donc :
</p>

<p>
\[\sigma(a,\epsilon) - \delta \le \sigma(a,\epsilon) - x\]
</p>

<p>
et :
</p>

<p>
\[x \le \delta\]
</p>

<p>
On a alors :
</p>

<p>
\[x \in \ ]0,\delta] \subseteq \Delta(a,\epsilon)\]
</p>

<p>
et donc \(x \in \Delta(a,\epsilon)\). Cette relation étant vérifiée pour tout \(x \in \intervalleouvert{0}{\sigma(a,\epsilon)}\), on a :
</p>

<p>
\[\intervalleouvert{0}{\sigma(a,\epsilon)} \subseteq \Delta(a,\epsilon)\]
</p>

<ul class="org-ul">
<li>Par définition du supremum, il ne peut avoir d'élément de \(\Delta(a,\epsilon)\) supérieur à \(\sigma(a,\epsilon)\), et on a également :</li>
</ul>

<p>
\[\Delta(a,\epsilon) \subseteq \intervallesemiouvertgauche{0}{\sigma(a,\epsilon)}\]
</p>

<ul class="org-ul">
<li>Les propositions sur l'intersection non vide et l'infimum strictement positif sont équivalentes. En effet, si \(I(\epsilon) \strictsuperieur 0\), on a :</li>
</ul>

<p>
\[\sigma(a,\epsilon) \ge I(\epsilon) \strictsuperieur 0\]
</p>

<p>
pour tout \(a\). On a donc :
</p>

<p>
\[\emptyset \ne \ ]0,I(\epsilon)[ \ \subseteq \bigcap_{a \in [\alpha,\beta]} (0,\sigma(a,\epsilon)) \subseteq \bigcap_{a \in [\alpha,\beta]} \Delta(a,\epsilon)\]
</p>

<p>
D'un autre coté, si l'intersection est non nulle, soit :
</p>

<p>
\[\delta \in \bigcap_{a \in [\alpha,\beta]} \Delta(a,\epsilon)\]
</p>

<p>
Comme \(\delta\) appartient à \(\Delta(a,\epsilon)\) pour tout \(a \in [\alpha,\beta]\), on a :
</p>

<p>
\[\sigma(a,\epsilon) \ge \delta \strictsuperieur 0\]
</p>

<p>
par définition du supremum localisé en \(a\). Il suffit alors de passer à l'infimum sur \(a\) pour obtenir :
</p>

<p>
\[I(\epsilon) = \inf_{a \in [\alpha,\beta]} \sigma(a,\epsilon) \ge \delta \strictsuperieur 0\]
</p>

<p>
Nous nous attelerons ici à démontrer que \(I(\epsilon) \strictsuperieur 0\).
</p>


\begin{demonstration}

Soit $\epsilon \strictsuperieur 0$. Considérons la suite d'infimums intermédiaires :

$$D(x) = \inf \{ \sigma(\xi,\epsilon) : \xi \in [\alpha,x] \}$$

où $x \in [\alpha,\beta]$. Nous considérons l'ensemble $\Psi$ des éléments tels que cet infimum soit non nul :

$$\Psi = \{ x \in [\alpha,\beta] : D(x) \strictsuperieur 0 \}$$

On a $D(\alpha) = \inf\{ \sigma(\alpha,\epsilon) \} = \sigma(\alpha,\epsilon) \strictsuperieur 0$. Donc $\alpha \in \Psi$. On a aussi $\Psi \subseteq [\alpha,\beta] \le \beta$. L'ensemble $\Psi$ est non vide et majoré. Il admet donc un supremum :

$$S = \sup \Psi \le \beta$$

  - Si $x \in \Psi$ et $a \in [\alpha,x]$, on a :

$$[\alpha,a] \subseteq [\alpha,x]$$

Les propriétés de l'infimum pour l'inclusion nous donnent alors :

$$D(a) \ge D(x) \strictsuperieur 0$$

On en conclut que :

$$[\alpha,x] \subseteq \Psi$$

pour tout $x \in \Psi$.

  - Soit $a \in [\alpha,\beta]$. Nous allons construire une zone autour de $a$ où l'infimum est strictement positif. Choisissons $\delta(a) \in \Delta(a,\epsilon/2)$ et posons :

$$\gamma(a) =  \unsur{2} \delta(a) \strictsuperieur 0$$

Considérons à présent l'ensemble :

$$U(a) = [a - \gamma(a), a + \gamma(a)] \cap [\alpha,\beta]$$

Pour tout $b \in U(a)$ et $x \in [b - \gamma(a), b + \gamma(a)] \cap [\alpha,\beta]$, on a alors :

\begin{align}
\abs{b - a} &\le \gamma(a) \strictinferieur \delta(a) \)

\(
\abs{x - b} &\le \gamma(a) \)

\(
\abs{x - a} &\le \abs{x - b} + \abs{b - a} \le 2 \gamma(a) \le \delta(a)
\end{align}

et :

#+BEGIN_CENTER
\(
\abs{f(b) - f(a)} \le \frac{\epsilon}{2} \\ \)

\(
\abs{f(x) - f(a)} \le \frac{\epsilon}{2}
\)
#+END_CENTER

On en déduit la borne supérieure :

\begin{align}
\abs{f(x) - f(b)} &\le \abs{f(x) - f(a)} + \abs{f(a) - f(b)} \)

\(
&\le \frac{\epsilon}{2} + \frac{\epsilon}{2} = \epsilon
\end{align}

On a donc :

$$\sigma(b,\epsilon) \ge \gamma(a) \strictsuperieur 0$$

pour tout $b \in U(a)$. En prenant l'infimum, il vient :

$$\inf_{b \in U(a)} \sigma(b,\epsilon) \ge \gamma(a) \strictsuperieur 0 $$

  - Supposons que $S = \alpha$ et posons :

$$\theta = \min \{ \gamma(\alpha) , \beta - \alpha \} \strictsuperieur 0$$

On a alors $U(S) = U(\alpha) = [\alpha, \alpha + \theta]$ et :

$$D(\alpha + \theta) = \inf_{b \in U(\alpha)} \sigma(b,\epsilon) \ge \gamma(\alpha) \strictsuperieur 0 $$

On en conclut que $\alpha + \theta \in \Psi$ avec $S \strictinferieur \alpha + \theta$, ce qui contredit $S = \sup \Psi$.

  - Supposons à présent que $\alpha \strictinferieur S \strictinferieur \beta$ et posons :

#+BEGIN_CENTER
\(
\eta = \min \{ \gamma(S) , S - \alpha \} \strictsuperieur 0 \)

\(
\theta = \min \{ \gamma(S) , \beta - S \} \strictsuperieur 0
\)
#+END_CENTER

On a alors $U(S) = [S - \eta, S + \theta]$. Voyons comment se comporte $\sigma(b,\epsilon)$ lorsquer $b$ voyage dans $[\alpha, S + \theta]$. Le supremum étant dans l'adhérence, on peut trouver $\psi \in \Psi$ tel que :

$$\abs{S - \psi} = S - \psi \le \eta$$

Si $b \in [\alpha,\psi]$, on a :

$$\sigma(b,\epsilon) \ge D(\psi) \strictsuperieur 0 $$

par définition de $\Psi$. Mais si $b \in [\psi, S + \theta]$, on a :

$$S - \eta \le \psi \le b \le S + \theta$$

Donc, $b \in [S - \eta, S + \theta] = U(S)$ et :

$$\sigma(b,\epsilon) \ge \gamma(S) \strictsuperieur 0$$

Il suffit donc de poser :

$$\varpi = \min \{ \gamma(S) , D(\psi) \} \strictsuperieur 0$$

pour avoir $\sigma(b,\epsilon) \ge \varpi$ sur $[\alpha, S + \theta]$. On en déduit que l'infimum est strictement positif :

$$D(S + \theta) \ge \varpi \strictsuperieur 0 $$

C'est à dire $S + \theta \in \Psi$ avec $S \strictinferieur S + \theta$, ce qui contredit la définition du supremum. On doit donc avoir $S = \sup \Psi = \beta$.

  - Posons :

$$\theta = \min \{ \gamma(\beta) , \beta - \alpha \} \strictsuperieur 0$$

On a alors $U(S) = U(\beta) = [S - \theta, S] = [\beta - \theta, \beta]$. Comme le supremum est dans l'adhérence, on peut trouver un $\psi \in \Psi$ tel que :

$$\abs{S - \psi} = S - \psi \le \theta$$

Si $b \in [\alpha,\psi]$, on a :

$$\sigma(b,\epsilon) \ge D(\psi) \strictsuperieur 0 $$

par définition de $\Psi$. Mais si $b \in [\psi, S] = [\psi, \beta]$, on a :

$$S - \theta \le \psi \le b \le \beta$$

Donc, $b \in [S - \theta, S] = U(S)$ et :

$$\sigma(b,\epsilon) \ge \gamma(S) = \gamma(\beta) \strictsuperieur 0$$

Il suffit donc de poser :

$$\varpi = \min \{ \gamma(\beta) , D(\psi) \} \strictsuperieur 0$$

pour avoir $\sigma(b,\epsilon) \ge \varpi$ sur $[\alpha, \beta]$. On en déduit que l'infimum est strictement positif :

$$D(\beta) \ge \varpi \strictsuperieur 0 $$

C'est à dire $\beta \in \Psi$ et :

$$\Psi = [\alpha,\beta]$$

L'infimum $D(x)$ est donc strictement positif sur tout l'intervalle $[\alpha,\beta]$ et on a :

#+BEGIN_CENTER
\(
I(\epsilon) = D(\beta) \strictsuperieur 0 \)

\(
\intervalleouvert{0}{I(\epsilon)} \subseteq \Gamma(\epsilon)
\)
#+END_CENTER


\end{demonstration}
</div>
</div>
<div id="outline-container-org5917e22" class="outline-4">
<h4 id="org5917e22"><span class="section-number-4">4.9.2.</span> Remarque</h4>
<div class="outline-text-4" id="text-4-9-2">
<p>
Le théorème {\em n'est pas} applicable aux intervalles ouverts ou semi-ouverts.
</p>
</div>
</div>
</div>
<div id="outline-container-orgb736e6f" class="outline-3">
<h3 id="orgb736e6f"><span class="section-number-3">4.10.</span> Variations bornées</h3>
<div class="outline-text-3" id="text-4-10">
<p>
On déduit de l'uniforme continuité des fonctions continues sur les intervalles que les fonctions continues y sont bornées. En effet, soit \(\epsilon \strictsuperieur\) et \(\delta \strictsuperieur 0\) tel que :
</p>

<p>
\[\abs{f(b) - f(a)} \le \epsilon\]
</p>

<p>
pour tout \(a,b \in [\alpha,\beta]\) tels que \(\abs{a - b} \le \delta\).
Choisissons \(N \in \setN\) tel que :
</p>

<p>
\[\frac{\beta - \alpha}{N} \le \delta\]
</p>

<p>
Choisissons à présent \(x,y \in [\alpha,\beta]\) et posons :
</p>

<p>
\[h = \frac{y - x}{N}\]
</p>

<p>
On a alors :
</p>

<p>
\[\abs{h} = \abs{\frac{y - x}{N}} \le \frac{\beta - \alpha}{N} \le \delta\]
</p>

<p>
Posons \(x_i = x + i \cdot h\) pour \(i = 0,1,2,...,N\). On a alors \(x_0 = x\) et \(x_N = y\). La variation est bornée par :
</p>

<p>
\[\abs{f(x) - f(y)} \le \sum_{i = 0}^N \abs{f(x_i) - f(x_{i - 1})} \le (N + 1) \cdot \epsilon\]
</p>
</div>
<div id="outline-container-orge5214af" class="outline-4">
<h4 id="orge5214af"><span class="section-number-4">4.10.1.</span> Norme</h4>
<div class="outline-text-4" id="text-4-10-1">
<p>
Comme \(N\) ne dépend ni de \(x\) ni de \(y\), on en conclut que les variations de \(f\) sont bornées sur l'intervalle. On a aussi :
</p>

<p>
\[\abs{f(x)} \le \abs{f(x) - f(\alpha)} + \abs{f(\alpha)} \le M\]
</p>

<p>
où \(M = (N + 1) \cdot \epsilon + \abs{f(\alpha)}\) ne dépend pas du choix de \(x\), ce qui prouve que \(f\) est bornée sur l'intervalle. En passant au supremum, on en déduit que la norme est finie et que :
</p>

<p>
\[\norme{f}_\infty \le M\]
</p>
</div>
</div>
</div>
<div id="outline-container-orgba320ce" class="outline-3">
<h3 id="orgba320ce"><span class="section-number-3">4.11.</span> Extrema</h3>
<div class="outline-text-3" id="text-4-11">
<p>
Soit \(f \in \continue([a,b],\setR)\). Comme \(f\) est bornée, on peut poser :
</p>

<p>
\[I = \inf \{ f(x) : x \in [a,b] \} = \inf f([a,b])\]
</p>

<p>
Comme la distance de l'infimum à l'ensemble est nulle, on peut construire une suite \(\{x_1,x_2,...\}\) convergente vers \(\lambda \in [a,b]\) :
</p>

<p>
\[\lambda = \lim_{n \to \infty} x_n\]
</p>

<p>
et telle que :
</p>

<p>
\[f(x_n) - I \le \unsur{2^n}\]
</p>

<p>
On voit que :
</p>

<p>
\[\lim_{n \to \infty} f(x_n) = I\]
</p>

<p>
Mais par continuité de \(f\), on a aussi :
</p>

<p>
\[\lim_{n \to \infty} f(x_n) = f(\lambda)\]
</p>

<p>
On en conclut que :
</p>

<p>
\[f(\lambda) = I\]
</p>

<p>
On peut donc trouver un réel dans l'intervalle qui minimise la fonction. L'infimum appartient à l'image \(f([a,b])\). Il est donc également un minimum et on a :
</p>

<p>
\[f(\lambda) = \inf f([a,b]) = \min f([a,b])\]
</p>

<p>
On construit de même un \(\sigma \in [a,b]\) qui atteint le supremum :
</p>

<p>
\[f(\sigma) = \sup f([a,b]) = \max f([a,b])\]
</p>
</div>
</div>
</div>
<div id="outline-container-orgc547a82" class="outline-2">
<h2 id="orgc547a82"><span class="section-number-2">5.</span> Applications linéaires</h2>
<div class="outline-text-2" id="text-5">
<div id="text-table-of-contents-5" role="doc-toc">
<ul>
<li><a href="#org8d3a0cd">5.1. Dépendances</a></li>
<li><a href="#orgbd7f89a">5.2. Définition</a></li>
<li><a href="#orgba0abd1">5.3. Identité</a></li>
<li><a href="#orgddc5c74">5.4. Inverse</a></li>
<li><a href="#orgce8385a">5.5. Valeur au vecteur nul</a></li>
<li><a href="#orgbc655f6">5.6. Norme des applications linéaires</a></li>
<li><a href="#orgcfaf2aa">5.7. Norme d'une composée</a></li>
<li><a href="#org6df2292">5.8. Norme d'une puissance</a></li>
<li><a href="#orgf06c7cd">5.9. Continuité</a></li>
<li><a href="#orgd435975">5.10. $n$-linéarité</a></li>
<li><a href="#org4e61e70">5.11. Représentation matricielle</a></li>
<li><a href="#org1db74c4">5.12. Produit matriciel</a></li>
<li><a href="#org5a4295b">5.13. Blocs</a></li>
<li><a href="#org6c9f76c">5.14. Matrice identité</a></li>
<li><a href="#orge58d81f">5.15. Inverse</a></li>
<li><a href="#orge561167">5.16. Inverse d'un produit</a></li>
<li><a href="#org2da251a">5.17. Puissance</a></li>
<li><a href="#org9a99d08">5.18. Polynômes matriciels</a></li>
</ul>
</div>

<p>
\label{chap:lineaire}
</p>
</div>
<div id="outline-container-org8d3a0cd" class="outline-3">
<h3 id="org8d3a0cd"><span class="section-number-3">5.1.</span> Dépendances</h3>
<div class="outline-text-3" id="text-5-1">
<ul class="org-ul">
<li>Chapitre \ref{chap:fonction} : Les fonctions</li>
</ul>
</div>
</div>
<div id="outline-container-orgbd7f89a" class="outline-3">
<h3 id="orgbd7f89a"><span class="section-number-3">5.2.</span> Définition</h3>
<div class="outline-text-3" id="text-5-2">
<p>
Soit les espaces vectoriels \(E\) et \(F\) sur \(\corps\) et la fonction \(f : E \mapsto F\). On dit que \(f\) est linéaire si, pour tout \(x,y \in E\) et \(\alpha, \beta \in \corps\), on a :
</p>

<p>
\[f(\alpha \cdot x + \beta \cdot y) = \alpha \cdot f(x) + \beta \cdot f(y)\]
</p>

<p>
On note \(\lineaire(E,F)\) l'ensemble des fonctions linéaires de \(E\) vers \(F\).
</p>
</div>
</div>
<div id="outline-container-orgba0abd1" class="outline-3">
<h3 id="orgba0abd1"><span class="section-number-3">5.3.</span> Identité</h3>
<div class="outline-text-3" id="text-5-3">
<p>
L'application identité est clairement linéaire.
</p>
</div>
</div>
<div id="outline-container-orgddc5c74" class="outline-3">
<h3 id="orgddc5c74"><span class="section-number-3">5.4.</span> Inverse</h3>
<div class="outline-text-3" id="text-5-4">
<p>
Soit \(u = f(x)\) et \(v = f(y)\). Si l'application inverse existe, on a \(x = f^{-1}(u)\) et \(y = f^{-1}(v)\). En composant à gauche par \(f^{-1}\) la définition de la linéarité, on obtient :
</p>

<p>
\[\alpha \cdot f^{-1}(u) + \beta \cdot f^{-1}(v) = f^{-1}(\alpha \cdot u + \beta \cdot v)\]
</p>

<p>
ce qui montre que l'inverse est également linéaire.
</p>
</div>
</div>
<div id="outline-container-orgce8385a" class="outline-3">
<h3 id="orgce8385a"><span class="section-number-3">5.5.</span> Valeur au vecteur nul</h3>
<div class="outline-text-3" id="text-5-5">
<p>
Choisissons un \(x \in E\). on voit que :
</p>

<p>
\[f(0) = f(0 \cdot x) = 0 \cdot f(x) = 0\]
</p>

<p>
La valeur d'une application linéaire s'annule au vecteur nul.
</p>
</div>
</div>
<div id="outline-container-orgbc655f6" class="outline-3">
<h3 id="orgbc655f6"><span class="section-number-3">5.6.</span> Norme des applications linéaires</h3>
<div class="outline-text-3" id="text-5-6">
<p>
La norme d'une application linéaire est définie comme étant l'extension maximale qu'elle produit :
</p>

<p>
\[\norme{f} = \sup \left\{ \frac{ \norme{f(x)} }{ \norme{x} } : x \in E, \ x \ne 0 \right\}\]
</p>

<p>
On a donc :
</p>

<p>
\[\norme{f(x)} \le \norme{f} \cdot \norme{x}\]
</p>

<p>
pour tout \(x \in E \setminus \{ 0 \}\).
</p>
</div>
<div id="outline-container-org988438f" class="outline-4">
<h4 id="org988438f"><span class="section-number-4">5.6.1.</span> Vérification</h4>
<div class="outline-text-4" id="text-5-6-1">
<p>
Nous allons vérifier qu'il s'agit bien d'une norme. On a \(\norme{f} \ge 0\) par positivité de la norme sur \(E\) et \(F\). La condition \(\norme{f} = 0\) implique \(\norme{f(x)} = 0\) et donc \(f(x) = 0\) pour tout \(x \ne 0\). Comme \(f\) est linéaire, on a aussi \(f(0) = 0\) et \(f = 0\).
</p>

<p>
Si \(f,g\) sont linéaires, on a :
</p>

<p>
\[\norme{f(x) + g(x)} \le \norme{f(x)} + \norme{g(x)} \le \norme{f} \cdot \norme{x} + \norme{g} \cdot \norme{x} = (\norme{f} + \norme{g}) \cdot \norme{x}\]
</p>

<p>
pour tout \(x \ne 0\). En divisant par \(\norme{x}\) et en passant au supremum, on obtient :
</p>

<p>
\[\norme{f + g} \le \norme{f} + \norme{g}\]
</p>

<p>
Enfin, si \(\alpha \in \corps\), on a :
</p>

<p>
\[\frac{ \norme{\alpha \cdot f(x)} }{ \norme{x} } = \abs{\alpha} \cdot \frac{ \norme{f(x)} }{ \norme{x} }\]
</p>

<p>
En passant au supremum, on obtient :
</p>

<p>
\[\norme{\alpha \cdot f} = \abs{\alpha} \cdot \norme{f}\]
</p>
</div>
</div>
<div id="outline-container-org7543415" class="outline-4">
<h4 id="org7543415"><span class="section-number-4">5.6.2.</span> Notation</h4>
<div class="outline-text-4" id="text-5-6-2">
<p>
Lorsqu'il est nécessaire de différentier la norme au sens des applications linéaires d'autres types de normes utilisées, on note :
</p>

<p>
\[\norme{f}_\lineaire = \norme{f}\]
</p>
</div>
</div>
<div id="outline-container-org96278bc" class="outline-4">
<h4 id="org96278bc"><span class="section-number-4">5.6.3.</span> Définition alternative</h4>
<div class="outline-text-4" id="text-5-6-3">
<p>
Soit \(N \in \corps\), avec \(N \strictsuperieur 0\) et :
</p>

<p>
\[B = \{ u \in E : \norme{u} = N \}\]
</p>

<p>
Soit \(x \in E\) avec \(x \ne 0\) et :
</p>

<p>
\[\lambda = \frac{ \norme{x} }{N}\]
</p>

<p>
Définissons :
</p>

<p>
\[u = \frac{x}{\lambda}\]
</p>

<p>
On voit que :
</p>

<p>
\[\norme{u} = \norme{\unsur{\lambda} \cdot x} = \unsur{\lambda} \cdot \norme{x} = \frac{N}{ \norme{x} } \cdot \norme{x} = N\]
</p>

<p>
On a donc \(u \in B\). Le rapport des normes s'écrit :
</p>

<p>
\[\frac{ \norme{f(x)} }{ \norme{x} } = \frac{ \norme{f(x)} }{ N \cdot \lambda } = \unsur{N} \norme{ \frac{f(x)}{ \lambda } } = \unsur{ \norme{u} } \cdot \norme{ f\left( \frac{x}{ \lambda } \right) } = \frac{ \norme{f(u)} }{ \norme{u} }\]
</p>

<p>
On en conclut que :
</p>

<p>
\[\frac{ \norme{f(x)} }{ \norme{x} } = \frac{ \norme{f(u)} }{ \norme{u} } \le \sup \Big\{ \frac{ \norme{f(v)} }{ \norme{v} } : v \in B \Big\}\]
</p>

<p>
Comme ce doit être valable quelque soit \(x \ne 0\), on obtient :
</p>

<p>
\[\norme{f} \le \sup \Big\{ \frac{ \norme{f(v)} }{ \norme{v} } : v \in B \Big\}\]
</p>

<p>
en passant au supremum sur \(x\).
</p>

<p>
Choisissons à présent \(u \in B\). On a alors :
</p>

<p>
\[\frac{ \norme{f(u)} }{ \norme{u} } \le \norme{f}\]
</p>

<p>
En passant au supremum sur \(u\), on obtient :
</p>

<p>
\[\sup \Big\{ \frac{ \norme{f(v)} }{ \norme{v} } : v \in B \Big\} \le \norme{f}\]
</p>

<p>
On en conclut que les deux supremums sont égaux :
</p>

<p>
\[\sup \left\{ \frac{ \norme{f(v)} }{ \norme{v} } : v \in B \right\} = \norme{f}\]
</p>
</div>
</div>
<div id="outline-container-orge4690c9" class="outline-4">
<h4 id="orge4690c9"><span class="section-number-4">5.6.4.</span> Norme unitaire</h4>
<div class="outline-text-4" id="text-5-6-4">
<p>
Une conséquence importante du résultat ci-dessus est le cas particulier \(N = 1\). On a alors :
</p>

<p>
\[\norme{f} = \sup \left\{ \norme{f(v)} : v \in E, \ \norme{v} = 1 \right\}\]
</p>
</div>
</div>
</div>
<div id="outline-container-orgcfaf2aa" class="outline-3">
<h3 id="orgcfaf2aa"><span class="section-number-3">5.7.</span> Norme d'une composée</h3>
<div class="outline-text-3" id="text-5-7">
<p>
Soit \(f : E \mapsto F\) et \(g : F \mapsto G\) deux applications linéaires de normes finies. Si \(x \in E\) avec \(x \ne 0\) on a \(f(x) \in F\) et :
</p>

<p>
\[\norme{g \circ f(x)} \le \norme{g} \cdot \norme{f(x)} \le \norme{g} \cdot \norme{f} \cdot \norme{x}\]
</p>

<p>
En divisant par \(\norme{x} \ne 0\) :
</p>

<p>
\[\frac{ \norme{g \circ f(x)} }{ \norme{x} } \le \norme{g} \cdot \norme{f}\]
</p>

<p>
et en passant au supremum sur \(x \ne 0\), on en conclut que :
</p>

<p>
\[\norme{g \circ f} \le \norme{g} \cdot \norme{f}\]
</p>
</div>
</div>
<div id="outline-container-org6df2292" class="outline-3">
<h3 id="org6df2292"><span class="section-number-3">5.8.</span> Norme d'une puissance</h3>
<div class="outline-text-3" id="text-5-8">
<p>
On a clairement :
</p>

<p>
\[\norme{f^n} = \norme{f \circ ... \circ f} \le \norme{f}^n\]
</p>
</div>
</div>
<div id="outline-container-orgf06c7cd" class="outline-3">
<h3 id="orgf06c7cd"><span class="section-number-3">5.9.</span> Continuité</h3>
<div class="outline-text-3" id="text-5-9">
<p>
Nous allons montrer que, pour tout \(f \in \lineaire(A,B)\), on a l'équivalence entre l'hypothèse d'une norme de \(f\) finie et l'hypothèse de \(f\) continue.
</p>

<p>
Si la norme est finie, on a :
</p>

<p>
\[\norme{f(x) - f(a)} = \norme{f(x - a)} \le \norme{f} \cdot \norme{x-a}\]
</p>

<p>
qui tend bien vers \(0\) lorsque \(x\) tend vers \(a\). Inversément, si \(f\) est continue, on peut trouver un \(\delta \strictsuperieur 0\) tel que :
</p>

<p>
\[\norme{f(x) - f(0)} \le 1\]
</p>

<p>
pour tout \(x\) vérifiant \(\distance(x,0) = \norme{x} \le \delta\). Posons \(B = \{ x \in A : \norme{x} = \delta \}\). On a alors :
</p>

<p>
\[\sup_{x \in B} \frac{\norme{f(x)}}{\norme{x}} = \unsur{\delta} \sup_{x \in B} \norme{f(x) - f(0)} \le \unsur{\delta}\]
</p>

<p>
La norme est donc finie :
</p>

<p>
\[\norme{f} = \sup_{x \in B} \frac{\norme{f(x)}}{\norme{x}} \le \unsur{\delta} \strictinferieur +\infty\]
</p>
</div>
</div>
<div id="outline-container-orgd435975" class="outline-3">
<h3 id="orgd435975"><span class="section-number-3">5.10.</span> $n$-linéarité</h3>
<div class="outline-text-3" id="text-5-10">
<p>
On dit que la fonction \(f : E_1 \times ... \times E_n \mapsto F\) est $n$-linéaire si elle est linéaire par rapport à chacune des composantes de son argument, les autres composantes restant inchangées :
</p>

<p>
\[f(...,\alpha x + \beta y,...) = \alpha \cdot f(...,x,...) + \beta \cdot f(...,y,...)\]
</p>

<p>
pour tout \(\alpha,\beta \in \corps\) et \(x,y \in E\). On note \(\lineaire_n(E_1,...,E_n,F)\) l'ensemble des fonctions $n$-linéaires de \(E_1 \times ... \times E_n\) vers \(F\).
</p>
</div>
<div id="outline-container-orgbce4c4e" class="outline-4">
<h4 id="orgbce4c4e"><span class="section-number-4">5.10.1.</span> Norme</h4>
<div class="outline-text-4" id="text-5-10-1">
<p>
La norme est définie dans ce cas par :
</p>

<p>
\[\norme{f} = \sup \left\{ \frac{ \norme{f(x_1,...,x_n)} }{ \prod_{i = 1}^n \norme{x_i} } : (x_1,...,x_n) \in E_1 \times ... \times E_n \right\}\]
</p>

<p>
Si cette norme est finie, on a :
</p>

<p>
\[\norme{f(x_1,...,x_n)} \le \norme{f} \cdot \prod_{i = 1}^n \norme{x_i}\]
</p>

<p>
pour tout \((x_1,...,x_n) \in E_1 \times ... \times E_n\).
</p>
</div>
</div>
<div id="outline-container-orgb75786f" class="outline-4">
<h4 id="orgb75786f"><span class="section-number-4">5.10.2.</span> Bilinéarité</h4>
<div class="outline-text-4" id="text-5-10-2">
<p>
On dit aussi des fonctions $2$-linéaires qu'elles sont bilinéaires. La norme d'une fonction \(f : E_1 \times E_2 \mapsto F\) bilinéaire est définie par :
</p>

<p>
\[\norme{f} = \sup \left\{ \frac{ \norme{f(u,v)} }{ \norme{u} \cdot \norme{v} } : (u,v) \in E_1 \times E_2 \right\}\]
</p>

<p>
Si cette norme est finie, on a :
</p>

<p>
\[\norme{f(u,v)} \le \norme{f} \cdot \norme{u} \cdot \norme{v}\]
</p>

<p>
pour tout \((u,v) \in E_1 \times E_2\).
</p>
</div>
</div>
</div>
<div id="outline-container-org4e61e70" class="outline-3">
<h3 id="org4e61e70"><span class="section-number-3">5.11.</span> Représentation matricielle</h3>
<div class="outline-text-3" id="text-5-11">
<p>
Soit une application linéaire \(\mathcal{A} : E \to F\). Choisissons \(x \in E\) et posons :
</p>

<p>
\[y = \mathcal{A}(x)\]
</p>

<p>
Si on dispose d'une base \((e_1,...,e_n)\) de \(E\) et d'une base \((f_1,...,f_m)\) de \(F\), on a :
</p>

<div class="org-center">
<p>
\(
x = \sum_{i = 1}^n x_i \cdot e_i \)
</p>

<p>
\(
y = \sum_{i = 1}^m y_i \cdot f_i
\)
</p>
</div>

<p>
pour certains \(x_i,y_i \in \corps\). La linéarité de \(\mathcal{A}\) implique que :
</p>

<p>
\[y = \sum_{j = 1}^n \mathcal{A}(e_j) \cdot x_j\]
</p>

<p>
Si les \(a_{ij} \in \corps\) sont les coordonnées de \(\mathcal{A}(e_j)\) dans la base des \(f_i\), on a :
</p>

<p>
\[\mathcal{A}(e_j) = \sum_{i = 1}^m a_{ij} \cdot f_i\]
</p>

<p>
En substituant cette expression, on obtient :
</p>

<p>
\[y = \sum_{i = 1}^m f_i \sum_{j = 1}^n a_{ij} \cdot x_j\]
</p>

<p>
La \(i^{ème}\) coordonnée de \(y\) est donc donnée par :
</p>

<p>
\[y_i = \sum_{j = 1}^n a_{ij} \cdot x_j\]
</p>

<p>
On définit la matrice \(A \in \matrice(\corps,m,n)\) associée à \(\mathcal{A}\) en posant :
</p>

<p>
\[A = ( a_{ij} )_{i,j}\]
</p>

<p>
Nous définissons ensuite le produit d'une matrice avec le « vecteur » \(x\) équivalent de \(\corps^n\) :
</p>

<p>
\[A \cdot x = \left( \sum_j a_{ij} \cdot x_j  \right)_i\]
</p>

<p>
de telle sorte que :
</p>

<p>
\[A \cdot x = \mathcal{A}(x)\]
</p>
</div>
<div id="outline-container-org592ca29" class="outline-4">
<h4 id="org592ca29"><span class="section-number-4">5.11.1.</span> Norme</h4>
<div class="outline-text-4" id="text-5-11-1">
<p>
La norme d'une matrice est la norme de l'application linéaire associée, c'est-à-dire :
</p>

<p>
\[\norme{A}_2 = \sup \left\{ \frac{ \norme{A \cdot x} }{ \norme{x} } : x \in \corps^n, \ x \ne 0 \right\}\]
</p>

<p>
Soit :
</p>

<p>
\[M = \max_{i,j} \abs{\composante_{ij} A}\]
</p>

<p>
On a alors :
</p>

<p>
\[\norme{A \cdot x} \le M \cdot m \cdot n \cdot \max_i x_i \le M \cdot m \cdot n \cdot \norme{x}\]
</p>

<p>
ce qui montre que :
</p>

<p>
\[\norme{A} \le M \cdot m \cdot n \strictinferieur \infty\]
</p>

<p>
La norme d'une matrice finie (\(m,n \strictinferieur \infty\)) existe toujours.
</p>
</div>
</div>
<div id="outline-container-orgb4aa3eb" class="outline-4">
<h4 id="orgb4aa3eb"><span class="section-number-4">5.11.2.</span> Image</h4>
<div class="outline-text-4" id="text-5-11-2">
<p>
L'image d'une matrice est l'image de l'application linéaire associée, c'est-à-dire :
</p>

<p>
\[\image A = \{ A \cdot x : x \in \corps^n \}\]
</p>

<p>
Si \(c_i = \colonne_i A\), on a :
</p>

<p>
\[A = [ c_1 \ c_2 \ ... \ c_n ]\]
</p>

<p>
On voit que :
</p>

<p>
\[A \cdot x = \sum_i c_i \cdot x_i\]
</p>

<p>
autrement dit l'image de \(A\) est l'espace vectoriel engendré par ses colonnes :
</p>

<p>
\[\image A = \combilin{c_1,c_2,...,c_n}\]
</p>
</div>
</div>
<div id="outline-container-org12950cf" class="outline-4">
<h4 id="org12950cf"><span class="section-number-4">5.11.3.</span> Noyau</h4>
<div class="outline-text-4" id="text-5-11-3">
<p>
Le noyau d'une matrice est le noyau de l'application linéaire associée, c'est-à-dire :
</p>

<p>
\[\noyau A = \{ x \in \corps^n : A \cdot x = 0 \}\]
</p>
</div>
</div>
</div>
<div id="outline-container-org1db74c4" class="outline-3">
<h3 id="org1db74c4"><span class="section-number-3">5.12.</span> Produit matriciel</h3>
<div class="outline-text-3" id="text-5-12">
<p>
Soit à présent les matrices \(A \in \matrice(\corps,m,n)\) et \(B \in \matrice(\corps,n,p)\) données par :
</p>

<div class="org-center">
<p>
\(
A = ( a_{ij} )_{i,j} \)
</p>

<p>
\(
B = ( b_{ij} )_{i,j}
\)
</p>
</div>

<p>
où les \(a_{ij},b_{ij} \in K\). Soit les applications linéaires \(\mathcal{B} : \corps^p \to \corps^n\) et \(\mathcal{A} : \corps^n \to \corps^m\) définies par :
</p>

<div class="org-center">
<p>
\(
\mathcal{B}(x) = B \cdot x \)
</p>

<p>
\(
\mathcal{A}(y) = A \cdot y
\)
</p>
</div>

<p>
pour tout \(x \in \corps^p\), \(y \in \corps^n\). Choisissons \(z \in \corps^m\) et relions \(x,y,z\) par :
</p>

<div class="org-center">
<p>
\(
y = \mathcal{B}(x) \)
</p>

<p>
\(
z = \mathcal{A}(y) = \big( \mathcal{A} \circ \mathcal{B} \big)(x)
\)
</p>
</div>

<p>
Examinons les composantes de \(z\) en fonction de celles de \(x\) :
</p>

<p>
\[z_i = \sum_k a_{ik} \cdot y_k = \sum_k a_{ik} \sum_j b_{kj} \cdot x_j = \sum_{k,j} a_{ik} \cdot b_{kj} \cdot x_j\]
</p>

<p>
On en déduit que la composée \(\mathcal{C} = \mathcal{A} \circ \mathcal{B}\) est représentée par la matrice \(C \in \matrice(\corps,m,p)\) de composantes :
</p>

<p>
\[\composante_{ij} C = c_{ij} = \sum_k a_{ik} \cdot b_{kj}\]
</p>

<p>
Il suffit donc de définir le produit matriciel \(A \cdot B\) par :
</p>

<p>
\[A \cdot B = \left(\sum_{k=1}^n a_{ik} \cdot b_{kj}\right)_{i,j}\]
</p>

<p>
pour avoir :
</p>

<p>
\[(A \cdot B) \cdot x = \big( \mathcal{A} \circ \mathcal{B} \big)(x)\]
</p>

<p>
Le produit matriciel représente donc une composée d'applications linéaires. Pour que ce produit soit bien défini, il est nécessaire que le nombre
de colonnes \(n\) de \(A\) et le nombre de lignes de \(B\) soient identiques.
</p>

<p>
On voit également que le produit matrice - vecteur défini précédemment en est un cas particulier lorsque \(p = 1\).
</p>
</div>
<div id="outline-container-org98398e2" class="outline-4">
<h4 id="org98398e2"><span class="section-number-4">5.12.1.</span> Notation</h4>
<div class="outline-text-4" id="text-5-12-1">
<p>
En pratique, on laisse souvent tomber le ``\(\cdot\)'' et on note \(A B\)
au lieu de \(A \cdot B\) lorsqu'il est évident que \(A\) et \(B\) sont deux
matrices différentes.
</p>
</div>
</div>
<div id="outline-container-org94f41de" class="outline-4">
<h4 id="org94f41de"><span class="section-number-4">5.12.2.</span> Taille</h4>
<div class="outline-text-4" id="text-5-12-2">
<p>
Le produit d'une matrice de taille \((m,n)\) par une matrice de taille \((n,p)\) est une matrice de taille \((m,p)\).
</p>
</div>
</div>
<div id="outline-container-orgfc228fa" class="outline-4">
<h4 id="orgfc228fa"><span class="section-number-4">5.12.3.</span> Lignes et colonnes</h4>
<div class="outline-text-4" id="text-5-12-3">
<p>
Si \(x_i^T = \ligne_i(A)\) et \(y_j = \colonne_j(B)\), on voit que :
</p>

<p>
\[\composante_{ij} (A \cdot B) = x_i^T \cdot y_j\]
</p>
</div>
</div>
<div id="outline-container-org8c27b46" class="outline-4">
<h4 id="org8c27b46"><span class="section-number-4">5.12.4.</span> Associativité</h4>
<div class="outline-text-4" id="text-5-12-4">
<p>
Soit les matrices \(A \in \matrice(\corps,m,n)\), \(B \in \matrice(\corps,n,p)\) et \(C \in \matrice(\corps,p,q)\) données par :
</p>

<div class="org-center">
<p>
\(
A = ( a_{ij} )_{i,j} \)
</p>

<p>
\(
B = ( b_{ij} )_{i,j} \)
</p>

<p>
\(
C = ( c_{ij} )_{i,j}
\)
</p>
</div>

<p>
où les \(a_{ij},b_{ij},c_{ij} \in K\). La relation :
</p>

<p>
\[A \cdot (B \cdot C) = \left( \sum_{k,l} a_{ik} \cdot b_{kl} \cdot c_{lj} \right)_{i,j} = (A \cdot B) \cdot C\]
</p>

<p>
nous montre que la multiplication entre matrices est associative. On définit :
</p>

<p>
\[A \cdot B \cdot C = A \cdot (B \cdot C) = (A \cdot B) \cdot C\]
</p>
</div>
</div>
<div id="outline-container-org6c6312d" class="outline-4">
<h4 id="org6c6312d"><span class="section-number-4">5.12.5.</span> Distributivité</h4>
<div class="outline-text-4" id="text-5-12-5">
<p>
On a aussi les propriétés de distribution :
</p>

<div class="org-center">
<p>
\(
A \cdot (B + C) = A \cdot B + A \cdot C \)
</p>

<p>
\(
(B + C) \cdot D = B \cdot D + C \cdot D
\)
</p>
</div>

<p>
où \(A \in \matrice(\corps,m,n)\), \(B,C \in \matrice(\corps,n,p)\) et \(D \in \matrice(\corps,p,q)\).
</p>
</div>
</div>
<div id="outline-container-org61f009a" class="outline-4">
<h4 id="org61f009a"><span class="section-number-4">5.12.6.</span> Non commutativité</h4>
<div class="outline-text-4" id="text-5-12-6">
<p>
Par contre, on peut trouver des matrices \(A\) et \(B\) telles que :
</p>

<p>
\[A \cdot B \ne B \cdot A\]
</p>

<p>
La multiplication matricielle n'est donc en général pas commutative. D'ailleurs, pour que ces deux produits existent simultanément, il faut que \(A\) et \(B\) soient toutes deux carrées, ce qui n'est pas forcément le cas.
</p>
</div>
</div>
<div id="outline-container-org057daa2" class="outline-4">
<h4 id="org057daa2"><span class="section-number-4">5.12.7.</span> Commutateur</h4>
<div class="outline-text-4" id="text-5-12-7">
<p>
La matrice associée au commutateur :
</p>

<p>
\[[\mathcal{A},\mathcal{B}] = \mathcal{A} \circ \mathcal{B} - \mathcal{B} \circ \mathcal{A}\]
</p>

<p>
est donnée par le commutateur équivalent :
</p>

<p>
\[[A,B] = A \cdot B - B \cdot A\]
</p>
</div>
</div>
<div id="outline-container-org65fe0c8" class="outline-4">
<h4 id="org65fe0c8"><span class="section-number-4">5.12.8.</span> Transposée</h4>
<div class="outline-text-4" id="text-5-12-8">
<p>
On vérifie que :
</p>

<p>
\[(A \cdot B)^T = B^T \cdot A^T\]
</p>
</div>
</div>
</div>
<div id="outline-container-org5a4295b" class="outline-3">
<h3 id="org5a4295b"><span class="section-number-3">5.13.</span> Blocs</h3>
<div class="outline-text-3" id="text-5-13">
<p>
En utilisant l'associativité de l'addition, on peut facilement vérifier que la formule de multiplication reste valable lorsqu'on considère des blocs de matrices au lieu des éléments, à condition de respecter l'ordre de multiplication. Un exemple fréquemment utilisé :
</p>

<div class="org-center">
<p>
\(
</p>
\begin{Matrix}{cc}
A_{11} & A_{12} \\ A_{21} & A_{22}
\end{Matrix}
<p>
&sdot;
</p>
\begin{Matrix}{cc}
B_{11} & B_{12} \\ B_{21} & B_{22}
\end{Matrix}
<p>
=
</p>
\begin{Matrix}{cc}
A_{11} \cdot B_{11} +  A_{12} \cdot B_{21} & A_{11} \cdot B_{12} +  A_{12} \cdot B_{22} \)

\(
A_{21} \cdot B_{11} +  A_{22} \cdot B_{21} & A_{21} \cdot B_{12} +  A_{22} \cdot B_{22}
\end{Matrix}
<p>
\)
</p>
</div>
</div>
<div id="outline-container-org3f08943" class="outline-4">
<h4 id="org3f08943"><span class="section-number-4">5.13.1.</span> Bloc-diagonale</h4>
<div class="outline-text-4" id="text-5-13-1">
<p>
Un cas particulier important :
</p>

<div class="org-center">
<p>
\(
</p>
\begin{Matrix}{cc}
A_1 & 0 \\ 0 & A_2
\end{Matrix}
<p>
&sdot;
</p>
\begin{Matrix}{cc}
B_1 & 0 \\ 0 & B_2
\end{Matrix}
<p>
=
</p>
\begin{Matrix}{cc}
A_1 \cdot B_1 & 0 \)

\(
0 & A_2 \cdot B_2
\end{Matrix}
<p>
\)
</p>
</div>
</div>
</div>
</div>
<div id="outline-container-org6c9f76c" class="outline-3">
<h3 id="org6c9f76c"><span class="section-number-3">5.14.</span> Matrice identité</h3>
<div class="outline-text-3" id="text-5-14">
<p>
La matrice identité \(I \in \matrice(\corps,n,n)\) correspond à la fonction \(\identité\). On a donc :
</p>

<p>
\[I \cdot x = x\]
</p>

<p>
pour tout \(x \in \corps^n\). Si \((\canonique_1,...\canonique_n)\) est la base canonique de \(\corps^n\), on a donc :
</p>

<p>
\[I \cdot \canonique_i = \canonique_i\]
</p>

<p>
ce qui entraîne directement :
</p>

<p>
\[I = ( \indicatrice_{ij} )_{i,j}\]
</p>

<p>
On remarque que :
</p>

<p>
\[I = [\canonique_1 \ \hdots \ \canonique_n]\]
</p>
</div>
<div id="outline-container-org484fb7f" class="outline-4">
<h4 id="org484fb7f"><span class="section-number-4">5.14.1.</span> Neutre</h4>
<div class="outline-text-4" id="text-5-14-1">
<p>
Comme la fonction identité est neutre pour la composition, la matrice unité correspondante \(I \in \matrice(\corps,m,n)\) doit être neutre pour la multiplication avec toutes les matrices de dimensions compatibles. Soit \(A \in \matrice(\corps,m,n)\) et \(B \in \matrice(\corps,n,p)\). On vérifie que l'on a bien :
</p>

<div class="org-center">
<p>
\(
A \cdot I = A \)
</p>

<p>
\(
I \cdot B = B
\)
</p>
</div>
</div>
</div>
<div id="outline-container-org0462fc9" class="outline-4">
<h4 id="org0462fc9"><span class="section-number-4">5.14.2.</span> Notation</h4>
<div class="outline-text-4" id="text-5-14-2">
<p>
On note aussi \(I_n\) pour préciser que \(I\) est de taille \((n,n)\).
</p>
</div>
</div>
</div>
<div id="outline-container-orge58d81f" class="outline-3">
<h3 id="orge58d81f"><span class="section-number-3">5.15.</span> Inverse</h3>
<div class="outline-text-3" id="text-5-15">
<p>
Lorsqu'elle existe, la matrice inverse de \(A\), notée \(A^{-1}\), reflète l'application linéaire inverse sous-jacente. Elle est donc l'unique matrice telle que :
</p>

<p>
\[A^{-1} \cdot A = A \cdot A^{-1} = I\]
</p>
</div>
</div>
<div id="outline-container-orge561167" class="outline-3">
<h3 id="orge561167"><span class="section-number-3">5.16.</span> Inverse d'un produit</h3>
<div class="outline-text-3" id="text-5-16">
<p>
Soit \(A\) et \(B\) deux matrices inversibles. Les relations \(C \cdot (A \cdot B) = I\) et \((A \cdot B) \cdot D = I\) nous donnent :
</p>

<p>
\[C = D = B^{-1} \cdot A^{-1}\]
</p>

<p>
et donc :
</p>

<p>
\[(A \cdot B)^{-1} = B^{-1} \cdot A^{-1}\]
</p>
</div>
<div id="outline-container-org9a77de1" class="outline-4">
<h4 id="org9a77de1"><span class="section-number-4">5.16.1.</span> Inverse à gauche et à droite</h4>
<div class="outline-text-4" id="text-5-16-1">
<p>
On dit que \(L\) est un inverse à gauche de \(A\) si \(L \cdot A = I\). On dit que \(R\) est un inverse à droite de \(A\) si \(A \cdot R = I\).
</p>
</div>
</div>
</div>
<div id="outline-container-org2da251a" class="outline-3">
<h3 id="org2da251a"><span class="section-number-3">5.17.</span> Puissance</h3>
<div class="outline-text-3" id="text-5-17">
<p>
Il est possible de multiplier une matrice carrée \(A\) avec elle-même.
On peut donc définir l'exposant par :
</p>

<div class="org-center">
<p>
\(
A^0 = I \)
</p>

<p>
\(
A^k = A \cdot A^{k-1}
\)
</p>
</div>
</div>
<div id="outline-container-org91e980f" class="outline-4">
<h4 id="org91e980f"><span class="section-number-4">5.17.1.</span> Négative</h4>
<div class="outline-text-4" id="text-5-17-1">
<p>
Si l'inverse \(A^{-1}\) existe, on définit également :
</p>

<p>
\[A^{-k} = (A^{-1})^k\]
</p>
</div>
</div>
</div>
<div id="outline-container-org9a99d08" class="outline-3">
<h3 id="org9a99d08"><span class="section-number-3">5.18.</span> Polynômes matriciels</h3>
<div class="outline-text-3" id="text-5-18">
<p>
Ici, \(\corps\) n'est plus un corps mais l'anneau des matrices \(X\) de taille \((N,N)\). On dit que \(p : \matrice(\corps,N,N) \mapsto \matrice(\corps,N,N)\) est un polynôme matriciel si il existe \(a_0,...,a_n \in \corps\) tels que :
</p>

<p>
\[p(X) = \sum_{i = 0}^n a_i \cdot X^i\]
</p>

<p>
pour tout \(X \in \matrice(\corps,N,N)\).
</p>
</div>
</div>
</div>
<div id="outline-container-org252bd7c" class="outline-2">
<h2 id="org252bd7c"><span class="section-number-2">6.</span> Géométrie</h2>
<div class="outline-text-2" id="text-6">
<div id="text-table-of-contents-6" role="doc-toc">
<ul>
<li><a href="#orgea27e62">6.1. Courbe</a></li>
<li><a href="#orgd6149a4">6.2. Segment</a></li>
<li><a href="#org1da0ff0">6.3. Enveloppe convexe</a></li>
<li><a href="#org9eca091">6.4. Surface</a></li>
</ul>
</div>
</div>
<div id="outline-container-orgea27e62" class="outline-3">
<h3 id="orgea27e62"><span class="section-number-3">6.1.</span> Courbe</h3>
<div class="outline-text-3" id="text-6-1">
<p>
Une courbe sur un espace vectoriel \(E\) (par exemple \(\setR^n\)) est de la forme :
</p>

<div class="org-center">
<p>
\(
\Lambda = \{ \lambda(s) : s \in [\alpha,\beta] \}
\)
</p>
</div>

<p>
où \(\lambda : [\alpha,\beta] \mapsto E\) est une fonction continue et où \(\alpha,\beta \in \setR\) vérifient \(\alpha \le \beta\).
</p>
</div>
</div>
<div id="outline-container-orgd6149a4" class="outline-3">
<h3 id="orgd6149a4"><span class="section-number-3">6.2.</span> Segment</h3>
<div class="outline-text-3" id="text-6-2">
<p>
Les segments sont une généralisation des intervalles. Un segment de \(u \in E\) vers \(v \in E\) est un cas particulier de courbe où \(\lambda : [0,1] \subseteq \setR \mapsto E\) est une fonction linéaire définie par :
</p>

<div class="org-center">
<p>
\(
\lambda(s) = u + s \cdot (v - u)
\)
</p>
</div>

<p>
pour tout \(s \in [0,1] \subseteq \setR\). On voit que \(\lambda(0) = u\) et que \(\lambda(1) = v\). On note aussi :
</p>

<div class="org-center">
<p>
\(\relax
[u,v] = \lambda([0,1]) = \{ u + s \cdot (v - u) : s \in [0,1] \} \subseteq E
\)
</p>
</div>
</div>
<div id="outline-container-org9e83b8d" class="outline-4">
<h4 id="org9e83b8d"><span class="section-number-4">6.2.1.</span> Alternative</h4>
<div class="outline-text-4" id="text-6-2-1">
<p>
On dispose aussi d'une définition alternative. On utilise :
</p>

<div class="org-center">
<p>
\(
L = \{ (s,t) \in \setR^2 : (s,t) \ge 0 \text{ et } s + t = 1 \}
\)
</p>
</div>

<p>
et la fonction \(\sigma : L \mapsto E\) définie par :
</p>

<div class="org-center">
<p>
\(\relax
\sigma(s,t) = s \cdot u + t \cdot v
\)
</p>
</div>

<p>
On a alors \([u,v] = \sigma(L)\). On voit aussi que \(\sigma(1,0) = u\) et \(\sigma(0,1) = v\).
</p>
</div>
</div>
</div>
<div id="outline-container-org1da0ff0" class="outline-3">
<h3 id="org1da0ff0"><span class="section-number-3">6.3.</span> Enveloppe convexe</h3>
<div class="outline-text-3" id="text-6-3">
<p>
Soit \(A \subseteq E\) et la collection des segments reliant deux points quelconques de \(A\) :
</p>

<div class="org-center">
<p>
\(
\mathcal{S} = \{ [u,v] : u,v \in A \}
\)
</p>
</div>

<p>
L'enveloppe convexe de \(A\) est l'union de tous ces segments :
</p>

<div class="org-center">
<p>
\(
\convexe(A) = \bigcup \mathcal{S}
\)
</p>
</div>

<p>
Pour tout \(u,v \in A\) et \((s,t) \in \setR^2\) tels que \(s,t \ge 0\) et \(s + t = 1\), on a donc :
</p>

<div class="org-center">
<p>
\(
s \cdot u + t \cdot v \in \convexe(A)
\)
</p>
</div>
</div>
<div id="outline-container-org91f7c0f" class="outline-4">
<h4 id="org91f7c0f"><span class="section-number-4">6.3.1.</span> Inclusion</h4>
<div class="outline-text-4" id="text-6-3-1">
<p>
Il suffit de considérer le choix \((s,t) = (1,0)\) pour voir que tout \(u \in A\) appartient à \(\convexe(A)\). On a donc \(A \subseteq \convexe(A)\).
</p>
</div>
</div>
<div id="outline-container-org82b606e" class="outline-4">
<h4 id="org82b606e"><span class="section-number-4">6.3.2.</span> Ensemble convexe</h4>
<div class="outline-text-4" id="text-6-3-2">
<p>
On dit qu'un ensemble \(C \subseteq E\) est convexe si \(\convexe(C) = C\).
</p>
</div>
</div>
</div>
<div id="outline-container-org9eca091" class="outline-3">
<h3 id="org9eca091"><span class="section-number-3">6.4.</span> Surface</h3>
<div class="outline-text-3" id="text-6-4">
<p>
Une surface de \(E\) est de la forme :
</p>

<div class="org-center">
<p>
\(
\Phi = \{ \varphi(s,t) : (s,t) \in [a,b] \times [c,d] \}
\)
</p>
</div>

<p>
où \(\varphi : [a,b] \times [c,d] \mapsto E\) est une fonction continue et où \(a,b,c,d \in \setR\) vérifient \(a \le b\) et \(c \le d\).
</p>
</div>
</div>
</div>
<div id="outline-container-orgae61dd2" class="outline-2">
<h2 id="orgae61dd2"><span class="section-number-2">7.</span> Formes linéaires</h2>
<div class="outline-text-2" id="text-7">
<div id="text-table-of-contents-7" role="doc-toc">
<ul>
<li><a href="#org8d2921b">7.1. Dépendances</a></li>
<li><a href="#orgc52976a">7.2. Définition</a></li>
<li><a href="#org4a1204c">7.3. Espace dual</a></li>
<li><a href="#org58e1ee2">7.4. Notation</a></li>
<li><a href="#org86e033a">7.5. Linéarité</a></li>
<li><a href="#org906e600">7.6. Biorthonormalité</a></li>
<li><a href="#orgd30247b">7.7. Similitude</a></li>
<li><a href="#org752833c">7.8. Espace bidual</a></li>
<li><a href="#org2dad56b">7.9. Application duale</a></li>
<li><a href="#org85cacf5">7.10. Formes bilinéaires</a></li>
<li><a href="#orgc8a20ec">7.11. Formes quadratiques</a></li>
<li><a href="#orgf189787">7.12. Représentation matricielle</a></li>
</ul>
</div>

<p>
\label{chap:forme}
</p>
</div>
<div id="outline-container-org8d2921b" class="outline-3">
<h3 id="org8d2921b"><span class="section-number-3">7.1.</span> Dépendances</h3>
<div class="outline-text-3" id="text-7-1">
<ul class="org-ul">
<li>Chapitre \ref{chap:relation} : Les fonctions</li>
<li>Chapitre \ref{chap:lineaire} : Les fonctions linéaires</li>
</ul>
</div>
</div>
<div id="outline-container-orgc52976a" class="outline-3">
<h3 id="orgc52976a"><span class="section-number-3">7.2.</span> Définition</h3>
<div class="outline-text-3" id="text-7-2">
<p>
Soit un espace vectoriel \(E\) sur \(\corps\). Une forme linéaire est une fonction linéaire continue
\(\varphi : E \mapsto \corps\).
</p>
</div>
</div>
<div id="outline-container-org4a1204c" class="outline-3">
<h3 id="org4a1204c"><span class="section-number-3">7.3.</span> Espace dual</h3>
<div class="outline-text-3" id="text-7-3">
<p>
L'espace dual \(E^\dual\) de \(E\) est l'ensemble des formes linéaires sur \(E\) , autrement dit
l'ensemble des fonctions linéaires continues de \(E\) vers \(\corps\) :
</p>

<p>
\[E^\dual = \{ \varphi \in \lineaire(E,\corps) : \norme{\varphi}_\lineaire \strictinferieur +\infty \}\]
</p>

<p>
Il s'agit d'un espace vectoriel pour les opérations d'addition et de multiplication mixte définies sur les fonctions.
</p>
</div>
</div>
<div id="outline-container-org58e1ee2" class="outline-3">
<h3 id="org58e1ee2"><span class="section-number-3">7.4.</span> Notation</h3>
<div class="outline-text-3" id="text-7-4">
<p>
Pour toute forme \(\varphi \in E^\dual\) et tout vecteur \(v \in E\), on note :
</p>

<p>
\[\forme{\varphi}{v} = \varphi(v)\]
</p>

<p>
ce qui définit implicitement la fonction \(\forme{}{} : E^\dual \times E \mapsto \corps\).
</p>
</div>
</div>
<div id="outline-container-org86e033a" class="outline-3">
<h3 id="org86e033a"><span class="section-number-3">7.5.</span> Linéarité</h3>
<div class="outline-text-3" id="text-7-5">
<p>
Soit \(\varphi,\psi \in E^\dual\), \(u,v \in E\) et \(\alpha,\beta \in S\). Comme \(\varphi\) est linéaire, on a :
</p>

<p>
\[\forme{\varphi}{\alpha \cdot u + \beta \cdot v}  = \alpha \cdot \forme{\varphi}{u} + \beta \cdot \forme{\varphi}{v}\]
</p>

<p>
Symétriquement, la définition des opérations sur les fonctions nous donne également :
</p>

<p>
\[\forme{\alpha \cdot \varphi + \beta \cdot \psi}{u}  = \alpha \cdot \forme{\varphi}{u} + \beta \cdot \forme{\psi}{u}\]
</p>

<p>
L'application \(\forme{}{}\) est donc bilinéaire.
</p>
</div>
</div>
<div id="outline-container-org906e600" class="outline-3">
<h3 id="org906e600"><span class="section-number-3">7.6.</span> Biorthonormalité</h3>
<div class="outline-text-3" id="text-7-6">
<p>
On dit que les suites \((\Phi_1,...,\Phi_m)\) de \(E^\dual\) et
\((e_1,...,e_n)\) de \(E\) sont biorthonormées si :
</p>

<p>
\[\forme{\Phi_i}{e_j} = \indicatrice_{ij}\]
</p>

<p>
pour tout \((i,j) \in \setZ(0,m) \times \setZ(0,n)\). De telles suites permettent d'évaluer facilement les coefficients des développements en série du type :
</p>

<p>
\[\varphi = \sum_{i = 1}^m \alpha_i \cdot \Phi_i\]
</p>

<p>
où \(\alpha_1,...,\alpha_m \in S\). En effet, il suffit d'évaluer :
</p>

<p>
\[\varphi(e_j) = \forme{\varphi}{e_j} = \sum_{i = 1}^m \alpha_i \cdot \forme{\Phi_i}{e_j} = \sum_{i = 1}^m \alpha_i \cdot \indicatrice_{ij} = \alpha_j\]
</p>

<p>
pour obtenir les valeurs des \(\alpha_j\).
</p>

<p>
Réciproquement, si :
</p>

<p>
\[u = \sum_{i = 1}^n \beta_i \cdot e_i\]
</p>

<p>
avec \(\beta_1,...,\beta_n \in S\), on a :
</p>

<p>
\[\Phi_j(u) = \forme{\Phi_j}{u} = \sum_{i = 1}^n \beta_i \cdot \forme{\Phi_j}{e_i} = \sum_{i = 1}^n \beta_i \cdot \indicatrice_{ij} = \beta_j\]
</p>

<p>
ce qui nous donne les valeurs des \(\beta_j\).
</p>

<p>
Forts de ces résultats, il est aisé d'évaluer :
</p>

<p>
\[\forme{\varphi}{u} = \sum_{i,j} \alpha_i \cdot \forme{\Phi_i}{u_j} \cdot \beta_j = \sum_{i,j} \alpha_i \cdot \indicatrice_{ij} \cdot \beta_j = \sum_i \alpha_i \cdot \beta_i\]
</p>

<p>
On a donc en définitive :
</p>

<p>
\[\forme{\varphi}{u} = \sum_i \forme{\varphi}{e_i} \cdot \forme{\Phi_i}{u}\]
</p>
</div>
</div>
<div id="outline-container-orgd30247b" class="outline-3">
<h3 id="orgd30247b"><span class="section-number-3">7.7.</span> Similitude</h3>
<div class="outline-text-3" id="text-7-7">
<p>
On dit que deux fonctions \(u,v \in E\) sont identique au sens des distributions si :
</p>

<p>
\[\forme{\varphi}{u} = \forme{\varphi}{v}\]
</p>

<p>
pour tout \(\varphi \in E^\dual\).
</p>

<p>
Symétriquement, les deux formes \(\varphi,\psi \in E^\dual\) sont identiques par définition si et seulement si :
</p>

<p>
\[\forme{\varphi}{u} = \forme{\psi}{u}\]
</p>

<p>
pour tout \(u \in E\).
</p>
</div>
</div>
<div id="outline-container-org752833c" class="outline-3">
<h3 id="org752833c"><span class="section-number-3">7.8.</span> Espace bidual</h3>
<div class="outline-text-3" id="text-7-8">
<p>
On définit l'espace bidual de \(E\), noté \(E^{\dual \dual}\), par :
</p>

<p>
\[E^{\dual \dual} = (E^\dual)^\dual\]
</p>

<p>
On associe à chaque élément \(u \in E\) un élément \(\hat{u} \in E^{\dual \dual}\) par la condition :
</p>

<p>
\[\hat{u}(\varphi) = \varphi(u)\]
</p>

<p>
qui doit être vérifiée pour tout \(\varphi \in E^\dual\). On a donc :
</p>

<p>
\[\forme{\hat{u}}{\varphi} = \forme{\varphi}{u}\]
</p>
</div>
</div>
<div id="outline-container-org2dad56b" class="outline-3">
<h3 id="org2dad56b"><span class="section-number-3">7.9.</span> Application duale</h3>
<div class="outline-text-3" id="text-7-9">
<p>
Soit les espaces vectoriels \(E\) et \(F\) sur \(\corps\) et une fonction \(A : E \mapsto F\). Le dual de \(A\) au sens des formes, s'il existe, est l'unique fonction \(A^\dual : F^\dual \mapsto E^\dual\) telle que :
</p>

<p>
\[\forme{ A^\dual(\varphi) }{u} = \forme{\varphi}{ A(u) }\]
</p>

<p>
pour tout \(u \in E\) et \(\varphi \in F^\dual\).
</p>
</div>
</div>
<div id="outline-container-org85cacf5" class="outline-3">
<h3 id="org85cacf5"><span class="section-number-3">7.10.</span> Formes bilinéaires</h3>
<div class="outline-text-3" id="text-7-10">
<p>
Soit les espaces vectoriels \(E\) et \(F\) sur \(\corps\). Une forme bilinéaire est une fonction bilinéaire continue \(\vartheta : F \times E \mapsto \corps\). On utilise une notation analogue à celle des formes :
</p>

<p>
\[\biforme{x}{\vartheta}{u} = \vartheta(x,u)\]
</p>

<p>
pour tout \(x \in F\) et \(u \in E\). On voit que :
</p>

<div class="org-center">
<p>
\(
\biforme{\alpha \cdot x + \beta \cdot y}{\vartheta}{u} = \alpha \cdot \biforme{x}{\vartheta}{u} + \beta \cdot \biforme{y}{\vartheta}{u} \)
</p>

<p>
\(
\biforme{x}{\vartheta}{\alpha \cdot u + \beta \cdot v} = \alpha \cdot \biforme{x}{\vartheta}{u} + \beta \cdot \biforme{x}{\vartheta}{v}
\)
</p>
</div>

<p>
pour tout \(\alpha,\beta \in \corps\), \(u,v \in E\) et \(x,y \in F\).
</p>
</div>
</div>
<div id="outline-container-orgc8a20ec" class="outline-3">
<h3 id="orgc8a20ec"><span class="section-number-3">7.11.</span> Formes quadratiques</h3>
<div class="outline-text-3" id="text-7-11">
<p>
Soit la forme bilinéaire $ &thetasym; : E &times; E \mapsto \corps$. Une forme quadratique \(\mathcal{Q} : E \mapsto \corps\) est une fonction de la forme :
</p>

<p>
\[\mathcal{Q}(x) = \biforme{x}{\vartheta}{x}\]
</p>
</div>
</div>
<div id="outline-container-orgf189787" class="outline-3">
<h3 id="orgf189787"><span class="section-number-3">7.12.</span> Représentation matricielle</h3>
<div class="outline-text-3" id="text-7-12">
<p>
On peut représenter toute forme linéaire \(\varphi \in \lineaire(\corps^n,\corps)\) par un vecteur matriciel \(\hat{\varphi} \in \corps^n\). Etant donné la base canonique \((e_1,...,e_n)\) de \(\corps^n\), il suffit de poser :
</p>

<p>
\[\hat{\varphi}_i = \forme{\varphi}{e_i}\]
</p>

<p>
pour avoir :
</p>

<p>
\[\forme{\varphi}{u} = \hat{\varphi}^T \cdot u\]
</p>

<p>
pour tout \(u \in \corps^n\).
</p>
</div>
<div id="outline-container-org4bfc0f4" class="outline-4">
<h4 id="org4bfc0f4"><span class="section-number-4">7.12.1.</span> Formes bilinéaires</h4>
<div class="outline-text-4" id="text-7-12-1">
<p>
On peut représenter toute forme bilinéaire \(\vartheta \in \lineaire(\corps^m \times \corps^n,\corps)\) par une matrice \(\Theta \in \matrice(K,m,n)\). Etant donné les bases canoniques \((f_1,...,f_m)\) de \(\corps^m\) et \((e_1,...,e_n)\) de \(\corps^n\), il suffit de poser :
</p>

<p>
\[\composante_{ij} \Theta = \biforme{f_i}{\vartheta}{e_j}\]
</p>

<p>
pour avoir :
</p>

<p>
\[\biforme{v}{\vartheta}{u} = v^T \cdot \Theta \cdot u\]
</p>

<p>
pour tout \(u \in \corps^n\) et tout \(v \in \corps^m\).
</p>
</div>
</div>
</div>
</div>
<div id="outline-container-orgbfee45f" class="outline-2">
<h2 id="orgbfee45f"><span class="section-number-2">8.</span> Produit scalaire</h2>
<div class="outline-text-2" id="text-8">
<div id="text-table-of-contents-8" role="doc-toc">
<ul>
<li><a href="#org420ca09">8.1. Dépendances</a></li>
<li><a href="#org6865c5b">8.2. Introduction</a></li>
<li><a href="#orgfe72bfd">8.3. Produit scalaire réel</a></li>
<li><a href="#orge250629">8.4. Produit scalaire complexe</a></li>
<li><a href="#org26c7a03">8.5. Espace orthogonal</a></li>
<li><a href="#org4fd3e59">8.6. Egalité</a></li>
<li><a href="#orge32c692">8.7. Base orthonormée</a></li>
<li><a href="#org134be39">8.8. Produit scalaire et coordonnées</a></li>
<li><a href="#orgf15359c">8.9. Application définie positive</a></li>
<li><a href="#org9e6763e">8.10. Produit scalaire sur \(\setR^n\)</a></li>
<li><a href="#org289598f">8.11. Produit scalaire sur \(\setC^n\)</a></li>
<li><a href="#orgf1428b7">8.12. Base orthonormée sur \(\corps^n\)</a></li>
<li><a href="#org13897d4">8.13. Représentation matricielle</a></li>
<li><a href="#orgfd8db15">8.14. Application linéaire</a></li>
<li><a href="#orgfec0f87">8.15. Matrice de produit scalaire</a></li>
<li><a href="#org2419c34">8.16. Bases de vecteurs matriciels</a></li>
<li><a href="#org5737dbd">8.17. Noyau</a></li>
</ul>
</div>

<p>
\label{chap:ps}
</p>
</div>
<div id="outline-container-org420ca09" class="outline-3">
<h3 id="org420ca09"><span class="section-number-3">8.1.</span> Dépendances</h3>
<div class="outline-text-3" id="text-8-1">
<ul class="org-ul">
<li>Chapitre \ref{chap:vecteur} : Les espaces vectoriels</li>
<li>Chapitre \ref{chap:norme} : Les normes</li>
</ul>
</div>
</div>
<div id="outline-container-org6865c5b" class="outline-3">
<h3 id="org6865c5b"><span class="section-number-3">8.2.</span> Introduction</h3>
<div class="outline-text-3" id="text-8-2">
<p>
Soit \(E\) un espace vectoriel sur \(\corps\) et une famille de fonctions linéaires \(\phi_u \in E^\dual\) où \(u \in E\) est un paramètre vectoriel. Nous pouvons écrire :
</p>

<p>
\[\phi_u(v) = \forme{\phi_u}{v}\]
</p>

<p>
pour tout \(u,v \in E\). Cette expression introduit implicitement le produit dérivé \(\scalaire{}{} : E \times E \mapsto \corps\) défini par :
</p>

<p>
\[\scalaire{u}{v} = \forme{\phi_u}{v}\]
</p>

<p>
pour tout \(u,v \in E\). Ce produit hérite bien entendu la linéarité à droite de la forme associée :
</p>

<p>
\[\scalaire{u}{\alpha \cdot v + \beta \cdot w}  = \alpha \cdot \scalaire{u}{v} + \beta \cdot \scalaire{u}{w}\]
</p>

<p>
pour tout \(u,v,w \in E\) et pour tout \(\alpha,\beta \in \corps\).
</p>

<p>
Les produits scalaires sont des cas particuliers de ce type de produit.
</p>
</div>
<div id="outline-container-org5702246" class="outline-4">
<h4 id="org5702246"><span class="section-number-4">8.2.1.</span> Notation</h4>
<div class="outline-text-4" id="text-8-2-1">
<p>
On note aussi \(u \cdot v = \scalaire{u}{v}\).
</p>
</div>
</div>
</div>
<div id="outline-container-orgfe72bfd" class="outline-3">
<h3 id="orgfe72bfd"><span class="section-number-3">8.3.</span> Produit scalaire réel</h3>
<div class="outline-text-3" id="text-8-3">
<p>
Considérons le cas particulier où \(\corps = \setR\). et un produit linéaire à droite \(\scalaire{}{} : E \times E \mapsto \setR\). Nous voudrions en plus que la valeur de \(\scalaire{u}{u}\) en chaque \(u \in E\) puisse représenter la norme de \(u\). Nous imposons donc la positivité :
</p>

<p>
\[\scalaire{u}{u} \ge 0\]
</p>

<p>
Pour compléter le caractère strictement défini positif, on impose également que le seul élément \(u \in E\) vérifiant :
</p>

<p>
\[\scalaire{u}{u} = 0\]
</p>

<p>
soit le vecteur nul \(u = 0\). Ce qui revient à dire que :
</p>

<p>
\[\scalaire{u}{u} > 0\]
</p>

<p>
pour tout \(u \in E \setminus \{ 0 \}\).
</p>

<p>
Si on peut également interchanger n'importe quels \(u,v \in E\) sans changer le résultat :
</p>

<p>
\[\scalaire{u}{v} = \scalaire{v}{u}\]
</p>

<p>
on dit que \(\scalaire{}{}\) est un produit scalaire réel sur \(E\).
</p>

<p>
Nous déduisons directement de la linéarité à droite et de la symétrie que :
</p>

<p>
\[\scalaire{\alpha \cdot u + \beta \cdot v}{w} = \alpha \cdot \scalaire{u}{w} + \beta \cdot \scalaire{v}{w}\]
</p>

<p>
pour tout \(\alpha,\beta \in \setR\) et \(u,v,w \in E\). Le produit scalaire réel est bilinéaire.
</p>
</div>
</div>
<div id="outline-container-orge250629" class="outline-3">
<h3 id="orge250629"><span class="section-number-3">8.4.</span> Produit scalaire complexe</h3>
<div class="outline-text-3" id="text-8-4">
<p>
Examinons à présent le cas \(\corps = \setC\). On demande qu'un produit scalaire \(\scalaire{}{} : E \times E \mapsto \setC\) soit strictement défini positif. Pour cela, les valeurs de \(\scalaire{u}{u}\) doivent être réelles et positives :
</p>

<div class="org-center">
<p>
\(
\scalaire{u}{u} \in \setR \)
</p>

<p>
\(
\scalaire{u}{u} \ge 0
\)
</p>
</div>

<p>
pour tout \(u \in E\). Ensuite, il faut également que le seul élément \(u \in E\) vérifiant :
</p>

<p>
\[\scalaire{u}{u} = 0\]
</p>

<p>
soit le vecteur nul \(u = 0\).
</p>

<p>
Le caractère réel de \(\scalaire{u}{u}\) implique que :
</p>

<p>
\[\scalaire{u}{u} = \conjaccent{\scalaire{u}{u}}\]
</p>

<p>
où la barre supérieure désigne comme d'habitude le complexe conjugué. Cette constatation nous mène à une variante de la symétrie. On impose :
</p>

<p>
\[\scalaire{u}{v} = \conjaccent{\scalaire{v}{u}}\]
</p>

<p>
pour tout \(u,v \in E\). On dit que le produit scalaire complexe est hermitien.
</p>

<p>
La linéarité à droite s'exprime simplement par :
</p>

<p>
\[\scalaire{u}{\alpha \cdot v + \beta \cdot w}  = \alpha \cdot \scalaire{u}{v} + \beta \cdot \scalaire{u}{w}\]
</p>

<p>
pour tout \(u,v,w \in E\) et pour tout \(\alpha,\beta \in \setC\). On déduit de la linéarité et du caractère hermitien du produit scalaire complexe que :
</p>

\begin{align}
\scalaire{\alpha \cdot u + \beta \cdot v}{w}  &= \conjaccent{\scalaire{w}{\alpha \cdot u + \beta \cdot v}} \)

\(
&= \conjaccent{\alpha} \cdot \conjaccent{\scalaire{w}{u}} + \conjaccent{\beta} \cdot \conjaccent{\scalaire{w}{v}}
\end{align}

<p>
et finalement :
</p>

<p>
\[\scalaire{\alpha \cdot u + \beta \cdot v}{w} = \conjaccent{\alpha} \cdot \scalaire{u}{w} + \conjaccent{\beta} \cdot \scalaire{v}{w}\]
</p>

<p>
On dit que le produit scalaire est antilinéaire à gauche.
</p>
</div>
<div id="outline-container-org2a986d2" class="outline-4">
<h4 id="org2a986d2"><span class="section-number-4">8.4.1.</span> Corollaire</h4>
<div class="outline-text-4" id="text-8-4-1">
<p>
En particulier, si \(u,v,w,x\) sont des vecteurs de \(E\) et
si \(\alpha = \scalaire{u}{v} \in \setC\), on a :
</p>

<div class="org-center">
<p>
\(
\scalaire{w}{\alpha \cdot x} = \scalaire{w}{ \scalaire{u}{v} \cdot x} = \scalaire{u}{v} \cdot \scalaire{w}{x} \)
</p>

<p>
\(
\scalaire{\alpha \cdot w}{x} = \scalaire{ \scalaire{u}{v} \cdot w}{x} = \scalaire{v}{u} \cdot \scalaire{w}{x}
\)
</p>
</div>
</div>
</div>
<div id="outline-container-orgbaee91a" class="outline-4">
<h4 id="orgbaee91a"><span class="section-number-4">8.4.2.</span> Cas particulier</h4>
<div class="outline-text-4" id="text-8-4-2">
<p>
Comme \(\conjaccent{x} = x\) pour tout \(x \in \setR \subseteq \setC\), on peut considérer le produit scalaire réel comme un cas particulier de produit scalaire complexe.
</p>
</div>
</div>
</div>
<div id="outline-container-org26c7a03" class="outline-3">
<h3 id="org26c7a03"><span class="section-number-3">8.5.</span> Espace orthogonal</h3>
<div class="outline-text-3" id="text-8-5">
</div>
<div id="outline-container-org49712ce" class="outline-4">
<h4 id="org49712ce"><span class="section-number-4">8.5.1.</span> A un vecteur</h4>
<div class="outline-text-4" id="text-8-5-1">
<p>
Soit \(x \in H\). On définit l'ensemble \(x^\orthogonal\) par :
</p>

<p>
\[x^\orthogonal = \{ z \in E : \scalaire{x}{z} = 0 \}\]
</p>

<p>
On dit des vecteurs de \(x^\orthogonal\) qu'ils sont orthogonaux à \(x\).
</p>
</div>
</div>
<div id="outline-container-org2400c3b" class="outline-4">
<h4 id="org2400c3b"><span class="section-number-4">8.5.2.</span> A un ensemble</h4>
<div class="outline-text-4" id="text-8-5-2">
<p>
Pour tout sous-ensemble \(V \subseteq E\), l'ensemble orthogonal à \(V\) est l'ensemble des vecteurs qui sont orthogonaux à tous les éléments de \(V\) :
</p>

<p>
\[V^\orthogonal = \bigcap_{x \in V} x^\orthogonal\]
</p>

<p>
Pour tout \(z \in V^\orthogonal\), on a donc \(\scalaire{x}{z} = 0\) quel que soit \(x \in V\).
</p>

<p>
Nous allons vérifier que \(V^\orthogonal\) est un sous-espace vectoriel. Soit \(z \in V\). Comme \(\scalaire{z}{0} = 0\), on a \(0 \in V^\orthogonal\). Soit \(x,y \in V^\orthogonal\), \(\alpha,\beta \in \corps\). On a :
</p>

<p>
\[\scalaire{z}{\alpha \cdot x + \beta \cdot y} = \alpha \cdot \scalaire{z}{x} + \beta \cdot \scalaire{z}{y} = 0 + 0 = 0\]
</p>

<p>
ce qui montre que \(\alpha \cdot x + \beta \cdot y \in V^\orthogonal\).
</p>
</div>
</div>
</div>
<div id="outline-container-org4fd3e59" class="outline-3">
<h3 id="org4fd3e59"><span class="section-number-3">8.6.</span> Egalité</h3>
<div class="outline-text-3" id="text-8-6">
<p>
Si \(u,v \in E\) sont tels que :
</p>

<p>
\[\scalaire{u}{w} = \scalaire{v}{w}\]
</p>

<p>
pour tout \(w \in E\), on a :
</p>

<p>
\[\scalaire{u - v}{w} = 0\]
</p>

<p>
Le choix \(w = u - v \in E\) nous donne alors :
</p>

<p>
\[\scalaire{u - v}{u - v} = 0\]
</p>

<p>
ce qui implique \(u - v = 0\) et donc \(u = v\).
</p>
</div>
</div>
<div id="outline-container-orge32c692" class="outline-3">
<h3 id="orge32c692"><span class="section-number-3">8.7.</span> Base orthonormée</h3>
<div class="outline-text-3" id="text-8-7">
<p>
Une base \((e_1,...,e_n)\) de \(E\) est dite orthonormée si le produit scalaire de deux vecteurs \(e_i \ne e_j\) s'annule, tandis que le produit scalaire d'un \(e_i\) avec lui-même donne l'unité :
</p>

<p>
\[\scalaire{e_i}{e_j} = \indicatrice_{ij}\]
</p>
</div>
<div id="outline-container-org93a3cac" class="outline-4">
<h4 id="org93a3cac"><span class="section-number-4">8.7.1.</span> Coordonnées</h4>
<div class="outline-text-4" id="text-8-7-1">
<p>
Soit \(u \in E\) de coordonnée \(u_i \in \corps\) :
</p>

<p>
\[u = \sum_{i = 1}^n u_i \cdot e_i\]
</p>

<p>
En effectuant le produit scalaire de \(u\) avec \(e_k\), on arrive à  :
</p>

<div class="org-center">
<p>
\(
\scalaire{e_k}{u} = \sum_{i = 1}^n u_i \cdot \scalaire{e_k}{e_i} \)
</p>

<p>
\(
\scalaire{e_k}{u} = \sum_{i = 1}^n u_i \cdot \indicatrice_{ik}
\)
</p>
</div>

<p>
Tous les termes de cette dernière somme s'annulent sauf lorsque \(i = k\), et on a :
</p>

<p>
\[\scalaire{e_k}{u} = u_k\]
</p>

<p>
On peut donc écrire :
</p>

<p>
\[y = \sum_{i = 1}^n \scalaire{e_i}{u} \cdot e_i\]
</p>
</div>
</div>
<div id="outline-container-org87539a2" class="outline-4">
<h4 id="org87539a2"><span class="section-number-4">8.7.2.</span> Indépendance linéaire</h4>
<div class="outline-text-4" id="text-8-7-2">
<p>
On peut voir que si une suite de vecteurs \(e_i\) est orthonormée,
(ils ne forment pas forcément une base) ils sont toujours linéairement
indépendant. En effet si les scalaires \(a_i\), sont tels que :
</p>

<p>
\[\sum_{i=1}^n a_i \cdot e_i = 0\]
</p>

<p>
on a alors :
</p>

<p>
\[a_i = \scalaire{e_i}{0} = 0\]
</p>
</div>
</div>
</div>
<div id="outline-container-org134be39" class="outline-3">
<h3 id="org134be39"><span class="section-number-3">8.8.</span> Produit scalaire et coordonnées</h3>
<div class="outline-text-3" id="text-8-8">
<p>
Soit \((e_1,...,e_n)\) une base de \(E\) et \(u,v \in E\). On a :
</p>

<div class="org-center">
<p>
\(
u = \sum_i u_i \cdot e_i \)
</p>

<p>
\(
v = \sum_i v_i \cdot e_i
\)
</p>
</div>

<p>
pour certains \(u_i,v_i \in \corps\). Posons :
</p>

<p>
\[g_{ij} = \scalaire{e_i}{e_j}\]
</p>

<p>
où \(\scalaire{}{} : E \times E \mapsto \setC\) est un produit scalaire complexe. Nous pouvons faire sortir les sommes en utilisant les propriétés du produit scalaire, ce qui nous donne :
</p>

<p>
\[\scalaire{u}{v} = \sum_{i,j} \conjaccent{u}_i \cdot g_{ij} \cdot v_j\]
</p>
</div>
<div id="outline-container-org84bf432" class="outline-4">
<h4 id="org84bf432"><span class="section-number-4">8.8.1.</span> Réel</h4>
<div class="outline-text-4" id="text-8-8-1">
<p>
Dans les cas d'un produit scalaire réel, on a \(\conjaccent{u}_i = u_i\) et l'expression devient :
</p>

<p>
\[\scalaire{u}{v} = \sum_{i,j} u_i \cdot g_{ij} \cdot v_j\]
</p>
</div>
</div>
<div id="outline-container-org02651b1" class="outline-4">
<h4 id="org02651b1"><span class="section-number-4">8.8.2.</span> Base orthonormée</h4>
<div class="outline-text-4" id="text-8-8-2">
<p>
Si la base \((e_1,...,e_n)\) est orthonormée, l'expression du produit scalaire se simplifie en :
</p>

<p>
\[\scalaire{u}{v} = \sum_i \conjaccent{u}_i \cdot v_i\]
</p>
</div>
</div>
</div>
<div id="outline-container-orgf15359c" class="outline-3">
<h3 id="orgf15359c"><span class="section-number-3">8.9.</span> Application définie positive</h3>
<div class="outline-text-3" id="text-8-9">
<p>
Soit une application linéaire \(A : E \mapsto E\). Si le produit scalaire de \(u\) avec \(A(u)\) est un réel positif :
</p>

<p>
\[\scalaire{u}{A(u)} = \scalaire{A(u)}{u} \ge 0\]
</p>

<p>
pour tout \(u \in E\), on dit que \(A\) est définie positive.
</p>
</div>
</div>
<div id="outline-container-org9e6763e" class="outline-3">
<h3 id="org9e6763e"><span class="section-number-3">8.10.</span> Produit scalaire sur \(\setR^n\)</h3>
<div class="outline-text-3" id="text-8-10">
<p>
Soit \(x,y \in \setR^n\) tels que :
</p>

<div class="org-center">
<p>
\(
x = (x_1,x_2,...,x_n) \)
</p>

<p>
\(
y = (y_1,y_2,...,y_n)
\)
</p>
</div>

<p>
pour certains \(x_i,y_i \in \setR\).
</p>

<p>
Le produit scalaire usuel sur \(\setR^n\) est défini par :
</p>

<p>
\[\scalaire{x}{y} = \sum_{i = 1}^n x_i y_i\]
</p>
</div>
</div>
<div id="outline-container-org289598f" class="outline-3">
<h3 id="org289598f"><span class="section-number-3">8.11.</span> Produit scalaire sur \(\setC^n\)</h3>
<div class="outline-text-3" id="text-8-11">
<p>
Soit \(x,y \in \setC^n\) tels que :
</p>

<div class="org-center">
<p>
\(
x = (x_1,x_2,...,x_n) \)
</p>

<p>
\(
y = (y_1,y_2,...,y_n)
\)
</p>
</div>

<p>
pour certains \(x_i,y_i \in \setC\).
</p>

<p>
Le produit scalaire usuel est défini par :
</p>

<p>
\[\scalaire{x}{y} = \sum_{i=1}^n \conjaccent{x}_i y_i\]
</p>
</div>
</div>
<div id="outline-container-orgf1428b7" class="outline-3">
<h3 id="orgf1428b7"><span class="section-number-3">8.12.</span> Base orthonormée sur \(\corps^n\)</h3>
<div class="outline-text-3" id="text-8-12">
<p>
Soit \(\corps \in \{ \setR , \setC \}\). Il est clair que la base canonique de \(\corps^n\) :
</p>

<p>
\[e_i = ( \indicatrice_{ij} )_{i,j}\]
</p>

<p>
vérifie :
</p>

<p>
\[\scalaire{e_i}{e_j} = \indicatrice_{ij}\]
</p>

<p>
pour le produit scalaire usuel sur \(\corps^n\). La suite \((e_1,...,e_n)\) forme une base orthonormée.
</p>
</div>
</div>
<div id="outline-container-org13897d4" class="outline-3">
<h3 id="org13897d4"><span class="section-number-3">8.13.</span> Représentation matricielle</h3>
<div class="outline-text-3" id="text-8-13">
<p>
Soit \(x = (x_1,..,x_n) , y = (y_1,...,y_n) \in \setC^n\). On définit les vecteurs colonne associé :
</p>

<div class="org-center">
<p>
\(
x =
</p>
\begin{Matrix}{c}
x_1 \)

\(
x_2 \)

\(
\vdots \)

\(
x_n
\end{Matrix}
<p>
\qquad \qquad
y =
</p>
\begin{Matrix}{c}
y_1 \)

\(
y_2 \)

\(
\vdots \)

\(
y_n
\end{Matrix}
<p>
\)
</p>
</div>

<p>
L'équivalence entre \(\setC^n\) et \(\matrice(\setC,n,1)\) nous amène à :
</p>

<p>
\[\scalaire{x}{y} = \sum_i \conjaccent{x}_i \cdot y_i\]
</p>

<p>
Le membre de droite n'est rien d'autre que le produit « matriciel » \(\conjaccent{x^T} \cdot y\) et on a donc :
</p>

<p>
\[\scalaire{x}{y} = \conjaccent{x^T} \cdot y\]
</p>

<p>
On vérifie que la base \((e_1,...,e_n)\) est orthonormée pour ce produit scalaire :
</p>

<p>
\[e_i^T \cdot e_j = \indicatrice_{ij}\]
</p>
</div>
</div>
<div id="outline-container-orgfd8db15" class="outline-3">
<h3 id="orgfd8db15"><span class="section-number-3">8.14.</span> Application linéaire</h3>
<div class="outline-text-3" id="text-8-14">
<p>
Soit les espaces vectoriels \(E,F\) sur \(\corps\) et une application linéaire \(\mathcal{A} : E \mapsto F\). On prend une base \((e_1,..,e_n)\) de \(E\) et une base orthonormée \((f_1,...,f_m)\) de \(F\). Comme les composantes de la matrice associée \(A\) sont les coordonnées de \(\mathcal{A}(e_j)\) dans la base des \(f_i\), on a :
</p>

<p>
\[\composante_{ij} A = \scalaire{f_i}{\mathcal{A}(e_j)}\]
</p>
</div>
</div>
<div id="outline-container-orgfec0f87" class="outline-3">
<h3 id="orgfec0f87"><span class="section-number-3">8.15.</span> Matrice de produit scalaire</h3>
<div class="outline-text-3" id="text-8-15">
<p>
Soit un espace vectoriel \(E\) sur \(\corps\) et un produit scalaire \(\scalaire{}{}\) quelconque défini sur \(E\). Soit \((e_1,...,e_n)\) une base quelconque de \(E\) (nous ne supposons pas qu'elle soit orthonormée). Si \(\hat{x},\hat{y} \in E\), on a :
</p>

<div class="org-center">
<p>
\(
\hat{x} = \sum_i x_i \cdot e_i \)
</p>

<p>
\(
\hat{y} = \sum_i y_i \cdot e_i
\)
</p>
</div>

<p>
pour certains \(x_i,y_i \in S\). Or, nous avons vu que :
</p>

<p>
\[\scalaire{\hat{x}}{\hat{y}} = \sum_{i,j} \conjaccent{x}_i \scalaire{e_i}{e_j} y_j\]
</p>

<p>
Si nous définissons la matrice des produits scalaires \(A \in \matrice(\corps,m,n)\) par :
</p>

<p>
\[\composante_{ij} A = \scalaire{e_i}{e_j}\]
</p>

<p>
nous pouvons réécrire le produit scalaire sous la forme :
</p>

<p>
\[\scalaire{\hat{x}}{\hat{y}} = \conjaccent{x^T} \cdot A \cdot y\]
</p>

<p>
où \(x,y\) sont les vecteurs colonne associés à \(\hat{x},\hat{y}\) :
</p>

<div class="org-center">
<p>
\(
x = [x_1 \ x_2 \ ... \ x_n]^T \)
</p>

<p>
\(
y = [y_1 \ y_2 \ ... \ y_n]^T
\)
</p>
</div>

<p>
Cette matrice possède d'importantes propriétés issues du produit scalaire. On a \(\conjaccent{x^T} \cdot A \cdot x > 0\) pour tout \(x \ne 0\). On dit que \(A\) est une matrice définie positive. Le caractère hermitien du produit scalaire nous donne aussi \(\conjaccent{A^T} = A\). On dit que \(A\) est une matrice hermitienne, ou auto-adjointe.
</p>
</div>
<div id="outline-container-org968f06e" class="outline-4">
<h4 id="org968f06e"><span class="section-number-4">8.15.1.</span> Réciproque</h4>
<div class="outline-text-4" id="text-8-15-1">
<p>
Si \(A\) est une matrice carrée définie positive et hermitienne, l'application définie par :
</p>

<p>
\[\scalaire{x}{y} = \conjaccent{x^T} \cdot A \cdot y\]
</p>

<p>
est bien un produit scalaire. En effet, la définie positivité de la matrice est équivalente à celle du produit ainsi défini. Pour le caractére hermitien, on a :
</p>

\begin{align}
\scalaire{x}{y} &= \sum_{i,j} \conjaccent{x}_i \cdot A_{ij} \cdot y_j = \sum_{i,j} \conjaccent{x}_i \cdot \conjaccent{A}_{ji} \cdot y_j \)

\(
&= \sum_{i,j} y_j \cdot \conjaccent{A}_{ji} \cdot \conjaccent{x}_i = \conjugue \sum_{i,j} \conjaccent{y}_j \cdot A_{ji} \cdot x_i \)

\(
&= \conjugue \scalaire{y}{x}
\end{align}
</div>
</div>
</div>
<div id="outline-container-org2419c34" class="outline-3">
<h3 id="org2419c34"><span class="section-number-3">8.16.</span> Bases de vecteurs matriciels</h3>
<div class="outline-text-3" id="text-8-16">
<p>
Un cas particulier important survient lorsque les vecteurs sont des vecteurs matriciels. Soit une suite de vecteurs linéairement indépendants \(u_1,u_2,...,u_m \in \matrice(\corps,n,1)\). Soit des \(x_i,y_i \in \corps\) et les vecteurs :
</p>

<div class="org-center">
<p>
\(
X = \sum_{i = 1}^m x_i \cdot u_i \)
</p>

<p>
\(
Y = \sum_{i = 1}^m y_i \cdot u_i
\)
</p>
</div>

<p>
Si nous considérons les vecteurs \(x,y \in \matrice(\corps,m,1)\) associés :
</p>

<div class="org-center">
<p>
\(
x = [x_1 \ x_2 \ ... \ x_m]^T \)
</p>

<p>
\(
y = [y_1 \ y_2 \ ... \ y_m]^T
\)
</p>
</div>

<p>
ainsi que la matrice \(U \in \matrice(\corps,n,m)\) rassemblant les \(u_i\) :
</p>

<p>
\[U = [u_1 \ u_2 \ ... \ u_m]\]
</p>

<p>
on peut réécrire la définition de \(x,y\) sous la forme :
</p>

<div class="org-center">
<p>
\(
X = U \cdot x \)
</p>

<p>
\(
Y = U \cdot y
\)
</p>
</div>

<p>
On a alors :
</p>

<p>
\[\scalaire{X}{Y} = \conjaccent{X^T} \cdot Y = \conjaccent{x^T} \cdot \conjaccent{U^T} \cdot U \cdot y\]
</p>

<p>
On en conclut que la matrice \(A = \conjaccent{U^T} \cdot U \in \matrice(\corps,m,m)\) est une matrice de produit scalaire
</p>
</div>
<div id="outline-container-orgc834d77" class="outline-4">
<h4 id="orgc834d77"><span class="section-number-4">8.16.1.</span> Réciproque</h4>
<div class="outline-text-4" id="text-8-16-1">
<p>
Soit \(U \in \matrice(\corps,n,m)\) telle que \(\noyau U = \{0\}\). La matrice \(A = \conjaccent{U^T} \cdot U \in \matrice(\corps,m,m)\) est une matrice de produit scalaire. En effet :
</p>

<p>
\[\conjaccent{x^T} \cdot A \cdot x = \conjaccent{x^T} \cdot \conjaccent{U^T} \cdot U \cdot x = \conjugue(U \cdot x)^T \cdot (U \cdot x) \ge 0\]
</p>

<p>
Si \(x \ne 0\), on a de plus \(U \cdot x \ne 0\) et \(\conjaccent{x}^T \cdot A \cdot x \strictsuperieur 0\). Par ailleurs, on a évidemment :
</p>

<p>
\[\conjaccent{A^T} = \conjaccent{(\conjaccent{U^T} \cdot U)^T} = \conjaccent{U^T} \cdot U = A\]
</p>
</div>
</div>
</div>
<div id="outline-container-org5737dbd" class="outline-3">
<h3 id="org5737dbd"><span class="section-number-3">8.17.</span> Noyau</h3>
<div class="outline-text-3" id="text-8-17">
<p>
Soit \(l_i = \ligne_i A\) et \(x \in \noyau A\). On a alors :
</p>

<p>
\[0 = \composante_i (A \cdot x) = l_i \cdot x\]
</p>

<p>
On en conclut que les lignes de \(A\) sont orthogonales aux \(\conjaccent{l}_i\). Il en va de même pour toute combinaison linéaire de ces lignes, et :
</p>

<p>
\[\noyau A = \combilin{\conjaccent{l}_1,...,\conjaccent{l}_m}^\orthogonal\]
</p>
</div>
</div>
</div>
<div id="outline-container-org18f2a1b" class="outline-2">
<h2 id="org18f2a1b"><span class="section-number-2">9.</span> Norme dérivée du produit scalaire</h2>
<div class="outline-text-2" id="text-9">
<div id="text-table-of-contents-9" role="doc-toc">
<ul>
<li><a href="#org2902096">9.1. Dépendances</a></li>
<li><a href="#orgba162cf">9.2. Introduction</a></li>
<li><a href="#org3e03eae">9.3. Addition</a></li>
<li><a href="#org4197684">9.4. Théorème de Pythagore</a></li>
<li><a href="#orgeebb7d3">9.5. Egalité du parallélogramme</a></li>
<li><a href="#org8e5c181">9.6. Inégalité de Cauchy-Schwartz</a></li>
<li><a href="#orgb8e2539">9.7. Norme et inégalité de Minkowski</a></li>
<li><a href="#org4f4de6c">9.8. Distance</a></li>
<li><a href="#org35e0595">9.9. Produit scalaire à partir de la norme</a></li>
<li><a href="#org34a1b7f">9.10. Norme et coordonnées</a></li>
<li><a href="#org881005a">9.11. Norme sur \(\corps^n\)</a></li>
<li><a href="#orge4d3068">9.12. Norme sur \(\setC\)</a></li>
<li><a href="#orgfb615e5">9.13. Représentation matricielle</a></li>
</ul>
</div>

<p>
\label{chap:ps}
</p>
</div>
<div id="outline-container-org2902096" class="outline-3">
<h3 id="org2902096"><span class="section-number-3">9.1.</span> Dépendances</h3>
<div class="outline-text-3" id="text-9-1">
<ul class="org-ul">
<li>Chapitre \ref{chap:vecteur} : Les espaces vectoriels</li>
<li>Chapitre \ref{chap:norme} : Les normes</li>
</ul>
</div>
</div>
<div id="outline-container-orgba162cf" class="outline-3">
<h3 id="orgba162cf"><span class="section-number-3">9.2.</span> Introduction</h3>
<div class="outline-text-3" id="text-9-2">
<p>
Soit un espace vectoriel \(E\) muni du produit scalaire \(\scalaire{}{}\). Nous allons analyser les propriétés de l'application \(\norme{.} : E \mapsto \corps\) associée au produit scalaire et définie par :
</p>

<p>
\[\norme{x} = \sqrt{ \scalaire{x}{x} }\]
</p>

<p>
pour tout \(x \in E\).
</p>
</div>
</div>
<div id="outline-container-org3e03eae" class="outline-3">
<h3 id="org3e03eae"><span class="section-number-3">9.3.</span> Addition</h3>
<div class="outline-text-3" id="text-9-3">
<p>
Soit \(x,y \in E\) et \(\alpha,\beta \in \setC\). On a :
</p>

\begin{align}
\norme{\alpha \cdot x + \beta \cdot y}^2 &= \scalaire{\alpha \cdot x + \beta \cdot y}{\alpha \cdot x + \beta \cdot y} \)

\(
&= \conjaccent{\alpha} \cdot \alpha \cdot \scalaire{x}{x} + \conjaccent{\alpha} \cdot \beta \cdot \scalaire{x}{y} + \conjaccent{\beta} \cdot \alpha \cdot \scalaire{y}{x} + \conjaccent{\beta} \cdot \beta \cdot \scalaire{y}{y} \)

\(
&= \abs{\alpha}^2 \cdot \norme{x}^2 + \conjaccent{\alpha} \cdot \beta \cdot \scalaire{x}{y} + \conjaccent{\beta} \cdot \alpha \cdot \scalaire{y}{x} + \abs{\beta}^2 \cdot \norme{y}^2
\end{align}

<p>
Dans le cas particulier où \(\beta = 1\), on a :
</p>

\begin{align}
\norme{y + \alpha \cdot x} &= \norme{y}^2 + \alpha \cdot \scalaire{y}{x} + \conjaccent{\alpha} \cdot \scalaire{x}{y} + \abs{\alpha}^2 \cdot \norme{x}^2 \)

\(
&= \norme{y}^2 + 2 \Re(\alpha \cdot \scalaire{y}{x}) + \abs{\alpha}^2 \cdot \norme{x}^2
\end{align}
</div>
</div>
<div id="outline-container-org4197684" class="outline-3">
<h3 id="org4197684"><span class="section-number-3">9.4.</span> Théorème de Pythagore</h3>
<div class="outline-text-3" id="text-9-4">
<p>
Si \(x,y \in E\) sont orthogonaux :
</p>

<p>
\[\scalaire{x}{y} = 0\]
</p>

<p>
on a également \(\scalaire{y}{x} = \conjugue \scalaire{x}{y} = 0\) et :
</p>

\begin{align}
\scalaire{x + y}{x + y} &= \scalaire{x}{x} + \scalaire{x}{y} + \scalaire{y}{x} + \scalaire{y}{y} \)

\(
&= \scalaire{x}{x} + \scalaire{y}{y}
\end{align}

<p>
En exprimant cette relation en terme de \(\norme{.}\), on obtient :
</p>

<p>
\[\norme{x + y}^2 = \norme{x}^2 + \norme{y}^2\]
</p>

<p>
résultat connu sous le nom de théorème de Pythagore.
</p>
</div>
</div>
<div id="outline-container-orgeebb7d3" class="outline-3">
<h3 id="orgeebb7d3"><span class="section-number-3">9.5.</span> Egalité du parallélogramme</h3>
<div class="outline-text-3" id="text-9-5">
<p>
En additionnant les équations :
</p>

<div class="org-center">
<p>
\(
\norme{x + y}^2 = \scalaire{x}{x} + \scalaire{x}{y} + \scalaire{y}{x} + \scalaire{y}{y} \)
</p>

<p>
\(
\norme{x - y}^2 = \scalaire{x}{x} - \scalaire{x}{y} - \scalaire{y}{x} + \scalaire{y}{y}
\)
</p>
</div>

<p>
on obtient :
</p>

<p>
\[\norme{x + y}^2 + \norme{x - y}^2 = 2(\scalaire{x}{x} + \scalaire{y}{y}) = 2 (\norme{x}^2 + \norme{y}^2)\]
</p>
</div>
</div>
<div id="outline-container-org8e5c181" class="outline-3">
<h3 id="org8e5c181"><span class="section-number-3">9.6.</span> Inégalité de Cauchy-Schwartz</h3>
<div class="outline-text-3" id="text-9-6">
<p>
Soit \(x,y \in E\) et \(\lambda \in \setC\). On a :
</p>

<p>
\[\norme{y - \lambda \cdot x}^2 = \scalaire{y}{y} - \lambda \cdot \scalaire{y}{x} - \conjaccent{\lambda} \cdot \scalaire{x}{y} + \conjaccent{\lambda} \cdot \lambda \cdot \scalaire{x}{x} \ge 0\]
</p>

<p>
Le choix magique de \(\lambda\) (nous verrons d'où il vient en étudiant les projections) est :
</p>

<p>
\[\lambda = \frac{ \scalaire{x}{y} }{ \scalaire{x}{x} }\]
</p>

<p>
On a alors :
</p>

<p>
\[\scalaire{y}{y} - \frac{ \scalaire{x}{y} \cdot \scalaire{y}{x} }{ \scalaire{x}{x} } - \frac{ \scalaire{y}{x} \cdot \scalaire{x}{y} }{ \scalaire{x}{x} } +  \frac{ \scalaire{y}{x} \cdot \scalaire{x}{y} }{ \scalaire{x}{x}^2 } \cdot \scalaire{x}{x} \ge 0\]
</p>

<p>
En simplifiant les termes, on arrive à :
</p>

<p>
\[\scalaire{y}{y}  - \frac{ \scalaire{x}{y} \cdot \scalaire{y}{x} }{ \scalaire{x}{x} } = \scalaire{y}{y}  - \frac{ \abs{\scalaire{x}{y}}^2 }{ \scalaire{x}{x} } \ge 0\]
</p>

<p>
En faisant passer le second terme dans le second membre et en multipliant par \(\scalaire{x}{x}\), on arrive finalement à :
</p>

<p>
\[\abs{\scalaire{x}{y}}^2 \le \scalaire{x}{x} \cdot \scalaire{y}{y}\]
</p>

<p>
En prenant la racine carrée, on obtient une relation connue sous le nom d'inégalite de Cauchy-Schwartz :
</p>

<p>
\[\abs{\scalaire{x}{y}} \le \sqrt{\scalaire{x}{x} \cdot \scalaire{y}{y}} = \norme{x} \cdot \norme{y}\]
</p>
</div>
</div>
<div id="outline-container-orgb8e2539" class="outline-3">
<h3 id="orgb8e2539"><span class="section-number-3">9.7.</span> Norme et inégalité de Minkowski</h3>
<div class="outline-text-3" id="text-9-7">
<p>
Nous allons à présent vérifier que l'application \(\norme{.}\) est bien une norme.
</p>
</div>
<div id="outline-container-orge8a3165" class="outline-4">
<h4 id="orge8a3165"><span class="section-number-4">9.7.1.</span> Définie positivité</h4>
<div class="outline-text-4" id="text-9-7-1">
<p>
On voit que notre application est strictement définie positive car \(\norme{x} \ge 0\) pour tout \(x \in E\) et :
</p>

<p>
\[\norme{x} = 0 \Rightarrow \scalaire{x}{x} = 0 \Rightarrow x = 0\]
</p>
</div>
</div>
<div id="outline-container-orgfb54ba4" class="outline-4">
<h4 id="orgfb54ba4"><span class="section-number-4">9.7.2.</span> Produit mixte</h4>
<div class="outline-text-4" id="text-9-7-2">
<p>
La multiplication par un scalaire \(\alpha \in \setC\) nous donne :
</p>

<p>
\[\norme{\alpha \cdot x} = \sqrt{ \abs{\alpha}^2 \cdot \scalaire{x}{x} } = \abs{\alpha} \cdot \sqrt{ \scalaire{x}{x} } = \abs{\alpha} \cdot \norme{x}\]
</p>
</div>
</div>
<div id="outline-container-orgbdf26f7" class="outline-4">
<h4 id="orgbdf26f7"><span class="section-number-4">9.7.3.</span> Inégalité de Minkowski</h4>
<div class="outline-text-4" id="text-9-7-3">
<p>
On a :
</p>

\begin{align}
\norme{x + y}^2 &= \scalaire{x}{x} + \scalaire{x}{y} + \scalaire{y}{x} + \scalaire{y}{y} \)

\(
&= \scalaire{x}{x} + 2 \Re(\scalaire{x}{y}) + \scalaire{y}{y}
\end{align}

<p>
Mais comme \(\abs{\Re(\scalaire{x}{y})} \le \abs{\scalaire{x}{y}} \le \norme{x} \cdot \norme{y}\), on a finalement :
</p>

<p>
\[\norme{x + y}^2 \le \norme{x}^2 + 2 \norme{x} \cdot \norme{y} + \norme{y}^2 = (\norme{x} + \norme{y})^2\]
</p>

<p>
d'où :
</p>

<p>
\[\norme{x + y} \le \norme{x} + \norme{y}\]
</p>

<p>
Cette troisième et dernière propriété étant vérifiée, l'application \(\norme{.} = \sqrt{\scalaire{}{}}\) est bien une norme.
</p>
</div>
</div>
</div>
<div id="outline-container-org4f4de6c" class="outline-3">
<h3 id="org4f4de6c"><span class="section-number-3">9.8.</span> Distance</h3>
<div class="outline-text-3" id="text-9-8">
<p>
On associe une distance à la norme et au produit scalaire par :
</p>

<p>
\[\distance(x,y) = \norme{x - y} = \sqrt{\scalaire{x - y}{x - y}}\]
</p>

<p>
pour tout \(x,y \in E\).
</p>
</div>
</div>
<div id="outline-container-org35e0595" class="outline-3">
<h3 id="org35e0595"><span class="section-number-3">9.9.</span> Produit scalaire à partir de la norme</h3>
<div class="outline-text-3" id="text-9-9">
<p>
En soutrayant les équations :
</p>

<div class="org-center">
<p>
\(
\norme{x + y}^2 = \scalaire{x}{x} + 2 \Re(\scalaire{x}{y}) + \scalaire{y}{y} \)
</p>

<p>
\(
\norme{x - y}^2 = \scalaire{x}{x} - 2 \Re(\scalaire{x}{y}) + \scalaire{y}{y}
\)
</p>
</div>

<p>
on obtient :
</p>

<p>
\[\norme{x + y}^2 - \norme{x - y}^2 = 4 \Re(\scalaire{x}{y})\]
</p>

<p>
Comme \(\Re(\img z) = - \Im(z)\), on a aussi :
</p>

\begin{align}
\norme{x + \img y}^2 &= \scalaire{x}{x} + 2 \Re(\img \scalaire{x}{y}) + \scalaire{y}{y} \)

\(
&=  \scalaire{x}{x} - 2 \Im(\scalaire{x}{y}) + \scalaire{y}{y} \)

\(
\norme{x - \img y}^2 &= \scalaire{x}{x} - 2 \Re(\img \scalaire{x}{y}) + \scalaire{y}{y} \)

\(
&=  \scalaire{x}{x} + 2 \Im(\scalaire{x}{y}) + \scalaire{y}{y}
\end{align}

<p>
En soustrayant ces deux résultats, on a donc :
</p>

<p>
\[\norme{x + \img y}^2 - \norme{x - \img y}^2 = - 4 \Im(\scalaire{x}{y})\]
</p>

<p>
On en conclut que :
</p>

\begin{align}
\scalaire{x}{y} &= \Re(\scalaire{x}{y}) + \img \Im(\scalaire{x}{y}) \)

\(
&= \unsur{4} (\norme{x + y}^2 + \norme{x - y}^2) + \frac{\img}{4} (\norme{x - \img y}^2 - \norme{x + \img y}^2)
\end{align}
</div>
</div>
<div id="outline-container-org34a1b7f" class="outline-3">
<h3 id="org34a1b7f"><span class="section-number-3">9.10.</span> Norme et coordonnées</h3>
<div class="outline-text-3" id="text-9-10">
<p>
Soit \((e_1,...,e_n)\) une base de \(E\) et \(u \in E\). On a :
</p>

<p>
\[u = \sum_i u_i \cdot e_i\]
</p>

<p>
pour certains \(u_i,v_i \in \corps\). La norme s'écrit alors :
</p>

<p>
\[\norme{u} = \sqrt{ \sum_{i,j} \conjaccent{u}_i \cdot \scalaire{e_i}{e_j} \cdot u_j }\]
</p>
</div>
<div id="outline-container-orgd59f0f2" class="outline-4">
<h4 id="orgd59f0f2"><span class="section-number-4">9.10.1.</span> Base orthonormée</h4>
<div class="outline-text-4" id="text-9-10-1">
<p>
Si la base est orthonormée, les seuls termes ne s'annulant pas sont ceux où \(i = j\), et on a :
</p>

<p>
\[\norme{u} = \sqrt{ \sum_i \abs{u_i}^2 }\]
</p>
</div>
</div>
</div>
<div id="outline-container-org881005a" class="outline-3">
<h3 id="org881005a"><span class="section-number-3">9.11.</span> Norme sur \(\corps^n\)</h3>
<div class="outline-text-3" id="text-9-11">
<p>
Soit \(\corps \in \{ \setR , \setC \}\). On définit une norme sur \(\corps^n\), dite norme euclidienne, à partir du produit scalaire :
</p>

<p>
\[\norme{x} = \sqrt{ \scalaire{x}{x} } = \sqrt{\sum_{i=1}^n \abs{x_i}^2}\]
</p>
</div>
<div id="outline-container-org0d9055e" class="outline-4">
<h4 id="org0d9055e"><span class="section-number-4">9.11.1.</span> Normes \(k\)</h4>
<div class="outline-text-4" id="text-9-11-1">
<p>
Par extension, on définit une série de normes \(k\) par :
</p>

<p>
\[\norme{x}_k = \left( \sum_i \abs{x_i}^k \right)^{1/k}\]
</p>

<p>
Lorsque \(k\) devient très grand, il est clair que la contribution du \(\abs{x_i}^k\) le plus grand en valeur absolue devient énorme par rapport aux autres contributions de la norme. On peut vérifier que :
</p>

<p>
\[\lim_{k \mapsto +\infty} \norme{x}_k = \max_{i = 1}^n \abs{x_i}\]
</p>

<p>
On s'inspire de ce résultat pour définir :
</p>

<p>
\[\norme{x}_\infty = \max_{i = 1}^n \abs{x_i}\]
</p>

<p>
On nomme \(\norme{.}_\infty\) la norme « max ».
</p>

<p>
Attention, une norme \(k\) quelconque ne dérive en général pas d'un produit scalaire et ne possède donc pas les propriétés que nous avons vu pour la norme \(\norme{.} = \norme{.}_2 = \sqrt{\scalaire{}{}}\).
</p>
</div>
</div>
</div>
<div id="outline-container-orge4d3068" class="outline-3">
<h3 id="orge4d3068"><span class="section-number-3">9.12.</span> Norme sur \(\setC\)</h3>
<div class="outline-text-3" id="text-9-12">
<p>
Soit \((a,b) \in \setR^2\) et \(z = a + \img b\). Il est clair que le module :
</p>

<p>
\[\abs{z} = \abs{a + \img b} = \sqrt{a^2 + b^2} = \norme{(a,b)}\]
</p>

<p>
définit une norme sur \(\setC\).
</p>
</div>
</div>
<div id="outline-container-orgfb615e5" class="outline-3">
<h3 id="orgfb615e5"><span class="section-number-3">9.13.</span> Représentation matricielle</h3>
<div class="outline-text-3" id="text-9-13">
<p>
Soit le vecteur matriciel \(x = [x_1 \ ... \ x_n]^T\). Sa norme s'écrit :
</p>

<p>
\[\norme{x} = \sqrt{ \scalaire{x}{x} } = \sqrt{\conjaccent{x}^T \cdot x}\]
</p>
</div>
</div>
</div>
<div id="outline-container-orgc77f955" class="outline-2">
<h2 id="orgc77f955"><span class="section-number-2">10.</span> Applications adjointes</h2>
<div class="outline-text-2" id="text-10">
<div id="text-table-of-contents-10" role="doc-toc">
<ul>
<li><a href="#org9c8668e">10.1. Dépendances</a></li>
<li><a href="#orgfbaf12b">10.2. Adjoint au sens des formes linéaires</a></li>
<li><a href="#orgd188c08">10.3. Adjoint au sens du produit scalaire</a></li>
<li><a href="#org6b40ff7">10.4. Identité</a></li>
<li><a href="#org99e4b05">10.5. Adjoint d'une combinaison linéaire</a></li>
<li><a href="#org0614c28">10.6. Bidual</a></li>
<li><a href="#org846dc54">10.7. Adjoint d'une composée</a></li>
<li><a href="#orgce07c80">10.8. Construction d'applications auto-adjointes</a></li>
<li><a href="#org240237b">10.9. Linéarité de l'adjoint</a></li>
<li><a href="#org100e8a8">10.10. Norme de l'adjoint</a></li>
<li><a href="#org0ac6540">10.11. Inverse</a></li>
<li><a href="#org83df995">10.12. Noyau et image</a></li>
<li><a href="#orga9a9cd3">10.13. Représentation matricielle</a></li>
<li><a href="#orgd36d2f1">10.14. Adjoint d'un produit</a></li>
</ul>
</div>

<p>
\label{chap:dualite}
</p>
</div>
<div id="outline-container-org9c8668e" class="outline-3">
<h3 id="org9c8668e"><span class="section-number-3">10.1.</span> Dépendances</h3>
<div class="outline-text-3" id="text-10-1">
<ul class="org-ul">
<li>Chapitre \ref{chap:forme} : Les formes linéaires</li>
<li>Chapitre \ref{chap:ps} : Les produits scalaires</li>
</ul>
</div>
</div>
<div id="outline-container-orgfbaf12b" class="outline-3">
<h3 id="orgfbaf12b"><span class="section-number-3">10.2.</span> Adjoint au sens des formes linéaires</h3>
<div class="outline-text-3" id="text-10-2">
<p>
Soit les espaces vectoriels \(E\) et \(F\) sur \(\corps\) et l'application linéaire
\(A : E \mapsto F\). Si \(A^\dual : F^\dual \mapsto E^\dual\) est l'unique fonction de \(E^F\) vérifiant :
</p>

<p>
\[\forme{\varphi}{A(u)} = \forme{A^\dual(\varphi)}{u}\]
</p>

<p>
pour tout \(\varphi \in F^\dual\) et \(u \in E\), on dit que \(A^\dual\) est l'application duale (ou adjointe) de \(A\) au sens des formes linéaires.
</p>
</div>
</div>
<div id="outline-container-orgd188c08" class="outline-3">
<h3 id="orgd188c08"><span class="section-number-3">10.3.</span> Adjoint au sens du produit scalaire</h3>
<div class="outline-text-3" id="text-10-3">
<p>
Soient \(E\) et \(F\) deux espaces vectoriels munis de produits scalaires et l'application linéaire \(A : E \mapsto F\). Si \(A^\dual : F \mapsto E\) est l'unique fonction de \(E^F\) vérifiant :
</p>

<p>
\[\scalaire{v}{A(u)} = \scalaire{A^\dual(v)}{u}\]
</p>

<p>
pour tout \((u,v) \in E \times F\), on dit que \(A^\dual\) est l'application duale (ou adjointe) de \(A\) au sens du produit scalaire. Nous supposons dans la suite que les applications rencontrées possèdent une application adjointe.
</p>
</div>
<div id="outline-container-org125cd16" class="outline-4">
<h4 id="org125cd16"><span class="section-number-4">10.3.1.</span> Applications auto-adjointes</h4>
<div class="outline-text-4" id="text-10-3-1">
<p>
Si \(E = F\) et \(A = A^\dual\), on dit que \(A\) est hermitienne ou auto-adjointe.
</p>
</div>
</div>
</div>
<div id="outline-container-org6b40ff7" class="outline-3">
<h3 id="org6b40ff7"><span class="section-number-3">10.4.</span> Identité</h3>
<div class="outline-text-3" id="text-10-4">
<p>
Comme :
</p>

<p>
\[\scalaire{v}{\identite(u)} = \scalaire{\identite(v)}{u} = \scalaire{v}{u}\]
</p>

<p>
on a bien évidemment \(\identite^\dual = \identite\).
</p>
</div>
</div>
<div id="outline-container-org99e4b05" class="outline-3">
<h3 id="org99e4b05"><span class="section-number-3">10.5.</span> Adjoint d'une combinaison linéaire</h3>
<div class="outline-text-3" id="text-10-5">
<p>
Soit deux applications linéaires \(A,B : E \mapsto F\) et \(\alpha,\beta \in \corps\). Si \((u,v) \in E \times F\), on a :
</p>

\begin{align}
\scalaire{(\conjaccent{\alpha} \cdot A^\dual + \conjaccent{\beta} \cdot B^\dual)(v)}{u} &= \alpha \cdot \scalaire{A^\dual(v)}{u} + \beta \cdot \scalaire{B^\dual(v)}{u} \)

\(
&= \alpha \cdot \scalaire{v}{A(u)} + \beta \cdot \scalaire{v}{A(u)} \)

\(
&= \scalaire{v}{(\alpha \cdot A + \beta \cdot B)(u)}
\end{align}

<p>
On en conclut que :
</p>

<p>
\[(\alpha \cdot A + \beta \cdot B)^\dual = \conjaccent{\alpha} \cdot A^\dual + \conjaccent{\beta} \cdot B^\dual\]
</p>

<p>
L'opérateur \(^\dual : A \mapsto A^\dual\) est antilinéaire.
</p>
</div>
</div>
<div id="outline-container-org0614c28" class="outline-3">
<h3 id="org0614c28"><span class="section-number-3">10.6.</span> Bidual</h3>
<div class="outline-text-3" id="text-10-6">
<p>
On remarque que :
</p>

<p>
\[\scalaire{u}{A^\dual(v)} = \conjaccent{\scalaire{A^\dual(v)}{u}} = \conjaccent{\scalaire{v}{A(u)}} = \scalaire{A(u)}{v}\]
</p>

<p>
pour tout \((u,v) \in E \times F\). On a donc :
</p>

<p>
\[\left( A^\dual \right)^\dual = A\]
</p>
</div>
</div>
<div id="outline-container-org846dc54" class="outline-3">
<h3 id="org846dc54"><span class="section-number-3">10.7.</span> Adjoint d'une composée</h3>
<div class="outline-text-3" id="text-10-7">
<p>
Soit un troisième espace vectoriel \(G\). Si les applications adjointes des applications linéaires \(A : E \mapsto F\) et \(B : F \mapsto G\) existent, on a :
</p>

<p>
\[\scalaire{v}{(A \circ B)(u)} = \scalaire{A^\dual(v)}{B(u)} = \scalaire{(B^\dual \circ A^\dual)(v)}{u}\]
</p>

<p>
pour tout \((u,v) \in E \times G\). On en conclut que :
</p>

<p>
\[(A \circ B)^\dual = B^\dual \circ A^\dual\]
</p>
</div>
</div>
<div id="outline-container-orgce07c80" class="outline-3">
<h3 id="orgce07c80"><span class="section-number-3">10.8.</span> Construction d'applications auto-adjointes</h3>
<div class="outline-text-3" id="text-10-8">
<p>
Nous allons voir que nous pouvons construire deux applications auto-adjointes à partir de n'importe quelle application linéaire \(A : E \mapsto F\) admettant une application duale \(A^\dual : F \mapsto E\).
</p>

<ul class="org-ul">
<li>L'application \(A^\dual \circ A : E \mapsto E\) vérifie :</li>
</ul>

<p>
\[\scalaire{A^\dual \circ A(v)}{u} = \scalaire{A(v)}{A(u)} = \scalaire{v}{A^\dual \circ A(u)}\]
</p>

<p>
pour tout \(u,v \in E\). On en déduit que :
</p>

<p>
\[(A^\dual \circ A)^\dual = A^\dual \circ A\]
</p>

<ul class="org-ul">
<li>L'application \(A \circ A^\dual : F \mapsto F\) vérifie :</li>
</ul>

<p>
\[\scalaire{A \circ A^\dual(x)}{y} =\scalaire{A^\dual(x)}{A^\dual(y)} = \scalaire{x}{A \circ A^\dual(y)}\]
</p>

<p>
pour tout \(x,y \in F\). On en déduit que :
</p>

<p>
\[(A \circ A^\dual)^\dual = A \circ A^\dual\]
</p>
</div>
</div>
<div id="outline-container-org240237b" class="outline-3">
<h3 id="org240237b"><span class="section-number-3">10.9.</span> Linéarité de l'adjoint</h3>
<div class="outline-text-3" id="text-10-9">
<p>
Soit \(u,v \in E\), \(x,y \in F\) et \(\alpha, \beta \in \corps\). On a :
</p>

\begin{align}
\scalaire{A^\dual(\alpha \cdot x + \beta \cdot y)}{u} &= \scalaire{\alpha \cdot x + \beta \cdot y}{A(u)} \)

\(
&= \conjaccent{\alpha} \cdot \scalaire{x}{A(u)} + \conjaccent{\beta} \cdot \scalaire{y}{A(u)} \)

\(
&= \conjaccent{\alpha} \cdot \scalaire{A^\dual(x)}{u} + \conjaccent{\beta} \cdot \scalaire{A^\dual(y)}{u} \)

\(
&=\scalaire{\alpha \cdot A^\dual(x) + \beta \cdot A^\dual(y)}{u}
\end{align}

<p>
Comme ce doit être valable pour tout \(u \in E\), on en conclut que l'application adjointe est linéaire :
</p>

<p>
\[A^\dual(\alpha \cdot x + \beta \cdot y) = \alpha \cdot A^\dual(x) + \beta \cdot A^\dual(y)\]
</p>
</div>
</div>
<div id="outline-container-org100e8a8" class="outline-3">
<h3 id="org100e8a8"><span class="section-number-3">10.10.</span> Norme de l'adjoint</h3>
<div class="outline-text-3" id="text-10-10">
<p>
Soit une application linéaire \(A : E \mapsto F\) de norme finie. Si \(x \in F\) est un vecteur non nul, on a :
</p>

<div class="org-center">
<p>
\(
\norme{A^\dual(x)}^2 = \scalaire{A^\dual(x)}{A^\dual(x)} = \scalaire{A \circ A^\dual(x)}{x} \le \norme{A \circ A^\dual(x)} \cdot \norme{x}
\)
</p>
</div>

<p>
et donc :
</p>

<p>
\[\norme{A^\dual(x)}^2 \le \norme{A} \cdot \norme{A^\dual(x)} \cdot \norme{x}\]
</p>

<p>
Si \(\norme{A^\dual(x)} \ne 0\), on peut diviser par \(\norme{A^\dual(x)}\). On obtient alors :
</p>

<p>
\[\norme{A^\dual(x)} \le \norme{A} \cdot \norme{x}\]
</p>

<p>
Par positivité des normes, on remarque que cette relation est également valable lorsque \(\norme{A^\dual(x)} = 0 \le \norme{A} \cdot \norme{x}\). En divisant par la norme de \(x\), on obtient :
</p>

<p>
\[\frac{ \norme{A^\dual(x)} }{ \norme{x} } \le \norme{A}\]
</p>

<p>
Il ne nous reste plus qu'à passer au supremum sur \(x\) pour en conclure que :
</p>

<p>
\[\norme{A^\dual} \le \norme{A}\]
</p>

<p>
Mais comme \((A^\dual)^\dual = A\), on a aussi :
</p>

<p>
\[\norme{A} = \norme{(A^\dual)^\dual} \le \norme{A^\dual}\]
</p>

<p>
Ces deux inégalités nous montrent que :
</p>

<p>
\[\norme{A^\dual} = \norme{A}\]
</p>
</div>
</div>
<div id="outline-container-org0ac6540" class="outline-3">
<h3 id="org0ac6540"><span class="section-number-3">10.11.</span> Inverse</h3>
<div class="outline-text-3" id="text-10-11">
<p>
Supposons que \(A\) soit inversible. On a :
</p>

<div class="org-center">
<p>
\(
\identite = \identite^\dual = (A^{-1} \circ A)^\dual = A^\dual \circ (A^{-1})^\dual \)
</p>

<p>
\(
\identite = \identite^\dual = (A \circ A^{-1})^\dual = (A^{-1})^\dual \circ A^\dual
\)
</p>
</div>

<p>
On en conclut que \(A^\dual\) est également inversible et que :
</p>

<p>
\[(A^\dual)^{-1} = (A^{-1})^\dual\]
</p>
</div>
</div>
<div id="outline-container-org83df995" class="outline-3">
<h3 id="org83df995"><span class="section-number-3">10.12.</span> Noyau et image</h3>
<div class="outline-text-3" id="text-10-12">
<p>
Soit l'application linéaire \(A : E \mapsto F\). Soit \(u \in \noyau A\) et \(v \in \image A^\dual\). On peut donc trouver un \(x \in F\) tel que \(v = A^\dual(x)\). On a :
</p>

<p>
\[\scalaire{u}{v} = \scalaire{u}{A^\dual(x)} = \scalaire{A(u)}{x} = \scalaire{0}{x} = 0\]
</p>

<p>
d'où \(u \in (\image A^\dual)^\orthogonal\) et \(\noyau A \subseteq (\image A^\dual)^\orthogonal\). Inversément, si \(u \in (\image A^\dual)^\orthogonal\), on a :
</p>

<p>
\[\scalaire{A(u)}{x} = \scalaire{u}{A^\dual(x)} = 0\]
</p>

<p>
pour tout \(x \in E\). On en conclut que \(A(u) = 0\), c'est-à-dire \(u \in \noyau A\). On a donc aussi \((\image A^\dual)^\orthogonal \subseteq \noyau A\). Ces deux inclusions nous montrent finalement que :
</p>

<p>
\[\noyau A = (\image A^\dual)^\orthogonal\]
</p>

<p>
Comme le bidual revient à l'application d'origine, on a aussi :
</p>

<p>
\[\noyau A^\dual = (\image A)^\orthogonal\]
</p>
</div>
</div>
<div id="outline-container-orga9a9cd3" class="outline-3">
<h3 id="orga9a9cd3"><span class="section-number-3">10.13.</span> Représentation matricielle</h3>
<div class="outline-text-3" id="text-10-13">
<p>
Soit \(\corps \in \{ \setR , \setC \}\) et la matrice \(A \in \matrice(\corps,m,n)\) représentant l'application linéaire \(\mathcal{A} : \corps^n \mapsto \corps^m\). Soit \(A^\dual \in \matrice(\corps,n,m)\) représentant \(\mathcal{A}^\dual\). On a :
</p>

<p>
\[\scalaire{y}{ \mathcal{A}(x) } = \conjaccent{y^T} \cdot A \cdot x\]
</p>

<p>
ainsi que :
</p>

<p>
\[\scalaire{\mathcal{A}^\dual(y)}{ x } = \big( \conjaccent{A^\dual} \cdot \conjaccent{y} \big)^T \cdot x = \conjaccent{y^T} \cdot \big( \conjaccent{A^\dual} \big)^T \cdot x\]
</p>

<p>
Les deux produits scalaires devant être égaux par définition de la dualité, on doit clairement avoir :
</p>

<p>
\[\big( \conjaccent{A^\dual} \big)^T = A\]
</p>

<p>
c'est-à-dire :
</p>

<p>
\[A^\dual = \conjaccent{A^T} = \conjugue A^T\]
</p>

<p>
Ce résultat prouve l'existence et l'unicité de l'adjoint dans le cas d'espaces de dimension finie.
</p>
</div>
<div id="outline-container-org1667f99" class="outline-4">
<h4 id="org1667f99"><span class="section-number-4">10.13.1.</span> Cas particulier</h4>
<div class="outline-text-4" id="text-10-13-1">
<p>
Dans le cas d'une matrice réelle, on a \(\conjaccent{A} = A\) et :
</p>

<p>
\[A^\dual = A^T\]
</p>
</div>
</div>
</div>
<div id="outline-container-orgd36d2f1" class="outline-3">
<h3 id="orgd36d2f1"><span class="section-number-3">10.14.</span> Adjoint d'un produit</h3>
<div class="outline-text-3" id="text-10-14">
<p>
On peut vérifier que :
</p>

<p>
\[(A \cdot B)^\dual = B^\dual \cdot A^\dual\]
</p>

<p>
pour toutes matrices \(A \in \matrice(\corps,m,n)\) et \(B \in \matrice(\corps,n,p)\) de dimensions compatibles pour la multiplication.
</p>
</div>
</div>
</div>
<div id="outline-container-org9e17826" class="outline-2">
<h2 id="org9e17826"><span class="section-number-2">11.</span> Tenseurs</h2>
<div class="outline-text-2" id="text-11">
<div id="text-table-of-contents-11" role="doc-toc">
<ul>
<li><a href="#org1d7a3b9">11.1. Dépendances</a></li>
<li><a href="#org3bb82d9">11.2. Introduction</a></li>
<li><a href="#org1d6ad26">11.3. Produit tensoriel de formes linéaires</a></li>
<li><a href="#org30846c7">11.4. Tenseurs de formes linéaires</a></li>
<li><a href="#org7243e9b">11.5. Produit tensoriel de deux vecteurs</a></li>
<li><a href="#org4f43b74">11.6. Contractions</a></li>
<li><a href="#orgd90d338">11.7. Contractions doubles</a></li>
<li><a href="#org928fe5e">11.8. Dualité</a></li>
<li><a href="#org2e1f585">11.9. Combinaison linéaire</a></li>
<li><a href="#org9a70b45">11.10. Tenseurs d'ordre deux</a></li>
<li><a href="#org4e67268">11.11. Tenseurs d'ordre un et zéro</a></li>
<li><a href="#orgded9ebd">11.12. Associativité</a></li>
<li><a href="#org091be69">11.13. Tenseur d'ordre \(n\)</a></li>
<li><a href="#org72f885b">11.14. Dualité</a></li>
<li><a href="#org7182a01">11.15. Réduction</a></li>
<li><a href="#org6a4ee94">11.16. Contraction généralisée</a></li>
<li><a href="#org0d981d5">11.17. Contraction double généralisée</a></li>
<li><a href="#org0c5392a">11.18. Coordonnées</a></li>
<li><a href="#org61b571c">11.19. Norme</a></li>
<li><a href="#orgd6e652f">11.20. Tenseur identité</a></li>
<li><a href="#org7f33ad3">11.21. Inverse</a></li>
<li><a href="#orgb8cb7ce">11.22. Trace</a></li>
<li><a href="#orga3e4d41">11.23. Cadre</a></li>
<li><a href="#org854b5d8">11.24. Représentation matricielle</a></li>
<li><a href="#orga4c57b5">11.25. Contraction d'ordre \(1\)</a></li>
<li><a href="#orgf5ef89d">11.26. Contraction maximale</a></li>
<li><a href="#org52ade76">11.27. Base canonique</a></li>
<li><a href="#orga819255">11.28. Normes matricielles</a></li>
</ul>
</div>

<p>
\label{chap:tenseur}
</p>
</div>
<div id="outline-container-org1d7a3b9" class="outline-3">
<h3 id="org1d7a3b9"><span class="section-number-3">11.1.</span> Dépendances</h3>
<div class="outline-text-3" id="text-11-1">
<ul class="org-ul">
<li>Chapitre \ref{chap:vecteur} : Les vecteurs</li>
<li>Chapitre \ref{chap:lineaire} : Les fonctions linéaires</li>
<li>Chapitre \ref{chap:forme} : Les formes</li>
<li>Chapitre \ref{chap:ps} : Les produits scalaires</li>
</ul>
</div>
</div>
<div id="outline-container-org3bb82d9" class="outline-3">
<h3 id="org3bb82d9"><span class="section-number-3">11.2.</span> Introduction</h3>
<div class="outline-text-3" id="text-11-2">
<p>
Nous présentons deux variantes de la définition des tenseurs. La première, classique, est basée sur les formes linéaires. La seconde, basée sur les vecteurs et la généralisation des produits scalaires, a l'avantage de mettre en relief la structure particulière des tenseurs, ainsi que la multitude de fonctions que l'on peut leur associer.
</p>
</div>
</div>
<div id="outline-container-org1d6ad26" class="outline-3">
<h3 id="org1d6ad26"><span class="section-number-3">11.3.</span> Produit tensoriel de formes linéaires</h3>
<div class="outline-text-3" id="text-11-3">
<p>
Soit deux espaces vectoriels \(E\) et \(F\) sur \(\corps\), ainsi que les fonctions \(\varphi \in \lineaire(E,S)\) et \(\psi \in \lineaire(F,S)\). On définit le produit tensoriel de ces deux fonctions, noté \(\varphi \otimes \psi\), par :
</p>

<p>
\[(\varphi \otimes \psi)(a,b) = \varphi(a) \cdot \psi(b)\]
</p>

<p>
pour tout \(a \in E\) et \(b \in F\). La fonction \(\varphi \otimes \psi\) est donc une forme bilinéaire vérifiant :
</p>

<p>
\[\biforme{a}{\varphi \otimes \psi}{b} = \forme{\varphi}{a} \cdot \forme{\psi}{b}\]
</p>
</div>
<div id="outline-container-orgdb986a3" class="outline-4">
<h4 id="orgdb986a3"><span class="section-number-4">11.3.1.</span> Forme associée</h4>
<div class="outline-text-4" id="text-11-3-1">
<p>
On peut réécrire la définition comme :
</p>

<p>
\[(\varphi \otimes \psi)(a,b) = \forme{\psi(b) \cdot \varphi}{a} = (\psi(b) \cdot \varphi)(a)\]
</p>

<p>
Nous pouvons donc associer à chaque \(b \in F\) une forme linéaire \(\psi(b) \cdot \varphi\) que nous notons :
</p>

<p>
\[\forme{\varphi \otimes \psi}{b} = \varphi \cdot \psi(b)\]
</p>
</div>
</div>
<div id="outline-container-org53f4375" class="outline-4">
<h4 id="org53f4375"><span class="section-number-4">11.3.2.</span> Dualité</h4>
<div class="outline-text-4" id="text-11-3-2">
<p>
On voit en échangeant \(\varphi\) et \(\psi\) que :
</p>

<p>
\[(\psi \otimes \varphi)(b,a) = \psi(b) \cdot \varphi(a) = (\varphi \otimes \psi)(a,b)\]
</p>
</div>
</div>
<div id="outline-container-org3776d4a" class="outline-4">
<h4 id="org3776d4a"><span class="section-number-4">11.3.3.</span> Associativité</h4>
<div class="outline-text-4" id="text-11-3-3">
<p>
Le produit tensoriel étant associatif, on note :
</p>

<p>
\[\varphi \otimes \psi \otimes \omega = \varphi \otimes (\psi \otimes \omega) = (\varphi \otimes \psi) \otimes \omega\]
</p>

<p>
pour toutes formes linéaires \(\varphi,\psi,\omega\) définies sur les espaces vectoriels \(E,F,G\).
</p>
</div>
</div>
<div id="outline-container-org0dbd174" class="outline-4">
<h4 id="org0dbd174"><span class="section-number-4">11.3.4.</span> Notation</h4>
<div class="outline-text-4" id="text-11-3-4">
<p>
On convient également de la notation :
</p>

<p>
\[\big(\forme{\psi \otimes \varphi}{u}\big)(x) = \psi(x) \cdot \forme{\varphi}{u}\]
</p>

<p>
pour tout \(x \in F\). On a donc :
</p>

<p>
\[\forme{\psi \otimes \varphi}{u} = \psi \cdot \forme{\varphi}{u}\]
</p>
</div>
</div>
</div>
<div id="outline-container-org30846c7" class="outline-3">
<h3 id="org30846c7"><span class="section-number-3">11.4.</span> Tenseurs de formes linéaires</h3>
<div class="outline-text-3" id="text-11-4">
<p>
Soit les espaces vectoriels \(E_1,E_2,...,E_n\) sur \(\corps\). On nomme tenseur d'ordre \(n\) les formes $n$-linéaires de \(E_1 \times E_2 \times ... \times E_n\) vers \(\corps\).
</p>
</div>
</div>
<div id="outline-container-org7243e9b" class="outline-3">
<h3 id="org7243e9b"><span class="section-number-3">11.5.</span> Produit tensoriel de deux vecteurs</h3>
<div class="outline-text-3" id="text-11-5">
<p>
Soit les espaces vectoriels \(E\) et \(F\) sur \(\corps\). Choisissons \(a \in F\) et \(b \in E\). Le produit tensoriel \(a \otimes b\) est l'application linéaire de \(E\) vers \(F\) définie par :
</p>

<p>
\[(a \otimes b)(c) = a \cdot \scalaire{b}{c}\]
</p>

<p>
pour tout \(c \in E\). On vérifie aisément que ce produit est bilinéaire.
</p>
</div>
<div id="outline-container-org3ce29d4" class="outline-4">
<h4 id="org3ce29d4"><span class="section-number-4">11.5.1.</span> Associativité mixte</h4>
<div class="outline-text-4" id="text-11-5-1">
<p>
Il est clair d'après les définitions des opérations que :
</p>

<div class="org-center">
<p>
\(
(\alpha \cdot a) \otimes b = \alpha \cdot (a \otimes b) \)
</p>

<p>
\(
a \otimes (b \cdot \alpha) = (a \otimes b) \cdot \alpha
\)
</p>
</div>

<p>
pour tout \(\alpha \in \corps\).
</p>
</div>
</div>
<div id="outline-container-orgc982f4b" class="outline-4">
<h4 id="orgc982f4b"><span class="section-number-4">11.5.2.</span> Distributivité</h4>
<div class="outline-text-4" id="text-11-5-2">
<p>
On a :
</p>

<div class="org-center">
<p>
\(
(a + b) \otimes c = a \otimes c + b \otimes c \)
</p>

<p>
\(
a \otimes (c + d) = a \otimes c + a \otimes d
\)
</p>
</div>

<p>
pour tout \(a,b \in F\) et \(c,d \in E\).
</p>
</div>
</div>
</div>
<div id="outline-container-org4f43b74" class="outline-3">
<h3 id="org4f43b74"><span class="section-number-3">11.6.</span> Contractions</h3>
<div class="outline-text-3" id="text-11-6">
<p>
Soit les espaces vectoriels \(E,F,G\) sur \(\corps\). On étend la notion de « produit scalaire » par les relations :
</p>

<div class="org-center">
<p>
\(
\scalaire{a \otimes b}{c} = a \cdot \scalaire{b}{c} \)
</p>

<p>
\(
\scalaire{b}{c \otimes d} = \scalaire{b}{c} \cdot d \)
</p>

<p>
\(
\scalaire{a \otimes b}{c \otimes d} = \scalaire{b}{c} \cdot (a \otimes d)
\)
</p>
</div>

<p>
valables pour tout \((a,b,c,d) \in G \times F \times F \times E\).
</p>
</div>
</div>
<div id="outline-container-orgd90d338" class="outline-3">
<h3 id="orgd90d338"><span class="section-number-3">11.7.</span> Contractions doubles</h3>
<div class="outline-text-3" id="text-11-7">
<p>
Soit \(b \in F\) et \(c \in E\). Afin de rester consistant avec le produit tensoriel des formes, nous définissons la forme associée à \(b \otimes c\) par :
</p>

<p>
\[\varphi(a,d) = \scalaire{a}{(b \otimes c)(d)} = \scalaire{a}{b} \cdot \scalaire{c}{d}\]
</p>

<p>
pour tout \(d \in E\) et \(a \in F\). On note en général cette forme au moyen de la double contraction :
</p>

<p>
\[\braket{a}{b \otimes c}{d} = \scalaire{a}{b} \cdot \scalaire{c}{d}\]
</p>
</div>
</div>
<div id="outline-container-org928fe5e" class="outline-3">
<h3 id="org928fe5e"><span class="section-number-3">11.8.</span> Dualité</h3>
<div class="outline-text-3" id="text-11-8">
<p>
Soit le scalaire \(\alpha \in \setC\) et les vecteurs \(a,u \in E_1\) et \(b,v \in E_2\). On a :
</p>

<p>
\[\scalaire{u}{\alpha \cdot (a \otimes b)(v)} = \alpha \cdot \scalaire{u}{a} \cdot \scalaire{b}{v}\]
</p>

<p>
et :
</p>

\begin{align}
\scalaire{\conjaccent{\alpha} \cdot (b \otimes a)(u)}{v} &= \alpha \cdot \conjugue(\scalaire{a}{u}) \cdot \scalaire{b}{v} \)

\(
&= \alpha \cdot \scalaire{u}{a} \cdot \scalaire{b}{v}
\end{align}

<p>
On en déduit que :
</p>

<p>
\[(\alpha \cdot a \otimes b)^\dual =  \conjaccent{\alpha} \cdot b \otimes a\]
</p>
</div>
</div>
<div id="outline-container-org2e1f585" class="outline-3">
<h3 id="org2e1f585"><span class="section-number-3">11.9.</span> Combinaison linéaire</h3>
<div class="outline-text-3" id="text-11-9">
<p>
Soit des vecteurs \(a_i,b_i,c_i,d_i\) et des scalaires \(\theta_{ij},\upsilon_{ij}\). On étend la définition des contractions à des combinaisons linéaires de la forme :
</p>

<div class="org-center">
<p>
\(
T = \sum_{i,j} \theta_{ij} \cdot a_i \otimes b_j \)
</p>

<p>
\(
U = \sum_{i,j} \upsilon_{ij} \cdot c_i \otimes d_j
\)
</p>
</div>

<p>
en imposant simplement la linéarité. On a donc par exemple :
</p>

<p>
\[\scalaire{T}{U} = \sum_{i,j,k,l} \theta_{ij} \cdot \upsilon_{kl} \cdot \scalaire{b_j}{c_k} \cdot (a_i \otimes d_l)\]
</p>
</div>
</div>
<div id="outline-container-org9a70b45" class="outline-3">
<h3 id="org9a70b45"><span class="section-number-3">11.10.</span> Tenseurs d'ordre deux</h3>
<div class="outline-text-3" id="text-11-10">
<p>
Soit \(a \in F\) et \(b \in E\). Un objet de la forme :
</p>

<p>
\[T = a \otimes b\]
</p>

<p>
est un cas particulier de « tenseur d'ordre deux ». Il est formellement défini comme une application linéaire de \(E\) vers \(F\), mais il est en fait bien plus riche puisqu'on peut associer différents types de fonctions et de formes à chaque contraction possible impliquant ce tenseur. Les notions de tenseur et de contraction sont en fait étroitement liées.
</p>

<p>
On note \(\tenseur_2(F,E)\) l'espace vectoriel généré par ce type de tenseurs d'ordre deux.
</p>
</div>
</div>
<div id="outline-container-org4e67268" class="outline-3">
<h3 id="org4e67268"><span class="section-number-3">11.11.</span> Tenseurs d'ordre un et zéro</h3>
<div class="outline-text-3" id="text-11-11">
<p>
La possibilité d'associer une forme linéaire à chaque vecteur \(a \in E\) par la contraction avec un autre vecteur, qui est dans ce cas un simple produit scalaire :
</p>

<p>
\[\varphi_a(b) = \scalaire{a}{b}\]
</p>

<p>
nous incite à considérer les vecteurs comme des « tenseurs d'ordre un ». Nommant \(\tenseur_1(E)\) l'ensemble des tenseurs d'ordre un, on a simplement \(\tenseur_1(E) = E\). Quant aux scalaires, ils seront considérés comme des « tenseurs d'ordre zéro ». On note \(\tenseur_0 = \corps\) l'ensemble des tenseurs d'ordre zéro.
</p>
</div>
</div>
<div id="outline-container-orgded9ebd" class="outline-3">
<h3 id="orgded9ebd"><span class="section-number-3">11.12.</span> Associativité</h3>
<div class="outline-text-3" id="text-11-12">
<p>
Soit \(a \in G\), \(b \in F\) et \(c \in E\). Comme \(\lineaire(E,F)\) est également un espace vectoriel, on a :
</p>

<p>
\[\Big[ a \otimes  \big[ (b \otimes c)(u) \big] \Big](v) = \big[ a \otimes b \scalaire{c}{u} \big](v) = a \scalaire{c}{u} \scalaire{b}{v}\]
</p>

<p>
valable pour tout \(u \in E\) et tout \(v \in F\). Mais \(\lineaire(F,G)\) est aussi un espace vectoriel, et l'on a aussi :
</p>

<p>
\[\Big[ \big[ (a \otimes b)(v) \big] \otimes c \Big](u) = \Big[ \scalaire{b}{v} a \otimes c \Big](u) = \scalaire{b}{v} \scalaire{c}{u} a\]
</p>

<p>
Le résultat étant le même, le produit tensoriel est associatif, et nous notons :
</p>

<p>
\[a \otimes b \otimes c = a \otimes (b \otimes c) = (a \otimes b) \otimes c\]
</p>

<p>
l'application linéaire définie par :
</p>

<p>
\[(a \otimes b \otimes c)(u,v) = \scalaire{c}{u} \scalaire{b}{v} a\]
</p>
</div>
</div>
<div id="outline-container-org091be69" class="outline-3">
<h3 id="org091be69"><span class="section-number-3">11.13.</span> Tenseur d'ordre \(n\)</h3>
<div class="outline-text-3" id="text-11-13">
<p>
Considérons les espaces vectoriels \(E_1,...,E_n\), les séries de vecteurs \(a_k^1,a_k^2,...a_k^{N_k} \in E_k\) et les scalaires \(\theta_{ij...r} \in \corps\). Un tenseur d'ordre \(n\) est une combinaison linéaire de la forme :
</p>

<p>
\[T = \sum_{i,j,...,r} \theta_{ij...r} \cdot a_1^i \otimes a_2^j \otimes ... \otimes a_n^r\]
</p>

<p>
On note \(\tenseur_n(E_1,E_2,...,E_n)\) l'espace des tenseurs d'ordre \(n\).
</p>

<p>
Lorsque tous les espaces vectoriels sont égaux, soit \(E = E_1 = E_2 = ... = E_n\), on note \(\tenseur_n(E) = \tenseur_n(E,E,...,E)\).
</p>
</div>
<div id="outline-container-org0c94526" class="outline-4">
<h4 id="org0c94526"><span class="section-number-4">11.13.1.</span> Indices covariants et contravariants</h4>
<div class="outline-text-4" id="text-11-13-1">
<p>
Les indices inférieurs (le \(i\) des vecteurs \(a_i^j\) par exemple) des
tenseurs sont appelés <i>indices covariants</i>.
</p>

<p>
Les indices supérieurs (le \(j\) des vecteurs \(a_i^j\) par exemple) des
tenseurs sont appelés <i>indices contravariants</i>.
</p>

<p>
Ne pas confondre ces <i>indices supérieurs</i> contravariants , très
utilisés en calcul tensoriel, avec les puissances ! Dans le contexte
des tenseurs, une éventuelle puissance d'un scalaire \(\theta_i^j\)
serait notée au besoin par :
</p>

<p>
\[\big( \theta_j^i \big)^m = \theta_j^i \cdot ... \cdot \theta_j^i\]
</p>
</div>
</div>
</div>
<div id="outline-container-org72f885b" class="outline-3">
<h3 id="org72f885b"><span class="section-number-3">11.14.</span> Dualité</h3>
<div class="outline-text-3" id="text-11-14">
<p>
Soit les séries de vecteurs \(a_k^i \in E_k\), les scalaires \(\theta_{ij...rs} \in \corps\), et le tenseur associé :
</p>

<p>
\[A = \sum_{i,j,...,r,s} \theta_{ij...rs} \cdot a_1^i \otimes a_2^j \otimes ... \otimes a_{n - 1}^r \otimes a_n^s\]
</p>

<p>
On vérifie que le dual s'obtient en inversant l'ordre des vecteurs et en conjuguant les coordonnées :
</p>

<p>
\[A^\dual =  \sum_{i,j,...,r,s} \conjaccent{\theta}_{ij...rs} \cdot a_n^s \otimes a_{n - 1}^r \otimes ... \otimes a_2^j \otimes a_1^i\]
</p>
</div>
</div>
<div id="outline-container-org7182a01" class="outline-3">
<h3 id="org7182a01"><span class="section-number-3">11.15.</span> Réduction</h3>
<div class="outline-text-3" id="text-11-15">
<p>
Soit les séries de vecteurs \(a_k^i \in E_k\), les scalaires \(\theta_{ij...rs} \in \corps\), et le tenseur associé :
</p>

<p>
\[A = \sum_{i,j,...,r,s} \theta_{ij...rs} \cdot a_1^i \otimes a_2^j \otimes ... \otimes a_{n - 1}^r \otimes a_n^s\]
</p>

<p>
Les opérations de réduction consistent à construire des tenseurs d'ordre \(n - 1\), notés \(A_-(s)\) et \(A^-(i)\), en retirant les vecteurs \(a_n^s\) (réduction à droite) ou les vecteurs \(a_1^i\) (réduction à gauche) :
</p>

<div class="org-center">
<p>
\(
A_-(s) =  \sum_{i,j,...,r} \theta_{ij...rs} \cdot a_1^i \otimes a_2^j \otimes ... \otimes a_{n - 1}^r \)
</p>

<p>
\(
A^-(i) =  \sum_{j,...,r,s} \theta_{ij...rs} \cdot a_2^j \otimes ... \otimes a_{n - 1}^r \otimes a_n^s
\)
</p>
</div>
</div>
</div>
<div id="outline-container-org6a4ee94" class="outline-3">
<h3 id="org6a4ee94"><span class="section-number-3">11.16.</span> Contraction généralisée</h3>
<div class="outline-text-3" id="text-11-16">
<p>
Soit les séries de vecteurs \(a_k^i \in E_k\) et \(b_k^j \in F_k\), les scalaires \(\eta_{i...r}, \theta_{j...s} \in \corps\), et les tenseurs :
</p>

<div class="org-center">
<p>
\(
A = \sum_{i,...,r} \eta_{i...r} \cdot a_1^i \otimes ... \otimes a_m^r \)
</p>

<p>
\(
B = \sum_{j,...,s} \theta_{j...s} \cdot b_1^j \otimes ... \otimes b_n^s
\)
</p>
</div>

<p>
La contraction d'ordre \(0\) consiste simplement à juxtaposer les deux tenseurs au moyen du produit tensoriel :
</p>

<p>
\[\contraction{A}{0}{B} = \sum_{i,...,r,j,...,s} \eta_{i...r} \cdot \theta_{j...s} \cdot a_1^i \otimes ... \otimes a_m^r \otimes b_1^j \otimes ... \otimes b_n^s\]
</p>

<p>
Soit \(p \in \setN\) tel que \(0 \le p \le \min \{m,n\}\). Si :
</p>

<div class="org-center">
<p>
\(
E_m = F_1 \)
</p>

<p>
\(
E_{m - 1} = F_2 \)
</p>

<p>
\(
\vdots \)
</p>

<p>
\(
E_{m - p + 1} = F_p \)
</p>

<p>
\(
\)
</p>
</div>

<p>
on peut définir la contraction \(\contraction{}{p}{}\)  d'ordre \(p\) de deux tenseurs par récurrence :
</p>

<p>
\[\contraction{A}{p}{B} = \sum_{r,j} \scalaire{a_m^r}{b_1^j} \cdot \contraction{A_-(r)}{p - 1}{B^-(j)}\]
</p>
</div>
<div id="outline-container-org181ddcb" class="outline-4">
<h4 id="org181ddcb"><span class="section-number-4">11.16.1.</span> Notation</h4>
<div class="outline-text-4" id="text-11-16-1">
<p>
Soit \(A\) un tenseur d'ordre \(m\) et \(B\) un tenseur d'ordre \(n\). Le produit tensoriel est bien évidemment identique à la contraction d'ordre \(0\). On le note :
</p>

<p>
\[A \otimes B = \contraction{A}{0}{B}\]
</p>

<p>
La contraction d'ordre \(1\) est notée comme un produit scalaire :
</p>

<p>
\[\scalaire{A}{B} = \contraction{A}{1}{B}\]
</p>

<p>
ou comme un produit lorsqu'il n'y a pas de confusion possible :
</p>

<p>
\[A \cdot B = \scalaire{A}{B}\]
</p>

<p>
Enfin, la contraction maximale d'ordre \(M = \min \{m,n\}\) est notée par :
</p>

<p>
\[A : B = \contraction{A}{M}{B}\]
</p>
</div>
</div>
<div id="outline-container-org2e0de44" class="outline-4">
<h4 id="org2e0de44"><span class="section-number-4">11.16.2.</span> Exemple</h4>
<div class="outline-text-4" id="text-11-16-2">
<p>
Considérons le cas :
</p>

<div class="org-center">
<p>
\(
A = a_1 \otimes ... \otimes a_m \)
</p>

<p>
\(
B = b_1 \otimes ... \otimes b_n
\)
</p>
</div>

<p>
En utilisant \(p\) fois la récurrence, on obtient :
</p>

<p>
\[\contraction{A}{p}{B} = \scalaire{a_m}{b_1} \cdot ... \cdot \scalaire{a_{m-p+1}}{b_p} \cdot a_1 \otimes ... \otimes a_{m-p} \otimes b_{p+1} \otimes ... \otimes b_n\]
</p>

<p>
soit un tenseur d'ordre \(m - p + n - p\).
</p>
</div>
</div>
</div>
<div id="outline-container-org0d981d5" class="outline-3">
<h3 id="org0d981d5"><span class="section-number-3">11.17.</span> Contraction double généralisée</h3>
<div class="outline-text-3" id="text-11-17">
<p>
On définit également la contraction double utilisant trois facteurs par :
</p>

<p>
\[\dblecont{Y}{m}{T}{n}{X} = \contraction{ Y }{m}{ \contraction{T}{n}{X} } = \contraction{  \contraction{Y}{m}{T} }{n}{ X }\]
</p>
</div>
<div id="outline-container-org8eeaac4" class="outline-4">
<h4 id="org8eeaac4"><span class="section-number-4">11.17.1.</span> Notation</h4>
<div class="outline-text-4" id="text-11-17-1">
<p>
On note :
</p>

<div class="org-center">
<p>
\(
\braket{y}{A}{x} = \dblecont{y}{1}{A}{1}{x} \)
</p>

<p>
\(
y \cdot A \cdot x = \braket{y}{A}{x}
\)
</p>
</div>
</div>
</div>
</div>
<div id="outline-container-org0c5392a" class="outline-3">
<h3 id="org0c5392a"><span class="section-number-3">11.18.</span> Coordonnées</h3>
<div class="outline-text-3" id="text-11-18">
<p>
Si chaque \(E_k\) dispose d'une base de vecteurs \((e_k^1,e_k^2,...e_k^{N_k})\), tout tenseur \(T \in \tenseur_n(E_1,E_2,...,E_n)\) peut être écrit sous la forme :
</p>

<p>
\[T = \sum_{i,...,r} \theta_{i...r} \cdot e_1^i \otimes ... \otimes e_n^r\]
</p>

<p>
On dit que les \(\theta_{i...r}\) sont les coordonnées de \(T\). L'indépendance linéaire des \(e_k^i\) nous garantit que ces coordonnées sont uniques.
</p>

<p>
Il y a donc :
</p>

<p>
\[N = \prod_i N_i\]
</p>

<p>
coordonnées déterminant un tenseur.
</p>
</div>
<div id="outline-container-orgab06e72" class="outline-4">
<h4 id="orgab06e72"><span class="section-number-4">11.18.1.</span> Bases orthonormées</h4>
<div class="outline-text-4" id="text-11-18-1">
<p>
Si les bases sont orthonormées :
</p>

<p>
\[\scalaire{e_k^i}{e_k^j} = \indicatrice_{ij}\]
</p>

<p>
les coordonnées s'obtiennent aisément au moyen des contractions. On évalue :
</p>

<div class="org-center">
<p>
\(
\contraction{e_n^s \otimes ... \otimes e_1^j}{n}{T} = \sum_{i,...,r} \theta_{i...r} \cdot \scalaire{e_1^j}{e_1^i} \cdot ... \cdot \scalaire{e_n^s}{e_n^r} \)
</p>

<p>
\(
\contraction{e_n^s \otimes ... \otimes e_1^j}{n}{T} = \sum_{i,...,s} \theta_{i...r} \cdot \indicatrice_{ji} \cdot ... \cdot \indicatrice_{sr}
\)
</p>
</div>

<p>
Le produit des deltas de Kronecker s'annulant partout sauf aux indices \((i,...,r) = (j,...,s)\), on a finalement :
</p>

<p>
\[\contraction{e_n^s \otimes ... \otimes e_1^j}{n}{T} = \theta_{j...s}\]
</p>

<p>
ce qui nous donne les valeurs des \(\theta_{j...s}\).
</p>
</div>
</div>
</div>
<div id="outline-container-org61b571c" class="outline-3">
<h3 id="org61b571c"><span class="section-number-3">11.19.</span> Norme</h3>
<div class="outline-text-3" id="text-11-19">
<p>
La norme d'un tenseur \(A\) est analogue aux normes dérivées d'un produit scalaire. On utilise ici la contraction maximale et le tenseur duel de \(A\), afin que les espaces des vecteurs soient compatibles :
</p>

<p>
\[\norm{A} = \sqrt{A^\dual : A}\]
</p>
</div>
<div id="outline-container-org4643b48" class="outline-4">
<h4 id="org4643b48"><span class="section-number-4">11.19.1.</span> Bases orthonormées</h4>
<div class="outline-text-4" id="text-11-19-1">
<p>
Supposons que \(A\) soit représenté par rapport aux bases orthonormées \((e_k^1,e_k^2,...e_k^{N_k})\) :
</p>

<p>
\[A = \sum_{i,...,r} \theta_{i...r} \cdot e_1^i \otimes ... \otimes e_n^r\]
</p>

<p>
pour certains \(\theta_{i...r} \in \corps\). La norme nous donne dans ce cas :
</p>

<p>
\[\norme{A} = \sqrt{A^\dual : A} = \sqrt{\sum_{i,...,r} \abs{\theta_{i...r}}^2\]
</p>
</div>
</div>
</div>
<div id="outline-container-orgd6e652f" class="outline-3">
<h3 id="orgd6e652f"><span class="section-number-3">11.20.</span> Tenseur identité</h3>
<div class="outline-text-3" id="text-11-20">
<p>
Le tenseur identité \(\tenseuridentite \in \tenseur_2(E)\) est défini par :
</p>

<p>
\[\tenseuridentite \cdot u = \contraction{\tenseuridentite}{1}{u} = u\]
</p>

<p>
pour tout \(u \in E\).
</p>
</div>
<div id="outline-container-org1e74a31" class="outline-4">
<h4 id="org1e74a31"><span class="section-number-4">11.20.1.</span> Base orthonormée</h4>
<div class="outline-text-4" id="text-11-20-1">
<p>
Soit \((e_1,...,e_n)\) une base orthonormée de \(E\). On a alors :
</p>

<p>
\[\tenseuridentite \cdot e_i = e_i\]
</p>

<p>
et :
</p>

<p>
\[\contraction{\tenseuridentite}{2}{e_j \otimes e_i} = \scalaire{\tenseuridentite \cdot e_i}{e_j} = \indicatrice_{ij}\]
</p>

<p>
On en conclut que :
</p>

<p>
\[\tenseuridentite = \sum_{i,j} \indicatrice_{ij} \cdot e_i \otimes e_j = \sum_i e_i \otimes e_i\]
</p>
</div>
</div>
</div>
<div id="outline-container-org7f33ad3" class="outline-3">
<h3 id="org7f33ad3"><span class="section-number-3">11.21.</span> Inverse</h3>
<div class="outline-text-3" id="text-11-21">
<p>
Soit un tenseur \(T \in \tenseur_2(E)\). Si il existe un tenseur \(T^{-1} \in \tenseur_2(E)\) dont les contractions d'ordre \(1\) avec \(T\) donnent le tenseur identité :
</p>

<p>
\[T \cdot T^{-1} = \contraction{T}{1}{T^{-1}} = T^{-1} \cdot T = \contraction{T^{-1}}{1}{T} = \tenseuridentite\]
</p>

<p>
on dit que \(T^{-1}\) est l'inverse de \(T\).
</p>
</div>
</div>
<div id="outline-container-orgb8cb7ce" class="outline-3">
<h3 id="orgb8cb7ce"><span class="section-number-3">11.22.</span> Trace</h3>
<div class="outline-text-3" id="text-11-22">
<p>
Soit \((e_1,...,e_n)\) est une base orthonormée de \(E\) et le tenseur \(T \in \tenseur_2(E)\) :
</p>

<p>
\[T = \sum_{i,j} \theta_{ij} \cdot e_i \otimes e_j\]
</p>

<p>
On définit sa trace par :
</p>

<p>
\[\trace T = T : \tenseuridentite = \sum_{i,j} \theta_{ij} \cdot \indicatrice_{ij} = \sum_i \theta_{ii}\]
</p>
</div>
</div>
<div id="outline-container-orga3e4d41" class="outline-3">
<h3 id="orga3e4d41"><span class="section-number-3">11.23.</span> Cadre</h3>
<div class="outline-text-3" id="text-11-23">
<p>
On dit que les vecteurs \(e_i \in E\) forment un cadre de \(E\) si :
</p>

<p>
\[\sum_i e_i \otimes e_i = \tenseuridentite\]
</p>

<p>
On a alors, pour tout vecteur \(u\) de \(E\) :
</p>

<p>
\[\sum_i \scalaire{e_i \otimes e_i}{u} = u\]
</p>

<p>
c'est-à-dire :
</p>

<p>
\[u = \sum_i \scalaire{e_i}{u} e_i\]
</p>

<p>
par définition de la contraction. Prenant le produit scalaire avec un autre vecteur \(v \in F\), on obtient :
</p>

<p>
\[\scalaire{v}{u} = \sum_i \scalaire{v}{e_i}\scalaire{e_i}{u}\]
</p>
</div>
</div>
<div id="outline-container-org854b5d8" class="outline-3">
<h3 id="org854b5d8"><span class="section-number-3">11.24.</span> Représentation matricielle</h3>
<div class="outline-text-3" id="text-11-24">
<p>
Soit les vecteurs \(u \in F = \matrice(K,m,1) \equiv \corps^m\) et \(v,w \in E = \matrice(K,n,1) \equiv \corps^n\). La matrice :
</p>

<div class="org-center">
<p>
\(
A = u &sdot; v^\dual =
</p>
\begin{Matrix}{cccc}
u_1 \cdot \bar{v}_1 & u_1 \cdot \bar{v}_2 & \ldots & u_1 \cdot \bar{v}_n \)

\(
u_21 \cdot \bar{v}_1 & u_2 \cdot \bar{v}_2 & \ldots & u_2 \cdot \bar{v}_n \)

\(
\vdots &        & \ddots &  \vdots \)

\(
u_m \cdot \bar{v}_1 & u_m \cdot \bar{v}_2 & \ldots & u_m \cdot \bar{v}_n
\end{Matrix}
<p>
\)
</p>
</div>

<p>
représente une application linéaire de \(\corps^n\) vers \(\corps^m\). Comme le produit matriciel est associatif, cette fonction possède la propriété :
</p>

<p>
\[A \cdot w = (u \cdot v^\dual) \cdot w = u \cdot (v^\dual \cdot w) = u \cdot \scalaire{v}{w}\]
</p>

<p>
Ce résultat étant identique à la définition d'un tenseur d'ordre deux, on voit que les matrices sont équivalentes à des tenseurs d'ordre deux : \(\matrice(K,m,n) \equiv \tenseur_2(\corps^m,\corps^n)\). On parle donc de tenseurs matriciels pour désigner cette représentation. On définit par équivalence le produit tensoriel de deux vecteurs matriciels par :
</p>

<p>
\[u \otimes v = u \cdot v^\dual\]
</p>
</div>
</div>
<div id="outline-container-orga4c57b5" class="outline-3">
<h3 id="orga4c57b5"><span class="section-number-3">11.25.</span> Contraction d'ordre \(1\)</h3>
<div class="outline-text-3" id="text-11-25">
<p>
Soit les espaces vectoriels \(E_1,E_2,E_3\) sur \(\corps\) et les suites de vecteurs \((e_k^1,e_k^2,...e_k^{N_k})\) formant des bases orthonormées des \(E_k\). Soit les tenseurs :
</p>

<div class="org-center">
<p>
\(
\mathcal{A} = \sum_{i,k} a_{ik} \cdot e_1^i \otimes e_2^k \)
</p>

<p>
\(
\mathcal{B} = \sum_{l,j} b_{lj} \cdot e_2^l \otimes e_3^j
\)
</p>
</div>

<p>
où \(a_{ij},b_{ij} \in \corps\). Calculons leur contraction d'ordre \(1\) :
</p>

<div class="org-center">
<p>
\(
\mathcal{C} = \contraction{ \mathcal{A} }{1}{ \mathcal{B} } = \sum_{i,j,k,l} a_{ik} \cdot b_{lj} \cdot \scalaire{e_2^k}{e_2^l} \cdot e_1^i \otimes e_3^j \)
</p>

<p>
\(
\mathcal{C} = \sum_{i,j,k,l} a_{ik} \cdot b_{lj} \cdot \indicatrice_{kl} \cdot e_1^i \otimes e_3^j \)
</p>

<p>
\(
\mathcal{C} = \sum_{i,j,k} a_{ik} \cdot b_{kj} \cdot e_1^i \otimes e_3^j
\)
</p>
</div>

<p>
On voit que les coordonnées obtenues :
</p>

<p>
\[c_{ij} = \sum_k a_{ik} \cdot b_{kj}\]
</p>

<p>
correspondent à une matrice \(C = ( c_{ij} )_{i,j}\) telle que :
</p>

<p>
\[C = A \cdot B\]
</p>

<p>
où les matrices \(A\) et \(B\) sont données par :
</p>

<div class="org-center">
<p>
\(
A = ( a_{ij} )_{i,j} \)
</p>

<p>
\(
B = ( b_{ij} )_{i,j}
\)
</p>
</div>

<p>
La contraction d'ordre \(1\) correspond donc au produit matriciel, qui correspond lui-même à la composition d'application linéaires.
</p>
</div>
</div>
<div id="outline-container-orgf5ef89d" class="outline-3">
<h3 id="orgf5ef89d"><span class="section-number-3">11.26.</span> Contraction maximale</h3>
<div class="outline-text-3" id="text-11-26">
<p>
Soit les espaces vectoriels \(E_1,E_2,E_3\) sur \(\corps\) et les suites de vecteurs \((e_k^1,e_k^2,...e_k^{N_k})\) formant des bases orthonormées des \(E_k\). Soit les tenseurs :
</p>

<div class="org-center">
<p>
\(
\mathcal{A} = \sum_{i,k} a_{ik} \cdot e_1^i \otimes e_2^k \)
</p>

<p>
\(
\mathcal{B} = \sum_{l,j} b_{lj} \cdot e_2^l \otimes e_3^j
\)
</p>
</div>

<p>
où \(a_{ij},b_{ij} \in \corps\). Calculons leur contraction maximale :
</p>

<div class="org-center">
<p>
\(
\alpha = \mathcal{A} : \mathcal{B} = \contraction{ \mathcal{A} }{2}{ \mathcal{B} } = \sum_{i,j,k,l} a_{ik} \cdot b_{lj} \cdot \indicatrice_{kl} \cdot \indicatrice_{ij} \)
</p>

<p>
\(
\alpha = \sum_{i,k} a_{ik} \cdot b_{ki}
\)
</p>
</div>

<p>
Ce résultat nous incite à définir l'opération équivalente sur les matrices associées \(A = ( a_{ij} )_{i,j}\) et \(B = ( b_{ij} )_{i,j}\) :
</p>

<p>
\[A : B = \sum_{i,j} a_{ij} \cdot b_{ji}\]
</p>
</div>
</div>
<div id="outline-container-org52ade76" class="outline-3">
<h3 id="org52ade76"><span class="section-number-3">11.27.</span> Base canonique</h3>
<div class="outline-text-3" id="text-11-27">
<p>
Soit \(\canonique_{m,i}\) les vecteurs de la base canonique de \(\corps^m\) et \(\canonique_{n,i}\) les vecteurs de la base canonique de \(\corps^n\). Si \(A = (a_{ij})_{i,j} \in \matrice(\corps,m,n)\), on a clairement :
</p>

<p>
\[A = \sum_{i = 1}^m \sum_{j = 1}^n a_{ij} \cdot \canonique_{m,i} \otimes \canonique_{n,j} = \sum_{i = 1}^m \sum_{j = 1}^n a_{ij} \cdot \canonique_{m,i} \cdot \canonique_{n,j}^\dual\]
</p>
</div>
<div id="outline-container-orgacb6b44" class="outline-4">
<h4 id="orgacb6b44"><span class="section-number-4">11.27.1.</span> Lignes</h4>
<div class="outline-text-4" id="text-11-27-1">
<p>
On a :
</p>

<p>
\[\canonique_{m,k}^\dual \cdot A = \sum_{k,i,j} a_{ij} \cdot \indicatrice_{ik} \cdot \canonique_{n,j}^\dual = \sum_j a_{kj} \cdot \canonique_{n,j}^\dual\]
</p>

<p>
La matrice de taille \((1,n)\) obtenue est donc la \(k^{eme}\) ligne de \(A\) :
</p>

<p>
\[\canonique_{m,k}^\dual \cdot A = \ligne_k A\]
</p>
</div>
</div>
<div id="outline-container-org65773b8" class="outline-4">
<h4 id="org65773b8"><span class="section-number-4">11.27.2.</span> Colonnes</h4>
<div class="outline-text-4" id="text-11-27-2">
<p>
On a :
</p>

<p>
\[A \cdot \canonique_{n,k} = \sum_{k,i,j} a_{ij} \cdot \canonique_{m,i} \cdot \indicatrice_{jk} = \sum_i a_{ik} \cdot \canonique_{m,i}\]
</p>

<p>
La matrice de taille \((m,1)\) obtenue est donc la \(k^{eme}\) colonne de \(A\) :
</p>

<p>
\[A \cdot \canonique_{n,k} = \colonne_k A\]
</p>
</div>
</div>
<div id="outline-container-org88347b3" class="outline-4">
<h4 id="org88347b3"><span class="section-number-4">11.27.3.</span> Identité</h4>
<div class="outline-text-4" id="text-11-27-3">
<p>
Dans le cas où \(m = n\), on a en particulier :
</p>

<p>
\[\sum_{i = 1}^n \canonique_{n,i} \otimes \canonique_{n,i} = \sum_{i = 1}^n \canonique_{n,i} \cdot \canonique_{n,i}^\dual = I\]
</p>
</div>
</div>
</div>
<div id="outline-container-orga819255" class="outline-3">
<h3 id="orga819255"><span class="section-number-3">11.28.</span> Normes matricielles</h3>
<div class="outline-text-3" id="text-11-28">
<p>
La norme de Frobenius d'une matrice \(A = ( a_{ij} )_{i,j}\) est la norme du tenseur associé. On a donc :
</p>

<p>
\[\norm{A}_F = \sqrt{A^\dual : A} = \sqrt{\sum_{i,j} \bar{a}_{ij} \cdot a_{ij} } = \sqrt{ \sum_{i,j} \abs{a_{ij}}^2 }\]
</p>
</div>
<div id="outline-container-org7d7de3e" class="outline-4">
<h4 id="org7d7de3e"><span class="section-number-4">11.28.1.</span> Trace</h4>
<div class="outline-text-4" id="text-11-28-1">
<p>
La trace d'une matrice \(A = (a_{ij})_{i,j} \in \matrice(K,m,n)\) est la trace du tenseur sous-jacent, et donc :
</p>

<p>
\[\trace(A) = \sum_{i \in I} a_{ii}\]
</p>

<p>
où \(I = \{1, 2, ..., \min \{ m , n \} \}\).
</p>
</div>
</div>
</div>
</div>
<div id="outline-container-orgd6d1377" class="outline-2">
<h2 id="orgd6d1377"><span class="section-number-2">12.</span> Produit extérieur</h2>
<div class="outline-text-2" id="text-12">
<div id="text-table-of-contents-12" role="doc-toc">
<ul>
<li><a href="#orga36685f">12.1. Dépendances</a></li>
<li><a href="#org4f691db">12.2. Dimension 2</a></li>
<li><a href="#orgbba9cfd">12.3. Dimension 3</a></li>
<li><a href="#org62aa35b">12.4. Permutations en dimension \(N\)</a></li>
<li><a href="#org3ff525d">12.5. Tenseur de permutation</a></li>
<li><a href="#orgc4ff3eb">12.6. Produit extérieur généralisé</a></li>
<li><a href="#orgb0872ad">12.7. Déterminant d'une matrice</a></li>
</ul>
</div>

<p>
\label{chap:prdext}
</p>
</div>
<div id="outline-container-orga36685f" class="outline-3">
<h3 id="orga36685f"><span class="section-number-3">12.1.</span> Dépendances</h3>
<div class="outline-text-3" id="text-12-1">
<ul class="org-ul">
<li>Chapitre \ref{chap:vecteur} : Les vecteurs</li>
<li>Chapitre \ref{chap:tenseur} : Les tenseurs</li>
</ul>
</div>
</div>
<div id="outline-container-org4f691db" class="outline-3">
<h3 id="org4f691db"><span class="section-number-3">12.2.</span> Dimension 2</h3>
<div class="outline-text-3" id="text-12-2">
<p>
Soit l'espace vectoriel \(E = \combilin{e_1,e_2}\) sur \(S\), où \((e_1,e_2)\) forme est une base orthonormée. Soit \(u,v \in E\). On a :
</p>

<div class="org-center">
<p>
\(
u = \sum_{i = 1}^2 u_i e_i \)
</p>

<p>
\(
v = \sum_{i = 1}^2 v_i e_i
\)
</p>
</div>

<p>
pour certains \(u_i,v_i \in S\).
</p>

<p>
Les symboles \(\permutation_{ij}\quad (i,j=1,2)\) sont définis de telle sorte que pour tout \(u,v\) la double somme :
</p>

<p>
\[u \wedge v = \sum_{i,j=1}^2 \permutation_{ij} u_i v_j\]
</p>

<p>
représente (au signe près) la surface du parallélogramme dont les sommets
sont \((0,0)\), \((u_1,u_2)\), \((v_1,v_2)\) et \((u_1+v_1,u_2+v_2)\). On impose
de plus l'antisymétrie :
</p>

<p>
\[u \wedge v = - v \wedge u\]
</p>

<p>
afin de donner une orientation à ce parallélogramme. Ces contraintes nous donnent :
</p>

<div class="org-center">
<p>
\(
u = e_1, \quad v = e_1 \quad \Rightarrow \quad u \wedge v = 0 \)
</p>

<p>
\(
u = e_1, \quad v = e_2 \quad \Rightarrow \quad u \wedge v = 1 \)
</p>

<p>
\(
u = e_2, \quad v = e_2 \quad \Rightarrow \quad u \wedge v = 0 \)
</p>

<p>
\(
u = e_2, \quad v = e_1 \quad \Rightarrow \quad u \wedge v = -1
\)
</p>
</div>

<p>
ce qui nous amène à :
</p>

<div class="org-center">
<p>
\(
\permutation_{11} = \permutation_{22} = 0 \)
</p>

<p>
\(
\permutation_{12} = 1 \qquad \permutation_{21} = -1
\)
</p>
</div>

<p>
On voit qu'il y a un certain arbitraire dans notre choix, on aurait
pu également choisir :
</p>

<div class="org-center">
<p>
\(
\permutation_{11} = \permutation_{22} = 0 \)
</p>

<p>
\(
\permutation_{12} = -1 \qquad \permutation_{21} = 1
\)
</p>
</div>

<p>
Le choix du signe détermine ce que l'on appelle l'orientation de l'espace \(E\).
</p>
</div>
</div>
<div id="outline-container-orgbba9cfd" class="outline-3">
<h3 id="orgbba9cfd"><span class="section-number-3">12.3.</span> Dimension 3</h3>
<div class="outline-text-3" id="text-12-3">
<p>
Soit l'espace vectoriel \(E = \combilin{e_1,e_2,e_3}\) sur \(S\), où \((e_1,e_2,e_3)\) forme est une base orthonormée. Soit \(u,v,w \in E\). On a :
</p>

<div class="org-center">
<p>
\(
u = \sum_{i = 1}^3 u_i e_i \)
</p>

<p>
\(
v = \sum_{i = 1}^3 v_i e_i \)
</p>

<p>
\(
w = \sum_{i = 1}^3 w_i e_i
\)
</p>
</div>

<p>
pour certains \(u_i,v_i,w_i \in S\).
</p>

<p>
Les symboles \(\permutation_{ijk} \quad (i,j=1,2,3)\) sont définis de telle sorte que pour tout \(u,v,w\) le scalaire :
</p>

<p>
\[u \wedge v \wedge w = \sum_{i,j,k=1}^3 \permutation_{ijk} u_i v_j w_k\]
</p>

<p>
représente (au signe près) le volume du parallélipipède dont les côtés
sont définis par \(u\), \(v\) et \(w\). On impose également l'antisymétrie
</p>

<div class="org-center">
<p>
\(
u \wedge v \wedge w = - u \wedge w \wedge v \)
</p>

<p>
\(
u \wedge v \wedge w = - v \wedge u \wedge w
\)
</p>
</div>

<p>
Par un procédé analogue au cas bidimensionnel, on montre qu'un choix possible
est donné par :
</p>

<div class="org-center">
<p>
\(
\permutation_{123} = 1 \)
</p>

<p>
\(
\permutation_{ijj} = \permutation_{iij} = \permutation_{iji} = 0 \)
</p>

<p>
\(
\permutation_{ijk} = \permutation_{jki} = \permutation_{kij} \)
</p>

<p>
\(
\permutation_{ijk} = - \permutation_{jik} \qquad
\permutation_{ijk} = - \permutation_{ikj}
\)
</p>
</div>
</div>
<div id="outline-container-orgac1141a" class="outline-4">
<h4 id="orgac1141a"><span class="section-number-4">12.3.1.</span> Produit vectoriel</h4>
<div class="outline-text-4" id="text-12-3-1">
<p>
On peut également former un vecteur avec l'opérateur \(\wedge\). On pose
simplement :
</p>

<p>
\[u \wedge v = \sum_{i,j,k = 1}^3 \left( \permutation_{ijk} u_j v_k \right) e_i\]
</p>

<p>
Les composantes sont donc données par :
</p>

<div class="org-center">
<p>
\(
(u \wedge v)_1 = u_2 v_3 - u_3 v_2  \)
</p>

<p>
\(
(u \wedge v)_2 = u_3 v_1 - u_1 v_3  \)
</p>

<p>
\(
(u \wedge v)_3 = u_1 v_2 - u_2 v_1
\)
</p>
</div>

<p>
On peut relier le produit extérieur au produit scalaire en constatant que :
</p>

<p>
\[\scalaire{u}{v \wedge w} = \sum_{i,j,k} \permutation_{ijk} u_i v_j w_k = u \wedge v \wedge w\]
</p>
</div>
</div>
</div>
<div id="outline-container-org62aa35b" class="outline-3">
<h3 id="org62aa35b"><span class="section-number-3">12.4.</span> Permutations en dimension \(N\)</h3>
<div class="outline-text-3" id="text-12-4">
<p>
Voyons quelles sont les propriétés communes aux \(\permutation_*\) présentés
ci-dessus. On remarque que \(\permutation_{12} = \permutation_{123} = 1\). On note aussi que \(\permutation_*\) est antysimétrique puisque l'inversion de deux indices change le signe. Ces propriétés nous permettent de généraliser la définition du produit extérieur à un espace de dimension \(N\). On impose que le \(\permutation\) à \(N\) indices vérifie les conditions suivantes :
</p>

<p>
\label{def:eps}
</p>

<div class="org-center">
<p>
\(
\permutation_{1,2,...,N} = 1 \)
</p>

<p>
\(
\permutation_{i ... j ... k ... l} = - \permutation_{i ... k ... j ... l}
\)
</p>
</div>

<p>
Ces conditions nous permettent de retrouver la valeur de n'importe quel \(\permutation_{ijk...l}\). Si deux indices sont égaux, l'antisymétrie nous permet d'affirmer que :
</p>

<p>
\[\permutation_{i ... j ... j ... k} = - \permutation_{i ... j ... j ... k}\]
</p>

<p>
et donc :
</p>

<p>
\[\permutation_{i ... j ... j ... k} = 0\]
</p>

<p>
Les seuls \(\permutation\) non nuls sont donc ceux dont tous les indices \((i,j,k,...,s)\) sont distincts, c'est-à-dire les permutations de \((1,2,3,...,N)\). On se rend compte que si \(p \in \setN\) est le nombre de permutations de couples d'indices nécessaires pour obtenir \((i,j,k,...,s)\) à partir de \((1,2,3,...,N)\), on a :
</p>

<p>
\[\permutation_{i j k ... l} = (-1)^p\]
</p>
</div>
</div>
<div id="outline-container-org3ff525d" class="outline-3">
<h3 id="org3ff525d"><span class="section-number-3">12.5.</span> Tenseur de permutation</h3>
<div class="outline-text-3" id="text-12-5">
<p>
Soit \((e_1,e_2,...,e_N)\) une base orthonormée d'un espace vectoriel \(E\) sur \(S\). Nous définissons le tenseur de permutation \(\mathcal{E} \in \tenseur_N(E)\) par :
</p>

<p>
\[\mathcal{E} = \sum_{i_1,i_2,...,i_N} \permutation_{i_1,i_2,...,i_N} \cdot e_{i_1} \otimes e_{i_2} \otimes ... \otimes e_{i_N}\]
</p>
</div>
</div>
<div id="outline-container-orgc4ff3eb" class="outline-3">
<h3 id="orgc4ff3eb"><span class="section-number-3">12.6.</span> Produit extérieur généralisé</h3>
<div class="outline-text-3" id="text-12-6">
<p>
Soit \((e_1,e_2,...,e_N)\) une base orthonormée d'un espace vectoriel \(E\) sur \(\corps\),
\(M \le N\) et les vecteurs \(u_1,u_2,...u_M \in E\) de coordonnées \(u_k^i\) :
</p>

<p>
\[u_k = \sum_{i = 1}^N u_k^i e_i\]
</p>

<p>
Nous définissons leur produit extérieur par une contraction d'ordre \(M\) :
</p>

<p>
\[u_1 \wedge u_2 \wedge ... \wedge u_M = \contraction{ \mathcal{E} }{M}{ u_M \otimes ... \otimes u_1 }\]
</p>

<p>
Par orthonormalité de la base, on a :
</p>

<p>
\[\scalaire{e_j}{u_k} = u_k^j\]
</p>

<p>
Le produit extérieur s'écrit donc :
</p>

<p>
\[\mathcal{U} = u_1 \wedge u_2 \wedge ... \wedge u_M = \sum_{i_1,i_2,...i_N} \permutation_{i_1,i_2,...i_N} \cdot e_{i_1} \otimes ... \otimes e_{ i_{N - M} } \cdot u_M^{i_N} \cdot \hdots \cdot u_1^{ i_{N - M + 1} }\]
</p>

<p>
ce qui nous donne les coordonnées du tenseur \(\mathcal{U} \in \tenseur_{N-M}(E)\) par rapport à la base \((e_1,...,e_N)\) :
</p>

<p>
\[U_{ i_1,...,i_{N - M} } = \sum_{i_{N - M + 1},...,i_N} \permutation_{i_1,i_2,...i_N} \cdot u_1^{ i_{N - M + 1} } \cdot \hdots \cdot u_M^{i_N}\]
</p>

<p>
On vérifie les propriétés suivantes :
</p>

<div class="org-center">
<p>
\(
u \wedge v = - v \wedge u \)
</p>

<p>
\(
u \wedge u = 0 \)
</p>

<p>
\(
(\alpha u + \beta v) \wedge w = \alpha u \wedge w + \beta v \wedge w \)
</p>

<p>
\(
w \wedge (\alpha u + \beta v) = \alpha w \wedge u + \beta w \wedge v
\)
</p>
</div>

<p>
pour tout \(u,v,w,... \in E\) et \(\alpha,\beta \in S\).
</p>
</div>
<div id="outline-container-org1172f26" class="outline-4">
<h4 id="org1172f26"><span class="section-number-4">12.6.1.</span> \(N\) vecteurs</h4>
<div class="outline-text-4" id="text-12-6-1">
<p>
Dans le cas où \(M = N\), le produit extérieur est le scalaire :
</p>

<p>
\[\Delta = u_1 \wedge u_2 \wedge ... \wedge u_N = \sum_{i,j,...,k = 1}^N \permutation_{ij...k} \cdot u_1^i \cdot u_2^j \cdot \hdots \cdot u_N^k\]
</p>

<p>
On appelle le \(\Delta\) ainsi obtenu le déterminant des \(N\) vecteurs \(u_i\), et on le note :
</p>

<p>
\[\det(u_1,...,u_N) = u_1 \wedge u_2 \wedge ... \wedge u_N\]
</p>
</div>
</div>
</div>
<div id="outline-container-orgb0872ad" class="outline-3">
<h3 id="orgb0872ad"><span class="section-number-3">12.7.</span> Déterminant d'une matrice</h3>
<div class="outline-text-3" id="text-12-7">
<p>
Soit une matrice \(A \in \matrice(K,N,N)\) :
</p>

<p>
\[A = \left( a_{ij} \right)_{i,j}\]
</p>

<p>
et les vecteurs correspondants :
</p>

<p>
\[a_i = \sum_j a_{ij} e_j\]
</p>

<p>
On définit alors simplement :
</p>

<div class="org-center">
<p>
\(
\det(A) = a_1 \wedge ... \wedge a_N = \sum_{i_1,i_2,...i_N} \permutation_{i_1,i_2,...i_N} a_{1, i_1 } ...
a_{N, i_N}
\)
</p>
</div>
</div>
</div>
</div>
<div id="outline-container-org68b0c0f" class="outline-2">
<h2 id="org68b0c0f"><span class="section-number-2">13.</span> Matrices élémentaires</h2>
<div class="outline-text-2" id="text-13">
<div id="text-table-of-contents-13" role="doc-toc">
<ul>
<li><a href="#orge0958cb">13.1. Dépendances</a></li>
<li><a href="#org46a5289">13.2. Introduction</a></li>
<li><a href="#org08e614f">13.3. Inverse</a></li>
<li><a href="#orgfe9e92c">13.4. Matrices élémentaires de transformation</a></li>
<li><a href="#org12ad16d">13.5. Matrices élémentaires de permutation</a></li>
<li><a href="#orgbc04f86">13.6. Matrices de permutations</a></li>
</ul>
</div>
</div>
<div id="outline-container-orge0958cb" class="outline-3">
<h3 id="orge0958cb"><span class="section-number-3">13.1.</span> Dépendances</h3>
<div class="outline-text-3" id="text-13-1">
<ul class="org-ul">
<li>Chapitre \ref{chap:matrice} : Les matrices</li>
<li>Chapitre \ref{chap:tenseur} : Les tenseurs</li>
</ul>
</div>
</div>
<div id="outline-container-org46a5289" class="outline-3">
<h3 id="org46a5289"><span class="section-number-3">13.2.</span> Introduction</h3>
<div class="outline-text-3" id="text-13-2">
<p>
Les matrices élémentaires constituent une classe importante de matrice. Elles permettent d'obtenir d'importants résultats utiles tant sur le plan théorique que pour les applications numériques. Une matrice élémentaire de taille \((n,n)\) est déterminée par un scalaire \(\alpha \in \corps\) et le produit tensoriel de deux vecteurs \(u,v \in \corps^n\) :
</p>

<p>
\[\matelementaire(\alpha,u,v) = I + \alpha \cdot u \otimes v = I + \alpha \cdot u \cdot v^\dual\]
</p>
</div>
</div>
<div id="outline-container-org08e614f" class="outline-3">
<h3 id="org08e614f"><span class="section-number-3">13.3.</span> Inverse</h3>
<div class="outline-text-3" id="text-13-3">
<p>
Considérons le produit :
</p>

\begin{align}
\matelementaire(\alpha,u,v) \cdot \matelementaire(\beta,u,v) &= I + (\alpha + \beta) \cdot u \cdot v^\dual + \alpha \cdot \beta \cdot u \cdot v^\dual \cdot u \cdot v^\dual \)

\(
&= I + [\alpha + \beta + \alpha \cdot \beta \cdot (v^\dual \cdot u)] \cdot u \cdot v^\dual
\end{align}

<p>
On voit que si on peut trouver un scalaire \(\beta\) tel que :
</p>

<p>
\[\alpha + \beta + \alpha \cdot \beta \cdot (v^\dual \cdot u) = 0\]
</p>

<p>
le produit des deux matrices sera égal à la matrice identité. Ce sera possible si \(1 + \alpha \cdot v^\dual \cdot u \ne 0\). On a alors :
</p>

<p>
\[\beta = - \frac{\alpha}{1 + \alpha \cdot (v^\dual \cdot u)}\]
</p>

<p>
et :
</p>

<p>
\[\matelementaire(\alpha,u,v) \cdot \matelementaire(\beta,u,v) = I\]
</p>

<p>
Par symétrie, il est clair que le produit des deux matrices ne change pas lorsqu'on intervertit \(\alpha\) et \(\beta\). On a donc aussi :
</p>

<p>
\[\matelementaire(\beta,u,v) \cdot \matelementaire(\alpha,u,v) = I\]
</p>

<p>
Ces deux conditions étant remplies, on a :
</p>

<p>
\[\matelementaire(\beta,u,v) = \matelementaire(\alpha,u,v)^{-1}\]
</p>

<p>
Les matrices élémentaires sont donc très faciles à inverser.
</p>
</div>
</div>
<div id="outline-container-orgfe9e92c" class="outline-3">
<h3 id="orgfe9e92c"><span class="section-number-3">13.4.</span> Matrices élémentaires de transformation</h3>
<div class="outline-text-3" id="text-13-4">
<p>
Nous allons construire une matrice élémentaire qui transforme un vecteur  donné \(x \in \matrice(\corps,n,1)\) non nul en un autre vecteur donné \(y \in \matrice(\corps,n,1)\) de même taille.
</p>
</div>
<div id="outline-container-orgb7791bd" class="outline-4">
<h4 id="orgb7791bd"><span class="section-number-4">13.4.1.</span> Colonne</h4>
<div class="outline-text-4" id="text-13-4-1">
<p>
On cherche une matrice élémentaire \(E_{yx}\) telle que \(E_{yx} \cdot x = y\). L'équation :
</p>

<p>
\[(I + \alpha \cdot u \cdot v^\dual) \cdot x = x + \alpha \cdot u \cdot (v^\dual \cdot x) = y\]
</p>

<p>
nous donne la condition :
</p>

<p>
\[\alpha \cdot (v^\dual \cdot x) \cdot u = y - x\]
</p>

<p>
On peut donc choisir par exemple \(u = y - x\) et \(v = x\). On a alors \(\alpha = 1 / (x^\dual \cdot x) \ne 0\) et on se retrouve avec la matrice élémentaire :
</p>

<p>
\[E_{yx} = I + \unsur{x^\dual \cdot x} \cdot (y - x) \cdot x^\dual\]
</p>
</div>
</div>
<div id="outline-container-orgb19da34" class="outline-4">
<h4 id="orgb19da34"><span class="section-number-4">13.4.2.</span> Inverse</h4>
<div class="outline-text-4" id="text-13-4-2">
<p>
Si l'inverse existe, il s'agit d'une matrice élémentaire de paramètre scalaire :
</p>

<p>
\[\beta = - \unsur{x^\dual \cdot x + x^\dual \cdot (y - x)} = - \unsur{x^\dual \cdot y}\]
</p>

<p>
Sous réserve que \(x^\dual \cdot y \ne 0\), on a donc :
</p>

<p>
\[E_{yx}^{-1} = I - \unsur{x^\dual \cdot y} \cdot (y - x) \cdot x^\dual\]
</p>
</div>
</div>
<div id="outline-container-org6f42e2c" class="outline-4">
<h4 id="org6f42e2c"><span class="section-number-4">13.4.3.</span> Ligne</h4>
<div class="outline-text-4" id="text-13-4-3">
<p>
On cherche une matrice élémentaire \(E_{yx}\) telle que \(x^\dual \cdot E_{yx} = y^\dual\). L'équation :
</p>

<p>
\[x^\dual \cdot (I + \alpha \cdot u \cdot v^\dual) = x^\dual + \alpha \cdot (x^\dual \cdot u) \cdot v^\dual = y^\dual\]
</p>

<p>
nous donne la condition :
</p>

<p>
\[\alpha \cdot (x^\dual \cdot u) \cdot v^\dual = y^\dual - x^\dual\]
</p>

<p>
ou :
</p>

<p>
\[\conjaccent{\alpha} \cdot (u^\dual \cdot x) \cdot v = y - x\]
</p>

<p>
On peut donc choisir par exemple \(v = y - x\) et \(u = x\). On a alors \(\alpha = \conjaccent{\alpha} = 1 / (x^\dual \cdot x) \ne 0\) et on se retrouve avec la matrice élémentaire :
</p>

<p>
\[E_{yx} = I + \unsur{x^\dual \cdot x} \cdot x \cdot (y - x)^\dual\]
</p>
</div>
</div>
<div id="outline-container-orga24f4a1" class="outline-4">
<h4 id="orga24f4a1"><span class="section-number-4">13.4.4.</span> Inverse</h4>
<div class="outline-text-4" id="text-13-4-4">
<p>
Si l'inverse existe, il s'agit d'une matrice élémentaire de paramètre scalaire :
</p>

<p>
\[\beta = - \unsur{x^\dual \cdot x + (y - x)^\dual \cdot x} = - \unsur{y^\dual \cdot x}\]
</p>

<p>
Sous réserve que \(y^\dual \cdot x \ne 0\), on a donc :
</p>

<p>
\[E_{yx}^{-1} = I - \unsur{y^\dual \cdot x} \cdot x \cdot (y - x)^\dual\]
</p>
</div>
</div>
</div>
<div id="outline-container-org12ad16d" class="outline-3">
<h3 id="org12ad16d"><span class="section-number-3">13.5.</span> Matrices élémentaires de permutation</h3>
<div class="outline-text-3" id="text-13-5">
<p>
Les matrices élémentaires de permutations permettent de permuter deux lignes ou deux colonnes d'une matrice. Soit \(\canonique_1,...,\canonique_n\) les vecteurs de la base canonique de \(\corps^n\). La matrice de permutation élémentaire de taille \((n,n)\) et de paramètres \(i,j\) est définie par :
</p>

<p>
\[\matpermutation_{n,i,j} = I - (\canonique_i - \canonique_j) \cdot (\canonique_i - \canonique_j)^\dual\]
</p>

<p>
Dans la suite, nous considérons \(A \in \matrice(\corps,m,n)\) et \(P = \matpermutation_{n,i,j}\).
</p>
</div>
<div id="outline-container-orgef58e1c" class="outline-4">
<h4 id="orgef58e1c"><span class="section-number-4">13.5.1.</span> Permutation des colonnes</h4>
<div class="outline-text-4" id="text-13-5-1">
<p>
Soit les colonnes \(C_i = A \cdot \canonique_i\). On a :
</p>

<p>
\[A \cdot P = A - (C_i \cdot \canonique_i^\dual + C_j \cdot \canonique_j^\dual) + (C_j \cdot \canonique_i^\dual + C_i \cdot \canonique_j^\dual)\]
</p>

<p>
Les colonnes \(i\) et \(j\) de \(A\) sont donc permutées par multiplication à droite d'une matrice de permutation.
</p>
</div>
</div>
<div id="outline-container-orge40652f" class="outline-4">
<h4 id="orge40652f"><span class="section-number-4">13.5.2.</span> Permutation des lignes</h4>
<div class="outline-text-4" id="text-13-5-2">
<p>
Soit les lignes \(L_i = \canonique_i^\dual \cdot A\). On a :
</p>

<p>
\[P \cdot A = A - (\canonique_i \cdot L_i + \canonique_j \cdot L_j) + (\canonique_j \cdot L_i + \canonique_i \cdot L_j)\]
</p>

<p>
Les lignes \(i\) et \(j\) de \(A\) sont donc permutées par multiplication à gauche d'une matrice de permutation.
</p>
</div>
</div>
<div id="outline-container-orgc0fa5b2" class="outline-4">
<h4 id="orgc0fa5b2"><span class="section-number-4">13.5.3.</span> Symétrie</h4>
<div class="outline-text-4" id="text-13-5-3">
<p>
On constate que la transposée et la duale sont égales à la matrice elle-même :
</p>

<p>
\[\matpermutation_{n,i,j} = \matpermutation_{n,i,j}^T = \matpermutation_{n,i,j}^\dual\]
</p>
</div>
</div>
<div id="outline-container-org0d96691" class="outline-4">
<h4 id="org0d96691"><span class="section-number-4">13.5.4.</span> Inverse</h4>
<div class="outline-text-4" id="text-13-5-4">
<p>
Soit \(\Delta_{ij} = \canonique_i - \canonique_j\) et :
</p>

<p>
\[P = \matpermutation_{n,i,j} = I - \Delta_{ij} \cdot \Delta_{ij}^\dual\]
</p>

<p>
Le produit de cette matrice avec elle-même s'écrit :
</p>

<p>
\[P \cdot P = I - 2 \Delta_{ij} \cdot \Delta_{ij}^\dual + \Delta_{ij} \cdot \Delta_{ij}^\dual  \cdot \Delta_{ij} \cdot \Delta_{ij}^\dual\]
</p>

<p>
Mais comme \(\Delta_{ij}^\dual \cdot \Delta_{ij} = 2\), on a :
</p>

<p>
\[P \cdot P = I - 2 \Delta_{ij} \cdot \Delta_{ij}^\dual + 2 \Delta_{ij} \cdot \Delta_{ij}^\dual\]
</p>

<p>
Les deux derniers termes s'annihilent et :
</p>

<p>
\[P \cdot P = I\]
</p>

<p>
Les matrices de permutations élémentaires sont donc égales à leur propre inverse :
</p>

<p>
\[\matpermutation_{n,i,j} = \matpermutation_{n,i,j}^{-1}\]
</p>
</div>
</div>
</div>
<div id="outline-container-orgbc04f86" class="outline-3">
<h3 id="orgbc04f86"><span class="section-number-3">13.6.</span> Matrices de permutations</h3>
<div class="outline-text-3" id="text-13-6">
<p>
Une matrice de permutation \(P\) est une matrice de la forme :
</p>

<p>
\[P = P_1 \cdot ... \cdot P_n\]
</p>

<p>
où les \(P_i\) sont des matrices élémentaires de permutation.
</p>
</div>
<div id="outline-container-orgff52a8d" class="outline-4">
<h4 id="orgff52a8d"><span class="section-number-4">13.6.1.</span> Inverse</h4>
<div class="outline-text-4" id="text-13-6-1">
<div class="org-center">
<p>
\(
P^\dual \cdot P = P_n \cdot ... \cdot P_1 \cdot P_1 \cdot ... \cdot P_n = I \)
</p>

<p>
\(
P \cdot P^\dual = P_1 \cdot ... \cdot P_n \cdot P_n \cdot ... \cdot P_1 = I
\)
</p>
</div>

<p>
Donc \(P^\dual = P^{-1}\). Comme \(P^\dual = P^T\), la transposée d'une matrice de permutation est identique à son inverse. On dit que ces matrices sont orthogonales.
</p>
</div>
</div>
</div>
</div>
<div id="outline-container-org85034f8" class="outline-2">
<h2 id="org85034f8"><span class="section-number-2">14.</span> Systèmes linéaires et inverses</h2>
<div class="outline-text-2" id="text-14">
<div id="text-table-of-contents-14" role="doc-toc">
<ul>
<li><a href="#orgd5be695">14.1. Gauss-Jordan</a></li>
<li><a href="#orge8a728f">14.2. Systèmes linéaires</a></li>
<li><a href="#org1c14320">14.3. Matrices hautes</a></li>
<li><a href="#org4d13c62">14.4. Matrices longues</a></li>
<li><a href="#org66e0be7">14.5. Matrices carrées</a></li>
<li><a href="#org235d067">14.6. Complément de Schur</a></li>
<li><a href="#org0f1384a">14.7. Dimension</a></li>
<li><a href="#orgc28cc66">14.8. Indépendance linéaire</a></li>
</ul>
</div>

<p>
\label{chap:syslin}
</p>
</div>
<div id="outline-container-orgd5be695" class="outline-3">
<h3 id="orgd5be695"><span class="section-number-3">14.1.</span> Gauss-Jordan</h3>
<div class="outline-text-3" id="text-14-1">
<p>
Nous allons tenter de diagonaliser une matrice \(A \in \matrice(\corps,m,n)\) en une matrice diagonale aussi proche que possible de la matrice identité. Soit les \(\canonique_i\), vecteurs de la base canonique de \(\corps^m\) ou \(\corps^n\) suivant le contexte. Si \(A\) est non nulle, on peut trouver un \(i\) et un \(j\) tels que :
</p>

<p>
\[a = \composante_{ij} A \ne 0\]
</p>

<p>
On considère les matrices élémentaires de permutation \(P_0 = \matpermutation_{n,i,1}\) et \(Q_0 = \matpermutation_{n,j,1}\), et on évalue \(P_0 \cdot A \cdot Q_0\) pour inverser les lignes \(1\) et \(i\) et les colonnes \(1\) et \(j\) de \(A\). On se retrouve alors avec une matrice de la forme :
</p>

<p>
\[Q_0 \cdot A \cdot P_0 = [x \ c_2 \ ... \ c_n ]\]
</p>

<p>
où \(x\) est la première colonne et où \(x^\dual \cdot \canonique_1 = a \ne 0\). On utilise ensuite une première matrice élémentaire :
</p>

<p>
\[E_0 = I + \unsur{x^\dual \cdot x} \cdot (\canonique_1 - x) \cdot x^\dual\]
</p>

<p>
pour transformer la première colonne \(x\) en \(y = \canonique_1\). Partitionnant à part la première ligne et la première colonne du résultat, on a :
</p>

<div class="org-center">
<p>
\(
E<sub>0</sub> &sdot; P<sub>0</sub> &sdot; A &sdot; Q<sub>0</sub> =
</p>
\begin{Matrix}{cc}
1 & z^\dual \)

\(
0 & \ddots
\end{Matrix}
<p>
\)
</p>
</div>

<p>
Posons \(u^\dual = [1 \ z^\dual]\) pour la première ligne. On a \(\canonique_1^\dual \cdot u = 1 \ne 0\). On utilise une seconde matrice élémentaire :
</p>

<p>
\[F_0 = I + \unsur{u^\dual \cdot u} \cdot u \cdot (\canonique_1 - u)^\dual\]
</p>

<p>
pour transformer la première ligne \(u^\dual\) en \(v^\dual = \canonique_1^\dual\). Mais comme \((\canonique_1 - u)^\dual = [0 \ -z^\dual]\), on a :
</p>

<div class="org-center">
<p>
\(
u &sdot; (\canonique<sub>1</sub> - u)^\dual =
</p>
\begin{Matrix}{c}
1 \\ z
\end{Matrix}
<p>
&sdot;
</p>
\begin{Matrix}{cc}
0 & -z^\dual
\end{Matrix}
<p>
=
</p>
\begin{Matrix}{cc}
0 & -z^\dual \)

\(
0 & -z \cdot z^\dual
\end{Matrix}
<p>
\)
</p>
</div>

<p>
On voit aussi que \(u^\dual \cdot u = 1 + z^\dual \cdot z\). La matrice élémentaire \(F_0\) est donc de la forme :
</p>

<div class="org-center">
<p>
\(
F<sub>0</sub> = I + \unsur{1 + z^\dual \cdot z} &sdot;
</p>
\begin{Matrix}{cc}
0 & -z^\dual \)

\(
0 & -z \cdot z^\dual
\end{Matrix}
<p>
\)
</p>
</div>

<p>
On obtient alors une matrice modifiée de la forme :
</p>

<div class="org-center">
<p>
\(
E<sub>0</sub> &sdot; P<sub>0</sub> &sdot; A &sdot; Q<sub>0</sub> &sdot; F<sub>0</sub> =
</p>
\begin{Matrix}{cc}
1 & z^\dual \)

\(
0 & \ddots
\end{Matrix}
<ul class="org-ul">
<li>\unsur{1 + z^\dual \cdot z} &sdot;</li>
</ul>
\begin{Matrix}{cc}
1 & z^\dual \)

\(
0 & \ddots
\end{Matrix}
<p>
&sdot;
</p>
\begin{Matrix}{cc}
0 & -z^\dual \)

\(
0 & -z \cdot z^\dual
\end{Matrix}
<p>
\)
</p>
</div>

<p>
et finalement :
</p>

<div class="org-center">
<p>
\(
E<sub>0</sub> &sdot; P<sub>0</sub> &sdot; A &sdot; Q<sub>0</sub> &sdot; F<sub>0</sub> =
</p>
\begin{Matrix}{cc}
1 & 0 \)

\(
0 & A^{(n - 1)}
\end{Matrix}
<p>
\)
</p>
</div>

<p>
Recommençons le même procédé pour transformer, au moyen des matrices de permutation \(P^{(n-1)},Q^{(n-1)}\) et élémentaires \(E^{(n-1)},F^{(n-1)}\), la première colonne et la première ligne de \(A^{(n - 1)}\) en \(e_1\) et \(e_1^\dual\). Si on pose :
</p>

<div class="org-center">
<p>
\(
P<sub>1</sub> =
</p>
\begin{Matrix}{cc}
1 & 0 \)

\(
0 & P^{(n - 1)}
\end{Matrix}
<p>
=
</p>
\begin{Matrix}{cc}
I_1 & 0 \)

\(
0 & P^{(n - 1)}
\end{Matrix} \)

\(
Q_1 =
\begin{Matrix}{cc}
1 & 0 \)

\(
0 & Q^{(n - 1)}
\end{Matrix}
<p>
=
</p>
\begin{Matrix}{cc}
I_1 & 0 \)

\(
0 & Q^{(n - 1)}
\end{Matrix} \)

\(
E_1 =
\begin{Matrix}{cc}
1 & 0 \)

\(
0 & E^{(n - 1)}
\end{Matrix}
<p>
=
</p>
\begin{Matrix}{cc}
I_1 & 0 \)

\(
0 & E^{(n - 1)}
\end{Matrix} \)

\(
F_1 =
\begin{Matrix}{cc}
1 & 0 \)

\(
0 & F^{(n - 1)}
\end{Matrix}
<p>
=
</p>
\begin{Matrix}{cc}
I_1 & 0 \)

\(
0 & F^{(n - 1)}
\end{Matrix}
<p>
\)
</p>
</div>

<p>
il vient :
</p>

<div class="org-center">
<p>
\(
E<sub>1</sub> &sdot; P<sub>1</sub> &sdot; E<sub>0</sub> &sdot; P<sub>0</sub> &sdot; A &sdot; Q<sub>0</sub> &sdot; F<sub>0</sub> &sdot; Q<sub>1</sub> &sdot; F<sub>1</sub> =
</p>
\begin{Matrix}{ccc}
1 & 0 & 0 \)

\(
0 & 1 & 0 \)

\(
0 & 0 & A^{(n - 2)}
\end{Matrix}
<p>
\)
</p>
</div>

<p>
Soit \(p = \min\{m, n\}\). On peut répéter le même processus \(r\) fois en utilisant à l'étape \(k\) :
</p>

<div class="org-center">
<p>
\(
P<sub>k</sub> =
</p>
\begin{Matrix}{cc}
I_k & 0 \)

\(
0 & P^{(n - k)}
\end{Matrix} \)

\(
Q_k =
\begin{Matrix}{cc}
I_k & 0 \)

\(
0 & Q^{(n - k)}
\end{Matrix} \)

\(
E_k =
\begin{Matrix}{cc}
I_k & 0 \)

\(
0 & E^{(n - k)}
\end{Matrix} \)

\(
F_k =
\begin{Matrix}{cc}
I_k & 0 \)

\(
0 & F^{(n - k)}
\end{Matrix}
<p>
\)
</p>
</div>

<p>
jusqu'à ce que la matrice \(A^{(n - r)}\) soit nulle, ou jusqu'à ce qu'on ait atteint \(r = p\). Posons :
</p>

\begin{align}
E &= E_r \cdot P_r \cdot ... \cdot E_0 \cdot P_0 \)

\(
F &= Q_0 \cdot F_0 \cdot ... \cdot Q_r \cdot F_r
\end{align}

<p>
On a alors schématiquement :
</p>

<p>
\[E \cdot A \cdot F = C\]
</p>

<p>
où :
</p>

<div class="org-center">
<p>
\(
C &isin; \left\{
I<sub>p</sub> ,
</p>
\begin{Matrix}{c}
I_n \\ 0
\end{Matrix} ,
\begin{Matrix}{cc}
I_m & 0
\end{Matrix} ,
\begin{Matrix}{cc}
I_r & 0 \)

\(
0 & 0
\end{Matrix}
<p>
\right\}
\)
</p>
</div>

<p>
suivant que \(r = p = m = n\), \(r = p = n\), \(r = p = m\) ou \(r \strictinferieur p\).
</p>
</div>
<div id="outline-container-orgce065ef" class="outline-4">
<h4 id="orgce065ef"><span class="section-number-4">14.1.1.</span> Inverse</h4>
<div class="outline-text-4" id="text-14-1-1">
<p>
Les matrices élémentaires de permutation sont inversibles, d'inverse identique à elles-mêmes. On a donc \(P_i^{-1} = P_i\) et \(Q_i^{-1} = Q_i\). Les matrices élémentaires de transformation sont également inversibles. En effet, \(x^\dual \cdot y = x^\dual \cdot \canonique_1 = a \ne 0\). L'inverse de \(E_0\) existe donc et s'écrit :
</p>

<p>
\[E_0^{-1} = I - \unsur{a} \cdot (\canonique_1 - x) \cdot x^\dual\]
</p>

<p>
D'un autre coté \(v^\dual \cdot u = \canonique_1^\dual \cdot u = 1 \ne 0\). L'inverse de \(F_0\) existe aussi et s'écrit :
</p>

<p>
\[F_0^{-1} = I - u \cdot (\canonique_1 - u)^\dual\]
</p>

<p>
Comme on procède de même à chaque étape, l'inverse de chaque matrice élémentaire existe.  En appliquant la formule d'inversion d'un produit, on obtient :
</p>

\begin{align}
E^{-1} &= P_0 \cdot E_0^{-1} \cdot ... \cdot P_r \cdot E_r^{-1} \)

\(
F^{-1} &= F_r^{-1} \cdot Q_r \cdot ... \cdot F_0^{-1} \cdot Q_0
\end{align}

<p>
Posons \(L = E^{-1}\) et \(R = F^{-1}\). En multipliant l'équation \(E \cdot A \cdot F = C\) à gauche par \(L\) et à droite par \(R\), on obtient :
</p>

<p>
\[A = L \cdot C \cdot R\]
</p>

<p>
Une telle décomposition est appelée décomposition de Gauss-Jordan et notée :
</p>

<p>
\[(L,C,R) = \gaussjordan(A)\]
</p>
</div>
</div>
<div id="outline-container-orgd8fe89b" class="outline-4">
<h4 id="orgd8fe89b"><span class="section-number-4">14.1.2.</span> Rang</h4>
<div class="outline-text-4" id="text-14-1-2">
<p>
Le \(r\) ainsi obtenu est appelé rang de la matrice \(A\). On le note :
</p>

<p>
\[r = \rang A\]
</p>

<p>
On a par construction \(r \le p\).
</p>
</div>
</div>
</div>
<div id="outline-container-orge8a728f" class="outline-3">
<h3 id="orge8a728f"><span class="section-number-3">14.2.</span> Systèmes linéaires</h3>
<div class="outline-text-3" id="text-14-2">
<p>
Nous allons à présent utiliser la décomposition de Gauss-Jordan pour analyser les espaces de solutions :
</p>

<p>
\[S(y) = \{ x \in \corps^n : A \cdot x = y \}\]
</p>

<p>
pour tout \(y \in \corps^m\). On a :
</p>

<p>
\[A \cdot x = L \cdot C \cdot R \cdot x = y\]
</p>

<p>
Multiplions cette équation par \(L^{-1}\). Il vient :
</p>

<p>
\[C \cdot R \cdot x = L^{-1} \cdot y\]
</p>

<p>
On pose :
</p>

<div class="org-center">
<p>
\(
z = R \cdot x \)
</p>

<p>
\(
b = L^{-1} \cdot y
\)
</p>
</div>

<p>
Le système linéaire s'écrit alors :
</p>

<p>
\[C \cdot z = b\]
</p>
</div>
<div id="outline-container-orge6a7b98" class="outline-4">
<h4 id="orge6a7b98"><span class="section-number-4">14.2.1.</span> Plein rang, matrice carrée</h4>
<div class="outline-text-4" id="text-14-2-1">
<p>
Si \(r = m = n\), on a :
</p>

<p>
\[A = L \cdot I \cdot R = L \cdot R\]
</p>

<p>
La matrice \(A\) est un produit de matrices inversibles. Elle est donc inversible et :
</p>

<p>
\[A^{-1} = R^{-1} \cdot L^{-1}\]
</p>

<p>
L'équation \(A \cdot x = y\) admet donc pour unique solution \(x \in S(y)\) le vecteur :
</p>

<p>
\[x = A^{-1} \cdot y = R^{-1} \cdot L^{-1} \cdot y\]
</p>
</div>
</div>
<div id="outline-container-org16ad4da" class="outline-4">
<h4 id="org16ad4da"><span class="section-number-4">14.2.2.</span> Plein rang, matrice haute</h4>
<div class="outline-text-4" id="text-14-2-2">
<p>
Si \(r = n \strictinferieur m\), on a :
</p>

<div class="org-center">
<p>
\(
C =
</p>
\begin{Matrix}{c}
I_n \\ 0
\end{Matrix}
<p>
\)
</p>
</div>

<p>
Si on partitionne \(b = L^{-1} \cdot y\) en deux vecteurs \(b_1,b_2\) de tailles \((n,1)\) et \((m - n, 1)\), on a :
</p>

<div class="org-center">
<p>
\(
</p>
\begin{Matrix}{c}
I_n \\ 0
\end{Matrix}
<p>
&sdot;
z
=
</p>
\begin{Matrix}{c}
z \\ 0
\end{Matrix}
<p>
=
</p>
\begin{Matrix}{c}
b_1 \\ b_2
\end{Matrix}
<p>
\)
</p>
</div>

<p>
Il y a donc deux conditions pour que \(x = R^{-1} \cdot z\) soit dans \(S(y)\) :
</p>

<div class="org-center">
<p>
\(
z = b_1 \)
</p>

<p>
\(
0 = b_2
\)
</p>
</div>

<p>
Si \(b_2 \ne 0\), il n'existe pas de solution. Si \(b_2 = 0\), il existe une unique solution \(x \in S(y)\), qui s'écrit :
</p>

<p>
\[x = R^{-1} \cdot z = R^{-1} \cdot b_1\]
</p>

<p>
Remarquons que l'on peut toujours trouver un \(y\) tel qu'il existe au moins une solution. En effet, il suffit de choisir :
</p>

<div class="org-center">
<p>
\(
y = L &sdot;
</p>
\begin{Matrix}{c}
b_1 \\ 0
\end{Matrix}
<p>
\)
</p>
</div>

<p>
D'un autre coté, il existe toujours un \(y\) tel qu'il n'existe pas de solution. En effet, il suffit de choisir :
</p>

<div class="org-center">
<p>
\(
y = L &sdot;
</p>
\begin{Matrix}{c}
b_1 \\ b_2
\end{Matrix}
<p>
\)
</p>
</div>

<p>
où \(b_2 \ne 0\).
</p>
</div>
</div>
<div id="outline-container-orgb923449" class="outline-4">
<h4 id="orgb923449"><span class="section-number-4">14.2.3.</span> Plein rang, matrice longue</h4>
<div class="outline-text-4" id="text-14-2-3">
<p>
Si \(r = m \strictinferieur n\), on se retrouve alors avec :
</p>

<div class="org-center">
<p>
\(
C =
</p>
\begin{Matrix}{cc}
I_m & 0
\end{Matrix}
<p>
\)
</p>
</div>

<p>
Si on partitionne \(z\) en deux vecteurs \(z_1,z_2\) de tailles \((m,1)\) et \((n - m,1)\), on a :
</p>

<div class="org-center">
<p>
\(
C &sdot; z =
</p>
\begin{Matrix}{cc}
I_m & 0
\end{Matrix}
<p>
&sdot;
</p>
\begin{Matrix}{c}
z_1 \\ z_2
\end{Matrix}
<p>
=
I<sub>m</sub> &sdot; z<sub>1</sub> + 0 &sdot; z<sub>2</sub> = z<sub>1</sub>
\)
</p>
</div>

<p>
La condition pour que \(x = R^{-1} \cdot z\) soit dans \(S(y)\) se résume à :
</p>

<p>
\[z_1 = b = L^{-1} \cdot y\]
</p>

<p>
Nous n'avons par contre aucune condition sur \(z_2 \in \matrice(\corps, n - m)\). Il y a donc une infinité de solutions \(x \in S(y)\), de la forme :
</p>

<div class="org-center">
<p>
\(
x =R<sup>-1</sup> &sdot; z = R<sup>-1</sup> &sdot;
</p>
\begin{Matrix}{c}
L^{-1} \cdot y \\ z_2
\end{Matrix}
<p>
\)
</p>
</div>
</div>
</div>
<div id="outline-container-org976ec6a" class="outline-4">
<h4 id="org976ec6a"><span class="section-number-4">14.2.4.</span> Rang incomplet</h4>
<div class="outline-text-4" id="text-14-2-4">
<p>
Supposons que \(r \strictinferieur p\). On partitionne alors \(z\) en deux vecteurs \(z_1\) et \(z_2\) de tailles \((r,1)\) et \((n - r, 1)\) et \(b\) en deux vecteurs \(b_1\) et \(b_2\) de tailles \((r,1)\) et \((m - r,1)\). Avec ces notations, le produit \(C \cdot z\) s'écrit :
</p>

<div class="org-center">
<p>
\(
C &sdot; z =
</p>
\begin{Matrix}{cc}
I_r & 0 \)

\(
0 & 0
\end{Matrix}
<p>
&sdot;
</p>
\begin{Matrix}{c}
z_1 \\ z_2
\end{Matrix}
<p>
=
</p>
\begin{Matrix}{c}
z_1 \\ 0
\end{Matrix}
<p>
\)
</p>
</div>

<p>
L'équation \(C \cdot z = b\) prend donc la forme :
</p>

<div class="org-center">
<p>
\(
</p>
\begin{Matrix}{c}
z_1 \\ 0
\end{Matrix}
<p>
=
</p>
\begin{Matrix}{c}
b_1 \\ b_2
\end{Matrix}
<p>
\)
</p>
</div>

<p>
Les deux conditions pour que \(x = R^{-1} \cdot z\) soit dans \(S(y)\) sont donc que \(z_1 = b_1\) et que \(b_2 = 0\). Il n'y a aucune condition sur \(z_2\). Si \(b_2 \ne 0\) il n'y a pas de solution. Si \(b_2 = 0\), il existe une infinité de solutions \(x \in S(y)\) de la forme :
</p>

<div class="org-center">
<p>
\(
x = R<sup>-1</sup> &sdot;
</p>
\begin{Matrix}{c}
b_1 \\ z_2
\end{Matrix}
<p>
\)
</p>
</div>

<p>
Remarquons que l'on peut toujours trouver un \(y\) tel qu'il existe au moins une solution. En effet, il suffit de choisir :
</p>

<div class="org-center">
<p>
\(
y = L &sdot;
</p>
\begin{Matrix}{c}
b_1 \\ 0
\end{Matrix}
<p>
\)
</p>
</div>

<p>
Un choix particulier est par exemple \(y = b = 0\).
</p>

<p>
D'un autre coté, il existe toujours un \(y\) tel qu'il n'existe pas de solution. En effet, il suffit de choisir :
</p>

<div class="org-center">
<p>
\(
y = L &sdot;
</p>
\begin{Matrix}{c}
b_1 \\ b_2
\end{Matrix}
<p>
\)
</p>
</div>

<p>
où \(b_2 \ne 0\).
</p>
</div>
</div>
<div id="outline-container-org2ed61fb" class="outline-4">
<h4 id="org2ed61fb"><span class="section-number-4">14.2.5.</span> Rang et existence</h4>
<div class="outline-text-4" id="text-14-2-5">
<p>
On conclut de ce qui précède que si \(r \strictinferieur m\) ou \(r \strictinferieur p\), on peut toujours trouver un \(y\) tel qu'il n'existe pas de solution. Pour qu'il existe au moins une solution du système quel que soit \(y\), il faut donc avoir \(r = m = p\).
</p>
</div>
</div>
<div id="outline-container-org76821cd" class="outline-4">
<h4 id="org76821cd"><span class="section-number-4">14.2.6.</span> Rang et unicité</h4>
<div class="outline-text-4" id="text-14-2-6">
<p>
On conclut de ce qui précède que si \(r \strictinferieur n\) ou \(r \strictinferieur p\), on peut toujours trouver un \(y\) tel qu'il existe une infinité de solution, et même une infinité de solutions non nulles puisque \(z_2 \ne 0\) peut être quelconque.
</p>

<p>
Pour qu'il existe au maximum une solution du système quel que soit \(y\), il faut donc avoir \(r = n = p\).
</p>
</div>
</div>
</div>
<div id="outline-container-org1c14320" class="outline-3">
<h3 id="org1c14320"><span class="section-number-3">14.3.</span> Matrices hautes</h3>
<div class="outline-text-3" id="text-14-3">
<p>
Soit une matrice \(A \in \matrice(\corps,m,n)\) strictement haute (\(m \strictsuperieur n\)). Un inverse à droite \(R\) vérifiant \(A \cdot B = I\) ne peut pas exister, car sinon il suffirait de prendre \(x = B \cdot y\) pour avoir :
</p>

<p>
\[A \cdot x = A \cdot B \cdot y = I \cdot y = y\]
</p>

<p>
Le système \(A \cdot x = y\) admettrait toujours au moins une solution, ce qui contredit les résultats précédents.
</p>
</div>
</div>
<div id="outline-container-org4d13c62" class="outline-3">
<h3 id="org4d13c62"><span class="section-number-3">14.4.</span> Matrices longues</h3>
<div class="outline-text-3" id="text-14-4">
<p>
Soit une matrice \(A \in \matrice(\corps,m,n)\) strictement longue (\(n \strictsuperieur m\)). Un inverse à gauche \(B\) vérifiant \(B \cdot A = I\) ne peut pas exister, car sinon on aurait :
</p>

<p>
\[(B \cdot A)^\dual = A^\dual \cdot B^\dual = I^\dual = I\]
</p>

<p>
La matrice \(B^\dual\) serait donc un inverse à droite de la matrice \(A^\dual\). Or, \(A^\dual\) est de taille \((n,m)\), donc strictement haute, et ne peut pas admettre d'inverse à droite. On en conclut qu'une matrice strictement longue ne peut pas admettre d'inverse à gauche.
</p>
</div>
</div>
<div id="outline-container-org66e0be7" class="outline-3">
<h3 id="org66e0be7"><span class="section-number-3">14.5.</span> Matrices carrées</h3>
<div class="outline-text-3" id="text-14-5">
<p>
Supposons que \(m = n\) et considérons deux matrices carrées \(A,B \in \matrice(\corps,n,n)\) telles que :
</p>

<p>
\[A \cdot B = I\]
</p>

<p>
Il existe au moins une solution du système \(A \cdot x = y\) quel que soit \(y\), car il suffit de prendre \(x = B \cdot y\) pour avoir \(A \cdot x = A \cdot B \cdot y = y\). On a donc forcément \(r = m \le n\). Mais comme \(m = n\), cette solution est unique. On en déduit que l'application linéaire associée à \(A\)
est inversible, donc \(A\) aussi et :
</p>

<p>
\[A \cdot (B - A^{-1}) = I - I = 0\]
</p>

<p>
En multipliant à gauche par \(A^{-1}\), il vient simplement \(B - A^{-1} = 0\), c'est-à-dire \(B = A^{-1}\). On a donc également :
</p>

<p>
\[B \cdot A = A^{-1} \cdot A = I\]
</p>
</div>
</div>
<div id="outline-container-org235d067" class="outline-3">
<h3 id="org235d067"><span class="section-number-3">14.6.</span> Complément de Schur</h3>
<div class="outline-text-3" id="text-14-6">
<p>
Soit le système suivant :
</p>

<div class="org-center">
<p>
\(
</p>
\begin{Matrix}{cc}
A & B \\ C & D
\end{Matrix}
<p>
&sdot;
</p>
\begin{Matrix}{c}
x & y
\end{Matrix}
<p>
=
</p>
\begin{Matrix}{c}
F \\ G
\end{Matrix}
<p>
\)
</p>
</div>

<p>
où \(A,B,C,D,F,G\) et \(x,y\) sont respectivement des matrices et des vecteurs matriciels de tailles compatibles. On a :
</p>

<div class="org-center">
<p>
\(
A \cdot x + B \cdot y = F \)
</p>

<p>
\(
C \cdot x + D \cdot y = G
\)
</p>
</div>

<p>
Si \(A\) est carrée et inversible, la première relation nous permet d'éliminer \(x\) :
</p>

<p>
\[x = A^{-1} \cdot F - A^{-1} \cdot B \cdot y\]
</p>

<p>
On substitue alors dans la seconde relation, et on obtient :
</p>

<p>
\[C \cdot A^{-1} \cdot F - C \cdot A^{-1} \cdot B \cdot y + D \cdot y = G\]
</p>

<p>
En plaçant \(y\) en évidence, on obtient :
</p>

<p>
\[(D - C \cdot A^{-1} \cdot B) \cdot y = G - C \cdot A^{-1} \cdot F\]
</p>

<p>
Sous réserve d'inversibilité, il ne nous reste alors plus qu'à résoudre par rapport à \(y\) :
</p>

<p>
\[y = (D - C \cdot A^{-1} \cdot B)^{-1} \cdot (G - C \cdot A^{-1} \cdot F)\]
</p>

<p>
qui nous donne ensuite :
</p>

<p>
\[x = A^{-1} \cdot [ F - B \cdot (D - C \cdot A^{-1} \cdot B)^{-1} \cdot (G - C \cdot A^{-1} \cdot F) ]\]
</p>

<p>
Si \(A\) est facilement inversible, et si \(D - C \cdot A^{-1} \cdot B\) est de taille réduite comparativement à la taille de \(A\), il peut être avantageux de résoudre le système global en \((x,y)\) de cette façon.
</p>
</div>
</div>
<div id="outline-container-org0f1384a" class="outline-3">
<h3 id="org0f1384a"><span class="section-number-3">14.7.</span> Dimension</h3>
<div class="outline-text-3" id="text-14-7">
<p>
Soit un espace vectoriel \(E\) sur \(\corps\) possédant deux bases \((e_1,e_2,...,e_n)\) et \((f_1,f_2,...,f_m)\). Comme les \(f_i \in E\), on a :
</p>

<p>
\[f_i = \sum_{k = 1}^n a_{ik} \cdot e_k\]
</p>

<p>
pour certains \(a_{ij} \in \corps\). Comme les \(e_k \in E\), on a également :
</p>

<p>
\[e_k = \sum_{j = 1}^m b_{kj} \cdot f_j\]
</p>

<p>
pour certains \(b_{kj} \in \corps\). On en conclut que :
</p>

<p>
\[f_i = \sum_{j = 1}^m \sum_{k = 1}^n a_{ik} \cdot b_{kj} \cdot f_j\]
</p>

<p>
Par indépendance linéaire des \(f_i\), on doit donc avoir :
</p>

<p>
\[\sum_{k = 1}^m a_{ik} \cdot b_{kj} = \indicatrice_{ij}\]
</p>

<p>
Si nous introduisons les matrices \(A \in \matrice(\corps,m,n)\) et \(B \in \matrice(\corps,n,m)\) de composantes \(a_{ij}\) et \(b_{ij}\) respectivement, on a donc \(A \cdot B = I_m\). La matrice \(A\) admettant un inverse à droite, elle ne peut pas être strictement haute et on a \(m \le n\). Mais on a aussi :
</p>

<p>
\[e_k = \sum_{k = 1}^n \sum_{j = 1}^m b_{kj} \cdot a_{ji} \cdot e_i\]
</p>

<p>
d'où l'on conclut que \(B \cdot A = I_n\). La matrice \(A\) admettant un inverse à gauche, elle ne peut pas être strictement longue et on a également \(n \le m\). On conclut de ces deux inégalités que \(m = n\). Si on possède une base de \(E\) comptant \(n\) vecteurs, on peut-être sûr que toute autre base de \(E\) comptera également \(n\) vecteurs. On dit que \(m = n\) est la dimension de \(E\) et on le note :
</p>

<p>
\[\dim E = n\]
</p>
</div>
<div id="outline-container-orgf03c3e8" class="outline-4">
<h4 id="orgf03c3e8"><span class="section-number-4">14.7.1.</span> Corollaire</h4>
<div class="outline-text-4" id="text-14-7-1">
<p>
La base canonique de \(\corps^n\) comptant \(n\) éléments,
on en déduit que \(\corps^n\) est de dimension \(n\).
</p>
</div>
</div>
</div>
<div id="outline-container-orgc28cc66" class="outline-3">
<h3 id="orgc28cc66"><span class="section-number-3">14.8.</span> Indépendance linéaire</h3>
<div class="outline-text-3" id="text-14-8">
<p>
Soit un espace vectoriel \(E\) de dimension \(m\) et une suite de vecteurs linéairement indépendants \((u_1,...,u_n)\) de \(E\). Si \((e_1,...,e_m)\) est une base de \(E\), on peut trouver des coordonnées \(a_{ki} \in \corps\) telles que :
</p>

<p>
\[u_i = \sum_{k = 1}^m a_{ki} \cdot e_k\]
</p>

<p>
Pour tout \(w \in E\), on sait aussi que l'on peut trouver des coordonnées \(y_k \in \corps\) telles que :
</p>

<p>
\[w = \sum_{k = 1}^m y_k \cdot e_k\]
</p>

<p>
Nous allons à présent examiner si on peut également trouver des coordonnées \(x_i \in \corps\) de \(w\) par rapport aux \(u_i\) :
</p>

<p>
\[w = \sum_{i = 1}^n x_i \cdot u_i\]
</p>

<p>
Si cette hypothèse est vérifiée, on doit avoir :
</p>

<p>
\[\sum_{k = 1}^m y_k \cdot e_k = w = \sum_{k = 1}^m \sum_{i = 1}^n a_{ki} \cdot x_i \cdot e_k\]
</p>

<p>
L'indépendance linéaire des \(e_k\) implique alors que :
</p>

<p>
\[y_k = \sum_{i = 1}^n a_{ki} \cdot x_i\]
</p>

<p>
Utilisant les vecteurs matriciels associés \(x = [x_1 \ ... \ x_n]^\dual\) et \(y = [y_1 \ ... \ y_m]^\dual\) ainsi que la matrice \(A = (a_{ki})_{k,i}\) de taille \((m,n)\), on se retrouve avec le système :
</p>

<p>
\[y = A \cdot x\]
</p>

<p>
Soit \(r\) le rang de \(A\). On sait déjà que \(r \le \min \{m,n\}\). Examinons les différents cas :
</p>

<ul class="org-ul">
<li>Si on avait \(r \strictinferieur n\), on pourrait trouver un \(y\) tel qu'il existe une infinité de solution en \(x\), ce qui contredit l'unicité des coordonnées de \(w\) par rapport à la suite de vecteurs linéairement indépendant \((u_1,...,u_n)\). On doit donc avoir \(r = n \le m\).</li>

<li>Dans le cas où \(r = n \strictinferieur m\), on peut trouver un \(y\) tel que la solution n'existe pas : la suite de \(u_i\) ne forme donc pas une base de \(E\).</li>

<li>Dans le cas où \(r = n = m\), il existe toujours une unique solution, la suite des \(u_i\) forme alors une base de \(E\). Il suffit donc de trouver une suite de \(m\) vecteurs indépendants dans un espace de dimension \(m\) pour former une base de cet espace.</li>
</ul>
</div>
</div>
</div>
<div id="outline-container-org3ed17ed" class="outline-2">
<h2 id="org3ed17ed"><span class="section-number-2">15.</span> Matrices unitaires</h2>
<div class="outline-text-2" id="text-15">
<div id="text-table-of-contents-15" role="doc-toc">
<ul>
<li><a href="#org995f909">15.1. Conservation du produit scalaire</a></li>
<li><a href="#orgbab259e">15.2. Matrices élémentaires unitaires</a></li>
<li><a href="#org3bb660f">15.3. Matrices de Householder</a></li>
<li><a href="#orgec4a3a0">15.4. Décomposition de Householder</a></li>
</ul>
</div>

<p>
\label{chap:unitaire}
</p>
</div>
<div id="outline-container-org995f909" class="outline-3">
<h3 id="org995f909"><span class="section-number-3">15.1.</span> Conservation du produit scalaire</h3>
<div class="outline-text-3" id="text-15-1">
<p>
Les matrices unitaires sont une généralisation des rotations. Or, la propriété essentielle d'une rotation est qu'elle conserve les distances. Comme les distances usuelles découlent des normes usuelles, elles-mêmes dérivées du produit scalaire, on impose que ce soit le produit scalaire qui soit conservé. On veut donc que :
</p>

<p>
\[\scalaire{U \cdot x}{U \cdot y} = x^\dual \cdot U^\dual \cdot U \cdot y = \scalaire{x}{y} = x^\dual \cdot y\]
</p>

<p>
pour toute matrice unitaire \(U\) de taille \((m,n)\) et tout \(x,y \in \corps^n\). Comme cette relation doit être valable pour tout \(x,y\), elle doit l'être pour les vecteurs de la base canonique :
</p>

<p>
\[\composante_{ij} U = \canonique_i^\dual \cdot U^\dual \cdot U \cdot \canonique_j = \canonique_i^\dual \cdot \canonique_j = \indicatrice_{ij}\]
</p>

<p>
On en conclut que :
</p>

<p>
\[U^\dual \cdot U = I\]
</p>

<p>
Si cette condition est vérifiée, on dit que \(U\) est une matrice unitaire.
</p>
</div>
<div id="outline-container-org0f5fe46" class="outline-4">
<h4 id="org0f5fe46"><span class="section-number-4">15.1.1.</span> Norme</h4>
<div class="outline-text-4" id="text-15-1-1">
<p>
La conservation de la norme usuelle \(\norme{x} = \sqrt{\scalaire{x}{x} }\) découle de celle du produit scalaire. Si \(U\) est unitaire, on a donc \(\norme{U \cdot x} = \norme{x}\).
</p>
</div>
</div>
</div>
<div id="outline-container-orgbab259e" class="outline-3">
<h3 id="orgbab259e"><span class="section-number-3">15.2.</span> Matrices élémentaires unitaires</h3>
<div class="outline-text-3" id="text-15-2">
<p>
Soit la matrice élémentaire :
</p>

<p>
\[U = \matelementaire(\alpha,u,u) = I + \alpha \cdot u \cdot u^\dual\]
</p>

<p>
où \(\alpha \in \setR\). Si on veut que \(U\) soit unitaire, il faut que :
</p>

\begin{align}
I = U^\dual \cdot U &= (I + \alpha \cdot u \cdot u^\dual) \cdot (I + \alpha \cdot u \cdot u^\dual) \)

\(
&= I + [2 \cdot \alpha + \alpha^2 \cdot (u^\dual \cdot u)] \cdot u \cdot u^\dual
\end{align}

<p>
Il suffit donc que \(\alpha\) vérifie la condition :
</p>

<p>
\[2 \cdot \alpha + \alpha^2 \cdot (u^\dual \cdot u) = \alpha \cdot [2 + \alpha \cdot (u^\dual \cdot u)] = 0\]
</p>

<p>
Si on choisit \(\alpha = 0\), on a \(U = I\) qui est bien une matrice unitaire. Dans le cas où \(\alpha \ne 0\), on doit avoir :
</p>

<p>
\[2 + \alpha \cdot (u^\dual \cdot u) = 0\]
</p>

<p>
Si \(u\) est un vecteur non nul, on aura bien sûr \(u^\dual \cdot u \strictsuperieur 0\). Il suffit alors de prendre :
</p>

<p>
\[\alpha = - \frac{2}{u^\dual \cdot u}\]
</p>

<p>
On se retrouve donc avec des matrices élémentaires unitaires du type :
</p>

<p>
\[U = I - \frac{2}{u^\dual \cdot u} \cdot u \cdot u^\dual\]
</p>

<p>
On les note :
</p>

<p>
\[\matunitaire(u) = I - \frac{2}{u^\dual \cdot u} \cdot u \cdot u^\dual\]
</p>
</div>
<div id="outline-container-org6e015b8" class="outline-4">
<h4 id="org6e015b8"><span class="section-number-4">15.2.1.</span> Propriétés</h4>
<div class="outline-text-4" id="text-15-2-1">
<p>
On a donc \(U^\dual \cdot U = I\). Comme \(U\) est carrée, on a également \(U^{-1} = U^\dual\). On voit également que :
</p>

<p>
\[U^\dual = I - \frac{2}{u^\dual \cdot u} \cdot u \cdot u^\dual = U\]
</p>

<p>
On en conclut que \(U^{-1} = U\).
</p>
</div>
</div>
</div>
<div id="outline-container-org3bb660f" class="outline-3">
<h3 id="org3bb660f"><span class="section-number-3">15.3.</span> Matrices de Householder</h3>
<div class="outline-text-3" id="text-15-3">
<p>
Soit deux vecteurs \(x,y \in \corps^n\). On aimerait bien trouver une matrice élémentaire unitaire \(U\) telle que \(U \cdot x \approx y\). Par analogie avec les matrices de transformation élémentaire, on pose \(u = x - y\) et \(U = \matunitaire(u)\). On a alors :
</p>

\begin{align}
U \cdot x &= x - \frac{2 u^\dual \cdot x}{u^\dual \cdot u} \cdot u \\ \)

\(
&= \frac{ (u^\dual \cdot u^\dual) \cdot x - 2 (u^\dual \cdot x) \cdot x + 2 (u^\dual \cdot x) \cdot y }{u^\dual \cdot u^\dual} \\ \)

\(
&= \frac{ (u^\dual \cdot u^\dual - 2 u^\dual \cdot x) \cdot x + 2 (u^\dual \cdot x) \cdot y }{u^\dual \cdot u^\dual}
\end{align}

<p>
Développons le coefficient de \(x\) :
</p>

<p>
\[u^\dual \cdot u^\dual - 2 u^\dual \cdot x = (x^\dual \cdot x - x^\dual \cdot y - y^\dual \cdot x + y^\dual \cdot y) - (2 x^\dual \cdot x - 2 y^\dual \cdot x)\]
</p>

<p>
Comme il s'agit d'une transformation unitaire, il est logique de demander que le produit scalaire soit conservé. On a donc \(x^\dual \cdot x = y^\dual \cdot y\). Si on impose de plus que \(x^\dual \cdot y\) soit réel, on a :
</p>

<p>
\[x^\dual \cdot y = \conjugue(x^\dual \cdot y) = y^\dual \cdot x\]
</p>

<p>
et :
</p>

<p>
\[u^\dual \cdot u^\dual - 2 u^\dual \cdot x = (2 x^\dual \cdot x - 2 y^\dual \cdot x) - (2 x^\dual \cdot x - 2 y^\dual \cdot x) = 0\]
</p>

<p>
On a alors :
</p>

<p>
\[U \cdot x = \gamma \cdot y\]
</p>

<p>
où :
</p>

<p>
\[\gamma = \frac{2 u^\dual \cdot x}{u^\dual \cdot u^\dual} \in \setC\]
</p>

<p>
Le vecteur \(U \cdot x\) est donc identique à \(y\) à un scalaire près.
</p>
</div>
<div id="outline-container-orge36dd00" class="outline-4">
<h4 id="orge36dd00"><span class="section-number-4">15.3.1.</span> Base canonique</h4>
<div class="outline-text-4" id="text-15-3-1">
<p>
Un cas particulier intéressant est celui où \(y\) est proportionnel au \(i^{eme}\) vecteur de la base canonique :
</p>

<p>
\[y = \alpha \cdot \canonique_i\]
</p>

<p>
avec \(\alpha \in \setC\). Si \(x_i = \composante_i x\), on a alors :
</p>

<p>
\[y^\dual \cdot x = \conjaccent{\alpha} \cdot x_i\]
</p>

<ul class="org-ul">
<li>Si \(x_i = 0\), on a \(y^\dual \cdot x = 0 \in \setR\). Il suffit alors de choisir \(\alpha\) pour que le produit scalaire soit conservé :</li>
</ul>

<p>
\[y^\dual \cdot y = \abs{\alpha}^2 = x^\dual \cdot x\]
</p>

<p>
On peut donc prendre :
</p>

<p>
\[\alpha = \sqrt{x^\dual \cdot x} \in \setR\]
</p>

<ul class="org-ul">
<li>Considérons à présent le cas où \(x_i \ne 0\). Si on prend \(\alpha = \lambda \cdot x_i\) où \(\lambda \in \setR\) est quelconque, on a :</li>
</ul>

<p>
\[y^\dual \cdot x = \lambda \cdot \conjaccent{x}_i \cdot x_i = \lambda \cdot \abs{x_i}^2 \in \setR\]
</p>

<p>
Comme on exige que la norme soit conservée, il faut de plus que :
</p>

<p>
\[y^\dual \cdot y = \lambda^2 \cdot \abs{x_i}^2 = x^\dual \cdot x\]
</p>

<p>
On doit donc avoir \(\lambda^2 = x^\dual \cdot x / \abs{x_i}^2\) et :
</p>

<p>
\[\alpha = \lambda \cdot x_i = \sqrt{ \frac{ x^\dual \cdot x }{ \abs{x_i}^2 } } \cdot x_i\]
</p>


<p>
Il ne nous reste alors plus qu'à poser \(u = x - \alpha \cdot \canonique_i\) et \(U = \matunitaire(u)\) pour obtenir :
</p>

<p>
\[U \cdot x = \gamma \cdot \canonique_i\]
</p>

<p>
pour un certain \(\gamma \in \setC\).
</p>
</div>
</div>
</div>
<div id="outline-container-orgec4a3a0" class="outline-3">
<h3 id="orgec4a3a0"><span class="section-number-3">15.4.</span> Décomposition de Householder</h3>
<div class="outline-text-3" id="text-15-4">
<p>
Soit une matrice \(A\) de taille \((m,n)\) et le partitionnement en colonnes :
</p>

<p>
\[A = [x \ x_2 \ ... \ x_n]\]
</p>

<p>
Soit \(H_1\) la matrice de Householder qui permet de transformer \(x\) en \(\gamma \cdot \canonique_1\), pour un certain \(\gamma_1 \in \setC\). On a alors :
</p>

<div class="org-center">
<p>
\(
H<sub>1</sub> &sdot; A =
</p>
\begin{Matrix}{cc}
\gamma_1 & \hdots \)

\(
0 & A^{(n - 1)}
\end{Matrix}
<p>
\)
</p>
</div>

<p>
On peut répéter le même processus avec \(A^{(n-1)}\) et la matrice de Householder \(H^{(n-1)}\) correspondante. Si on pose alors :
</p>

<div class="org-center">
<p>
\(
H<sub>2</sub> =
</p>
\begin{Matrix}{cc}
1 & 0 \)

\(
0 & H^{(n-1)}
\end{Matrix}
<p>
=
</p>
\begin{Matrix}{cc}
I_1 & 0 \)

\(
0 & H^{(n-1)}
\end{Matrix}
<p>
\)
</p>
</div>

<p>
il vient :
</p>

<div class="org-center">
<p>
\(
H<sub>2</sub> &sdot; H<sub>1</sub> &sdot; A =
</p>
\begin{Matrix}{ccc}
\gamma_1 & \hdots & \hdots \)

\(
0 & \gamma_2 & \hdots \)

\(
0 & 0 & H^{(n-2)}
\end{Matrix}
<p>
\)
</p>
</div>

<p>
On peut répéter le processus \(p = \min\{m,n\}\) fois en utilisant à l'étape \(k + 1\) :
</p>

<div class="org-center">
<p>
\(
H<sub>k + 1</sub> =
</p>
\begin{Matrix}{cc}
I_k & 0 \)

\(
0 & H^{(n-k)}
\end{Matrix}
<p>
\)
</p>
</div>

<p>
Posons :
</p>

<p>
\[H = H_p \cdot ... \cdot H_1\]
</p>

<p>
Si \(m \strictinferieur n\), on obtient à la fin du processus :
</p>

<div class="org-center">
<p>
\(
H &sdot; A = R =
</p>
\begin{Matrix}{cccc}
\gamma_1 & \hdots & \hdots & \hdots \)

\(
0 & \ddots & \hdots & \hdots \)

\(
0 & 0 & \gamma_m & \hdots
\end{Matrix}
<p>
\)
</p>
</div>

<p>
Si \(m = n\), on obtient à la fin du processus :
</p>

<div class="org-center">
<p>
\(
H &sdot; A = R =
</p>
\begin{Matrix}{ccc}
\gamma_1 & \hdots & \hdots \)

\(
0 & \ddots & \hdots \)

\(
0 & 0 & \gamma_m
\end{Matrix}
<p>
\)
</p>
</div>

<p>
Si \(m \strictsuperieur n\), on obtient à la fin du processus :
</p>

<div class="org-center">
<p>
\(
H &sdot; A = R =
</p>
\begin{Matrix}{ccc}
\gamma_1 & \hdots & \hdots \)

\(
0 & \ddots & \hdots \)

\(
0 & 0 & \gamma_m \)

\(
0 & 0 & 0
\end{Matrix}
<p>
\)
</p>
</div>

<p>
On voit que dans tous les cas la matrice \(R\) est triangulaire supérieure. Posons :
</p>

<p>
\[Q = H^{-1} = H_1 \cdot ... \cdot H_p = H^\dual\]
</p>

<p>
On a alors la décomposition :
</p>

<p>
\[A = Q \cdot R\]
</p>

<p>
où \(Q\) est une matrice unitaire et \(R\) une matrice triangulaire supérieure. On note cette décomposition :
</p>

<p>
\[(Q,R) = \householder(A)\]
</p>
</div>
</div>
</div>
</div>
<div id="postamble" class="status">
<p class="author">Auteur: chimay</p>
<p class="date">Created: 2025-10-19 dim 10:53</p>
<p class="validation"><a href="https://validator.w3.org/check?uri=referer">Validate</a></p>
</div>
</body>
</html>
