
#+STARTUP: showall

#+TITLE: Eclats de vers : Matemat 06 : Vecteurs
#+AUTHOR: chimay
#+EMAIL: or du val chez gé courriel commercial
#+LANGUAGE: fr
#+LINK_HOME: file:../index.html
#+LINK_UP: file:index.html
#+HTML_HEAD: <link rel="stylesheet" type="text/css" href="../style/defaut.css" />

#+OPTIONS: H:6
#+OPTIONS: toc:nil

#+TAGS: noexport(n)

[[file:index.org][Index des Grimoires]]

#+INCLUDE: "../include/navigan-1.org"

#+INCLUDE: "../include/latex/latex.org"

#+TOC: headlines 1

#+INCLUDE: "../include/latex/latex.org"

* Espaces vectoriels

#+TOC: headlines 1 local

\label{chap:vecteur}


** Dépendances

  - Chapitre \ref{chap:algebre} : Les structures algébriques
  - Chapitre \ref{chap:somme} : Les sommes


** Introduction

Soit un corps $\corps$, un ensemble quelconque $A$ et $n \in \setN$. Le but des espaces vectoriels est de fournir un cadre général aux $n$-tuples de $\corps^n$ et aux fonctions de $\corps^A$. Nous avons vu la correspondance $\corps^n \leftrightarrow \corps^A$ dans le cas particulier où $A$ possède un nombre fini d'éléments. Mais le lien entre les deux types d'objets ne s'arrête pas là : la comparaison de deux fonctions se base sur le même principe (étendu) que la comparaison de deux $n$-tuples. Nous avons également défini des produits mixtes $\cdot : \corps \times \corps^n \to \corps^n$ et $\cdot : \corps \times \corps^A \to \corps^A$ semblables. Les matrices représentant des applications linéaires, nous pouvons également les ajouter dans la liste. Si $u,v \in E$, avec $E \in \{ \corps^n , \corps^A , \matrice(\corps,m,n) \}$, on a :

#+BEGIN_CENTER
\(
\label{eq:mixte}
(\alpha \cdot \beta) \cdot u = \alpha \cdot (\beta \cdot u) \\
(\alpha + \beta) \cdot x = \alpha \cdot x + \beta \cdot x \\
\alpha \cdot (u + v) = \alpha \cdot u + \alpha \cdot v \\
1 \cdot u = 1
\)
#+END_CENTER

pour tout $\alpha, \beta \in \corps$. De plus l'addition induite sur $E$ par l'addition de $\corps$ transforme $E$ en groupe commutatif. On ne peut toutefois pas parler de corps pour $E$, car la multiplication matricielle n'est pas une multiplication induite, et est non commutative.


*** Attention

Ne pas confondre les additions définies sur $E$ et $\corps$, ni la multiplication de $\corps$ avec la multiplication mixte, ni le neutre de $E$ avec celui de $\corps$. Lorsqu'il y a un risque d'ambiguité, on parle du vecteur nul $0 \in E$ et du scalaire nul $0 \in \corps$.


** Définition

Soit un groupe commutatif pour l'addition $E$, ainsi qu'un corps $\corps$. Si il existe une opération de multiplication mixte $\cdot : \corps \times E \mapsto E$ vérifiant les propriétés \ref{eq:mixte} ci-dessus, on dit que $E$ est un espace vectoriel sur $\corps$. On nomme alors « vecteurs » les éléments de $E$ et « scalaires » les éléments de $\corps$.


*** Notation

On note aussi :

#+BEGIN_CENTER
\(
x - y = x + (-1) \cdot y \\
x \cdot \alpha = \alpha \cdot x \\
\alpha \cdot \beta \cdot x = (\alpha \cdot \beta) \cdot x \\
\alpha x = \alpha \cdot x \\
\frac{x}{\alpha} = \alpha^{-1} \cdot x
\)
#+END_CENTER

Lorsque $\alpha$ a un inverse dans $\corps$, on a même les « fractions » :

$$\frac{x}{\alpha} = \unsur{\alpha} \cdot x$$


*** Corollaires

Les propriétés de la multiplication mixte nous montrent directement que :

#+BEGIN_CENTER
\(
0 \cdot u = (1 - 1) \cdot u = u - u = 0 \\
\alpha \cdot 0 = \alpha \cdot (u - u) = \alpha - \alpha = 0
\)
#+END_CENTER


*** Remarque

Le corps $\corps$ est souvent $\setR$ ou $\setC$.


** Sous-espace

On dit que $F \subseteq E$ est un sous-espace vectoriel de $E$ si $0 \in F$ et si :

$$z = \alpha \cdot x + \beta \cdot y$$

appartient à $F$ quels que soient les vecteurs $x,y \in F$ et les scalaires $\alpha,\beta \in \corps$.

On vérifie par exemple que $E$ est un sous-espace vectoriel de lui-même.


** Espace engendré

L'espace engendré par les vecteurs $e_1,e_2,...,e_n \in E$ est l'ensemble des combinaisons linéaires formées à partir des $e_i$ :

$$\combilin{e_1,...,e_n} = \left\{ \sum_{i=1}^{n} \alpha_i \cdot e_i : \alpha_i \in \corps \right\}$$

On vérifie que $\combilin{e_1,...,e_n}$ est un sous-espace vectoriel de $E$.


*** Remarque

Les espaces vectoriels ne pouvant pas s'exprimer comme ci-dessus sont dit de dimension infinie.


** Indépendance linéaire

On dit qu'une série de vecteurs $e_1,...,e_n$ est linéairement
indépendante si pour toute suite de scalaires $\alpha_i$, la condition :

$$\sum_{i=1}^{n} \alpha_i \cdot e_i = 0$$

implique que tous les scalaires soient nuls :

$$\alpha_i = 0$$

pour tout $i \in \{1,2,...,n\}$.


** Coordonnées

Soit les vecteurs linéairement indépendants $(e_1,...,e_n)$ et
$x \in \combilin{e_1,...,e_n}$. On peut trouver une suite de scalaire $\alpha_i$ tels que :

$$x = \sum_{i = 1}^n \alpha_i \cdot e_i$$

Supposons que l'on ait également :

$$x = \sum_{i=1}^n \beta_i \cdot e_i$$

pour une autre suite de scalaires $\beta_i$. En soustrayant les deux
équations, on obtient :

$$\sum_{i=1}^n (\alpha_i - \beta_i) \cdot e_i = 0$$

L'indépendance linéaire des $e_i$ implique alors que $\alpha_i - \beta_i = 0$, c'est-à-dire :

$$\alpha_i = \beta_i$$

pour tout $i \in \{1,2,...,n\}$.

On a donc unicité des coefficients scalaire de la combinaison linéaire. On dit que les $\alpha_i$ sont les coordonnées de $x$ par rapport aux $(e_1,...,e_n)$.


*** Base

Par contre, l'existence de telles coordonnées n'est pas garantie pour tout $x \in E$. Ce ne sera le cas que si :

$$E \subseteq \combilin{e_1,...,e_n}$$

On dit alors que $(e_1,...,e_n)$ forme une base de $E$.


*** Dimension finie

On dit qu'un espace vectoriel $E$ est de dimension finie s'il posséde au moins une base de la forme $(e_1,...,e_n)$, où $n \in \setN$ est fini. Dans le cas où $E$ {\em ne possède pas} une telle base, il est dit de dimension infinie.


*** Equivalence

On voit qu'étant donné une base de $E$, il y a équivalence entre un vecteur $x \in E$ et un élément $(x_1,x_2,...,x_n) \in \corps^n$ formé par ses coordonnées.

Nous noterons donc également (et abusivement) $x = (x_1,x_2,...,x_n)$, mais attention : il ne faut jamais perdre de vue que les $x_i$ dépendent de la base utilisée. Le vecteur $x$ est lui invariant sous changement de base.


** Absence de redondance

Soit $e_1,...,e_n \in E$ une suite de vecteurs linéairement indépendants.
Soit $i \in \{ 1,2,...,n \}$ et :

$$J(i) = \setZ(0,n) \setminus \{ i \}$$

Supposons que le vecteur $e_i$ soit une combinaison des autres vecteurs :

$$e_i = \sum_{ j \in J(i) } \alpha_j \cdot e_j$$

On a donc :

$$e_i - \sum_{ j \in J(i) } \alpha_j \cdot e_j = 0$$

L'hypothèse d'indépendance linéaire voudrait que tous les $\alpha_j$ et le $\alpha_i$ soient nuls. Ce qui n'est manifestement pas le cas puisque $\alpha_i = 1 \ne 0$ !

Aucun des vecteurs de la suite n'est donc combinaison des autres. On dit qu'aucun vecteur n'est redondant dans la suite.

** Base canonique sur $\corps^n$

Soit $\corps \in \{ \setR, \setC \}$. On note $\canonique_i$ l'élément de $\corps^n$ ayant un $1$ en $i^{ème}$ position et des $0$ partout ailleurs. On a donc :

\begin{align}
\canonique_1 &= (1,0,...,0) \\
\canonique_2 &= (0,1,0,...,0) \\
&\vdots& \\
\canonique_n &= (0,...,0,1)
\end{align}

On a alors, pour tout $x = (x_1,...,x_n) \in \corps^n$ :

$$x = \sum_{i = 1}^n x_i \cdot \canonique_i$$


** Représentation matricielle

On représente généralement les vecteurs de $\corps^n$ par des vecteurs lignes ou colonnes. On parle alors de « vecteurs matriciels ». Le $i^{ème}$ vecteur de la base canonique est défini par le vecteur colonne :

#+BEGIN_CENTER
\(
\canonique_i = ( \indicatrice_{ij} )_j =
\begin{Matrix}{c}
\vdots \\ 0 \\ 1 \\ 0 \\ \vdots
\end{Matrix}
\)
#+END_CENTER

soit :

#+BEGIN_CENTER
\(
\canonique_1 = [1 \ 0 \ \hdots \ 0]^T \\
\canonique_2 = [0 \ 1 \ \hdots \ 0]^T \\
\vdots \\
\canonique_n = [0 \ \hdots \ 0 \ 1]^T
\)
#+END_CENTER


* Norme

#+TOC: headlines 1 local

\label{chap:norme}


** Dépendances

  - Chapitre \ref{chap:distance} : Les distances
  - Chapitre \ref{chap:vecteur} : Les espaces vectoriels


** Norme

Soit $S$ un corps muni d'un ordre $\le$ et $E$ un espace vectoriel sur $\corps$. Une norme $\norme{.} : E \to \setR$ est intuitivement la « grandeur » d'un vecteur $x \in E$. Cette grandeur correspond à la distance séparant le vecteur nul de $x$ :

$$\norme{x} \equiv \distance(x,0)$$

Soit $x,y,z \in E$.

Comme $\distance(x,0) \ge 0$, on impose par analogie que :

$$\norme{x} \ge 0$$

De plus, le seul vecteur $x$ de $E$ vérifiant :

$$\norme{x} = 0$$

implique que notre distance équivalente $\distance(x,0) = 0$ soit nulle, ce qui n'est possible que si $x = 0$. Lorsqu'on fait référence à ces conditions, on dit que la norme est strictement définie positive.

Considérons à présent l'inégalité triangulaire :

$$\distance(0 , x + y) \le \distance(0,x) + \distance(x , x + y)$$

Comment faire correspondre $\distance(x , x + y)$ à une norme ? En demandant simplement que notre distance particulière soit invariante sous translation de $x$ :

$$\distance(x , x + y) = \distance(x - x , x + y - x) = \distance(0,y) \equiv \norme{y}$$

On impose donc l'inégalité triangulaire :

$$\norme{x + y} \le \norme{x} + \norme{y}$$

Par ailleurs, lorsqu'on allonge ou réduit un vecteur $x$ d'un facteur $\alpha \in \corps$, la distance parcourue sur $x$ devra être allongée ou réduite par la valeur absolue de $\alpha$ :

$$\norme{\alpha \cdot x} = \abs{\alpha} \cdot \norme{x}$$


** Borne inférieure

On déduit de la définition que :

$$\norme{-x} = \norme{(-1) \cdot x} = \abs{-1} \cdot \norme{x} = \norme{x}$$

En posant $z = x + y$, on a :

$$\norme{z} \le \norme{x} + \norme{y} = \norme{x}  + \norme{z-x}$$

c'est-à-dire :

$$\norme{z - x} \ge \norme{z} - \norme{x}$$

Mais comme $\norme{z-x} = \norme{(-1) \cdot (z-x)} = \norme{x-z}$, la propriété vaut également en interchangeant $z$ et $x$, et on obtient :

$$\norme{z - x} \ge \max\{ \norme{z} - \norme{x}, \norme{x} -\norme{z} \}$$


** Distance associée

On peut associer une distance $d$ à une norme $\norme{.}$ en posant :

$$\distance(x,y) = \norme{x - y}$$

En effet :

  - $\distance(x,x) = \norme{x - x} = \norme{0} = 0$
  - si $\distance(x,y) = 0 = \norme{x-y}$, on a forcément $x - y = 0$ et donc $x = y$
  - $\distance(x,y) = \norme{x-y} = \norme{y-x} = \distance(y,x)$
  - $\distance(x,y) + \distance(y,z) = \norme{x-y} + \norme{y-z} \ge \norme{x-y+y-z} = \norme{x-z} = \distance(x,z)$


** Normalisation

On peut toujours normaliser un vecteur $w \ne 0$ pour obtenir un vecteur $u$ de norme $1$. Comme $\norme{w} \ne 0$, on peut écrire :

$$u = \frac{w}{\norme{w}}$$

On a alors :

$$\norme{u} = \norme{ \frac{w}{\norme{w}} } = \unsur{\norme{w}} \cdot \norme{w} = 1$$


* Espaces de Banach

#+TOC: headlines 1 local

\label{chap:banach}


** Définition

On dit qu'un espace vectoriel $X$ est un espace de Banach si il est complet pour la distance issue de la norme $\distance(x,y) = \norme{x - y}$. Dans la suite, nous considérons un espace de Banach $X$ sur $\setR$ ou $\setC$.


** Application contractante

On dit qu'une application $A : X \mapsto X$ est contractante s'il existe un $c \in \intervallesemiouvertdroite{0}{1} \subseteq \setR$ tel que :

$$\distance\big( A(u) , A(v) \big) \le c \cdot \distance(u,v)$$

pour tout $u,v\in X$.


** Suite de Cauchy

Soit une application contractante $A : X \mapsto X$ et $u_0 \in X$. On définit la suite $u_0,u_1,u_2,...$ par :

$$u_n = A(u_{n - 1}) = ... = A^n(u_0)$$

pour tout $n \in \setN$. On a alors :

$$\distance( u_{n + 1} , u_n ) \le c \cdot \distance( u_n , u_{n - 1} ) \le ... \le c^n \cdot \distance( u_1 , u_0 )$$

Soit $m,n \in \setN$. Les propriétés des distances nous permettent d'écrire :

$$\distance( u_{n + m} , u_n ) \le \sum_{i = 0}^{m - 1} \distance( u_{n + i + 1} , u_{n + i} )$$

Mais comme $\distance( u_{n + i + 1} , u_{n + i} ) \le c^{n + i} \cdot \distance( u_1 , u_0 )$, on a :

\begin{align}
\distance( u_{n + m} , u_n ) &\le \sum_{i = 0}^{m - 1} c^{n + i} \cdot \distance( u_1 , u_0 ) \\
&\le c^n \cdot \distance( u_1 , u_0 ) \cdot \sum_{i = 0}^{m - 1} c^i \\
&\le c^n \cdot \frac{1 - c^m}{1-c}
\end{align}

Finalement, comme $1 - c^m \le 1$ quel que soit $m \in \setN$ on obtient une expression qui ne dépend pas de $m$ :

$$\distance( u_{n + m} , u_n ) \le \frac{c^n}{1 - c} \cdot \distance( u_1 , u_0 )$$

Les éléments de la suite sont donc de plus en plus proche l'un de l'autre
lorsque $n$ augmente. Soit $\epsilon \strictsuperieur 0$. Comme la suite $c^n$ converge vers $0$ lorsque $n \to \infty$, on peut toujours trouver $N$ tel que :

$$c^N \le \frac{\epsilon \cdot (1 - c)}{\distance( u_1 , u_0 )}$$

Il suffit donc de choisir $i,j \in \setN$ tels que $i,j \ge N$ pour avoir :

$$\distance( u_i , u_j ) = \distance( u_j , u_i ) \le \epsilon$$

On en conclut que la suite des $u_n$ est de Cauchy.


** Point fixe

Comme $X$ est complet, notre suite $u_n$ étant de Cauchy converge vers une certaine limite :

$$p = \lim_{n \to \infty} u_n$$

appartenant à $X$. Analysons le comportement de $A(p)$. On a la borne supérieure :

$$\distance\big( A(p) , p \big) \le \distance\big( A(p) , A(u_n) \big) + \distance\big( A(u_n) , u_n \big) + \distance\big( u_n , p \big)$$

On sait déjà que $\distance( u_n , p )$ converge vers $0$ par définition de $p$. On sait aussi que $\distance\big( A(u_n) , u_n \big) = \distance( u_{n + 1} , u_n ) \to 0$. On a également :

$$\distance\big( A(p) , A(u_n) \big) \le c \cdot \distance\big( p , u_n \big)$$

On en conclut que la suite $A(u_n)$ converge vers $A(p)$ :

$$A(p) = \lim_{n \to \infty} A(u_n)$$

Les trois termes de la borne supérieure convergeant chacun vers $0$, cette borne est aussi petite que l'on veut lorsque $n$ est assez grand. On a donc $\distance\big( A(p) , p \big) = 0$ et :

$$A(p) = p$$

L'élément $p \in X$ est un point fixe de $A$.


** Unicité

Si $p_1$ et $p_2$ sont deux points fixes, on a :

#+BEGIN_CENTER
\(
A(p_1) = p_1 \\
A(p_2) = p_2
\)
#+END_CENTER

et :

$$\distance( p_1 , p_2 ) \le c \cdot \distance\big( A(p_1) , A(p_2) \big) \le c \cdot \distance\left( p_1 , p_2 \right)$$

Comme $c \strictinferieur 1$, ce n'est possible que si $\distance(p_1,p_2) = 0$, c'est-à-dire :

$$p_1 = p_2$$

Le point fixe de $A$ est unique.


** Vitesse de convergence

Nous avons donc montré que la suite des $u_n$ converge vers l'unique point fixe $p$ de $A$, et ce quel que soit $u_0$. On a même la propriété suivante nous donnant une borne supérieure pour le taux de convergence :

$$\distance(u_n,p) \le \distance\big(A(u_{n - 1}),A(p)\big) \le c \cdot \distance(u_{n - 1},p) \le ... \le c^n \cdot \distance(u_0,p)$$


* Continuité

#+TOC: headlines 1 local

\label{chap:limite}


** Dépendances

  - Chapitre \ref{chap:limite} : Les limites
  - Chapitre \ref{chap:reel} : Les réels


** Fonctions continues

Une fonction $f : D \to F$ est dite continue en $a$ si :

$$\lim_{ \substack{ x \to a \\ x \in D } } f(x) = f(a)$$

Soit $A \subseteq D$. On dit qu'une fonction est continue sur $A$ si :

$$\lim_{ \substack{ x \to a \\ x \in A } } f(x) = f(a)$$

pour tout $a \in A$. On note $\continue(A,F)$ l'ensemble des fonctions $f : A \mapsto F$ continues sur $A$.


*** Remarque

Si $F$ est muni d'une norme, les limites s'évaluent au sens de la distance découlant de la norme.


** Espace vectoriel

Si $\corps$ est un corps, on vérifie que $\continue(A,\corps)$ est un espace vectoriel sur $\corps$. En effet, la fonction nulle $0$ est continue. Si $\alpha,\beta \in \corps$ et si $f,g \in \continue(A,\corps)$, on a :

\begin{align}
\lim_{x \to a} (\alpha \cdot f(x) + \beta \cdot g(x)) &= \alpha \cdot \lim_{x \to a} f(x) + \beta \cdot \lim_{x \to a} g(x) \\
&= \alpha \cdot f(a) + \beta \cdot g(a)
\end{align}

pour tout $a \in A$. On en conclut que $\alpha \cdot f + \beta \cdot g$ est également continue.


** Norme des fonctions continues

Si l'ensemble $F$ est muni d'une norme, on peut définir la norme $\norme{.}_\continue$ d'une fonction continue $u$ par :

$$\norme{u}_\continue = \sup \big\{ \norme{u(x)} : x \in A \big\}$$


*** Notation

On note aussi :

$$\norme{u}_\infty = \norme{u}_\continue$$


*** Convergence uniforme

Cette norme est surtout utilisée lorsqu'il s'agit de mesurer l'écart entre deux fonctions $f,g : A \to B$, en particulier lorsque $g$ représente une approximation de $f$. Dans ce cas, l'écart $e = f - g$ représente l'erreur la plus élevée de l'estimation :

$$\norme{e}_\infty = \norme{f - g}_\infty = \sup \{ \norme{f(x) - g(x)} : x \in A \}$$

Lorsque cette norme particulière de l'erreur tend vers zéro, on parle de convergence uniforme.


** Théorème des valeurs intermédiaires

Les fonctions continues possèdent l'importante propriété suivante.

\begin{theoreme}

$$ $$


  - Soit $f \in \continue(I,\setR)$ où $I = [a,b]$ est un intervalle inclus dans $\setR$. On suppose que :

$$f(a) \strictinferieur f(b)$$

Soit le réel $\varphi$ vérifiant $f(a) \strictinferieur \varphi \strictinferieur f(b)$. On peut alors trouver un $c \in \intervalleouvert{a}{b}$ tel que $f(c) = \varphi$.

  - Soit $g \in \continue(I,\setR)$. On suppose que :

$$g(a) \strictsuperieur g(b)$$

Soit le réel $\varphi$ vérifiant $g(a) \strictsuperieur \varphi \strictsuperieur g(b)$. On peut alors trouver un $c \in \intervalleouvert{a}{b}$ tel que $g(c) = \varphi$.


\end{theoreme}

\begin{demonstration}

Nous allons démontrer ce résultat par l'absurde.


  - Considérons le cas où $f(a) \strictinferieur \varphi \strictinferieur f(b)$. On définit les ensembles :

#+BEGIN_CENTER
\(
A^+ = \{ x \in I : f(x) \strictsuperieur \varphi \} \\
A^- = \{ x \in I : f(x) \strictinferieur \varphi \}
\)
#+END_CENTER

Si aucun $c \in I$ ne vérifie $f(c) = \varphi$, on doit avoir clairement $A^+ \cup A^- = I$.

Nous définissons $\alpha = \sup A^-$. Comme $A^-\subseteq I$, on à clairement
$\alpha \in I$. Si $\alpha \in A^-$, alors par continuité de $f$ en $\alpha$,
on peut trouver $\delta \strictsuperieur 0$ tel que :

$$\abs{ f(\alpha + \delta) - f(\alpha) } \le \epsilon = \unsur{2}(\varphi - f(\alpha))$$

On a alors clairement $f(\alpha + \delta) \strictinferieur \varphi$ et $\alpha \strictinferieur \alpha + \delta \in A^-$ ce qui contredit l'hypothèse de suprémum pour $\alpha$.

On doit donc avoir $\alpha \notin A^-$. Mais alors $\alpha \in I \setminus A^- = A^+$. Donc $f(\alpha) \strictsuperieur \varphi$. Par continuité de $f$ en $\alpha$, on peut trouver $\delta \strictsuperieur 0$ tel que :

$$\abs{ f(\alpha) - f(x) } \le \epsilon = \unsur{2}(f(\alpha)-\varphi)$$

pour tout $x \in \intervalleouvert{\alpha - \delta}{\alpha}$. On a alors clairement $f(x) \strictsuperieur \varphi$ pour tout $x \in \intervalleouvert{\alpha - \delta}{\alpha}$.

Soit $\beta \in \intervalleouvert{\alpha - \delta}{\alpha}$. Par définition de $\alpha$, on ne peut pas avoir $\beta \ge A^-$, donc il existe $\gamma \in \intervalleouvert{\beta}{\alpha} \subseteq \intervalleouvert{\alpha - \delta}{\alpha}$ tel que $\gamma \in A^-$. On a donc $f(\gamma) \strictinferieur \varphi$ ce qui contredit la propriété ci-dessus en $x = \gamma$.

  - Considérons à présent le cas $g(a) \strictsuperieur \varphi \strictsuperieur g(b)$. On pose :

$$f = -g$$

On a :

$$f(a) \strictinferieur -\varphi \strictinferieur f(b)$$

On peut donc trouver un $c \in \intervalleouvert{a}{b}$ tel que :

$$f(c) = -\varphi$$

On en conclut que :

$$g(c) = -f(c) = \varphi$$


\end{demonstration}


*** Généralisation

Soit le réel $\varphi$ tel que $f(a) \le \varphi \le f(b)$. Si $\varphi \in \{f(a),f(b)\}$, il suffit de prendre $c \in \{a,b\}$ pour avoir $f(a) \le f(c) \le f(b)$. Sinon, on applique le théorème des valeurs intermédiaires et on trouve un $c$ vérifiant $f(a) \strictinferieur f(c) \strictinferieur f(b)$. Mais dans tous les cas, on pourra trouver un $c \in [a,b]$ tel que $f(a) \le f(c) \le f(b)$. De même si $f(a) \strictsuperieur f(b)$.


** Théorème de la bijection

\begin{theoreme}

Soit $f \in \continue(I,J)$ où $I = [a,b]$ est un intervalle inclus dans $\setR$ et où $J = f(I)$. Si $f$ est strictement croissante (ou décroissante), alors $f$ est inversible et :

$$f(I) = [\alpha,\beta]$$

avec :

#+BEGIN_CENTER
\(
\alpha = \min \{ f(a), f(b) \} \\
\beta = \max \{ f(a), f(b) \}
\)
#+END_CENTER

\end{theoreme}

\begin{demonstration}

Comme $f : I \mapsto J = f(I)$ est strictement croissante ou décroissante, on a vu dans le chapitre traitant des bijections que $f$ est inversible. Choisissons un réel $\varphi \in [\alpha,\beta]$. Le théorème des valeurs intermédiaires nous dit qu'on peut trouver un réel $c \in [a,b]$ tel que $f(c) = \varphi$. On en conclut que $\varphi \in f(I)$. Cette relation étant vérifiée pour tout $\varphi \in [\alpha,\beta]$, on a :

$$[\alpha,\beta] \subseteq f(I)$$

Soit $x \in I$. Comme $f$ est croissante ou décroissante, on doit avoir :

$$f(a) \le f(x) \le f(b)$$

ou :

$$f(a) \ge f(x) \ge f(b)$$

On a donc également :

$$f(I) \subseteq [\alpha,\beta]$$

L'inclusion étant réciproque, on a :

$$f(I) = [\alpha,\beta]$$

\end{demonstration}


** Continuité uniforme

\label{sec:continuite_uniforme}

On dit qu'une fonction $f$ est uniformément continue sur $A$, si pour toute précision $\epsilon \strictsuperieur 0$, on peut trouver un $\delta \strictsuperieur 0$ tel que :

$$\abs{f(s) - f(t)} \le \epsilon$$

pour tout $s,t \in A$ vérifiant $\abs{s - t} \le \delta$.


*** Continuité simple

Si $f$ est uniformément continue sur $A$, on a clairement :

$$\lim_{s \to t} \abs{f(s) - f(t)} = 0$$

et donc :

$$\lim_{s \to t} f(s) = f(t)$$

pour tout $t \in A$. Toute fonction uniformément continue est donc continue.


** Polynômes

Soit $n \in \setN$ et $\alpha,\beta \in \setR$ avec $\alpha \le \beta$. Nous allons analyser la continuité du monôme $\mu : [\alpha,\beta] \mapsto \setR$ défini par :

$$\mu : x \mapsto x^n$$

pour tout $x \in [\alpha,\beta]$. Reprenant les résultats de la section \ref{sec:factorisation_progression_geometrique}, nous avons :

$$s^n - t^n = (s - t) \sum_{i = 0}^{n - 1} s^{n - 1 - i} \cdot t^i$$

Quelque soient $s,t \in [\alpha,\beta]$, il est clair que

$$\abs{s}, \abs{t} \le M = \max \{ \abs{\alpha} , \abs{\beta} \}$$

On a donc :

$$\abs{s^n - t^n} \le \abs{s - t} \cdot n \cdot M^{n - 1}$$

Fixons à présent $\epsilon \strictsuperieur 0$. Il suffit de prendre :

$$\abs{s - t} \le \delta \le \frac{\epsilon}{ n \cdot M^{n - 1} }$$

pour avoir :

$$\abs{s^n - t^n} \le \delta \cdot n \cdot M^n \le \epsilon$$

Comme le choix de $\delta$ ne dépend ni de $s$ ni de $t$, le monôme $\mu$ est uniformément continu sur $[\alpha,\beta]$.

Pour généraliser aux polynômes de la forme :

$$p(x) = \sum_{i = 0}^n a_i \cdot x^i$$

on part de :

$$p(s) - p(t) = \sum_{i = 0}^n a_i \cdot (s^i - t^i)$$

On a donc :

$$\abs{p(s) - p(t)} \le \sum_{i = 0}^n \abs{a_i} \cdot \abs{s^i - t^i}$$

Mais comme on peut trouver des $\delta_k$ tels que :

$$\abs{s^k - t^k} \le \frac{\epsilon}{\sum_j \abs{a_j}}$$

il suffit de choisir $\delta = \min \{ \delta_0, \delta_1, ..., \delta_n \}$ pour avoir :

$$\abs{p(s) - p(t)} \le \epsilon \cdot \frac{ \sum_i \abs{a_i} }{ \sum_j \abs{a_j} } = \epsilon$$

Cette généralisation montre aussi que toute combinaison linéaire de fonctions uniformément continues est uniformément continue.


*** Continuité simple

Qu'en est-il de la continuité sur $\setR$ ? Choisissons $a \in \setR$ et considérons l'intervalle $I = [a - 1, a + 1]$. Le polynôme est uniformément continu sur cette intervalle, et $a \in \interieur I$. Plus précisément, $\distance(a,\setR \setminus I) = 1 \strictsuperieur 0$. Donc, si $\abs{x - a} \le 1$, on a forcément $x \in I$. On peut donc se servir de la continuité uniforme sur $I$ pour trouver un $\delta \in \intervalleouvert{0}{1}$ tel que $\abs{p(x) - p(a)} \le \epsilon$ lorsque $\abs{x - a} \le \delta$. Les polynômes sont donc continus en tout point de $\setR$, et par conséquent continus sur $\setR$.


** Uniformité

Nous allons à présent montrer que toute fonction continue sur un intervalle de la forme $[\alpha,\beta]$ y est uniformément continue.

\begin{theoreme}

Soit la fonction $f \in \continue([\alpha,\beta],\setR)$. Etant donné un $\epsilon \strictsuperieur 0$ et un $a \in [\alpha,\beta]$, on note $\Delta(a,\epsilon)$ l'ensemble des écarts strictement positifs offrant la précision demandée. Pour tout $\delta \in \Delta(a,\epsilon)$, on aura donc $\delta \strictsuperieur 0$ et :

$$\abs{f(a + h) - f(a)} \le \epsilon$$

pourvu que $h \in \setR$ vérifie :

#+BEGIN_CENTER
\(
\abs{h} \le \delta \\
a + h \in [\alpha,\beta]
\)
#+END_CENTER

On note les supremums de cette famille d'ensemble par :

$$\sigma(a,\epsilon) = \sup \Delta(a,\epsilon)$$

Nous allons voir que l'intersection de ces ensembles est non vide, même après avoir parcouru tout l'intervalle :

$$\Gamma(\epsilon) = \bigcap_{a \in [\alpha,\beta]} \Delta(a,\epsilon) \ne \emptyset$$

et que l'infimum des supremums est strictement positif :

$$I(\epsilon) = \inf \{ \sigma(a,\epsilon) : a \in [\alpha,\beta] \} \strictsuperieur 0$$

Etant donné un $\epsilon \strictsuperieur 0$, on peut donc trouver un $\delta  \in \Gamma(\epsilon)$ tel que :

$$\abs{f(x + h) - f(x)} \le \epsilon$$

pour tout $x \in [\alpha,\beta]$ et pour tout $h$ vérifiant :

#+BEGIN_CENTER
\(
\abs{h} \le \delta \\
x + h \in [\alpha,\beta]
\)
#+END_CENTER

Posant $s = x + h$ et $t = x$, cela revient à dire que :

$$\abs{f(s) - f(t)} \le \epsilon$$

pour tout $s,t \in [\alpha,\beta]$ vérifiant $\abs{s - t} \le \delta$. La fonction $f$ est donc uniformément continue sur $[\alpha,\beta]$.

\end{theoreme}


*** Remarques


  - La continuité de $f$ nous garantit que ces écarts strictement positifs existent bien, c'est à dire que :

$$\Delta(a,\epsilon) \ne \emptyset$$

quelles que soient les valeurs de $\epsilon \strictsuperieur 0$ et de $a \in [\alpha,\beta]$.

  - Par ailleurs, si :

$$0 \strictinferieur \gamma \le \delta \in \Delta(a,\epsilon)$$

tous les réels présentant un écart inférieur à $\gamma$ (par rapport à $a$) auront a fortiori un écart inférieur à $\delta$ et satisferont donc la précision $\epsilon$ :

$$f\big( [a - \gamma, a + \gamma] \big) \subseteq [f(a) - \epsilon, f(a) + \epsilon]$$

Par conséquent, $\gamma$ appartient à $\Delta(a,\epsilon)$ et :

$$]0,\delta] \subseteq \Delta(a,\epsilon)$$

pour tout $\delta \in \Delta(a,\epsilon)$.

  -  Si $x \in \intervalleouvert{0}{\sigma(a,\epsilon)}$, on a :

$$\psi = \sigma(a,\epsilon) - x \strictsuperieur 0$$

Comme le supremum est dans l'adhérence, la distance à son ensemble est nulle et on peut trouver un $\delta \in \Delta(a,\epsilon)$ tel que :

$$\abs{\sigma(a,\epsilon) - \delta} \le \psi$$

On a donc :

$$\sigma(a,\epsilon) - \delta \le \sigma(a,\epsilon) - x$$

et :

$$x \le \delta$$

On a alors :

$$x \in \ ]0,\delta] \subseteq \Delta(a,\epsilon)$$

et donc $x \in \Delta(a,\epsilon)$. Cette relation étant vérifiée pour tout $x \in \intervalleouvert{0}{\sigma(a,\epsilon)}$, on a :

$$\intervalleouvert{0}{\sigma(a,\epsilon)} \subseteq \Delta(a,\epsilon)$$

  - Par définition du supremum, il ne peut avoir d'élément de $\Delta(a,\epsilon)$ supérieur à $\sigma(a,\epsilon)$, et on a également :

$$\Delta(a,\epsilon) \subseteq \intervallesemiouvertgauche{0}{\sigma(a,\epsilon)}$$

  - Les propositions sur l'intersection non vide et l'infimum strictement positif sont équivalentes. En effet, si $I(\epsilon) \strictsuperieur 0$, on a :

$$\sigma(a,\epsilon) \ge I(\epsilon) \strictsuperieur 0$$

pour tout $a$. On a donc :

$$\emptyset \ne \ ]0,I(\epsilon)[ \ \subseteq \bigcap_{a \in [\alpha,\beta]} (0,\sigma(a,\epsilon)) \subseteq \bigcap_{a \in [\alpha,\beta]} \Delta(a,\epsilon)$$

D'un autre coté, si l'intersection est non nulle, soit :

$$\delta \in \bigcap_{a \in [\alpha,\beta]} \Delta(a,\epsilon)$$

Comme $\delta$ appartient à $\Delta(a,\epsilon)$ pour tout $a \in [\alpha,\beta]$, on a :

$$\sigma(a,\epsilon) \ge \delta \strictsuperieur 0$$

par définition du supremum localisé en $a$. Il suffit alors de passer à l'infimum sur $a$ pour obtenir :

$$I(\epsilon) = \inf_{a \in [\alpha,\beta]} \sigma(a,\epsilon) \ge \delta \strictsuperieur 0$$

Nous nous attelerons ici à démontrer que $I(\epsilon) \strictsuperieur 0$.


\begin{demonstration}

Soit $\epsilon \strictsuperieur 0$. Considérons la suite d'infimums intermédiaires :

$$D(x) = \inf \{ \sigma(\xi,\epsilon) : \xi \in [\alpha,x] \}$$

où $x \in [\alpha,\beta]$. Nous considérons l'ensemble $\Psi$ des éléments tels que cet infimum soit non nul :

$$\Psi = \{ x \in [\alpha,\beta] : D(x) \strictsuperieur 0 \}$$

On a $D(\alpha) = \inf\{ \sigma(\alpha,\epsilon) \} = \sigma(\alpha,\epsilon) \strictsuperieur 0$. Donc $\alpha \in \Psi$. On a aussi $\Psi \subseteq [\alpha,\beta] \le \beta$. L'ensemble $\Psi$ est non vide et majoré. Il admet donc un supremum :

$$S = \sup \Psi \le \beta$$

  - Si $x \in \Psi$ et $a \in [\alpha,x]$, on a :

$$[\alpha,a] \subseteq [\alpha,x]$$

Les propriétés de l'infimum pour l'inclusion nous donnent alors :

$$D(a) \ge D(x) \strictsuperieur 0$$

On en conclut que :

$$[\alpha,x] \subseteq \Psi$$

pour tout $x \in \Psi$.

  - Soit $a \in [\alpha,\beta]$. Nous allons construire une zone autour de $a$ où l'infimum est strictement positif. Choisissons $\delta(a) \in \Delta(a,\epsilon/2)$ et posons :

$$\gamma(a) =  \unsur{2} \delta(a) \strictsuperieur 0$$

Considérons à présent l'ensemble :

$$U(a) = [a - \gamma(a), a + \gamma(a)] \cap [\alpha,\beta]$$

Pour tout $b \in U(a)$ et $x \in [b - \gamma(a), b + \gamma(a)] \cap [\alpha,\beta]$, on a alors :

\begin{align}
\abs{b - a} &\le \gamma(a) \strictinferieur \delta(a) \\
\abs{x - b} &\le \gamma(a) \\
\abs{x - a} &\le \abs{x - b} + \abs{b - a} \le 2 \gamma(a) \le \delta(a)
\end{align}

et :

#+BEGIN_CENTER
\(
\abs{f(b) - f(a)} \le \frac{\epsilon}{2} \\ \\
\abs{f(x) - f(a)} \le \frac{\epsilon}{2}
\)
#+END_CENTER

On en déduit la borne supérieure :

\begin{align}
\abs{f(x) - f(b)} &\le \abs{f(x) - f(a)} + \abs{f(a) - f(b)} \\
&\le \frac{\epsilon}{2} + \frac{\epsilon}{2} = \epsilon
\end{align}

On a donc :

$$\sigma(b,\epsilon) \ge \gamma(a) \strictsuperieur 0$$

pour tout $b \in U(a)$. En prenant l'infimum, il vient :

$$\inf_{b \in U(a)} \sigma(b,\epsilon) \ge \gamma(a) \strictsuperieur 0 $$

  - Supposons que $S = \alpha$ et posons :

$$\theta = \min \{ \gamma(\alpha) , \beta - \alpha \} \strictsuperieur 0$$

On a alors $U(S) = U(\alpha) = [\alpha, \alpha + \theta]$ et :

$$D(\alpha + \theta) = \inf_{b \in U(\alpha)} \sigma(b,\epsilon) \ge \gamma(\alpha) \strictsuperieur 0 $$

On en conclut que $\alpha + \theta \in \Psi$ avec $S \strictinferieur \alpha + \theta$, ce qui contredit $S = \sup \Psi$.

  - Supposons à présent que $\alpha \strictinferieur S \strictinferieur \beta$ et posons :

#+BEGIN_CENTER
\(
\eta = \min \{ \gamma(S) , S - \alpha \} \strictsuperieur 0 \\
\theta = \min \{ \gamma(S) , \beta - S \} \strictsuperieur 0
\)
#+END_CENTER

On a alors $U(S) = [S - \eta, S + \theta]$. Voyons comment se comporte $\sigma(b,\epsilon)$ lorsquer $b$ voyage dans $[\alpha, S + \theta]$. Le supremum étant dans l'adhérence, on peut trouver $\psi \in \Psi$ tel que :

$$\abs{S - \psi} = S - \psi \le \eta$$

Si $b \in [\alpha,\psi]$, on a :

$$\sigma(b,\epsilon) \ge D(\psi) \strictsuperieur 0 $$

par définition de $\Psi$. Mais si $b \in [\psi, S + \theta]$, on a :

$$S - \eta \le \psi \le b \le S + \theta$$

Donc, $b \in [S - \eta, S + \theta] = U(S)$ et :

$$\sigma(b,\epsilon) \ge \gamma(S) \strictsuperieur 0$$

Il suffit donc de poser :

$$\varpi = \min \{ \gamma(S) , D(\psi) \} \strictsuperieur 0$$

pour avoir $\sigma(b,\epsilon) \ge \varpi$ sur $[\alpha, S + \theta]$. On en déduit que l'infimum est strictement positif :

$$D(S + \theta) \ge \varpi \strictsuperieur 0 $$

C'est à dire $S + \theta \in \Psi$ avec $S \strictinferieur S + \theta$, ce qui contredit la définition du supremum. On doit donc avoir $S = \sup \Psi = \beta$.

  - Posons :

$$\theta = \min \{ \gamma(\beta) , \beta - \alpha \} \strictsuperieur 0$$

On a alors $U(S) = U(\beta) = [S - \theta, S] = [\beta - \theta, \beta]$. Comme le supremum est dans l'adhérence, on peut trouver un $\psi \in \Psi$ tel que :

$$\abs{S - \psi} = S - \psi \le \theta$$

Si $b \in [\alpha,\psi]$, on a :

$$\sigma(b,\epsilon) \ge D(\psi) \strictsuperieur 0 $$

par définition de $\Psi$. Mais si $b \in [\psi, S] = [\psi, \beta]$, on a :

$$S - \theta \le \psi \le b \le \beta$$

Donc, $b \in [S - \theta, S] = U(S)$ et :

$$\sigma(b,\epsilon) \ge \gamma(S) = \gamma(\beta) \strictsuperieur 0$$

Il suffit donc de poser :

$$\varpi = \min \{ \gamma(\beta) , D(\psi) \} \strictsuperieur 0$$

pour avoir $\sigma(b,\epsilon) \ge \varpi$ sur $[\alpha, \beta]$. On en déduit que l'infimum est strictement positif :

$$D(\beta) \ge \varpi \strictsuperieur 0 $$

C'est à dire $\beta \in \Psi$ et :

$$\Psi = [\alpha,\beta]$$

L'infimum $D(x)$ est donc strictement positif sur tout l'intervalle $[\alpha,\beta]$ et on a :

#+BEGIN_CENTER
\(
I(\epsilon) = D(\beta) \strictsuperieur 0 \\
\intervalleouvert{0}{I(\epsilon)} \subseteq \Gamma(\epsilon)
\)
#+END_CENTER


\end{demonstration}


*** Remarque

Le théorème {\em n'est pas} applicable aux intervalles ouverts ou semi-ouverts.


** Variations bornées

On déduit de l'uniforme continuité des fonctions continues sur les intervalles que les fonctions continues y sont bornées. En effet, soit $\epsilon \strictsuperieur$ et $\delta \strictsuperieur 0$ tel que :

$$\abs{f(b) - f(a)} \le \epsilon$$

pour tout $a,b \in [\alpha,\beta]$ tels que $\abs{a - b} \le \delta$.
Choisissons $N \in \setN$ tel que :

$$\frac{\beta - \alpha}{N} \le \delta$$

Choisissons à présent $x,y \in [\alpha,\beta]$ et posons :

$$h = \frac{y - x}{N}$$

On a alors :

$$\abs{h} = \abs{\frac{y - x}{N}} \le \frac{\beta - \alpha}{N} \le \delta$$

Posons $x_i = x + i \cdot h$ pour $i = 0,1,2,...,N$. On a alors $x_0 = x$ et $x_N = y$. La variation est bornée par :

$$\abs{f(x) - f(y)} \le \sum_{i = 0}^N \abs{f(x_i) - f(x_{i - 1})} \le (N + 1) \cdot \epsilon$$


*** Norme

Comme $N$ ne dépend ni de $x$ ni de $y$, on en conclut que les variations de $f$ sont bornées sur l'intervalle. On a aussi :

$$\abs{f(x)} \le \abs{f(x) - f(\alpha)} + \abs{f(\alpha)} \le M$$

où $M = (N + 1) \cdot \epsilon + \abs{f(\alpha)}$ ne dépend pas du choix de $x$, ce qui prouve que $f$ est bornée sur l'intervalle. En passant au supremum, on en déduit que la norme est finie et que :

$$\norme{f}_\infty \le M$$


** Extrema

Soit $f \in \continue([a,b],\setR)$. Comme $f$ est bornée, on peut poser :

$$I = \inf \{ f(x) : x \in [a,b] \} = \inf f([a,b])$$

Comme la distance de l'infimum à l'ensemble est nulle, on peut construire une suite $\{x_1,x_2,...\}$ convergente vers $\lambda \in [a,b]$ :

$$\lambda = \lim_{n \to \infty} x_n$$

et telle que :

$$f(x_n) - I \le \unsur{2^n}$$

On voit que :

$$\lim_{n \to \infty} f(x_n) = I$$

Mais par continuité de $f$, on a aussi :

$$\lim_{n \to \infty} f(x_n) = f(\lambda)$$

On en conclut que :

$$f(\lambda) = I$$

On peut donc trouver un réel dans l'intervalle qui minimise la fonction. L'infimum appartient à l'image $f([a,b])$. Il est donc également un minimum et on a :

$$f(\lambda) = \inf f([a,b]) = \min f([a,b])$$

On construit de même un $\sigma \in [a,b]$ qui atteint le supremum :

$$f(\sigma) = \sup f([a,b]) = \max f([a,b])$$


* Applications linéaires

#+TOC: headlines 1 local

\label{chap:lineaire}


** Dépendances

  - Chapitre \ref{chap:fonction} : Les fonctions


** Définition

Soit les espaces vectoriels $E$ et $F$ sur $\corps$ et la fonction $f : E \mapsto F$. On dit que $f$ est linéaire si, pour tout $x,y \in E$ et $\alpha, \beta \in \corps$, on a :

$$f(\alpha \cdot x + \beta \cdot y) = \alpha \cdot f(x) + \beta \cdot f(y)$$

On note $\lineaire(E,F)$ l'ensemble des fonctions linéaires de $E$ vers $F$.


** Identité

L'application identité est clairement linéaire.


** Inverse

Soit $u = f(x)$ et $v = f(y)$. Si l'application inverse existe, on a $x = f^{-1}(u)$ et $y = f^{-1}(v)$. En composant à gauche par $f^{-1}$ la définition de la linéarité, on obtient :

$$\alpha \cdot f^{-1}(u) + \beta \cdot f^{-1}(v) = f^{-1}(\alpha \cdot u + \beta \cdot v)$$

ce qui montre que l'inverse est également linéaire.


** Valeur au vecteur nul

Choisissons un $x \in E$. on voit que :

$$f(0) = f(0 \cdot x) = 0 \cdot f(x) = 0$$

La valeur d'une application linéaire s'annule au vecteur nul.


** Norme des applications linéaires

La norme d'une application linéaire est définie comme étant l'extension maximale qu'elle produit :

$$\norme{f} = \sup \left\{ \frac{ \norme{f(x)} }{ \norme{x} } : x \in E, \ x \ne 0 \right\}$$

On a donc :

$$\norme{f(x)} \le \norme{f} \cdot \norme{x}$$

pour tout $x \in E \setminus \{ 0 \}$.


*** Vérification

Nous allons vérifier qu'il s'agit bien d'une norme. On a $\norme{f} \ge 0$ par positivité de la norme sur $E$ et $F$. La condition $\norme{f} = 0$ implique $\norme{f(x)} = 0$ et donc $f(x) = 0$ pour tout $x \ne 0$. Comme $f$ est linéaire, on a aussi $f(0) = 0$ et $f = 0$.

Si $f,g$ sont linéaires, on a :

$$\norme{f(x) + g(x)} \le \norme{f(x)} + \norme{g(x)} \le \norme{f} \cdot \norme{x} + \norme{g} \cdot \norme{x} = (\norme{f} + \norme{g}) \cdot \norme{x}$$

pour tout $x \ne 0$. En divisant par $\norme{x}$ et en passant au supremum, on obtient :

$$\norme{f + g} \le \norme{f} + \norme{g}$$

Enfin, si $\alpha \in \corps$, on a :

$$\frac{ \norme{\alpha \cdot f(x)} }{ \norme{x} } = \abs{\alpha} \cdot \frac{ \norme{f(x)} }{ \norme{x} }$$

En passant au supremum, on obtient :

$$\norme{\alpha \cdot f} = \abs{\alpha} \cdot \norme{f}$$


*** Notation

Lorsqu'il est nécessaire de différentier la norme au sens des applications linéaires d'autres types de normes utilisées, on note :

$$\norme{f}_\lineaire = \norme{f}$$


*** Définition alternative

Soit $N \in \corps$, avec $N \strictsuperieur 0$ et :

$$B = \{ u \in E : \norme{u} = N \}$$

Soit $x \in E$ avec $x \ne 0$ et :

$$\lambda = \frac{ \norme{x} }{N}$$

Définissons :

$$u = \frac{x}{\lambda}$$

On voit que :

$$\norme{u} = \norme{\unsur{\lambda} \cdot x} = \unsur{\lambda} \cdot \norme{x} = \frac{N}{ \norme{x} } \cdot \norme{x} = N$$

On a donc $u \in B$. Le rapport des normes s'écrit :

$$\frac{ \norme{f(x)} }{ \norme{x} } = \frac{ \norme{f(x)} }{ N \cdot \lambda } = \unsur{N} \norme{ \frac{f(x)}{ \lambda } } = \unsur{ \norme{u} } \cdot \norme{ f\left( \frac{x}{ \lambda } \right) } = \frac{ \norme{f(u)} }{ \norme{u} }$$

On en conclut que :

$$\frac{ \norme{f(x)} }{ \norme{x} } = \frac{ \norme{f(u)} }{ \norme{u} } \le \sup \Big\{ \frac{ \norme{f(v)} }{ \norme{v} } : v \in B \Big\}$$

Comme ce doit être valable quelque soit $x \ne 0$, on obtient :

$$\norme{f} \le \sup \Big\{ \frac{ \norme{f(v)} }{ \norme{v} } : v \in B \Big\}$$

en passant au supremum sur $x$.

Choisissons à présent $u \in B$. On a alors :

$$\frac{ \norme{f(u)} }{ \norme{u} } \le \norme{f}$$

En passant au supremum sur $u$, on obtient :

$$\sup \Big\{ \frac{ \norme{f(v)} }{ \norme{v} } : v \in B \Big\} \le \norme{f}$$

On en conclut que les deux supremums sont égaux :

$$\sup \left\{ \frac{ \norme{f(v)} }{ \norme{v} } : v \in B \right\} = \norme{f}$$


*** Norme unitaire

Une conséquence importante du résultat ci-dessus est le cas particulier $N = 1$. On a alors :

$$\norme{f} = \sup \left\{ \norme{f(v)} : v \in E, \ \norme{v} = 1 \right\}$$


** Norme d'une composée

Soit $f : E \mapsto F$ et $g : F \mapsto G$ deux applications linéaires de normes finies. Si $x \in E$ avec $x \ne 0$ on a $f(x) \in F$ et :

$$\norme{g \circ f(x)} \le \norme{g} \cdot \norme{f(x)} \le \norme{g} \cdot \norme{f} \cdot \norme{x}$$

En divisant par $\norme{x} \ne 0$ :

$$\frac{ \norme{g \circ f(x)} }{ \norme{x} } \le \norme{g} \cdot \norme{f}$$

et en passant au supremum sur $x \ne 0$, on en conclut que :

$$\norme{g \circ f} \le \norme{g} \cdot \norme{f}$$


** Norme d'une puissance

On a clairement :

$$\norme{f^n} = \norme{f \circ ... \circ f} \le \norme{f}^n$$


** Continuité

Nous allons montrer que, pour tout $f \in \lineaire(A,B)$, on a l'équivalence entre l'hypothèse d'une norme de $f$ finie et l'hypothèse de $f$ continue.

Si la norme est finie, on a :

$$\norme{f(x) - f(a)} = \norme{f(x - a)} \le \norme{f} \cdot \norme{x-a}$$

qui tend bien vers $0$ lorsque $x$ tend vers $a$. Inversément, si $f$ est continue, on peut trouver un $\delta \strictsuperieur 0$ tel que :

$$\norme{f(x) - f(0)} \le 1$$

pour tout $x$ vérifiant $\distance(x,0) = \norme{x} \le \delta$. Posons $B = \{ x \in A : \norme{x} = \delta \}$. On a alors :

$$\sup_{x \in B} \frac{\norme{f(x)}}{\norme{x}} = \unsur{\delta} \sup_{x \in B} \norme{f(x) - f(0)} \le \unsur{\delta}$$

La norme est donc finie :

$$\norme{f} = \sup_{x \in B} \frac{\norme{f(x)}}{\norme{x}} \le \unsur{\delta} \strictinferieur +\infty$$


** $n$-linéarité

On dit que la fonction $f : E_1 \times ... \times E_n \mapsto F$ est $n$-linéaire si elle est linéaire par rapport à chacune des composantes de son argument, les autres composantes restant inchangées :

$$f(...,\alpha x + \beta y,...) = \alpha \cdot f(...,x,...) + \beta \cdot f(...,y,...)$$

pour tout $\alpha,\beta \in \corps$ et $x,y \in E$. On note $\lineaire_n(E_1,...,E_n,F)$ l'ensemble des fonctions $n$-linéaires de $E_1 \times ... \times E_n$ vers $F$.


*** Norme

La norme est définie dans ce cas par :

$$\norme{f} = \sup \left\{ \frac{ \norme{f(x_1,...,x_n)} }{ \prod_{i = 1}^n \norme{x_i} } : (x_1,...,x_n) \in E_1 \times ... \times E_n \right\}$$

Si cette norme est finie, on a :

$$\norme{f(x_1,...,x_n)} \le \norme{f} \cdot \prod_{i = 1}^n \norme{x_i}$$

pour tout $(x_1,...,x_n) \in E_1 \times ... \times E_n$.


*** Bilinéarité

On dit aussi des fonctions $2$-linéaires qu'elles sont bilinéaires. La norme d'une fonction $f : E_1 \times E_2 \mapsto F$ bilinéaire est définie par :

$$\norme{f} = \sup \left\{ \frac{ \norme{f(u,v)} }{ \norme{u} \cdot \norme{v} } : (u,v) \in E_1 \times E_2 \right\}$$

Si cette norme est finie, on a :

$$\norme{f(u,v)} \le \norme{f} \cdot \norme{u} \cdot \norme{v}$$

pour tout $(u,v) \in E_1 \times E_2$.


** Représentation matricielle

Soit une application linéaire $\mathcal{A} : E \to F$. Choisissons $x \in E$ et posons :

$$y = \mathcal{A}(x)$$

Si on dispose d'une base $(e_1,...,e_n)$ de $E$ et d'une base $(f_1,...,f_m)$ de $F$, on a :

#+BEGIN_CENTER
\(
x = \sum_{i = 1}^n x_i \cdot e_i \\
y = \sum_{i = 1}^m y_i \cdot f_i
\)
#+END_CENTER

pour certains $x_i,y_i \in \corps$. La linéarité de $\mathcal{A}$ implique que :

$$y = \sum_{j = 1}^n \mathcal{A}(e_j) \cdot x_j$$

Si les $a_{ij} \in \corps$ sont les coordonnées de $\mathcal{A}(e_j)$ dans la base des $f_i$, on a :

$$\mathcal{A}(e_j) = \sum_{i = 1}^m a_{ij} \cdot f_i$$

En substituant cette expression, on obtient :

$$y = \sum_{i = 1}^m f_i \sum_{j = 1}^n a_{ij} \cdot x_j$$

La $i^{ème}$ coordonnée de $y$ est donc donnée par :

$$y_i = \sum_{j = 1}^n a_{ij} \cdot x_j$$

On définit la matrice $A \in \matrice(\corps,m,n)$ associée à $\mathcal{A}$ en posant :

$$A = ( a_{ij} )_{i,j}$$

Nous définissons ensuite le produit d'une matrice avec le « vecteur » $x$ équivalent de $\corps^n$ :

$$A \cdot x = \left( \sum_j a_{ij} \cdot x_j  \right)_i$$

de telle sorte que :

$$A \cdot x = \mathcal{A}(x)$$


*** Norme

La norme d'une matrice est la norme de l'application linéaire associée, c'est-à-dire :

$$\norme{A}_2 = \sup \left\{ \frac{ \norme{A \cdot x} }{ \norme{x} } : x \in \corps^n, \ x \ne 0 \right\}$$

Soit :

$$M = \max_{i,j} \abs{\composante_{ij} A}$$

On a alors :

$$\norme{A \cdot x} \le M \cdot m \cdot n \cdot \max_i x_i \le M \cdot m \cdot n \cdot \norme{x}$$

ce qui montre que :

$$\norme{A} \le M \cdot m \cdot n \strictinferieur \infty$$

La norme d'une matrice finie ($m,n \strictinferieur \infty$) existe toujours.


*** Image

L'image d'une matrice est l'image de l'application linéaire associée, c'est-à-dire :

$$\image A = \{ A \cdot x : x \in \corps^n \}$$

Si $c_i = \colonne_i A$, on a :

$$A = [ c_1 \ c_2 \ ... \ c_n ]$$

On voit que :

$$A \cdot x = \sum_i c_i \cdot x_i$$

autrement dit l'image de $A$ est l'espace vectoriel engendré par ses colonnes :

$$\image A = \combilin{c_1,c_2,...,c_n}$$


*** Noyau

Le noyau d'une matrice est le noyau de l'application linéaire associée, c'est-à-dire :

$$\noyau A = \{ x \in \corps^n : A \cdot x = 0 \}$$


** Produit matriciel

Soit à présent les matrices $A \in \matrice(\corps,m,n)$ et $B \in \matrice(\corps,n,p)$ données par :

#+BEGIN_CENTER
\(
A = ( a_{ij} )_{i,j} \\
B = ( b_{ij} )_{i,j}
\)
#+END_CENTER

où les $a_{ij},b_{ij} \in K$. Soit les applications linéaires $\mathcal{B} : \corps^p \to \corps^n$ et $\mathcal{A} : \corps^n \to \corps^m$ définies par :

#+BEGIN_CENTER
\(
\mathcal{B}(x) = B \cdot x \\
\mathcal{A}(y) = A \cdot y
\)
#+END_CENTER

pour tout $x \in \corps^p$, $y \in \corps^n$. Choisissons $z \in \corps^m$ et relions $x,y,z$ par :

#+BEGIN_CENTER
\(
y = \mathcal{B}(x) \\
z = \mathcal{A}(y) = \big( \mathcal{A} \circ \mathcal{B} \big)(x)
\)
#+END_CENTER

Examinons les composantes de $z$ en fonction de celles de $x$ :

$$z_i = \sum_k a_{ik} \cdot y_k = \sum_k a_{ik} \sum_j b_{kj} \cdot x_j = \sum_{k,j} a_{ik} \cdot b_{kj} \cdot x_j$$

On en déduit que la composée $\mathcal{C} = \mathcal{A} \circ \mathcal{B}$ est représentée par la matrice $C \in \matrice(\corps,m,p)$ de composantes :

$$\composante_{ij} C = c_{ij} = \sum_k a_{ik} \cdot b_{kj}$$

Il suffit donc de définir le produit matriciel $A \cdot B$ par :

$$A \cdot B = \left(\sum_{k=1}^n a_{ik} \cdot b_{kj}\right)_{i,j}$$

pour avoir :

$$(A \cdot B) \cdot x = \big( \mathcal{A} \circ \mathcal{B} \big)(x)$$

Le produit matriciel représente donc une composée d'applications linéaires. Pour que ce produit soit bien défini, il est nécessaire que le nombre
de colonnes $n$ de $A$ et le nombre de lignes de $B$ soient identiques.

On voit également que le produit matrice - vecteur défini précédemment en est un cas particulier lorsque $p = 1$.


*** Notation

En pratique, on laisse souvent tomber le ``$\cdot$'' et on note $A B$
au lieu de $A \cdot B$ lorsqu'il est évident que $A$ et $B$ sont deux
matrices différentes.


*** Taille

Le produit d'une matrice de taille $(m,n)$ par une matrice de taille $(n,p)$ est une matrice de taille $(m,p)$.


*** Lignes et colonnes

Si $x_i^T = \ligne_i(A)$ et $y_j = \colonne_j(B)$, on voit que :

$$\composante_{ij} (A \cdot B) = x_i^T \cdot y_j$$


*** Associativité

Soit les matrices $A \in \matrice(\corps,m,n)$, $B \in \matrice(\corps,n,p)$ et $C \in \matrice(\corps,p,q)$ données par :

#+BEGIN_CENTER
\(
A = ( a_{ij} )_{i,j} \\
B = ( b_{ij} )_{i,j} \\
C = ( c_{ij} )_{i,j}
\)
#+END_CENTER

où les $a_{ij},b_{ij},c_{ij} \in K$. La relation :

$$A \cdot (B \cdot C) = \left( \sum_{k,l} a_{ik} \cdot b_{kl} \cdot c_{lj} \right)_{i,j} = (A \cdot B) \cdot C$$

nous montre que la multiplication entre matrices est associative. On définit :

$$A \cdot B \cdot C = A \cdot (B \cdot C) = (A \cdot B) \cdot C$$


*** Distributivité

On a aussi les propriétés de distribution :

#+BEGIN_CENTER
\(
A \cdot (B + C) = A \cdot B + A \cdot C \\
(B + C) \cdot D = B \cdot D + C \cdot D
\)
#+END_CENTER

où $A \in \matrice(\corps,m,n)$, $B,C \in \matrice(\corps,n,p)$ et $D \in \matrice(\corps,p,q)$.


*** Non commutativité

Par contre, on peut trouver des matrices $A$ et $B$ telles que :

$$A \cdot B \ne B \cdot A$$

La multiplication matricielle n'est donc en général pas commutative. D'ailleurs, pour que ces deux produits existent simultanément, il faut que $A$ et $B$ soient toutes deux carrées, ce qui n'est pas forcément le cas.


*** Commutateur

La matrice associée au commutateur :

$$[\mathcal{A},\mathcal{B}] = \mathcal{A} \circ \mathcal{B} - \mathcal{B} \circ \mathcal{A}$$

est donnée par le commutateur équivalent :

$$[A,B] = A \cdot B - B \cdot A$$


*** Transposée

On vérifie que :

$$(A \cdot B)^T = B^T \cdot A^T$$


** Blocs

En utilisant l'associativité de l'addition, on peut facilement vérifier que la formule de multiplication reste valable lorsqu'on considère des blocs de matrices au lieu des éléments, à condition de respecter l'ordre de multiplication. Un exemple fréquemment utilisé :

#+BEGIN_CENTER
\(
\begin{Matrix}{cc}
A_{11} & A_{12} \\ A_{21} & A_{22}
\end{Matrix}
\cdot
\begin{Matrix}{cc}
B_{11} & B_{12} \\ B_{21} & B_{22}
\end{Matrix}
=
\begin{Matrix}{cc}
A_{11} \cdot B_{11} +  A_{12} \cdot B_{21} & A_{11} \cdot B_{12} +  A_{12} \cdot B_{22} \\
A_{21} \cdot B_{11} +  A_{22} \cdot B_{21} & A_{21} \cdot B_{12} +  A_{22} \cdot B_{22}
\end{Matrix}
\)
#+END_CENTER


*** Bloc-diagonale

Un cas particulier important :

#+BEGIN_CENTER
\(
\begin{Matrix}{cc}
A_1 & 0 \\ 0 & A_2
\end{Matrix}
\cdot
\begin{Matrix}{cc}
B_1 & 0 \\ 0 & B_2
\end{Matrix}
=
\begin{Matrix}{cc}
A_1 \cdot B_1 & 0 \\
0 & A_2 \cdot B_2
\end{Matrix}
\)
#+END_CENTER


** Matrice identité

La matrice identité $I \in \matrice(\corps,n,n)$ correspond à la fonction $\identité$. On a donc :

$$I \cdot x = x$$

pour tout $x \in \corps^n$. Si $(\canonique_1,...\canonique_n)$ est la base canonique de $\corps^n$, on a donc :

$$I \cdot \canonique_i = \canonique_i$$

ce qui entraîne directement :

$$I = ( \indicatrice_{ij} )_{i,j}$$

On remarque que :

$$I = [\canonique_1 \ \hdots \ \canonique_n]$$


*** Neutre

Comme la fonction identité est neutre pour la composition, la matrice unité correspondante $I \in \matrice(\corps,m,n)$ doit être neutre pour la multiplication avec toutes les matrices de dimensions compatibles. Soit $A \in \matrice(\corps,m,n)$ et $B \in \matrice(\corps,n,p)$. On vérifie que l'on a bien :

#+BEGIN_CENTER
\(
A \cdot I = A \\
I \cdot B = B
\)
#+END_CENTER


*** Notation

On note aussi $I_n$ pour préciser que $I$ est de taille $(n,n)$.


** Inverse

Lorsqu'elle existe, la matrice inverse de $A$, notée $A^{-1}$, reflète l'application linéaire inverse sous-jacente. Elle est donc l'unique matrice telle que :

$$A^{-1} \cdot A = A \cdot A^{-1} = I$$


** Inverse d'un produit

Soit $A$ et $B$ deux matrices inversibles. Les relations $C \cdot (A \cdot B) = I$ et $(A \cdot B) \cdot D = I$ nous donnent :

$$C = D = B^{-1} \cdot A^{-1}$$

et donc :

$$(A \cdot B)^{-1} = B^{-1} \cdot A^{-1}$$


*** Inverse à gauche et à droite

On dit que $L$ est un inverse à gauche de $A$ si $L \cdot A = I$. On dit que $R$ est un inverse à droite de $A$ si $A \cdot R = I$.


** Puissance

Il est possible de multiplier une matrice carrée $A$ avec elle-même.
On peut donc définir l'exposant par :

#+BEGIN_CENTER
\(
A^0 = I \\
A^k = A \cdot A^{k-1}
\)
#+END_CENTER


*** Négative

Si l'inverse $A^{-1}$ existe, on définit également :

$$A^{-k} = (A^{-1})^k$$


** Polynômes matriciels

Ici, $\corps$ n'est plus un corps mais l'anneau des matrices $X$ de taille $(N,N)$. On dit que $p : \matrice(\corps,N,N) \mapsto \matrice(\corps,N,N)$ est un polynôme matriciel si il existe $a_0,...,a_n \in \corps$ tels que :

$$p(X) = \sum_{i = 0}^n a_i \cdot X^i$$

pour tout $X \in \matrice(\corps,N,N)$.


* Géométrie

#+TOC: headlines 1 local


** Courbe

Une courbe sur un espace vectoriel $E$ (par exemple $\setR^n$) est de la forme :

#+BEGIN_CENTER
\(
\Lambda = \{ \lambda(s) : s \in [\alpha,\beta] \}
\)
#+END_CENTER

où $\lambda : [\alpha,\beta] \mapsto E$ est une fonction continue et où $\alpha,\beta \in \setR$ vérifient $\alpha \le \beta$.


** Segment

Les segments sont une généralisation des intervalles. Un segment de $u \in E$ vers $v \in E$ est un cas particulier de courbe où $\lambda : [0,1] \subseteq \setR \mapsto E$ est une fonction linéaire définie par :

#+BEGIN_CENTER
\(
\lambda(s) = u + s \cdot (v - u)
\)
#+END_CENTER

pour tout $s \in [0,1] \subseteq \setR$. On voit que $\lambda(0) = u$ et que $\lambda(1) = v$. On note aussi :

#+BEGIN_CENTER
\(\relax
[u,v] = \lambda([0,1]) = \{ u + s \cdot (v - u) : s \in [0,1] \} \subseteq E
\)
#+END_CENTER


*** Alternative

On dispose aussi d'une définition alternative. On utilise :

#+BEGIN_CENTER
\(
L = \{ (s,t) \in \setR^2 : (s,t) \ge 0 \text{ et } s + t = 1 \}
\)
#+END_CENTER

et la fonction $\sigma : L \mapsto E$ définie par :

#+BEGIN_CENTER
\(\relax
\sigma(s,t) = s \cdot u + t \cdot v
\)
#+END_CENTER

On a alors $[u,v] = \sigma(L)$. On voit aussi que $\sigma(1,0) = u$ et $\sigma(0,1) = v$.


** Enveloppe convexe

Soit $A \subseteq E$ et la collection des segments reliant deux points quelconques de $A$ :

#+BEGIN_CENTER
\(
\mathcal{S} = \{ [u,v] : u,v \in A \}
\)
#+END_CENTER

L'enveloppe convexe de $A$ est l'union de tous ces segments :

#+BEGIN_CENTER
\(
\convexe(A) = \bigcup \mathcal{S}
\)
#+END_CENTER

Pour tout $u,v \in A$ et $(s,t) \in \setR^2$ tels que $s,t \ge 0$ et $s + t = 1$, on a donc :

#+BEGIN_CENTER
\(
s \cdot u + t \cdot v \in \convexe(A)
\)
#+END_CENTER


*** Inclusion

Il suffit de considérer le choix $(s,t) = (1,0)$ pour voir que tout $u \in A$ appartient à $\convexe(A)$. On a donc $A \subseteq \convexe(A)$.


*** Ensemble convexe

On dit qu'un ensemble $C \subseteq E$ est convexe si $\convexe(C) = C$.


** Surface

Une surface de $E$ est de la forme :

#+BEGIN_CENTER
\(
\Phi = \{ \varphi(s,t) : (s,t) \in [a,b] \times [c,d] \}
\)
#+END_CENTER

où $\varphi : [a,b] \times [c,d] \mapsto E$ est une fonction continue et où $a,b,c,d \in \setR$ vérifient $a \le b$ et $c \le d$.


* Formes linéaires

#+TOC: headlines 1 local

\label{chap:forme}


** Dépendances

  - Chapitre \ref{chap:relation} : Les fonctions
  - Chapitre \ref{chap:lineaire} : Les fonctions linéaires


** Définition

Soit un espace vectoriel $E$ sur $\corps$. Une forme linéaire est une fonction linéaire continue
$\varphi : E \mapsto \corps$.


** Espace dual

L'espace dual $E^\dual$ de $E$ est l'ensemble des formes linéaires sur $E$ , autrement dit
l'ensemble des fonctions linéaires continues de $E$ vers $\corps$ :

$$E^\dual = \{ \varphi \in \lineaire(E,\corps) : \norme{\varphi}_\lineaire \strictinferieur +\infty \}$$

Il s'agit d'un espace vectoriel pour les opérations d'addition et de multiplication mixte définies sur les fonctions.


** Notation

Pour toute forme $\varphi \in E^\dual$ et tout vecteur $v \in E$, on note :

$$\forme{\varphi}{v} = \varphi(v)$$

ce qui définit implicitement la fonction $\forme{}{} : E^\dual \times E \mapsto \corps$.


** Linéarité

Soit $\varphi,\psi \in E^\dual$, $u,v \in E$ et $\alpha,\beta \in S$. Comme $\varphi$ est linéaire, on a :

$$\forme{\varphi}{\alpha \cdot u + \beta \cdot v}  = \alpha \cdot \forme{\varphi}{u} + \beta \cdot \forme{\varphi}{v}$$

Symétriquement, la définition des opérations sur les fonctions nous donne également :

$$\forme{\alpha \cdot \varphi + \beta \cdot \psi}{u}  = \alpha \cdot \forme{\varphi}{u} + \beta \cdot \forme{\psi}{u}$$

L'application $\forme{}{}$ est donc bilinéaire.


** Biorthonormalité

On dit que les suites $(\Phi_1,...,\Phi_m)$ de $E^\dual$ et
$(e_1,...,e_n)$ de $E$ sont biorthonormées si :

$$\forme{\Phi_i}{e_j} = \indicatrice_{ij}$$

pour tout $(i,j) \in \setZ(0,m) \times \setZ(0,n)$. De telles suites permettent d'évaluer facilement les coefficients des développements en série du type :

$$\varphi = \sum_{i = 1}^m \alpha_i \cdot \Phi_i$$

où $\alpha_1,...,\alpha_m \in S$. En effet, il suffit d'évaluer :

$$\varphi(e_j) = \forme{\varphi}{e_j} = \sum_{i = 1}^m \alpha_i \cdot \forme{\Phi_i}{e_j} = \sum_{i = 1}^m \alpha_i \cdot \indicatrice_{ij} = \alpha_j$$

pour obtenir les valeurs des $\alpha_j$.

Réciproquement, si :

$$u = \sum_{i = 1}^n \beta_i \cdot e_i$$

avec $\beta_1,...,\beta_n \in S$, on a :

$$\Phi_j(u) = \forme{\Phi_j}{u} = \sum_{i = 1}^n \beta_i \cdot \forme{\Phi_j}{e_i} = \sum_{i = 1}^n \beta_i \cdot \indicatrice_{ij} = \beta_j$$

ce qui nous donne les valeurs des $\beta_j$.

Forts de ces résultats, il est aisé d'évaluer :

$$\forme{\varphi}{u} = \sum_{i,j} \alpha_i \cdot \forme{\Phi_i}{u_j} \cdot \beta_j = \sum_{i,j} \alpha_i \cdot \indicatrice_{ij} \cdot \beta_j = \sum_i \alpha_i \cdot \beta_i$$

On a donc en définitive :

$$\forme{\varphi}{u} = \sum_i \forme{\varphi}{e_i} \cdot \forme{\Phi_i}{u}$$


** Similitude

On dit que deux fonctions $u,v \in E$ sont identique au sens des distributions si :

$$\forme{\varphi}{u} = \forme{\varphi}{v}$$

pour tout $\varphi \in E^\dual$.

Symétriquement, les deux formes $\varphi,\psi \in E^\dual$ sont identiques par définition si et seulement si :

$$\forme{\varphi}{u} = \forme{\psi}{u}$$

pour tout $u \in E$.


** Espace bidual

On définit l'espace bidual de $E$, noté $E^{\dual \dual}$, par :

$$E^{\dual \dual} = (E^\dual)^\dual$$

On associe à chaque élément $u \in E$ un élément $\hat{u} \in E^{\dual \dual}$ par la condition :

$$\hat{u}(\varphi) = \varphi(u)$$

qui doit être vérifiée pour tout $\varphi \in E^\dual$. On a donc :

$$\forme{\hat{u}}{\varphi} = \forme{\varphi}{u}$$


** Application duale

Soit les espaces vectoriels $E$ et $F$ sur $\corps$ et une fonction $A : E \mapsto F$. Le dual de $A$ au sens des formes, s'il existe, est l'unique fonction $A^\dual : F^\dual \mapsto E^\dual$ telle que :

$$\forme{ A^\dual(\varphi) }{u} = \forme{\varphi}{ A(u) }$$

pour tout $u \in E$ et $\varphi \in F^\dual$.


** Formes bilinéaires

Soit les espaces vectoriels $E$ et $F$ sur $\corps$. Une forme bilinéaire est une fonction bilinéaire continue $\vartheta : F \times E \mapsto \corps$. On utilise une notation analogue à celle des formes :

$$\biforme{x}{\vartheta}{u} = \vartheta(x,u)$$

pour tout $x \in F$ et $u \in E$. On voit que :

#+BEGIN_CENTER
\(
\biforme{\alpha \cdot x + \beta \cdot y}{\vartheta}{u} = \alpha \cdot \biforme{x}{\vartheta}{u} + \beta \cdot \biforme{y}{\vartheta}{u} \\
\biforme{x}{\vartheta}{\alpha \cdot u + \beta \cdot v} = \alpha \cdot \biforme{x}{\vartheta}{u} + \beta \cdot \biforme{x}{\vartheta}{v}
\)
#+END_CENTER

pour tout $\alpha,\beta \in \corps$, $u,v \in E$ et $x,y \in F$.


** Formes quadratiques

Soit la forme bilinéaire $ \vartheta : E \times E \mapsto \corps$. Une forme quadratique $\mathcal{Q} : E \mapsto \corps$ est une fonction de la forme :

$$\mathcal{Q}(x) = \biforme{x}{\vartheta}{x}$$


** Représentation matricielle

On peut représenter toute forme linéaire $\varphi \in \lineaire(\corps^n,\corps)$ par un vecteur matriciel $\hat{\varphi} \in \corps^n$. Etant donné la base canonique $(e_1,...,e_n)$ de $\corps^n$, il suffit de poser :

$$\hat{\varphi}_i = \forme{\varphi}{e_i}$$

pour avoir :

$$\forme{\varphi}{u} = \hat{\varphi}^T \cdot u$$

pour tout $u \in \corps^n$.


*** Formes bilinéaires

On peut représenter toute forme bilinéaire $\vartheta \in \lineaire(\corps^m \times \corps^n,\corps)$ par une matrice $\Theta \in \matrice(K,m,n)$. Etant donné les bases canoniques $(f_1,...,f_m)$ de $\corps^m$ et $(e_1,...,e_n)$ de $\corps^n$, il suffit de poser :

$$\composante_{ij} \Theta = \biforme{f_i}{\vartheta}{e_j}$$

pour avoir :

$$\biforme{v}{\vartheta}{u} = v^T \cdot \Theta \cdot u$$

pour tout $u \in \corps^n$ et tout $v \in \corps^m$.


* Produit scalaire

#+TOC: headlines 1 local

\label{chap:ps}


** Dépendances

  - Chapitre \ref{chap:vecteur} : Les espaces vectoriels
  - Chapitre \ref{chap:norme} : Les normes


** Introduction

Soit $E$ un espace vectoriel sur $\corps$ et une famille de fonctions linéaires $\phi_u \in E^\dual$ où $u \in E$ est un paramètre vectoriel. Nous pouvons écrire :

$$\phi_u(v) = \forme{\phi_u}{v}$$

pour tout $u,v \in E$. Cette expression introduit implicitement le produit dérivé $\scalaire{}{} : E \times E \mapsto \corps$ défini par :

$$\scalaire{u}{v} = \forme{\phi_u}{v}$$

pour tout $u,v \in E$. Ce produit hérite bien entendu la linéarité à droite de la forme associée :

$$\scalaire{u}{\alpha \cdot v + \beta \cdot w}  = \alpha \cdot \scalaire{u}{v} + \beta \cdot \scalaire{u}{w}$$

pour tout $u,v,w \in E$ et pour tout $\alpha,\beta \in \corps$.

Les produits scalaires sont des cas particuliers de ce type de produit.


*** Notation

On note aussi $u \cdot v = \scalaire{u}{v}$.


** Produit scalaire réel

Considérons le cas particulier où $\corps = \setR$. et un produit linéaire à droite $\scalaire{}{} : E \times E \mapsto \setR$. Nous voudrions en plus que la valeur de $\scalaire{u}{u}$ en chaque $u \in E$ puisse représenter la norme de $u$. Nous imposons donc la positivité :

$$\scalaire{u}{u} \ge 0$$

Pour compléter le caractère strictement défini positif, on impose également que le seul élément $u \in E$ vérifiant :

$$\scalaire{u}{u} = 0$$

soit le vecteur nul $u = 0$. Ce qui revient à dire que :

$$\scalaire{u}{u} > 0$$

pour tout $u \in E \setminus \{ 0 \}$.

Si on peut également interchanger n'importe quels $u,v \in E$ sans changer le résultat :

$$\scalaire{u}{v} = \scalaire{v}{u}$$

on dit que $\scalaire{}{}$ est un produit scalaire réel sur $E$.

Nous déduisons directement de la linéarité à droite et de la symétrie que :

$$\scalaire{\alpha \cdot u + \beta \cdot v}{w} = \alpha \cdot \scalaire{u}{w} + \beta \cdot \scalaire{v}{w}$$

pour tout $\alpha,\beta \in \setR$ et $u,v,w \in E$. Le produit scalaire réel est bilinéaire.


** Produit scalaire complexe

Examinons à présent le cas $\corps = \setC$. On demande qu'un produit scalaire $\scalaire{}{} : E \times E \mapsto \setC$ soit strictement défini positif. Pour cela, les valeurs de $\scalaire{u}{u}$ doivent être réelles et positives :

#+BEGIN_CENTER
\(
\scalaire{u}{u} \in \setR \\
\scalaire{u}{u} \ge 0
\)
#+END_CENTER

pour tout $u \in E$. Ensuite, il faut également que le seul élément $u \in E$ vérifiant :

$$\scalaire{u}{u} = 0$$

soit le vecteur nul $u = 0$.

Le caractère réel de $\scalaire{u}{u}$ implique que :

$$\scalaire{u}{u} = \conjaccent{\scalaire{u}{u}}$$

où la barre supérieure désigne comme d'habitude le complexe conjugué. Cette constatation nous mène à une variante de la symétrie. On impose :

$$\scalaire{u}{v} = \conjaccent{\scalaire{v}{u}}$$

pour tout $u,v \in E$. On dit que le produit scalaire complexe est hermitien.

La linéarité à droite s'exprime simplement par :

$$\scalaire{u}{\alpha \cdot v + \beta \cdot w}  = \alpha \cdot \scalaire{u}{v} + \beta \cdot \scalaire{u}{w}$$

pour tout $u,v,w \in E$ et pour tout $\alpha,\beta \in \setC$. On déduit de la linéarité et du caractère hermitien du produit scalaire complexe que :

\begin{align}
\scalaire{\alpha \cdot u + \beta \cdot v}{w}  &= \conjaccent{\scalaire{w}{\alpha \cdot u + \beta \cdot v}} \\
&= \conjaccent{\alpha} \cdot \conjaccent{\scalaire{w}{u}} + \conjaccent{\beta} \cdot \conjaccent{\scalaire{w}{v}}
\end{align}

et finalement :

$$\scalaire{\alpha \cdot u + \beta \cdot v}{w} = \conjaccent{\alpha} \cdot \scalaire{u}{w} + \conjaccent{\beta} \cdot \scalaire{v}{w}$$

On dit que le produit scalaire est antilinéaire à gauche.


*** Corollaire

En particulier, si $u,v,w,x$ sont des vecteurs de $E$ et
si $\alpha = \scalaire{u}{v} \in \setC$, on a :

#+BEGIN_CENTER
\(
\scalaire{w}{\alpha \cdot x} = \scalaire{w}{ \scalaire{u}{v} \cdot x} = \scalaire{u}{v} \cdot \scalaire{w}{x} \\
\scalaire{\alpha \cdot w}{x} = \scalaire{ \scalaire{u}{v} \cdot w}{x} = \scalaire{v}{u} \cdot \scalaire{w}{x}
\)
#+END_CENTER


*** Cas particulier

Comme $\conjaccent{x} = x$ pour tout $x \in \setR \subseteq \setC$, on peut considérer le produit scalaire réel comme un cas particulier de produit scalaire complexe.


** Espace orthogonal


*** A un vecteur

Soit $x \in H$. On définit l'ensemble $x^\orthogonal$ par :

$$x^\orthogonal = \{ z \in E : \scalaire{x}{z} = 0 \}$$

On dit des vecteurs de $x^\orthogonal$ qu'ils sont orthogonaux à $x$.


*** A un ensemble

Pour tout sous-ensemble $V \subseteq E$, l'ensemble orthogonal à $V$ est l'ensemble des vecteurs qui sont orthogonaux à tous les éléments de $V$ :

$$V^\orthogonal = \bigcap_{x \in V} x^\orthogonal$$

Pour tout $z \in V^\orthogonal$, on a donc $\scalaire{x}{z} = 0$ quel que soit $x \in V$.

Nous allons vérifier que $V^\orthogonal$ est un sous-espace vectoriel. Soit $z \in V$. Comme $\scalaire{z}{0} = 0$, on a $0 \in V^\orthogonal$. Soit $x,y \in V^\orthogonal$, $\alpha,\beta \in \corps$. On a :

$$\scalaire{z}{\alpha \cdot x + \beta \cdot y} = \alpha \cdot \scalaire{z}{x} + \beta \cdot \scalaire{z}{y} = 0 + 0 = 0$$

ce qui montre que $\alpha \cdot x + \beta \cdot y \in V^\orthogonal$.


** Egalité

Si $u,v \in E$ sont tels que :

$$\scalaire{u}{w} = \scalaire{v}{w}$$

pour tout $w \in E$, on a :

$$\scalaire{u - v}{w} = 0$$

Le choix $w = u - v \in E$ nous donne alors :

$$\scalaire{u - v}{u - v} = 0$$

ce qui implique $u - v = 0$ et donc $u = v$.


** Base orthonormée

Une base $(e_1,...,e_n)$ de $E$ est dite orthonormée si le produit scalaire de deux vecteurs $e_i \ne e_j$ s'annule, tandis que le produit scalaire d'un $e_i$ avec lui-même donne l'unité :

$$\scalaire{e_i}{e_j} = \indicatrice_{ij}$$


*** Coordonnées

Soit $u \in E$ de coordonnée $u_i \in \corps$ :

$$u = \sum_{i = 1}^n u_i \cdot e_i$$

En effectuant le produit scalaire de $u$ avec $e_k$, on arrive à  :

#+BEGIN_CENTER
\(
\scalaire{e_k}{u} = \sum_{i = 1}^n u_i \cdot \scalaire{e_k}{e_i} \\
\scalaire{e_k}{u} = \sum_{i = 1}^n u_i \cdot \indicatrice_{ik}
\)
#+END_CENTER

Tous les termes de cette dernière somme s'annulent sauf lorsque $i = k$, et on a :

$$\scalaire{e_k}{u} = u_k$$

On peut donc écrire :

$$y = \sum_{i = 1}^n \scalaire{e_i}{u} \cdot e_i$$


*** Indépendance linéaire

On peut voir que si une suite de vecteurs $e_i$ est orthonormée,
(ils ne forment pas forcément une base) ils sont toujours linéairement
indépendant. En effet si les scalaires $a_i$, sont tels que :

$$\sum_{i=1}^n a_i \cdot e_i = 0$$

on a alors :

$$a_i = \scalaire{e_i}{0} = 0$$


** Produit scalaire et coordonnées

Soit $(e_1,...,e_n)$ une base de $E$ et $u,v \in E$. On a :

#+BEGIN_CENTER
\(
u = \sum_i u_i \cdot e_i \\
v = \sum_i v_i \cdot e_i
\)
#+END_CENTER

pour certains $u_i,v_i \in \corps$. Posons :

$$g_{ij} = \scalaire{e_i}{e_j}$$

où $\scalaire{}{} : E \times E \mapsto \setC$ est un produit scalaire complexe. Nous pouvons faire sortir les sommes en utilisant les propriétés du produit scalaire, ce qui nous donne :

$$\scalaire{u}{v} = \sum_{i,j} \conjaccent{u}_i \cdot g_{ij} \cdot v_j$$


*** Réel

Dans les cas d'un produit scalaire réel, on a $\conjaccent{u}_i = u_i$ et l'expression devient :

$$\scalaire{u}{v} = \sum_{i,j} u_i \cdot g_{ij} \cdot v_j$$


*** Base orthonormée

Si la base $(e_1,...,e_n)$ est orthonormée, l'expression du produit scalaire se simplifie en :

$$\scalaire{u}{v} = \sum_i \conjaccent{u}_i \cdot v_i$$


** Application définie positive

Soit une application linéaire $A : E \mapsto E$. Si le produit scalaire de $u$ avec $A(u)$ est un réel positif :

$$\scalaire{u}{A(u)} = \scalaire{A(u)}{u} \ge 0$$

pour tout $u \in E$, on dit que $A$ est définie positive.

** Produit scalaire sur $\setR^n$

Soit $x,y \in \setR^n$ tels que :

#+BEGIN_CENTER
\(
x = (x_1,x_2,...,x_n) \\
y = (y_1,y_2,...,y_n)
\)
#+END_CENTER

pour certains $x_i,y_i \in \setR$.

Le produit scalaire usuel sur $\setR^n$ est défini par :

$$\scalaire{x}{y} = \sum_{i = 1}^n x_i y_i$$

** Produit scalaire sur $\setC^n$

Soit $x,y \in \setC^n$ tels que :

#+BEGIN_CENTER
\(
x = (x_1,x_2,...,x_n) \\
y = (y_1,y_2,...,y_n)
\)
#+END_CENTER

pour certains $x_i,y_i \in \setC$.

Le produit scalaire usuel est défini par :

$$\scalaire{x}{y} = \sum_{i=1}^n \conjaccent{x}_i y_i$$

** Base orthonormée sur $\corps^n$

Soit $\corps \in \{ \setR , \setC \}$. Il est clair que la base canonique de $\corps^n$ :

$$e_i = ( \indicatrice_{ij} )_{i,j}$$

vérifie :

$$\scalaire{e_i}{e_j} = \indicatrice_{ij}$$

pour le produit scalaire usuel sur $\corps^n$. La suite $(e_1,...,e_n)$ forme une base orthonormée.


** Représentation matricielle

Soit $x = (x_1,..,x_n) , y = (y_1,...,y_n) \in \setC^n$. On définit les vecteurs colonne associé :

#+BEGIN_CENTER
\(
x =
\begin{Matrix}{c}
x_1 \\
x_2 \\
\vdots \\
x_n
\end{Matrix}
\qquad \qquad
y =
\begin{Matrix}{c}
y_1 \\
y_2 \\
\vdots \\
y_n
\end{Matrix}
\)
#+END_CENTER

L'équivalence entre $\setC^n$ et $\matrice(\setC,n,1)$ nous amène à :

$$\scalaire{x}{y} = \sum_i \conjaccent{x}_i \cdot y_i$$

Le membre de droite n'est rien d'autre que le produit « matriciel » $\conjaccent{x^T} \cdot y$ et on a donc :

$$\scalaire{x}{y} = \conjaccent{x^T} \cdot y$$

On vérifie que la base $(e_1,...,e_n)$ est orthonormée pour ce produit scalaire :

$$e_i^T \cdot e_j = \indicatrice_{ij}$$


** Application linéaire

Soit les espaces vectoriels $E,F$ sur $\corps$ et une application linéaire $\mathcal{A} : E \mapsto F$. On prend une base $(e_1,..,e_n)$ de $E$ et une base orthonormée $(f_1,...,f_m)$ de $F$. Comme les composantes de la matrice associée $A$ sont les coordonnées de $\mathcal{A}(e_j)$ dans la base des $f_i$, on a :

$$\composante_{ij} A = \scalaire{f_i}{\mathcal{A}(e_j)}$$


** Matrice de produit scalaire

Soit un espace vectoriel $E$ sur $\corps$ et un produit scalaire $\scalaire{}{}$ quelconque défini sur $E$. Soit $(e_1,...,e_n)$ une base quelconque de $E$ (nous ne supposons pas qu'elle soit orthonormée). Si $\hat{x},\hat{y} \in E$, on a :

#+BEGIN_CENTER
\(
\hat{x} = \sum_i x_i \cdot e_i \\
\hat{y} = \sum_i y_i \cdot e_i
\)
#+END_CENTER

pour certains $x_i,y_i \in S$. Or, nous avons vu que :

$$\scalaire{\hat{x}}{\hat{y}} = \sum_{i,j} \conjaccent{x}_i \scalaire{e_i}{e_j} y_j$$

Si nous définissons la matrice des produits scalaires $A \in \matrice(\corps,m,n)$ par :

$$\composante_{ij} A = \scalaire{e_i}{e_j}$$

nous pouvons réécrire le produit scalaire sous la forme :

$$\scalaire{\hat{x}}{\hat{y}} = \conjaccent{x^T} \cdot A \cdot y$$

où $x,y$ sont les vecteurs colonne associés à $\hat{x},\hat{y}$ :

#+BEGIN_CENTER
\(
x = [x_1 \ x_2 \ ... \ x_n]^T \\
y = [y_1 \ y_2 \ ... \ y_n]^T
\)
#+END_CENTER

Cette matrice possède d'importantes propriétés issues du produit scalaire. On a $\conjaccent{x^T} \cdot A \cdot x > 0$ pour tout $x \ne 0$. On dit que $A$ est une matrice définie positive. Le caractère hermitien du produit scalaire nous donne aussi $\conjaccent{A^T} = A$. On dit que $A$ est une matrice hermitienne, ou auto-adjointe.


*** Réciproque

Si $A$ est une matrice carrée définie positive et hermitienne, l'application définie par :

$$\scalaire{x}{y} = \conjaccent{x^T} \cdot A \cdot y$$

est bien un produit scalaire. En effet, la définie positivité de la matrice est équivalente à celle du produit ainsi défini. Pour le caractére hermitien, on a :

\begin{align}
\scalaire{x}{y} &= \sum_{i,j} \conjaccent{x}_i \cdot A_{ij} \cdot y_j = \sum_{i,j} \conjaccent{x}_i \cdot \conjaccent{A}_{ji} \cdot y_j \\
&= \sum_{i,j} y_j \cdot \conjaccent{A}_{ji} \cdot \conjaccent{x}_i = \conjugue \sum_{i,j} \conjaccent{y}_j \cdot A_{ji} \cdot x_i \\
&= \conjugue \scalaire{y}{x}
\end{align}


** Bases de vecteurs matriciels

Un cas particulier important survient lorsque les vecteurs sont des vecteurs matriciels. Soit une suite de vecteurs linéairement indépendants $u_1,u_2,...,u_m \in \matrice(\corps,n,1)$. Soit des $x_i,y_i \in \corps$ et les vecteurs :

#+BEGIN_CENTER
\(
X = \sum_{i = 1}^m x_i \cdot u_i \\
Y = \sum_{i = 1}^m y_i \cdot u_i
\)
#+END_CENTER

Si nous considérons les vecteurs $x,y \in \matrice(\corps,m,1)$ associés :

#+BEGIN_CENTER
\(
x = [x_1 \ x_2 \ ... \ x_m]^T \\
y = [y_1 \ y_2 \ ... \ y_m]^T
\)
#+END_CENTER

ainsi que la matrice $U \in \matrice(\corps,n,m)$ rassemblant les $u_i$ :

$$U = [u_1 \ u_2 \ ... \ u_m]$$

on peut réécrire la définition de $x,y$ sous la forme :

#+BEGIN_CENTER
\(
X = U \cdot x \\
Y = U \cdot y
\)
#+END_CENTER

On a alors :

$$\scalaire{X}{Y} = \conjaccent{X^T} \cdot Y = \conjaccent{x^T} \cdot \conjaccent{U^T} \cdot U \cdot y$$

On en conclut que la matrice $A = \conjaccent{U^T} \cdot U \in \matrice(\corps,m,m)$ est une matrice de produit scalaire


*** Réciproque

Soit $U \in \matrice(\corps,n,m)$ telle que $\noyau U = \{0\}$. La matrice $A = \conjaccent{U^T} \cdot U \in \matrice(\corps,m,m)$ est une matrice de produit scalaire. En effet :

$$\conjaccent{x^T} \cdot A \cdot x = \conjaccent{x^T} \cdot \conjaccent{U^T} \cdot U \cdot x = \conjugue(U \cdot x)^T \cdot (U \cdot x) \ge 0$$

Si $x \ne 0$, on a de plus $U \cdot x \ne 0$ et $\conjaccent{x}^T \cdot A \cdot x \strictsuperieur 0$. Par ailleurs, on a évidemment :

$$\conjaccent{A^T} = \conjaccent{(\conjaccent{U^T} \cdot U)^T} = \conjaccent{U^T} \cdot U = A$$


** Noyau

Soit $l_i = \ligne_i A$ et $x \in \noyau A$. On a alors :

$$0 = \composante_i (A \cdot x) = l_i \cdot x$$

On en conclut que les lignes de $A$ sont orthogonales aux $\conjaccent{l}_i$. Il en va de même pour toute combinaison linéaire de ces lignes, et :

$$\noyau A = \combilin{\conjaccent{l}_1,...,\conjaccent{l}_m}^\orthogonal$$


* Norme dérivée du produit scalaire

#+TOC: headlines 1 local

\label{chap:ps}


** Dépendances

  - Chapitre \ref{chap:vecteur} : Les espaces vectoriels
  - Chapitre \ref{chap:norme} : Les normes


** Introduction

Soit un espace vectoriel $E$ muni du produit scalaire $\scalaire{}{}$. Nous allons analyser les propriétés de l'application $\norme{.} : E \mapsto \corps$ associée au produit scalaire et définie par :

$$\norme{x} = \sqrt{ \scalaire{x}{x} }$$

pour tout $x \in E$.


** Addition

Soit $x,y \in E$ et $\alpha,\beta \in \setC$. On a :

\begin{align}
\norme{\alpha \cdot x + \beta \cdot y}^2 &= \scalaire{\alpha \cdot x + \beta \cdot y}{\alpha \cdot x + \beta \cdot y} \\
&= \conjaccent{\alpha} \cdot \alpha \cdot \scalaire{x}{x} + \conjaccent{\alpha} \cdot \beta \cdot \scalaire{x}{y} + \conjaccent{\beta} \cdot \alpha \cdot \scalaire{y}{x} + \conjaccent{\beta} \cdot \beta \cdot \scalaire{y}{y} \\
&= \abs{\alpha}^2 \cdot \norme{x}^2 + \conjaccent{\alpha} \cdot \beta \cdot \scalaire{x}{y} + \conjaccent{\beta} \cdot \alpha \cdot \scalaire{y}{x} + \abs{\beta}^2 \cdot \norme{y}^2
\end{align}

Dans le cas particulier où $\beta = 1$, on a :

\begin{align}
\norme{y + \alpha \cdot x} &= \norme{y}^2 + \alpha \cdot \scalaire{y}{x} + \conjaccent{\alpha} \cdot \scalaire{x}{y} + \abs{\alpha}^2 \cdot \norme{x}^2 \\
&= \norme{y}^2 + 2 \Re(\alpha \cdot \scalaire{y}{x}) + \abs{\alpha}^2 \cdot \norme{x}^2
\end{align}


** Théorème de Pythagore

Si $x,y \in E$ sont orthogonaux :

$$\scalaire{x}{y} = 0$$

on a également $\scalaire{y}{x} = \conjugue \scalaire{x}{y} = 0$ et :

\begin{align}
\scalaire{x + y}{x + y} &= \scalaire{x}{x} + \scalaire{x}{y} + \scalaire{y}{x} + \scalaire{y}{y} \\
&= \scalaire{x}{x} + \scalaire{y}{y}
\end{align}

En exprimant cette relation en terme de $\norme{.}$, on obtient :

$$\norme{x + y}^2 = \norme{x}^2 + \norme{y}^2$$

résultat connu sous le nom de théorème de Pythagore.


** Egalité du parallélogramme

En additionnant les équations :

#+BEGIN_CENTER
\(
\norme{x + y}^2 = \scalaire{x}{x} + \scalaire{x}{y} + \scalaire{y}{x} + \scalaire{y}{y} \\
\norme{x - y}^2 = \scalaire{x}{x} - \scalaire{x}{y} - \scalaire{y}{x} + \scalaire{y}{y}
\)
#+END_CENTER

on obtient :

$$\norme{x + y}^2 + \norme{x - y}^2 = 2(\scalaire{x}{x} + \scalaire{y}{y}) = 2 (\norme{x}^2 + \norme{y}^2)$$


** Inégalité de Cauchy-Schwartz

Soit $x,y \in E$ et $\lambda \in \setC$. On a :

$$\norme{y - \lambda \cdot x}^2 = \scalaire{y}{y} - \lambda \cdot \scalaire{y}{x} - \conjaccent{\lambda} \cdot \scalaire{x}{y} + \conjaccent{\lambda} \cdot \lambda \cdot \scalaire{x}{x} \ge 0$$

Le choix magique de $\lambda$ (nous verrons d'où il vient en étudiant les projections) est :

$$\lambda = \frac{ \scalaire{x}{y} }{ \scalaire{x}{x} }$$

On a alors :

$$\scalaire{y}{y} - \frac{ \scalaire{x}{y} \cdot \scalaire{y}{x} }{ \scalaire{x}{x} } - \frac{ \scalaire{y}{x} \cdot \scalaire{x}{y} }{ \scalaire{x}{x} } +  \frac{ \scalaire{y}{x} \cdot \scalaire{x}{y} }{ \scalaire{x}{x}^2 } \cdot \scalaire{x}{x} \ge 0$$

En simplifiant les termes, on arrive à :

$$\scalaire{y}{y}  - \frac{ \scalaire{x}{y} \cdot \scalaire{y}{x} }{ \scalaire{x}{x} } = \scalaire{y}{y}  - \frac{ \abs{\scalaire{x}{y}}^2 }{ \scalaire{x}{x} } \ge 0$$

En faisant passer le second terme dans le second membre et en multipliant par $\scalaire{x}{x}$, on arrive finalement à :

$$\abs{\scalaire{x}{y}}^2 \le \scalaire{x}{x} \cdot \scalaire{y}{y}$$

En prenant la racine carrée, on obtient une relation connue sous le nom d'inégalite de Cauchy-Schwartz :

$$\abs{\scalaire{x}{y}} \le \sqrt{\scalaire{x}{x} \cdot \scalaire{y}{y}} = \norme{x} \cdot \norme{y}$$


** Norme et inégalité de Minkowski

Nous allons à présent vérifier que l'application $\norme{.}$ est bien une norme.


*** Définie positivité

On voit que notre application est strictement définie positive car $\norme{x} \ge 0$ pour tout $x \in E$ et :

$$\norme{x} = 0 \Rightarrow \scalaire{x}{x} = 0 \Rightarrow x = 0$$


*** Produit mixte

La multiplication par un scalaire $\alpha \in \setC$ nous donne :

$$\norme{\alpha \cdot x} = \sqrt{ \abs{\alpha}^2 \cdot \scalaire{x}{x} } = \abs{\alpha} \cdot \sqrt{ \scalaire{x}{x} } = \abs{\alpha} \cdot \norme{x}$$


*** Inégalité de Minkowski

On a :

\begin{align}
\norme{x + y}^2 &= \scalaire{x}{x} + \scalaire{x}{y} + \scalaire{y}{x} + \scalaire{y}{y} \\
&= \scalaire{x}{x} + 2 \Re(\scalaire{x}{y}) + \scalaire{y}{y}
\end{align}

Mais comme $\abs{\Re(\scalaire{x}{y})} \le \abs{\scalaire{x}{y}} \le \norme{x} \cdot \norme{y}$, on a finalement :

$$\norme{x + y}^2 \le \norme{x}^2 + 2 \norme{x} \cdot \norme{y} + \norme{y}^2 = (\norme{x} + \norme{y})^2$$

d'où :

$$\norme{x + y} \le \norme{x} + \norme{y}$$

Cette troisième et dernière propriété étant vérifiée, l'application $\norme{.} = \sqrt{\scalaire{}{}}$ est bien une norme.


** Distance

On associe une distance à la norme et au produit scalaire par :

$$\distance(x,y) = \norme{x - y} = \sqrt{\scalaire{x - y}{x - y}}$$

pour tout $x,y \in E$.


** Produit scalaire à partir de la norme

En soutrayant les équations :

#+BEGIN_CENTER
\(
\norme{x + y}^2 = \scalaire{x}{x} + 2 \Re(\scalaire{x}{y}) + \scalaire{y}{y} \\
\norme{x - y}^2 = \scalaire{x}{x} - 2 \Re(\scalaire{x}{y}) + \scalaire{y}{y}
\)
#+END_CENTER

on obtient :

$$\norme{x + y}^2 - \norme{x - y}^2 = 4 \Re(\scalaire{x}{y})$$

Comme $\Re(\img z) = - \Im(z)$, on a aussi :

\begin{align}
\norme{x + \img y}^2 &= \scalaire{x}{x} + 2 \Re(\img \scalaire{x}{y}) + \scalaire{y}{y} \\
&=  \scalaire{x}{x} - 2 \Im(\scalaire{x}{y}) + \scalaire{y}{y} \\
\norme{x - \img y}^2 &= \scalaire{x}{x} - 2 \Re(\img \scalaire{x}{y}) + \scalaire{y}{y} \\
&=  \scalaire{x}{x} + 2 \Im(\scalaire{x}{y}) + \scalaire{y}{y}
\end{align}

En soustrayant ces deux résultats, on a donc :

$$\norme{x + \img y}^2 - \norme{x - \img y}^2 = - 4 \Im(\scalaire{x}{y})$$

On en conclut que :

\begin{align}
\scalaire{x}{y} &= \Re(\scalaire{x}{y}) + \img \Im(\scalaire{x}{y}) \\
&= \unsur{4} (\norme{x + y}^2 + \norme{x - y}^2) + \frac{\img}{4} (\norme{x - \img y}^2 - \norme{x + \img y}^2)
\end{align}


** Norme et coordonnées

Soit $(e_1,...,e_n)$ une base de $E$ et $u \in E$. On a :

$$u = \sum_i u_i \cdot e_i$$

pour certains $u_i,v_i \in \corps$. La norme s'écrit alors :

$$\norme{u} = \sqrt{ \sum_{i,j} \conjaccent{u}_i \cdot \scalaire{e_i}{e_j} \cdot u_j }$$


*** Base orthonormée

Si la base est orthonormée, les seuls termes ne s'annulant pas sont ceux où $i = j$, et on a :

$$\norme{u} = \sqrt{ \sum_i \abs{u_i}^2 }$$

** Norme sur $\corps^n$

Soit $\corps \in \{ \setR , \setC \}$. On définit une norme sur $\corps^n$, dite norme euclidienne, à partir du produit scalaire :

$$\norme{x} = \sqrt{ \scalaire{x}{x} } = \sqrt{\sum_{i=1}^n \abs{x_i}^2}$$


*** Normes $k$

Par extension, on définit une série de normes $k$ par :

$$\norme{x}_k = \left( \sum_i \abs{x_i}^k \right)^{1/k}$$

Lorsque $k$ devient très grand, il est clair que la contribution du $\abs{x_i}^k$ le plus grand en valeur absolue devient énorme par rapport aux autres contributions de la norme. On peut vérifier que :

$$\lim_{k \mapsto +\infty} \norme{x}_k = \max_{i = 1}^n \abs{x_i}$$

On s'inspire de ce résultat pour définir :

$$\norme{x}_\infty = \max_{i = 1}^n \abs{x_i}$$

On nomme $\norme{.}_\infty$ la norme « max ».

Attention, une norme $k$ quelconque ne dérive en général pas d'un produit scalaire et ne possède donc pas les propriétés que nous avons vu pour la norme $\norme{.} = \norme{.}_2 = \sqrt{\scalaire{}{}}$.

** Norme sur $\setC$

Soit $(a,b) \in \setR^2$ et $z = a + \img b$. Il est clair que le module :

$$\abs{z} = \abs{a + \img b} = \sqrt{a^2 + b^2} = \norme{(a,b)}$$

définit une norme sur $\setC$.


** Représentation matricielle

Soit le vecteur matriciel $x = [x_1 \ ... \ x_n]^T$. Sa norme s'écrit :

$$\norme{x} = \sqrt{ \scalaire{x}{x} } = \sqrt{\conjaccent{x}^T \cdot x}$$


* Applications adjointes

#+TOC: headlines 1 local

\label{chap:dualite}


** Dépendances

  - Chapitre \ref{chap:forme} : Les formes linéaires
  - Chapitre \ref{chap:ps} : Les produits scalaires


** Adjoint au sens des formes linéaires

Soit les espaces vectoriels $E$ et $F$ sur $\corps$ et l'application linéaire
$A : E \mapsto F$. Si $A^\dual : F^\dual \mapsto E^\dual$ est l'unique fonction de $E^F$ vérifiant :

$$\forme{\varphi}{A(u)} = \forme{A^\dual(\varphi)}{u}$$

pour tout $\varphi \in F^\dual$ et $u \in E$, on dit que $A^\dual$ est l'application duale (ou adjointe) de $A$ au sens des formes linéaires.


** Adjoint au sens du produit scalaire

Soient $E$ et $F$ deux espaces vectoriels munis de produits scalaires et l'application linéaire $A : E \mapsto F$. Si $A^\dual : F \mapsto E$ est l'unique fonction de $E^F$ vérifiant :

$$\scalaire{v}{A(u)} = \scalaire{A^\dual(v)}{u}$$

pour tout $(u,v) \in E \times F$, on dit que $A^\dual$ est l'application duale (ou adjointe) de $A$ au sens du produit scalaire. Nous supposons dans la suite que les applications rencontrées possèdent une application adjointe.


*** Applications auto-adjointes

Si $E = F$ et $A = A^\dual$, on dit que $A$ est hermitienne ou auto-adjointe.


** Identité

Comme :

$$\scalaire{v}{\identite(u)} = \scalaire{\identite(v)}{u} = \scalaire{v}{u}$$

on a bien évidemment $\identite^\dual = \identite$.


** Adjoint d'une combinaison linéaire

Soit deux applications linéaires $A,B : E \mapsto F$ et $\alpha,\beta \in \corps$. Si $(u,v) \in E \times F$, on a :

\begin{align}
\scalaire{(\conjaccent{\alpha} \cdot A^\dual + \conjaccent{\beta} \cdot B^\dual)(v)}{u} &= \alpha \cdot \scalaire{A^\dual(v)}{u} + \beta \cdot \scalaire{B^\dual(v)}{u} \\
&= \alpha \cdot \scalaire{v}{A(u)} + \beta \cdot \scalaire{v}{A(u)} \\
&= \scalaire{v}{(\alpha \cdot A + \beta \cdot B)(u)}
\end{align}

On en conclut que :

$$(\alpha \cdot A + \beta \cdot B)^\dual = \conjaccent{\alpha} \cdot A^\dual + \conjaccent{\beta} \cdot B^\dual$$

L'opérateur $^\dual : A \mapsto A^\dual$ est antilinéaire.


** Bidual

On remarque que :

$$\scalaire{u}{A^\dual(v)} = \conjaccent{\scalaire{A^\dual(v)}{u}} = \conjaccent{\scalaire{v}{A(u)}} = \scalaire{A(u)}{v}$$

pour tout $(u,v) \in E \times F$. On a donc :

$$\left( A^\dual \right)^\dual = A$$


** Adjoint d'une composée

Soit un troisième espace vectoriel $G$. Si les applications adjointes des applications linéaires $A : E \mapsto F$ et $B : F \mapsto G$ existent, on a :

$$\scalaire{v}{(A \circ B)(u)} = \scalaire{A^\dual(v)}{B(u)} = \scalaire{(B^\dual \circ A^\dual)(v)}{u}$$

pour tout $(u,v) \in E \times G$. On en conclut que :

$$(A \circ B)^\dual = B^\dual \circ A^\dual$$


** Construction d'applications auto-adjointes

Nous allons voir que nous pouvons construire deux applications auto-adjointes à partir de n'importe quelle application linéaire $A : E \mapsto F$ admettant une application duale $A^\dual : F \mapsto E$.

  - L'application $A^\dual \circ A : E \mapsto E$ vérifie :

$$\scalaire{A^\dual \circ A(v)}{u} = \scalaire{A(v)}{A(u)} = \scalaire{v}{A^\dual \circ A(u)}$$

pour tout $u,v \in E$. On en déduit que :

$$(A^\dual \circ A)^\dual = A^\dual \circ A$$

  - L'application $A \circ A^\dual : F \mapsto F$ vérifie :

$$\scalaire{A \circ A^\dual(x)}{y} =\scalaire{A^\dual(x)}{A^\dual(y)} = \scalaire{x}{A \circ A^\dual(y)}$$

pour tout $x,y \in F$. On en déduit que :

$$(A \circ A^\dual)^\dual = A \circ A^\dual$$



** Linéarité de l'adjoint

Soit $u,v \in E$, $x,y \in F$ et $\alpha, \beta \in \corps$. On a :

\begin{align}
\scalaire{A^\dual(\alpha \cdot x + \beta \cdot y)}{u} &= \scalaire{\alpha \cdot x + \beta \cdot y}{A(u)} \\
&= \conjaccent{\alpha} \cdot \scalaire{x}{A(u)} + \conjaccent{\beta} \cdot \scalaire{y}{A(u)} \\
&= \conjaccent{\alpha} \cdot \scalaire{A^\dual(x)}{u} + \conjaccent{\beta} \cdot \scalaire{A^\dual(y)}{u} \\
&=\scalaire{\alpha \cdot A^\dual(x) + \beta \cdot A^\dual(y)}{u}
\end{align}

Comme ce doit être valable pour tout $u \in E$, on en conclut que l'application adjointe est linéaire :

$$A^\dual(\alpha \cdot x + \beta \cdot y) = \alpha \cdot A^\dual(x) + \beta \cdot A^\dual(y)$$


** Norme de l'adjoint

Soit une application linéaire $A : E \mapsto F$ de norme finie. Si $x \in F$ est un vecteur non nul, on a :

#+BEGIN_CENTER
\(
\norme{A^\dual(x)}^2 = \scalaire{A^\dual(x)}{A^\dual(x)} = \scalaire{A \circ A^\dual(x)}{x} \le \norme{A \circ A^\dual(x)} \cdot \norme{x}
\)
#+END_CENTER

et donc :

$$\norme{A^\dual(x)}^2 \le \norme{A} \cdot \norme{A^\dual(x)} \cdot \norme{x}$$

Si $\norme{A^\dual(x)} \ne 0$, on peut diviser par $\norme{A^\dual(x)}$. On obtient alors :

$$\norme{A^\dual(x)} \le \norme{A} \cdot \norme{x}$$

Par positivité des normes, on remarque que cette relation est également valable lorsque $\norme{A^\dual(x)} = 0 \le \norme{A} \cdot \norme{x}$. En divisant par la norme de $x$, on obtient :

$$\frac{ \norme{A^\dual(x)} }{ \norme{x} } \le \norme{A}$$

Il ne nous reste plus qu'à passer au supremum sur $x$ pour en conclure que :

$$\norme{A^\dual} \le \norme{A}$$

Mais comme $(A^\dual)^\dual = A$, on a aussi :

$$\norme{A} = \norme{(A^\dual)^\dual} \le \norme{A^\dual}$$

Ces deux inégalités nous montrent que :

$$\norme{A^\dual} = \norme{A}$$


** Inverse

Supposons que $A$ soit inversible. On a :

#+BEGIN_CENTER
\(
\identite = \identite^\dual = (A^{-1} \circ A)^\dual = A^\dual \circ (A^{-1})^\dual \\
\identite = \identite^\dual = (A \circ A^{-1})^\dual = (A^{-1})^\dual \circ A^\dual
\)
#+END_CENTER

On en conclut que $A^\dual$ est également inversible et que :

$$(A^\dual)^{-1} = (A^{-1})^\dual$$


** Noyau et image

Soit l'application linéaire $A : E \mapsto F$. Soit $u \in \noyau A$ et $v \in \image A^\dual$. On peut donc trouver un $x \in F$ tel que $v = A^\dual(x)$. On a :

$$\scalaire{u}{v} = \scalaire{u}{A^\dual(x)} = \scalaire{A(u)}{x} = \scalaire{0}{x} = 0$$

d'où $u \in (\image A^\dual)^\orthogonal$ et $\noyau A \subseteq (\image A^\dual)^\orthogonal$. Inversément, si $u \in (\image A^\dual)^\orthogonal$, on a :

$$\scalaire{A(u)}{x} = \scalaire{u}{A^\dual(x)} = 0$$

pour tout $x \in E$. On en conclut que $A(u) = 0$, c'est-à-dire $u \in \noyau A$. On a donc aussi $(\image A^\dual)^\orthogonal \subseteq \noyau A$. Ces deux inclusions nous montrent finalement que :

$$\noyau A = (\image A^\dual)^\orthogonal$$

Comme le bidual revient à l'application d'origine, on a aussi :

$$\noyau A^\dual = (\image A)^\orthogonal$$


** Représentation matricielle

Soit $\corps \in \{ \setR , \setC \}$ et la matrice $A \in \matrice(\corps,m,n)$ représentant l'application linéaire $\mathcal{A} : \corps^n \mapsto \corps^m$. Soit $A^\dual \in \matrice(\corps,n,m)$ représentant $\mathcal{A}^\dual$. On a :

$$\scalaire{y}{ \mathcal{A}(x) } = \conjaccent{y^T} \cdot A \cdot x$$

ainsi que :

$$\scalaire{\mathcal{A}^\dual(y)}{ x } = \big( \conjaccent{A^\dual} \cdot \conjaccent{y} \big)^T \cdot x = \conjaccent{y^T} \cdot \big( \conjaccent{A^\dual} \big)^T \cdot x$$

Les deux produits scalaires devant être égaux par définition de la dualité, on doit clairement avoir :

$$\big( \conjaccent{A^\dual} \big)^T = A$$

c'est-à-dire :

$$A^\dual = \conjaccent{A^T} = \conjugue A^T$$

Ce résultat prouve l'existence et l'unicité de l'adjoint dans le cas d'espaces de dimension finie.


*** Cas particulier

Dans le cas d'une matrice réelle, on a $\conjaccent{A} = A$ et :

$$A^\dual = A^T$$


** Adjoint d'un produit

On peut vérifier que :

$$(A \cdot B)^\dual = B^\dual \cdot A^\dual$$

pour toutes matrices $A \in \matrice(\corps,m,n)$ et $B \in \matrice(\corps,n,p)$ de dimensions compatibles pour la multiplication.


* Tenseurs

#+TOC: headlines 1 local

\label{chap:tenseur}


** Dépendances

  - Chapitre \ref{chap:vecteur} : Les vecteurs
  - Chapitre \ref{chap:lineaire} : Les fonctions linéaires
  - Chapitre \ref{chap:forme} : Les formes
  - Chapitre \ref{chap:ps} : Les produits scalaires


** Introduction

Nous présentons deux variantes de la définition des tenseurs. La première, classique, est basée sur les formes linéaires. La seconde, basée sur les vecteurs et la généralisation des produits scalaires, a l'avantage de mettre en relief la structure particulière des tenseurs, ainsi que la multitude de fonctions que l'on peut leur associer.


** Produit tensoriel de formes linéaires

Soit deux espaces vectoriels $E$ et $F$ sur $\corps$, ainsi que les fonctions $\varphi \in \lineaire(E,S)$ et $\psi \in \lineaire(F,S)$. On définit le produit tensoriel de ces deux fonctions, noté $\varphi \otimes \psi$, par :

$$(\varphi \otimes \psi)(a,b) = \varphi(a) \cdot \psi(b)$$

pour tout $a \in E$ et $b \in F$. La fonction $\varphi \otimes \psi$ est donc une forme bilinéaire vérifiant :

$$\biforme{a}{\varphi \otimes \psi}{b} = \forme{\varphi}{a} \cdot \forme{\psi}{b}$$


*** Forme associée

On peut réécrire la définition comme :

$$(\varphi \otimes \psi)(a,b) = \forme{\psi(b) \cdot \varphi}{a} = (\psi(b) \cdot \varphi)(a)$$

Nous pouvons donc associer à chaque $b \in F$ une forme linéaire $\psi(b) \cdot \varphi$ que nous notons :

$$\forme{\varphi \otimes \psi}{b} = \varphi \cdot \psi(b)$$


*** Dualité

On voit en échangeant $\varphi$ et $\psi$ que :

$$(\psi \otimes \varphi)(b,a) = \psi(b) \cdot \varphi(a) = (\varphi \otimes \psi)(a,b)$$


*** Associativité

Le produit tensoriel étant associatif, on note :

$$\varphi \otimes \psi \otimes \omega = \varphi \otimes (\psi \otimes \omega) = (\varphi \otimes \psi) \otimes \omega$$

pour toutes formes linéaires $\varphi,\psi,\omega$ définies sur les espaces vectoriels $E,F,G$.


*** Notation

On convient également de la notation :

$$\big(\forme{\psi \otimes \varphi}{u}\big)(x) = \psi(x) \cdot \forme{\varphi}{u}$$

pour tout $x \in F$. On a donc :

$$\forme{\psi \otimes \varphi}{u} = \psi \cdot \forme{\varphi}{u}$$


** Tenseurs de formes linéaires

Soit les espaces vectoriels $E_1,E_2,...,E_n$ sur $\corps$. On nomme tenseur d'ordre $n$ les formes $n$-linéaires de $E_1 \times E_2 \times ... \times E_n$ vers $\corps$.


** Produit tensoriel de deux vecteurs

Soit les espaces vectoriels $E$ et $F$ sur $\corps$. Choisissons $a \in F$ et $b \in E$. Le produit tensoriel $a \otimes b$ est l'application linéaire de $E$ vers $F$ définie par :

$$(a \otimes b)(c) = a \cdot \scalaire{b}{c}$$

pour tout $c \in E$. On vérifie aisément que ce produit est bilinéaire.


*** Associativité mixte

Il est clair d'après les définitions des opérations que :

#+BEGIN_CENTER
\(
(\alpha \cdot a) \otimes b = \alpha \cdot (a \otimes b) \\
a \otimes (b \cdot \alpha) = (a \otimes b) \cdot \alpha
\)
#+END_CENTER

pour tout $\alpha \in \corps$.


*** Distributivité

On a :

#+BEGIN_CENTER
\(
(a + b) \otimes c = a \otimes c + b \otimes c \\
a \otimes (c + d) = a \otimes c + a \otimes d
\)
#+END_CENTER

pour tout $a,b \in F$ et $c,d \in E$.


** Contractions

Soit les espaces vectoriels $E,F,G$ sur $\corps$. On étend la notion de « produit scalaire » par les relations :

#+BEGIN_CENTER
\(
\scalaire{a \otimes b}{c} = a \cdot \scalaire{b}{c} \\
\scalaire{b}{c \otimes d} = \scalaire{b}{c} \cdot d \\
\scalaire{a \otimes b}{c \otimes d} = \scalaire{b}{c} \cdot (a \otimes d)
\)
#+END_CENTER

valables pour tout $(a,b,c,d) \in G \times F \times F \times E$.


** Contractions doubles

Soit $b \in F$ et $c \in E$. Afin de rester consistant avec le produit tensoriel des formes, nous définissons la forme associée à $b \otimes c$ par :

$$\varphi(a,d) = \scalaire{a}{(b \otimes c)(d)} = \scalaire{a}{b} \cdot \scalaire{c}{d}$$

pour tout $d \in E$ et $a \in F$. On note en général cette forme au moyen de la double contraction :

$$\braket{a}{b \otimes c}{d} = \scalaire{a}{b} \cdot \scalaire{c}{d}$$


** Dualité

Soit le scalaire $\alpha \in \setC$ et les vecteurs $a,u \in E_1$ et $b,v \in E_2$. On a :

$$\scalaire{u}{\alpha \cdot (a \otimes b)(v)} = \alpha \cdot \scalaire{u}{a} \cdot \scalaire{b}{v}$$

et :

\begin{align}
\scalaire{\conjaccent{\alpha} \cdot (b \otimes a)(u)}{v} &= \alpha \cdot \conjugue(\scalaire{a}{u}) \cdot \scalaire{b}{v} \\
&= \alpha \cdot \scalaire{u}{a} \cdot \scalaire{b}{v}
\end{align}

On en déduit que :

$$(\alpha \cdot a \otimes b)^\dual =  \conjaccent{\alpha} \cdot b \otimes a$$


** Combinaison linéaire

Soit des vecteurs $a_i,b_i,c_i,d_i$ et des scalaires $\theta_{ij},\upsilon_{ij}$. On étend la définition des contractions à des combinaisons linéaires de la forme :

#+BEGIN_CENTER
\(
T = \sum_{i,j} \theta_{ij} \cdot a_i \otimes b_j \\
U = \sum_{i,j} \upsilon_{ij} \cdot c_i \otimes d_j
\)
#+END_CENTER

en imposant simplement la linéarité. On a donc par exemple :

$$\scalaire{T}{U} = \sum_{i,j,k,l} \theta_{ij} \cdot \upsilon_{kl} \cdot \scalaire{b_j}{c_k} \cdot (a_i \otimes d_l)$$


** Tenseurs d'ordre deux

Soit $a \in F$ et $b \in E$. Un objet de la forme :

$$T = a \otimes b$$

est un cas particulier de « tenseur d'ordre deux ». Il est formellement défini comme une application linéaire de $E$ vers $F$, mais il est en fait bien plus riche puisqu'on peut associer différents types de fonctions et de formes à chaque contraction possible impliquant ce tenseur. Les notions de tenseur et de contraction sont en fait étroitement liées.

On note $\tenseur_2(F,E)$ l'espace vectoriel généré par ce type de tenseurs d'ordre deux.


** Tenseurs d'ordre un et zéro

La possibilité d'associer une forme linéaire à chaque vecteur $a \in E$ par la contraction avec un autre vecteur, qui est dans ce cas un simple produit scalaire :

$$\varphi_a(b) = \scalaire{a}{b}$$

nous incite à considérer les vecteurs comme des « tenseurs d'ordre un ». Nommant $\tenseur_1(E)$ l'ensemble des tenseurs d'ordre un, on a simplement $\tenseur_1(E) = E$. Quant aux scalaires, ils seront considérés comme des « tenseurs d'ordre zéro ». On note $\tenseur_0 = \corps$ l'ensemble des tenseurs d'ordre zéro.


** Associativité

Soit $a \in G$, $b \in F$ et $c \in E$. Comme $\lineaire(E,F)$ est également un espace vectoriel, on a :

$$\Big[ a \otimes  \big[ (b \otimes c)(u) \big] \Big](v) = \big[ a \otimes b \scalaire{c}{u} \big](v) = a \scalaire{c}{u} \scalaire{b}{v}$$

valable pour tout $u \in E$ et tout $v \in F$. Mais $\lineaire(F,G)$ est aussi un espace vectoriel, et l'on a aussi :

$$\Big[ \big[ (a \otimes b)(v) \big] \otimes c \Big](u) = \Big[ \scalaire{b}{v} a \otimes c \Big](u) = \scalaire{b}{v} \scalaire{c}{u} a$$

Le résultat étant le même, le produit tensoriel est associatif, et nous notons :

$$a \otimes b \otimes c = a \otimes (b \otimes c) = (a \otimes b) \otimes c$$

l'application linéaire définie par :

$$(a \otimes b \otimes c)(u,v) = \scalaire{c}{u} \scalaire{b}{v} a$$


** Tenseur d'ordre $n$

Considérons les espaces vectoriels $E_1,...,E_n$, les séries de vecteurs $a_k^1,a_k^2,...a_k^{N_k} \in E_k$ et les scalaires $\theta_{ij...r} \in \corps$. Un tenseur d'ordre $n$ est une combinaison linéaire de la forme :

$$T = \sum_{i,j,...,r} \theta_{ij...r} \cdot a_1^i \otimes a_2^j \otimes ... \otimes a_n^r$$

On note $\tenseur_n(E_1,E_2,...,E_n)$ l'espace des tenseurs d'ordre $n$.

Lorsque tous les espaces vectoriels sont égaux, soit $E = E_1 = E_2 = ... = E_n$, on note $\tenseur_n(E) = \tenseur_n(E,E,...,E)$.


*** Indices covariants et contravariants

Les indices inférieurs (le $i$ des vecteurs $a_i^j$ par exemple) des
tenseurs sont appelés /indices covariants/.

Les indices supérieurs (le $j$ des vecteurs $a_i^j$ par exemple) des
tenseurs sont appelés /indices contravariants/.

Ne pas confondre ces /indices supérieurs/ contravariants , très
utilisés en calcul tensoriel, avec les puissances ! Dans le contexte
des tenseurs, une éventuelle puissance d'un scalaire $\theta_i^j$
serait notée au besoin par :

$$\big( \theta_j^i \big)^m = \theta_j^i \cdot ... \cdot \theta_j^i$$


** Dualité

Soit les séries de vecteurs $a_k^i \in E_k$, les scalaires $\theta_{ij...rs} \in \corps$, et le tenseur associé :

$$A = \sum_{i,j,...,r,s} \theta_{ij...rs} \cdot a_1^i \otimes a_2^j \otimes ... \otimes a_{n - 1}^r \otimes a_n^s$$

On vérifie que le dual s'obtient en inversant l'ordre des vecteurs et en conjuguant les coordonnées :

$$A^\dual =  \sum_{i,j,...,r,s} \conjaccent{\theta}_{ij...rs} \cdot a_n^s \otimes a_{n - 1}^r \otimes ... \otimes a_2^j \otimes a_1^i$$


** Réduction

Soit les séries de vecteurs $a_k^i \in E_k$, les scalaires $\theta_{ij...rs} \in \corps$, et le tenseur associé :

$$A = \sum_{i,j,...,r,s} \theta_{ij...rs} \cdot a_1^i \otimes a_2^j \otimes ... \otimes a_{n - 1}^r \otimes a_n^s$$

Les opérations de réduction consistent à construire des tenseurs d'ordre $n - 1$, notés $A_-(s)$ et $A^-(i)$, en retirant les vecteurs $a_n^s$ (réduction à droite) ou les vecteurs $a_1^i$ (réduction à gauche) :

#+BEGIN_CENTER
\(
A_-(s) =  \sum_{i,j,...,r} \theta_{ij...rs} \cdot a_1^i \otimes a_2^j \otimes ... \otimes a_{n - 1}^r \\
A^-(i) =  \sum_{j,...,r,s} \theta_{ij...rs} \cdot a_2^j \otimes ... \otimes a_{n - 1}^r \otimes a_n^s
\)
#+END_CENTER


** Contraction généralisée

Soit les séries de vecteurs $a_k^i \in E_k$ et $b_k^j \in F_k$, les scalaires $\eta_{i...r}, \theta_{j...s} \in \corps$, et les tenseurs :

#+BEGIN_CENTER
\(
A = \sum_{i,...,r} \eta_{i...r} \cdot a_1^i \otimes ... \otimes a_m^r \\
B = \sum_{j,...,s} \theta_{j...s} \cdot b_1^j \otimes ... \otimes b_n^s
\)
#+END_CENTER

La contraction d'ordre $0$ consiste simplement à juxtaposer les deux tenseurs au moyen du produit tensoriel :

$$\contraction{A}{0}{B} = \sum_{i,...,r,j,...,s} \eta_{i...r} \cdot \theta_{j...s} \cdot a_1^i \otimes ... \otimes a_m^r \otimes b_1^j \otimes ... \otimes b_n^s$$

Soit $p \in \setN$ tel que $0 \le p \le \min \{m,n\}$. Si :

#+BEGIN_CENTER
\(
E_m = F_1 \\
E_{m - 1} = F_2 \\
\vdots \\
E_{m - p + 1} = F_p \\
\)
#+END_CENTER

on peut définir la contraction $\contraction{}{p}{}$  d'ordre $p$ de deux tenseurs par récurrence :

$$\contraction{A}{p}{B} = \sum_{r,j} \scalaire{a_m^r}{b_1^j} \cdot \contraction{A_-(r)}{p - 1}{B^-(j)}$$


*** Notation

Soit $A$ un tenseur d'ordre $m$ et $B$ un tenseur d'ordre $n$. Le produit tensoriel est bien évidemment identique à la contraction d'ordre $0$. On le note :

$$A \otimes B = \contraction{A}{0}{B}$$

La contraction d'ordre $1$ est notée comme un produit scalaire :

$$\scalaire{A}{B} = \contraction{A}{1}{B}$$

ou comme un produit lorsqu'il n'y a pas de confusion possible :

$$A \cdot B = \scalaire{A}{B}$$

Enfin, la contraction maximale d'ordre $M = \min \{m,n\}$ est notée par :

$$A : B = \contraction{A}{M}{B}$$


*** Exemple

Considérons le cas :

#+BEGIN_CENTER
\(
A = a_1 \otimes ... \otimes a_m \\
B = b_1 \otimes ... \otimes b_n
\)
#+END_CENTER

En utilisant $p$ fois la récurrence, on obtient :

$$\contraction{A}{p}{B} = \scalaire{a_m}{b_1} \cdot ... \cdot \scalaire{a_{m-p+1}}{b_p} \cdot a_1 \otimes ... \otimes a_{m-p} \otimes b_{p+1} \otimes ... \otimes b_n$$

soit un tenseur d'ordre $m - p + n - p$.


** Contraction double généralisée

On définit également la contraction double utilisant trois facteurs par :

$$\dblecont{Y}{m}{T}{n}{X} = \contraction{ Y }{m}{ \contraction{T}{n}{X} } = \contraction{  \contraction{Y}{m}{T} }{n}{ X }$$


*** Notation

On note :

#+BEGIN_CENTER
\(
\braket{y}{A}{x} = \dblecont{y}{1}{A}{1}{x} \\
y \cdot A \cdot x = \braket{y}{A}{x}
\)
#+END_CENTER


** Coordonnées

Si chaque $E_k$ dispose d'une base de vecteurs $(e_k^1,e_k^2,...e_k^{N_k})$, tout tenseur $T \in \tenseur_n(E_1,E_2,...,E_n)$ peut être écrit sous la forme :

$$T = \sum_{i,...,r} \theta_{i...r} \cdot e_1^i \otimes ... \otimes e_n^r$$

On dit que les $\theta_{i...r}$ sont les coordonnées de $T$. L'indépendance linéaire des $e_k^i$ nous garantit que ces coordonnées sont uniques.

Il y a donc :

$$N = \prod_i N_i$$

coordonnées déterminant un tenseur.


*** Bases orthonormées

Si les bases sont orthonormées :

$$\scalaire{e_k^i}{e_k^j} = \indicatrice_{ij}$$

les coordonnées s'obtiennent aisément au moyen des contractions. On évalue :

#+BEGIN_CENTER
\(
\contraction{e_n^s \otimes ... \otimes e_1^j}{n}{T} = \sum_{i,...,r} \theta_{i...r} \cdot \scalaire{e_1^j}{e_1^i} \cdot ... \cdot \scalaire{e_n^s}{e_n^r} \\
\contraction{e_n^s \otimes ... \otimes e_1^j}{n}{T} = \sum_{i,...,s} \theta_{i...r} \cdot \indicatrice_{ji} \cdot ... \cdot \indicatrice_{sr}
\)
#+END_CENTER

Le produit des deltas de Kronecker s'annulant partout sauf aux indices $(i,...,r) = (j,...,s)$, on a finalement :

$$\contraction{e_n^s \otimes ... \otimes e_1^j}{n}{T} = \theta_{j...s}$$

ce qui nous donne les valeurs des $\theta_{j...s}$.


** Norme

La norme d'un tenseur $A$ est analogue aux normes dérivées d'un produit scalaire. On utilise ici la contraction maximale et le tenseur duel de $A$, afin que les espaces des vecteurs soient compatibles :

$$\norm{A} = \sqrt{A^\dual : A}$$


*** Bases orthonormées

Supposons que $A$ soit représenté par rapport aux bases orthonormées $(e_k^1,e_k^2,...e_k^{N_k})$ :

$$A = \sum_{i,...,r} \theta_{i...r} \cdot e_1^i \otimes ... \otimes e_n^r$$

pour certains $\theta_{i...r} \in \corps$. La norme nous donne dans ce cas :

$$\norme{A} = \sqrt{A^\dual : A} = \sqrt{\sum_{i,...,r} \abs{\theta_{i...r}}^2$$


** Tenseur identité

Le tenseur identité $\tenseuridentite \in \tenseur_2(E)$ est défini par :

$$\tenseuridentite \cdot u = \contraction{\tenseuridentite}{1}{u} = u$$

pour tout $u \in E$.


*** Base orthonormée

Soit $(e_1,...,e_n)$ une base orthonormée de $E$. On a alors :

$$\tenseuridentite \cdot e_i = e_i$$

et :

$$\contraction{\tenseuridentite}{2}{e_j \otimes e_i} = \scalaire{\tenseuridentite \cdot e_i}{e_j} = \indicatrice_{ij}$$

On en conclut que :

$$\tenseuridentite = \sum_{i,j} \indicatrice_{ij} \cdot e_i \otimes e_j = \sum_i e_i \otimes e_i$$


** Inverse

Soit un tenseur $T \in \tenseur_2(E)$. Si il existe un tenseur $T^{-1} \in \tenseur_2(E)$ dont les contractions d'ordre $1$ avec $T$ donnent le tenseur identité :

$$T \cdot T^{-1} = \contraction{T}{1}{T^{-1}} = T^{-1} \cdot T = \contraction{T^{-1}}{1}{T} = \tenseuridentite$$

on dit que $T^{-1}$ est l'inverse de $T$.


** Trace

Soit $(e_1,...,e_n)$ est une base orthonormée de $E$ et le tenseur $T \in \tenseur_2(E)$ :

$$T = \sum_{i,j} \theta_{ij} \cdot e_i \otimes e_j$$

On définit sa trace par :

$$\trace T = T : \tenseuridentite = \sum_{i,j} \theta_{ij} \cdot \indicatrice_{ij} = \sum_i \theta_{ii}$$


** Cadre

On dit que les vecteurs $e_i \in E$ forment un cadre de $E$ si :

$$\sum_i e_i \otimes e_i = \tenseuridentite$$

On a alors, pour tout vecteur $u$ de $E$ :

$$\sum_i \scalaire{e_i \otimes e_i}{u} = u$$

c'est-à-dire :

$$u = \sum_i \scalaire{e_i}{u} e_i$$

par définition de la contraction. Prenant le produit scalaire avec un autre vecteur $v \in F$, on obtient :

$$\scalaire{v}{u} = \sum_i \scalaire{v}{e_i}\scalaire{e_i}{u}$$


** Représentation matricielle

Soit les vecteurs $u \in F = \matrice(K,m,1) \equiv \corps^m$ et $v,w \in E = \matrice(K,n,1) \equiv \corps^n$. La matrice :

#+BEGIN_CENTER
\(
A = u \cdot v^\dual =
\begin{Matrix}{cccc}
u_1 \cdot \bar{v}_1 & u_1 \cdot \bar{v}_2 & \ldots & u_1 \cdot \bar{v}_n \\
u_21 \cdot \bar{v}_1 & u_2 \cdot \bar{v}_2 & \ldots & u_2 \cdot \bar{v}_n \\
\vdots &        & \ddots &  \vdots \\
u_m \cdot \bar{v}_1 & u_m \cdot \bar{v}_2 & \ldots & u_m \cdot \bar{v}_n
\end{Matrix}
\)
#+END_CENTER

représente une application linéaire de $\corps^n$ vers $\corps^m$. Comme le produit matriciel est associatif, cette fonction possède la propriété :

$$A \cdot w = (u \cdot v^\dual) \cdot w = u \cdot (v^\dual \cdot w) = u \cdot \scalaire{v}{w}$$

Ce résultat étant identique à la définition d'un tenseur d'ordre deux, on voit que les matrices sont équivalentes à des tenseurs d'ordre deux : $\matrice(K,m,n) \equiv \tenseur_2(\corps^m,\corps^n)$. On parle donc de tenseurs matriciels pour désigner cette représentation. On définit par équivalence le produit tensoriel de deux vecteurs matriciels par :

$$u \otimes v = u \cdot v^\dual$$


** Contraction d'ordre $1$

Soit les espaces vectoriels $E_1,E_2,E_3$ sur $\corps$ et les suites de vecteurs $(e_k^1,e_k^2,...e_k^{N_k})$ formant des bases orthonormées des $E_k$. Soit les tenseurs :

#+BEGIN_CENTER
\(
\mathcal{A} = \sum_{i,k} a_{ik} \cdot e_1^i \otimes e_2^k \\
\mathcal{B} = \sum_{l,j} b_{lj} \cdot e_2^l \otimes e_3^j
\)
#+END_CENTER

où $a_{ij},b_{ij} \in \corps$. Calculons leur contraction d'ordre $1$ :

#+BEGIN_CENTER
\(
\mathcal{C} = \contraction{ \mathcal{A} }{1}{ \mathcal{B} } = \sum_{i,j,k,l} a_{ik} \cdot b_{lj} \cdot \scalaire{e_2^k}{e_2^l} \cdot e_1^i \otimes e_3^j \\
\mathcal{C} = \sum_{i,j,k,l} a_{ik} \cdot b_{lj} \cdot \indicatrice_{kl} \cdot e_1^i \otimes e_3^j \\
\mathcal{C} = \sum_{i,j,k} a_{ik} \cdot b_{kj} \cdot e_1^i \otimes e_3^j
\)
#+END_CENTER

On voit que les coordonnées obtenues :

$$c_{ij} = \sum_k a_{ik} \cdot b_{kj}$$

correspondent à une matrice $C = ( c_{ij} )_{i,j}$ telle que :

$$C = A \cdot B$$

où les matrices $A$ et $B$ sont données par :

#+BEGIN_CENTER
\(
A = ( a_{ij} )_{i,j} \\
B = ( b_{ij} )_{i,j}
\)
#+END_CENTER

La contraction d'ordre $1$ correspond donc au produit matriciel, qui correspond lui-même à la composition d'application linéaires.


** Contraction maximale

Soit les espaces vectoriels $E_1,E_2,E_3$ sur $\corps$ et les suites de vecteurs $(e_k^1,e_k^2,...e_k^{N_k})$ formant des bases orthonormées des $E_k$. Soit les tenseurs :

#+BEGIN_CENTER
\(
\mathcal{A} = \sum_{i,k} a_{ik} \cdot e_1^i \otimes e_2^k \\
\mathcal{B} = \sum_{l,j} b_{lj} \cdot e_2^l \otimes e_3^j
\)
#+END_CENTER

où $a_{ij},b_{ij} \in \corps$. Calculons leur contraction maximale :

#+BEGIN_CENTER
\(
\alpha = \mathcal{A} : \mathcal{B} = \contraction{ \mathcal{A} }{2}{ \mathcal{B} } = \sum_{i,j,k,l} a_{ik} \cdot b_{lj} \cdot \indicatrice_{kl} \cdot \indicatrice_{ij} \\
\alpha = \sum_{i,k} a_{ik} \cdot b_{ki}
\)
#+END_CENTER

Ce résultat nous incite à définir l'opération équivalente sur les matrices associées $A = ( a_{ij} )_{i,j}$ et $B = ( b_{ij} )_{i,j}$ :

$$A : B = \sum_{i,j} a_{ij} \cdot b_{ji}$$


** Base canonique

Soit $\canonique_{m,i}$ les vecteurs de la base canonique de $\corps^m$ et $\canonique_{n,i}$ les vecteurs de la base canonique de $\corps^n$. Si $A = (a_{ij})_{i,j} \in \matrice(\corps,m,n)$, on a clairement :

$$A = \sum_{i = 1}^m \sum_{j = 1}^n a_{ij} \cdot \canonique_{m,i} \otimes \canonique_{n,j} = \sum_{i = 1}^m \sum_{j = 1}^n a_{ij} \cdot \canonique_{m,i} \cdot \canonique_{n,j}^\dual$$


*** Lignes

On a :

$$\canonique_{m,k}^\dual \cdot A = \sum_{k,i,j} a_{ij} \cdot \indicatrice_{ik} \cdot \canonique_{n,j}^\dual = \sum_j a_{kj} \cdot \canonique_{n,j}^\dual$$

La matrice de taille $(1,n)$ obtenue est donc la $k^{eme}$ ligne de $A$ :

$$\canonique_{m,k}^\dual \cdot A = \ligne_k A$$


*** Colonnes

On a :

$$A \cdot \canonique_{n,k} = \sum_{k,i,j} a_{ij} \cdot \canonique_{m,i} \cdot \indicatrice_{jk} = \sum_i a_{ik} \cdot \canonique_{m,i}$$

La matrice de taille $(m,1)$ obtenue est donc la $k^{eme}$ colonne de $A$ :

$$A \cdot \canonique_{n,k} = \colonne_k A$$


*** Identité

Dans le cas où $m = n$, on a en particulier :

$$\sum_{i = 1}^n \canonique_{n,i} \otimes \canonique_{n,i} = \sum_{i = 1}^n \canonique_{n,i} \cdot \canonique_{n,i}^\dual = I$$


** Normes matricielles

La norme de Frobenius d'une matrice $A = ( a_{ij} )_{i,j}$ est la norme du tenseur associé. On a donc :

$$\norm{A}_F = \sqrt{A^\dual : A} = \sqrt{\sum_{i,j} \bar{a}_{ij} \cdot a_{ij} } = \sqrt{ \sum_{i,j} \abs{a_{ij}}^2 }$$


*** Trace

La trace d'une matrice $A = (a_{ij})_{i,j} \in \matrice(K,m,n)$ est la trace du tenseur sous-jacent, et donc :

$$\trace(A) = \sum_{i \in I} a_{ii}$$

où $I = \{1, 2, ..., \min \{ m , n \} \}$.


* Produit extérieur

#+TOC: headlines 1 local

\label{chap:prdext}


** Dépendances

  - Chapitre \ref{chap:vecteur} : Les vecteurs
  - Chapitre \ref{chap:tenseur} : Les tenseurs


** Dimension 2

Soit l'espace vectoriel $E = \combilin{e_1,e_2}$ sur $S$, où $(e_1,e_2)$ forme est une base orthonormée. Soit $u,v \in E$. On a :

#+BEGIN_CENTER
\(
u = \sum_{i = 1}^2 u_i e_i \\
v = \sum_{i = 1}^2 v_i e_i
\)
#+END_CENTER

pour certains $u_i,v_i \in S$.

Les symboles $\permutation_{ij}\quad (i,j=1,2)$ sont définis de telle sorte que pour tout $u,v$ la double somme :

$$u \wedge v = \sum_{i,j=1}^2 \permutation_{ij} u_i v_j$$

représente (au signe près) la surface du parallélogramme dont les sommets
sont $(0,0)$, $(u_1,u_2)$, $(v_1,v_2)$ et $(u_1+v_1,u_2+v_2)$. On impose
de plus l'antisymétrie :

$$u \wedge v = - v \wedge u$$

afin de donner une orientation à ce parallélogramme. Ces contraintes nous donnent :

#+BEGIN_CENTER
\(
u = e_1, \quad v = e_1 \quad \Rightarrow \quad u \wedge v = 0 \\
u = e_1, \quad v = e_2 \quad \Rightarrow \quad u \wedge v = 1 \\
u = e_2, \quad v = e_2 \quad \Rightarrow \quad u \wedge v = 0 \\
u = e_2, \quad v = e_1 \quad \Rightarrow \quad u \wedge v = -1
\)
#+END_CENTER

ce qui nous amène à :

#+BEGIN_CENTER
\(
\permutation_{11} = \permutation_{22} = 0 \\
\permutation_{12} = 1 \qquad \permutation_{21} = -1
\)
#+END_CENTER

On voit qu'il y a un certain arbitraire dans notre choix, on aurait
pu également choisir :

#+BEGIN_CENTER
\(
\permutation_{11} = \permutation_{22} = 0 \\
\permutation_{12} = -1 \qquad \permutation_{21} = 1
\)
#+END_CENTER

Le choix du signe détermine ce que l'on appelle l'orientation de l'espace $E$.


** Dimension 3

Soit l'espace vectoriel $E = \combilin{e_1,e_2,e_3}$ sur $S$, où $(e_1,e_2,e_3)$ forme est une base orthonormée. Soit $u,v,w \in E$. On a :

#+BEGIN_CENTER
\(
u = \sum_{i = 1}^3 u_i e_i \\
v = \sum_{i = 1}^3 v_i e_i \\
w = \sum_{i = 1}^3 w_i e_i
\)
#+END_CENTER

pour certains $u_i,v_i,w_i \in S$.

Les symboles $\permutation_{ijk} \quad (i,j=1,2,3)$ sont définis de telle sorte que pour tout $u,v,w$ le scalaire :

$$u \wedge v \wedge w = \sum_{i,j,k=1}^3 \permutation_{ijk} u_i v_j w_k$$

représente (au signe près) le volume du parallélipipède dont les côtés
sont définis par $u$, $v$ et $w$. On impose également l'antisymétrie

#+BEGIN_CENTER
\(
u \wedge v \wedge w = - u \wedge w \wedge v \\
u \wedge v \wedge w = - v \wedge u \wedge w
\)
#+END_CENTER

Par un procédé analogue au cas bidimensionnel, on montre qu'un choix possible
est donné par :

#+BEGIN_CENTER
\(
\permutation_{123} = 1 \\
\permutation_{ijj} = \permutation_{iij} = \permutation_{iji} = 0 \\
\permutation_{ijk} = \permutation_{jki} = \permutation_{kij} \\
\permutation_{ijk} = - \permutation_{jik} \qquad
\permutation_{ijk} = - \permutation_{ikj}
\)
#+END_CENTER


*** Produit vectoriel

On peut également former un vecteur avec l'opérateur $\wedge$. On pose
simplement :

$$u \wedge v = \sum_{i,j,k = 1}^3 \left( \permutation_{ijk} u_j v_k \right) e_i$$

Les composantes sont donc données par :

#+BEGIN_CENTER
\(
(u \wedge v)_1 = u_2 v_3 - u_3 v_2  \\
(u \wedge v)_2 = u_3 v_1 - u_1 v_3  \\
(u \wedge v)_3 = u_1 v_2 - u_2 v_1
\)
#+END_CENTER

On peut relier le produit extérieur au produit scalaire en constatant que :

$$\scalaire{u}{v \wedge w} = \sum_{i,j,k} \permutation_{ijk} u_i v_j w_k = u \wedge v \wedge w$$


** Permutations en dimension $N$

Voyons quelles sont les propriétés communes aux $\permutation_*$ présentés
ci-dessus. On remarque que $\permutation_{12} = \permutation_{123} = 1$. On note aussi que $\permutation_*$ est antysimétrique puisque l'inversion de deux indices change le signe. Ces propriétés nous permettent de généraliser la définition du produit extérieur à un espace de dimension $N$. On impose que le $\permutation$ à $N$ indices vérifie les conditions suivantes :

\label{def:eps}

#+BEGIN_CENTER
\(
\permutation_{1,2,...,N} = 1 \\
\permutation_{i ... j ... k ... l} = - \permutation_{i ... k ... j ... l}
\)
#+END_CENTER

Ces conditions nous permettent de retrouver la valeur de n'importe quel $\permutation_{ijk...l}$. Si deux indices sont égaux, l'antisymétrie nous permet d'affirmer que :

$$\permutation_{i ... j ... j ... k} = - \permutation_{i ... j ... j ... k}$$

et donc :

$$\permutation_{i ... j ... j ... k} = 0$$

Les seuls $\permutation$ non nuls sont donc ceux dont tous les indices $(i,j,k,...,s)$ sont distincts, c'est-à-dire les permutations de $(1,2,3,...,N)$. On se rend compte que si $p \in \setN$ est le nombre de permutations de couples d'indices nécessaires pour obtenir $(i,j,k,...,s)$ à partir de $(1,2,3,...,N)$, on a :

$$\permutation_{i j k ... l} = (-1)^p$$


** Tenseur de permutation

Soit $(e_1,e_2,...,e_N)$ une base orthonormée d'un espace vectoriel $E$ sur $S$. Nous définissons le tenseur de permutation $\mathcal{E} \in \tenseur_N(E)$ par :

$$\mathcal{E} = \sum_{i_1,i_2,...,i_N} \permutation_{i_1,i_2,...,i_N} \cdot e_{i_1} \otimes e_{i_2} \otimes ... \otimes e_{i_N}$$


** Produit extérieur généralisé

Soit $(e_1,e_2,...,e_N)$ une base orthonormée d'un espace vectoriel $E$ sur $\corps$,
$M \le N$ et les vecteurs $u_1,u_2,...u_M \in E$ de coordonnées $u_k^i$ :

$$u_k = \sum_{i = 1}^N u_k^i e_i$$

Nous définissons leur produit extérieur par une contraction d'ordre $M$ :

$$u_1 \wedge u_2 \wedge ... \wedge u_M = \contraction{ \mathcal{E} }{M}{ u_M \otimes ... \otimes u_1 }$$

Par orthonormalité de la base, on a :

$$\scalaire{e_j}{u_k} = u_k^j$$

Le produit extérieur s'écrit donc :

$$\mathcal{U} = u_1 \wedge u_2 \wedge ... \wedge u_M = \sum_{i_1,i_2,...i_N} \permutation_{i_1,i_2,...i_N} \cdot e_{i_1} \otimes ... \otimes e_{ i_{N - M} } \cdot u_M^{i_N} \cdot \hdots \cdot u_1^{ i_{N - M + 1} }$$

ce qui nous donne les coordonnées du tenseur $\mathcal{U} \in \tenseur_{N-M}(E)$ par rapport à la base $(e_1,...,e_N)$ :

$$U_{ i_1,...,i_{N - M} } = \sum_{i_{N - M + 1},...,i_N} \permutation_{i_1,i_2,...i_N} \cdot u_1^{ i_{N - M + 1} } \cdot \hdots \cdot u_M^{i_N}$$

On vérifie les propriétés suivantes :

#+BEGIN_CENTER
\(
u \wedge v = - v \wedge u \\
u \wedge u = 0 \\
(\alpha u + \beta v) \wedge w = \alpha u \wedge w + \beta v \wedge w \\
w \wedge (\alpha u + \beta v) = \alpha w \wedge u + \beta w \wedge v
\)
#+END_CENTER

pour tout $u,v,w,... \in E$ et $\alpha,\beta \in S$.


*** $N$ vecteurs

Dans le cas où $M = N$, le produit extérieur est le scalaire :

$$\Delta = u_1 \wedge u_2 \wedge ... \wedge u_N = \sum_{i,j,...,k = 1}^N \permutation_{ij...k} \cdot u_1^i \cdot u_2^j \cdot \hdots \cdot u_N^k$$

On appelle le $\Delta$ ainsi obtenu le déterminant des $N$ vecteurs $u_i$, et on le note :

$$\det(u_1,...,u_N) = u_1 \wedge u_2 \wedge ... \wedge u_N$$


** Déterminant d'une matrice

Soit une matrice $A \in \matrice(K,N,N)$ :

$$A = \left( a_{ij} \right)_{i,j}$$

et les vecteurs correspondants :

$$a_i = \sum_j a_{ij} e_j$$

On définit alors simplement :

#+BEGIN_CENTER
\(
\det(A) = a_1 \wedge ... \wedge a_N = \sum_{i_1,i_2,...i_N} \permutation_{i_1,i_2,...i_N} a_{1, i_1 } ...
a_{N, i_N}
\)
#+END_CENTER


* Matrices élémentaires

#+TOC: headlines 1 local


** Dépendances

  - Chapitre \ref{chap:matrice} : Les matrices
  - Chapitre \ref{chap:tenseur} : Les tenseurs


** Introduction

Les matrices élémentaires constituent une classe importante de matrice. Elles permettent d'obtenir d'importants résultats utiles tant sur le plan théorique que pour les applications numériques. Une matrice élémentaire de taille $(n,n)$ est déterminée par un scalaire $\alpha \in \corps$ et le produit tensoriel de deux vecteurs $u,v \in \corps^n$ :

$$\matelementaire(\alpha,u,v) = I + \alpha \cdot u \otimes v = I + \alpha \cdot u \cdot v^\dual$$


** Inverse

Considérons le produit :

\begin{align}
\matelementaire(\alpha,u,v) \cdot \matelementaire(\beta,u,v) &= I + (\alpha + \beta) \cdot u \cdot v^\dual + \alpha \cdot \beta \cdot u \cdot v^\dual \cdot u \cdot v^\dual \\
&= I + [\alpha + \beta + \alpha \cdot \beta \cdot (v^\dual \cdot u)] \cdot u \cdot v^\dual
\end{align}

On voit que si on peut trouver un scalaire $\beta$ tel que :

$$\alpha + \beta + \alpha \cdot \beta \cdot (v^\dual \cdot u) = 0$$

le produit des deux matrices sera égal à la matrice identité. Ce sera possible si $1 + \alpha \cdot v^\dual \cdot u \ne 0$. On a alors :

$$\beta = - \frac{\alpha}{1 + \alpha \cdot (v^\dual \cdot u)}$$

et :

$$\matelementaire(\alpha,u,v) \cdot \matelementaire(\beta,u,v) = I$$

Par symétrie, il est clair que le produit des deux matrices ne change pas lorsqu'on intervertit $\alpha$ et $\beta$. On a donc aussi :

$$\matelementaire(\beta,u,v) \cdot \matelementaire(\alpha,u,v) = I$$

Ces deux conditions étant remplies, on a :

$$\matelementaire(\beta,u,v) = \matelementaire(\alpha,u,v)^{-1}$$

Les matrices élémentaires sont donc très faciles à inverser.


** Matrices élémentaires de transformation

Nous allons construire une matrice élémentaire qui transforme un vecteur  donné $x \in \matrice(\corps,n,1)$ non nul en un autre vecteur donné $y \in \matrice(\corps,n,1)$ de même taille.


*** Colonne

On cherche une matrice élémentaire $E_{yx}$ telle que $E_{yx} \cdot x = y$. L'équation :

$$(I + \alpha \cdot u \cdot v^\dual) \cdot x = x + \alpha \cdot u \cdot (v^\dual \cdot x) = y$$

nous donne la condition :

$$\alpha \cdot (v^\dual \cdot x) \cdot u = y - x$$

On peut donc choisir par exemple $u = y - x$ et $v = x$. On a alors $\alpha = 1 / (x^\dual \cdot x) \ne 0$ et on se retrouve avec la matrice élémentaire :

$$E_{yx} = I + \unsur{x^\dual \cdot x} \cdot (y - x) \cdot x^\dual$$


*** Inverse

Si l'inverse existe, il s'agit d'une matrice élémentaire de paramètre scalaire :

$$\beta = - \unsur{x^\dual \cdot x + x^\dual \cdot (y - x)} = - \unsur{x^\dual \cdot y}$$

Sous réserve que $x^\dual \cdot y \ne 0$, on a donc :

$$E_{yx}^{-1} = I - \unsur{x^\dual \cdot y} \cdot (y - x) \cdot x^\dual$$


*** Ligne

On cherche une matrice élémentaire $E_{yx}$ telle que $x^\dual \cdot E_{yx} = y^\dual$. L'équation :

$$x^\dual \cdot (I + \alpha \cdot u \cdot v^\dual) = x^\dual + \alpha \cdot (x^\dual \cdot u) \cdot v^\dual = y^\dual$$

nous donne la condition :

$$\alpha \cdot (x^\dual \cdot u) \cdot v^\dual = y^\dual - x^\dual$$

ou :

$$\conjaccent{\alpha} \cdot (u^\dual \cdot x) \cdot v = y - x$$

On peut donc choisir par exemple $v = y - x$ et $u = x$. On a alors $\alpha = \conjaccent{\alpha} = 1 / (x^\dual \cdot x) \ne 0$ et on se retrouve avec la matrice élémentaire :

$$E_{yx} = I + \unsur{x^\dual \cdot x} \cdot x \cdot (y - x)^\dual$$


*** Inverse

Si l'inverse existe, il s'agit d'une matrice élémentaire de paramètre scalaire :

$$\beta = - \unsur{x^\dual \cdot x + (y - x)^\dual \cdot x} = - \unsur{y^\dual \cdot x}$$

Sous réserve que $y^\dual \cdot x \ne 0$, on a donc :

$$E_{yx}^{-1} = I - \unsur{y^\dual \cdot x} \cdot x \cdot (y - x)^\dual$$


** Matrices élémentaires de permutation

Les matrices élémentaires de permutations permettent de permuter deux lignes ou deux colonnes d'une matrice. Soit $\canonique_1,...,\canonique_n$ les vecteurs de la base canonique de $\corps^n$. La matrice de permutation élémentaire de taille $(n,n)$ et de paramètres $i,j$ est définie par :

$$\matpermutation_{n,i,j} = I - (\canonique_i - \canonique_j) \cdot (\canonique_i - \canonique_j)^\dual$$

Dans la suite, nous considérons $A \in \matrice(\corps,m,n)$ et $P = \matpermutation_{n,i,j}$.


*** Permutation des colonnes

Soit les colonnes $C_i = A \cdot \canonique_i$. On a :

$$A \cdot P = A - (C_i \cdot \canonique_i^\dual + C_j \cdot \canonique_j^\dual) + (C_j \cdot \canonique_i^\dual + C_i \cdot \canonique_j^\dual)$$

Les colonnes $i$ et $j$ de $A$ sont donc permutées par multiplication à droite d'une matrice de permutation.


*** Permutation des lignes

Soit les lignes $L_i = \canonique_i^\dual \cdot A$. On a :

$$P \cdot A = A - (\canonique_i \cdot L_i + \canonique_j \cdot L_j) + (\canonique_j \cdot L_i + \canonique_i \cdot L_j)$$

Les lignes $i$ et $j$ de $A$ sont donc permutées par multiplication à gauche d'une matrice de permutation.


*** Symétrie

On constate que la transposée et la duale sont égales à la matrice elle-même :

$$\matpermutation_{n,i,j} = \matpermutation_{n,i,j}^T = \matpermutation_{n,i,j}^\dual$$


*** Inverse

Soit $\Delta_{ij} = \canonique_i - \canonique_j$ et :

$$P = \matpermutation_{n,i,j} = I - \Delta_{ij} \cdot \Delta_{ij}^\dual$$

Le produit de cette matrice avec elle-même s'écrit :

$$P \cdot P = I - 2 \Delta_{ij} \cdot \Delta_{ij}^\dual + \Delta_{ij} \cdot \Delta_{ij}^\dual  \cdot \Delta_{ij} \cdot \Delta_{ij}^\dual$$

Mais comme $\Delta_{ij}^\dual \cdot \Delta_{ij} = 2$, on a :

$$P \cdot P = I - 2 \Delta_{ij} \cdot \Delta_{ij}^\dual + 2 \Delta_{ij} \cdot \Delta_{ij}^\dual$$

Les deux derniers termes s'annihilent et :

$$P \cdot P = I$$

Les matrices de permutations élémentaires sont donc égales à leur propre inverse :

$$\matpermutation_{n,i,j} = \matpermutation_{n,i,j}^{-1}$$


** Matrices de permutations

Une matrice de permutation $P$ est une matrice de la forme :

$$P = P_1 \cdot ... \cdot P_n$$

où les $P_i$ sont des matrices élémentaires de permutation.


*** Inverse

#+BEGIN_CENTER
\(
P^\dual \cdot P = P_n \cdot ... \cdot P_1 \cdot P_1 \cdot ... \cdot P_n = I \\
P \cdot P^\dual = P_1 \cdot ... \cdot P_n \cdot P_n \cdot ... \cdot P_1 = I
\)
#+END_CENTER

Donc $P^\dual = P^{-1}$. Comme $P^\dual = P^T$, la transposée d'une matrice de permutation est identique à son inverse. On dit que ces matrices sont orthogonales.


* Systèmes linéaires et inverses

#+TOC: headlines 1 local

\label{chap:syslin}


** Gauss-Jordan

Nous allons tenter de diagonaliser une matrice $A \in \matrice(\corps,m,n)$ en une matrice diagonale aussi proche que possible de la matrice identité. Soit les $\canonique_i$, vecteurs de la base canonique de $\corps^m$ ou $\corps^n$ suivant le contexte. Si $A$ est non nulle, on peut trouver un $i$ et un $j$ tels que :

$$a = \composante_{ij} A \ne 0$$

On considère les matrices élémentaires de permutation $P_0 = \matpermutation_{n,i,1}$ et $Q_0 = \matpermutation_{n,j,1}$, et on évalue $P_0 \cdot A \cdot Q_0$ pour inverser les lignes $1$ et $i$ et les colonnes $1$ et $j$ de $A$. On se retrouve alors avec une matrice de la forme :

$$Q_0 \cdot A \cdot P_0 = [x \ c_2 \ ... \ c_n ]$$

où $x$ est la première colonne et où $x^\dual \cdot \canonique_1 = a \ne 0$. On utilise ensuite une première matrice élémentaire :

$$E_0 = I + \unsur{x^\dual \cdot x} \cdot (\canonique_1 - x) \cdot x^\dual$$

pour transformer la première colonne $x$ en $y = \canonique_1$. Partitionnant à part la première ligne et la première colonne du résultat, on a :

#+BEGIN_CENTER
\(
E_0 \cdot P_0 \cdot A \cdot Q_0 =
\begin{Matrix}{cc}
1 & z^\dual \\
0 & \ddots
\end{Matrix}
\)
#+END_CENTER

Posons $u^\dual = [1 \ z^\dual]$ pour la première ligne. On a $\canonique_1^\dual \cdot u = 1 \ne 0$. On utilise une seconde matrice élémentaire :

$$F_0 = I + \unsur{u^\dual \cdot u} \cdot u \cdot (\canonique_1 - u)^\dual$$

pour transformer la première ligne $u^\dual$ en $v^\dual = \canonique_1^\dual$. Mais comme $(\canonique_1 - u)^\dual = [0 \ -z^\dual]$, on a :

#+BEGIN_CENTER
\(
u \cdot (\canonique_1 - u)^\dual =
\begin{Matrix}{c}
1 \\ z
\end{Matrix}
\cdot
\begin{Matrix}{cc}
0 & -z^\dual
\end{Matrix}
=
\begin{Matrix}{cc}
0 & -z^\dual \\
0 & -z \cdot z^\dual
\end{Matrix}
\)
#+END_CENTER

On voit aussi que $u^\dual \cdot u = 1 + z^\dual \cdot z$. La matrice élémentaire $F_0$ est donc de la forme :

#+BEGIN_CENTER
\(
F_0 = I + \unsur{1 + z^\dual \cdot z} \cdot
\begin{Matrix}{cc}
0 & -z^\dual \\
0 & -z \cdot z^\dual
\end{Matrix}
\)
#+END_CENTER

On obtient alors une matrice modifiée de la forme :

#+BEGIN_CENTER
\(
E_0 \cdot P_0 \cdot A \cdot Q_0 \cdot F_0 =
\begin{Matrix}{cc}
1 & z^\dual \\
0 & \ddots
\end{Matrix}
+ \unsur{1 + z^\dual \cdot z} \cdot
\begin{Matrix}{cc}
1 & z^\dual \\
0 & \ddots
\end{Matrix}
\cdot
\begin{Matrix}{cc}
0 & -z^\dual \\
0 & -z \cdot z^\dual
\end{Matrix}
\)
#+END_CENTER

et finalement :

#+BEGIN_CENTER
\(
E_0 \cdot P_0 \cdot A \cdot Q_0 \cdot F_0 =
\begin{Matrix}{cc}
1 & 0 \\
0 & A^{(n - 1)}
\end{Matrix}
\)
#+END_CENTER

Recommençons le même procédé pour transformer, au moyen des matrices de permutation $P^{(n-1)},Q^{(n-1)}$ et élémentaires $E^{(n-1)},F^{(n-1)}$, la première colonne et la première ligne de $A^{(n - 1)}$ en $e_1$ et $e_1^\dual$. Si on pose :

#+BEGIN_CENTER
\(
P_1 =
\begin{Matrix}{cc}
1 & 0 \\
0 & P^{(n - 1)}
\end{Matrix}
=
\begin{Matrix}{cc}
I_1 & 0 \\
0 & P^{(n - 1)}
\end{Matrix} \\
Q_1 =
\begin{Matrix}{cc}
1 & 0 \\
0 & Q^{(n - 1)}
\end{Matrix}
=
\begin{Matrix}{cc}
I_1 & 0 \\
0 & Q^{(n - 1)}
\end{Matrix} \\
E_1 =
\begin{Matrix}{cc}
1 & 0 \\
0 & E^{(n - 1)}
\end{Matrix}
=
\begin{Matrix}{cc}
I_1 & 0 \\
0 & E^{(n - 1)}
\end{Matrix} \\
F_1 =
\begin{Matrix}{cc}
1 & 0 \\
0 & F^{(n - 1)}
\end{Matrix}
=
\begin{Matrix}{cc}
I_1 & 0 \\
0 & F^{(n - 1)}
\end{Matrix}
\)
#+END_CENTER

il vient :

#+BEGIN_CENTER
\(
E_1 \cdot P_1 \cdot E_0 \cdot P_0 \cdot A \cdot Q_0 \cdot F_0 \cdot Q_1 \cdot F_1 =
\begin{Matrix}{ccc}
1 & 0 & 0 \\
0 & 1 & 0 \\
0 & 0 & A^{(n - 2)}
\end{Matrix}
\)
#+END_CENTER

Soit $p = \min\{m, n\}$. On peut répéter le même processus $r$ fois en utilisant à l'étape $k$ :

#+BEGIN_CENTER
\(
P_k =
\begin{Matrix}{cc}
I_k & 0 \\
0 & P^{(n - k)}
\end{Matrix} \\
Q_k =
\begin{Matrix}{cc}
I_k & 0 \\
0 & Q^{(n - k)}
\end{Matrix} \\
E_k =
\begin{Matrix}{cc}
I_k & 0 \\
0 & E^{(n - k)}
\end{Matrix} \\
F_k =
\begin{Matrix}{cc}
I_k & 0 \\
0 & F^{(n - k)}
\end{Matrix}
\)
#+END_CENTER

jusqu'à ce que la matrice $A^{(n - r)}$ soit nulle, ou jusqu'à ce qu'on ait atteint $r = p$. Posons :

\begin{align}
E &= E_r \cdot P_r \cdot ... \cdot E_0 \cdot P_0 \\
F &= Q_0 \cdot F_0 \cdot ... \cdot Q_r \cdot F_r
\end{align}

On a alors schématiquement :

$$E \cdot A \cdot F = C$$

où :

#+BEGIN_CENTER
\(
C \in \left\{
I_p ,
\begin{Matrix}{c}
I_n \\ 0
\end{Matrix} ,
\begin{Matrix}{cc}
I_m & 0
\end{Matrix} ,
\begin{Matrix}{cc}
I_r & 0 \\
0 & 0
\end{Matrix}
\right\}
\)
#+END_CENTER

suivant que $r = p = m = n$, $r = p = n$, $r = p = m$ ou $r \strictinferieur p$.


*** Inverse

Les matrices élémentaires de permutation sont inversibles, d'inverse identique à elles-mêmes. On a donc $P_i^{-1} = P_i$ et $Q_i^{-1} = Q_i$. Les matrices élémentaires de transformation sont également inversibles. En effet, $x^\dual \cdot y = x^\dual \cdot \canonique_1 = a \ne 0$. L'inverse de $E_0$ existe donc et s'écrit :

$$E_0^{-1} = I - \unsur{a} \cdot (\canonique_1 - x) \cdot x^\dual$$

D'un autre coté $v^\dual \cdot u = \canonique_1^\dual \cdot u = 1 \ne 0$. L'inverse de $F_0$ existe aussi et s'écrit :

$$F_0^{-1} = I - u \cdot (\canonique_1 - u)^\dual$$

Comme on procède de même à chaque étape, l'inverse de chaque matrice élémentaire existe.  En appliquant la formule d'inversion d'un produit, on obtient :

\begin{align}
E^{-1} &= P_0 \cdot E_0^{-1} \cdot ... \cdot P_r \cdot E_r^{-1} \\
F^{-1} &= F_r^{-1} \cdot Q_r \cdot ... \cdot F_0^{-1} \cdot Q_0
\end{align}

Posons $L = E^{-1}$ et $R = F^{-1}$. En multipliant l'équation $E \cdot A \cdot F = C$ à gauche par $L$ et à droite par $R$, on obtient :

$$A = L \cdot C \cdot R$$

Une telle décomposition est appelée décomposition de Gauss-Jordan et notée :

$$(L,C,R) = \gaussjordan(A)$$


*** Rang

Le $r$ ainsi obtenu est appelé rang de la matrice $A$. On le note :

$$r = \rang A$$

On a par construction $r \le p$.


** Systèmes linéaires

Nous allons à présent utiliser la décomposition de Gauss-Jordan pour analyser les espaces de solutions :

$$S(y) = \{ x \in \corps^n : A \cdot x = y \}$$

pour tout $y \in \corps^m$. On a :

$$A \cdot x = L \cdot C \cdot R \cdot x = y$$

Multiplions cette équation par $L^{-1}$. Il vient :

$$C \cdot R \cdot x = L^{-1} \cdot y$$

On pose :

#+BEGIN_CENTER
\(
z = R \cdot x \\
b = L^{-1} \cdot y
\)
#+END_CENTER

Le système linéaire s'écrit alors :

$$C \cdot z = b$$


*** Plein rang, matrice carrée

Si $r = m = n$, on a :

$$A = L \cdot I \cdot R = L \cdot R$$

La matrice $A$ est un produit de matrices inversibles. Elle est donc inversible et :

$$A^{-1} = R^{-1} \cdot L^{-1}$$

L'équation $A \cdot x = y$ admet donc pour unique solution $x \in S(y)$ le vecteur :

$$x = A^{-1} \cdot y = R^{-1} \cdot L^{-1} \cdot y$$


*** Plein rang, matrice haute

Si $r = n \strictinferieur m$, on a :

#+BEGIN_CENTER
\(
C =
\begin{Matrix}{c}
I_n \\ 0
\end{Matrix}
\)
#+END_CENTER

Si on partitionne $b = L^{-1} \cdot y$ en deux vecteurs $b_1,b_2$ de tailles $(n,1)$ et $(m - n, 1)$, on a :

#+BEGIN_CENTER
\(
\begin{Matrix}{c}
I_n \\ 0
\end{Matrix}
\cdot
z
=
\begin{Matrix}{c}
z \\ 0
\end{Matrix}
=
\begin{Matrix}{c}
b_1 \\ b_2
\end{Matrix}
\)
#+END_CENTER

Il y a donc deux conditions pour que $x = R^{-1} \cdot z$ soit dans $S(y)$ :

#+BEGIN_CENTER
\(
z = b_1 \\
0 = b_2
\)
#+END_CENTER

Si $b_2 \ne 0$, il n'existe pas de solution. Si $b_2 = 0$, il existe une unique solution $x \in S(y)$, qui s'écrit :

$$x = R^{-1} \cdot z = R^{-1} \cdot b_1$$

Remarquons que l'on peut toujours trouver un $y$ tel qu'il existe au moins une solution. En effet, il suffit de choisir :

#+BEGIN_CENTER
\(
y = L \cdot
\begin{Matrix}{c}
b_1 \\ 0
\end{Matrix}
\)
#+END_CENTER

D'un autre coté, il existe toujours un $y$ tel qu'il n'existe pas de solution. En effet, il suffit de choisir :

#+BEGIN_CENTER
\(
y = L \cdot
\begin{Matrix}{c}
b_1 \\ b_2
\end{Matrix}
\)
#+END_CENTER

où $b_2 \ne 0$.


*** Plein rang, matrice longue

Si $r = m \strictinferieur n$, on se retrouve alors avec :

#+BEGIN_CENTER
\(
C =
\begin{Matrix}{cc}
I_m & 0
\end{Matrix}
\)
#+END_CENTER

Si on partitionne $z$ en deux vecteurs $z_1,z_2$ de tailles $(m,1)$ et $(n - m,1)$, on a :

#+BEGIN_CENTER
\(
C \cdot z =
\begin{Matrix}{cc}
I_m & 0
\end{Matrix}
\cdot
\begin{Matrix}{c}
z_1 \\ z_2
\end{Matrix}
=
I_m \cdot z_1 + 0 \cdot z_2 = z_1
\)
#+END_CENTER

La condition pour que $x = R^{-1} \cdot z$ soit dans $S(y)$ se résume à :

$$z_1 = b = L^{-1} \cdot y$$

Nous n'avons par contre aucune condition sur $z_2 \in \matrice(\corps, n - m)$. Il y a donc une infinité de solutions $x \in S(y)$, de la forme :

#+BEGIN_CENTER
\(
x =R^{-1} \cdot z = R^{-1} \cdot
\begin{Matrix}{c}
L^{-1} \cdot y \\ z_2
\end{Matrix}
\)
#+END_CENTER


*** Rang incomplet

Supposons que $r \strictinferieur p$. On partitionne alors $z$ en deux vecteurs $z_1$ et $z_2$ de tailles $(r,1)$ et $(n - r, 1)$ et $b$ en deux vecteurs $b_1$ et $b_2$ de tailles $(r,1)$ et $(m - r,1)$. Avec ces notations, le produit $C \cdot z$ s'écrit :

#+BEGIN_CENTER
\(
C \cdot z =
\begin{Matrix}{cc}
I_r & 0 \\
0 & 0
\end{Matrix}
\cdot
\begin{Matrix}{c}
z_1 \\ z_2
\end{Matrix}
=
\begin{Matrix}{c}
z_1 \\ 0
\end{Matrix}
\)
#+END_CENTER

L'équation $C \cdot z = b$ prend donc la forme :

#+BEGIN_CENTER
\(
\begin{Matrix}{c}
z_1 \\ 0
\end{Matrix}
=
\begin{Matrix}{c}
b_1 \\ b_2
\end{Matrix}
\)
#+END_CENTER

Les deux conditions pour que $x = R^{-1} \cdot z$ soit dans $S(y)$ sont donc que $z_1 = b_1$ et que $b_2 = 0$. Il n'y a aucune condition sur $z_2$. Si $b_2 \ne 0$ il n'y a pas de solution. Si $b_2 = 0$, il existe une infinité de solutions $x \in S(y)$ de la forme :

#+BEGIN_CENTER
\(
x = R^{-1} \cdot
\begin{Matrix}{c}
b_1 \\ z_2
\end{Matrix}
\)
#+END_CENTER

Remarquons que l'on peut toujours trouver un $y$ tel qu'il existe au moins une solution. En effet, il suffit de choisir :

#+BEGIN_CENTER
\(
y = L \cdot
\begin{Matrix}{c}
b_1 \\ 0
\end{Matrix}
\)
#+END_CENTER

Un choix particulier est par exemple $y = b = 0$.

D'un autre coté, il existe toujours un $y$ tel qu'il n'existe pas de solution. En effet, il suffit de choisir :

#+BEGIN_CENTER
\(
y = L \cdot
\begin{Matrix}{c}
b_1 \\ b_2
\end{Matrix}
\)
#+END_CENTER

où $b_2 \ne 0$.


*** Rang et existence

On conclut de ce qui précède que si $r \strictinferieur m$ ou $r \strictinferieur p$, on peut toujours trouver un $y$ tel qu'il n'existe pas de solution. Pour qu'il existe au moins une solution du système quel que soit $y$, il faut donc avoir $r = m = p$.


*** Rang et unicité

On conclut de ce qui précède que si $r \strictinferieur n$ ou $r \strictinferieur p$, on peut toujours trouver un $y$ tel qu'il existe une infinité de solution, et même une infinité de solutions non nulles puisque $z_2 \ne 0$ peut être quelconque.

Pour qu'il existe au maximum une solution du système quel que soit $y$, il faut donc avoir $r = n = p$.


** Matrices hautes

Soit une matrice $A \in \matrice(\corps,m,n)$ strictement haute ($m \strictsuperieur n$). Un inverse à droite $R$ vérifiant $A \cdot B = I$ ne peut pas exister, car sinon il suffirait de prendre $x = B \cdot y$ pour avoir :

$$A \cdot x = A \cdot B \cdot y = I \cdot y = y$$

Le système $A \cdot x = y$ admettrait toujours au moins une solution, ce qui contredit les résultats précédents.


** Matrices longues

Soit une matrice $A \in \matrice(\corps,m,n)$ strictement longue ($n \strictsuperieur m$). Un inverse à gauche $B$ vérifiant $B \cdot A = I$ ne peut pas exister, car sinon on aurait :

$$(B \cdot A)^\dual = A^\dual \cdot B^\dual = I^\dual = I$$

La matrice $B^\dual$ serait donc un inverse à droite de la matrice $A^\dual$. Or, $A^\dual$ est de taille $(n,m)$, donc strictement haute, et ne peut pas admettre d'inverse à droite. On en conclut qu'une matrice strictement longue ne peut pas admettre d'inverse à gauche.


** Matrices carrées

Supposons que $m = n$ et considérons deux matrices carrées $A,B \in \matrice(\corps,n,n)$ telles que :

$$A \cdot B = I$$

Il existe au moins une solution du système $A \cdot x = y$ quel que soit $y$, car il suffit de prendre $x = B \cdot y$ pour avoir $A \cdot x = A \cdot B \cdot y = y$. On a donc forcément $r = m \le n$. Mais comme $m = n$, cette solution est unique. On en déduit que l'application linéaire associée à $A$
est inversible, donc $A$ aussi et :

$$A \cdot (B - A^{-1}) = I - I = 0$$

En multipliant à gauche par $A^{-1}$, il vient simplement $B - A^{-1} = 0$, c'est-à-dire $B = A^{-1}$. On a donc également :

$$B \cdot A = A^{-1} \cdot A = I$$


** Complément de Schur

Soit le système suivant :

#+BEGIN_CENTER
\(
\begin{Matrix}{cc}
A & B \\ C & D
\end{Matrix}
\cdot
\begin{Matrix}{c}
x & y
\end{Matrix}
=
\begin{Matrix}{c}
F \\ G
\end{Matrix}
\)
#+END_CENTER

où $A,B,C,D,F,G$ et $x,y$ sont respectivement des matrices et des vecteurs matriciels de tailles compatibles. On a :

#+BEGIN_CENTER
\(
A \cdot x + B \cdot y = F \\
C \cdot x + D \cdot y = G
\)
#+END_CENTER

Si $A$ est carrée et inversible, la première relation nous permet d'éliminer $x$ :

$$x = A^{-1} \cdot F - A^{-1} \cdot B \cdot y$$

On substitue alors dans la seconde relation, et on obtient :

$$C \cdot A^{-1} \cdot F - C \cdot A^{-1} \cdot B \cdot y + D \cdot y = G$$

En plaçant $y$ en évidence, on obtient :

$$(D - C \cdot A^{-1} \cdot B) \cdot y = G - C \cdot A^{-1} \cdot F$$

Sous réserve d'inversibilité, il ne nous reste alors plus qu'à résoudre par rapport à $y$ :

$$y = (D - C \cdot A^{-1} \cdot B)^{-1} \cdot (G - C \cdot A^{-1} \cdot F)$$

qui nous donne ensuite :

$$x = A^{-1} \cdot [ F - B \cdot (D - C \cdot A^{-1} \cdot B)^{-1} \cdot (G - C \cdot A^{-1} \cdot F) ]$$

Si $A$ est facilement inversible, et si $D - C \cdot A^{-1} \cdot B$ est de taille réduite comparativement à la taille de $A$, il peut être avantageux de résoudre le système global en $(x,y)$ de cette façon.


** Dimension

Soit un espace vectoriel $E$ sur $\corps$ possédant deux bases $(e_1,e_2,...,e_n)$ et $(f_1,f_2,...,f_m)$. Comme les $f_i \in E$, on a :

$$f_i = \sum_{k = 1}^n a_{ik} \cdot e_k$$

pour certains $a_{ij} \in \corps$. Comme les $e_k \in E$, on a également :

$$e_k = \sum_{j = 1}^m b_{kj} \cdot f_j$$

pour certains $b_{kj} \in \corps$. On en conclut que :

$$f_i = \sum_{j = 1}^m \sum_{k = 1}^n a_{ik} \cdot b_{kj} \cdot f_j$$

Par indépendance linéaire des $f_i$, on doit donc avoir :

$$\sum_{k = 1}^m a_{ik} \cdot b_{kj} = \indicatrice_{ij}$$

Si nous introduisons les matrices $A \in \matrice(\corps,m,n)$ et $B \in \matrice(\corps,n,m)$ de composantes $a_{ij}$ et $b_{ij}$ respectivement, on a donc $A \cdot B = I_m$. La matrice $A$ admettant un inverse à droite, elle ne peut pas être strictement haute et on a $m \le n$. Mais on a aussi :

$$e_k = \sum_{k = 1}^n \sum_{j = 1}^m b_{kj} \cdot a_{ji} \cdot e_i$$

d'où l'on conclut que $B \cdot A = I_n$. La matrice $A$ admettant un inverse à gauche, elle ne peut pas être strictement longue et on a également $n \le m$. On conclut de ces deux inégalités que $m = n$. Si on possède une base de $E$ comptant $n$ vecteurs, on peut-être sûr que toute autre base de $E$ comptera également $n$ vecteurs. On dit que $m = n$ est la dimension de $E$ et on le note :

$$\dim E = n$$


*** Corollaire

La base canonique de $\corps^n$ comptant $n$ éléments,
on en déduit que $\corps^n$ est de dimension $n$.


** Indépendance linéaire

Soit un espace vectoriel $E$ de dimension $m$ et une suite de vecteurs linéairement indépendants $(u_1,...,u_n)$ de $E$. Si $(e_1,...,e_m)$ est une base de $E$, on peut trouver des coordonnées $a_{ki} \in \corps$ telles que :

$$u_i = \sum_{k = 1}^m a_{ki} \cdot e_k$$

Pour tout $w \in E$, on sait aussi que l'on peut trouver des coordonnées $y_k \in \corps$ telles que :

$$w = \sum_{k = 1}^m y_k \cdot e_k$$

Nous allons à présent examiner si on peut également trouver des coordonnées $x_i \in \corps$ de $w$ par rapport aux $u_i$ :

$$w = \sum_{i = 1}^n x_i \cdot u_i$$

Si cette hypothèse est vérifiée, on doit avoir :

$$\sum_{k = 1}^m y_k \cdot e_k = w = \sum_{k = 1}^m \sum_{i = 1}^n a_{ki} \cdot x_i \cdot e_k$$

L'indépendance linéaire des $e_k$ implique alors que :

$$y_k = \sum_{i = 1}^n a_{ki} \cdot x_i$$

Utilisant les vecteurs matriciels associés $x = [x_1 \ ... \ x_n]^\dual$ et $y = [y_1 \ ... \ y_m]^\dual$ ainsi que la matrice $A = (a_{ki})_{k,i}$ de taille $(m,n)$, on se retrouve avec le système :

$$y = A \cdot x$$

Soit $r$ le rang de $A$. On sait déjà que $r \le \min \{m,n\}$. Examinons les différents cas :

  - Si on avait $r \strictinferieur n$, on pourrait trouver un $y$ tel qu'il existe une infinité de solution en $x$, ce qui contredit l'unicité des coordonnées de $w$ par rapport à la suite de vecteurs linéairement indépendant $(u_1,...,u_n)$. On doit donc avoir $r = n \le m$.

  - Dans le cas où $r = n \strictinferieur m$, on peut trouver un $y$ tel que la solution n'existe pas : la suite de $u_i$ ne forme donc pas une base de $E$.

  - Dans le cas où $r = n = m$, il existe toujours une unique solution, la suite des $u_i$ forme alors une base de $E$. Il suffit donc de trouver une suite de $m$ vecteurs indépendants dans un espace de dimension $m$ pour former une base de cet espace.


* Matrices unitaires

#+TOC: headlines 1 local

\label{chap:unitaire}


** Conservation du produit scalaire

Les matrices unitaires sont une généralisation des rotations. Or, la propriété essentielle d'une rotation est qu'elle conserve les distances. Comme les distances usuelles découlent des normes usuelles, elles-mêmes dérivées du produit scalaire, on impose que ce soit le produit scalaire qui soit conservé. On veut donc que :

$$\scalaire{U \cdot x}{U \cdot y} = x^\dual \cdot U^\dual \cdot U \cdot y = \scalaire{x}{y} = x^\dual \cdot y$$

pour toute matrice unitaire $U$ de taille $(m,n)$ et tout $x,y \in \corps^n$. Comme cette relation doit être valable pour tout $x,y$, elle doit l'être pour les vecteurs de la base canonique :

$$\composante_{ij} U = \canonique_i^\dual \cdot U^\dual \cdot U \cdot \canonique_j = \canonique_i^\dual \cdot \canonique_j = \indicatrice_{ij}$$

On en conclut que :

$$U^\dual \cdot U = I$$

Si cette condition est vérifiée, on dit que $U$ est une matrice unitaire.


*** Norme

La conservation de la norme usuelle $\norme{x} = \sqrt{\scalaire{x}{x} }$ découle de celle du produit scalaire. Si $U$ est unitaire, on a donc $\norme{U \cdot x} = \norme{x}$.


** Matrices élémentaires unitaires

Soit la matrice élémentaire :

$$U = \matelementaire(\alpha,u,u) = I + \alpha \cdot u \cdot u^\dual$$

où $\alpha \in \setR$. Si on veut que $U$ soit unitaire, il faut que :

\begin{align}
I = U^\dual \cdot U &= (I + \alpha \cdot u \cdot u^\dual) \cdot (I + \alpha \cdot u \cdot u^\dual) \\
&= I + [2 \cdot \alpha + \alpha^2 \cdot (u^\dual \cdot u)] \cdot u \cdot u^\dual
\end{align}

Il suffit donc que $\alpha$ vérifie la condition :

$$2 \cdot \alpha + \alpha^2 \cdot (u^\dual \cdot u) = \alpha \cdot [2 + \alpha \cdot (u^\dual \cdot u)] = 0$$

Si on choisit $\alpha = 0$, on a $U = I$ qui est bien une matrice unitaire. Dans le cas où $\alpha \ne 0$, on doit avoir :

$$2 + \alpha \cdot (u^\dual \cdot u) = 0$$

Si $u$ est un vecteur non nul, on aura bien sûr $u^\dual \cdot u \strictsuperieur 0$. Il suffit alors de prendre :

$$\alpha = - \frac{2}{u^\dual \cdot u}$$

On se retrouve donc avec des matrices élémentaires unitaires du type :

$$U = I - \frac{2}{u^\dual \cdot u} \cdot u \cdot u^\dual$$

On les note :

$$\matunitaire(u) = I - \frac{2}{u^\dual \cdot u} \cdot u \cdot u^\dual$$


*** Propriétés

On a donc $U^\dual \cdot U = I$. Comme $U$ est carrée, on a également $U^{-1} = U^\dual$. On voit également que :

$$U^\dual = I - \frac{2}{u^\dual \cdot u} \cdot u \cdot u^\dual = U$$

On en conclut que $U^{-1} = U$.


** Matrices de Householder

Soit deux vecteurs $x,y \in \corps^n$. On aimerait bien trouver une matrice élémentaire unitaire $U$ telle que $U \cdot x \approx y$. Par analogie avec les matrices de transformation élémentaire, on pose $u = x - y$ et $U = \matunitaire(u)$. On a alors :

\begin{align}
U \cdot x &= x - \frac{2 u^\dual \cdot x}{u^\dual \cdot u} \cdot u \\ \\
&= \frac{ (u^\dual \cdot u^\dual) \cdot x - 2 (u^\dual \cdot x) \cdot x + 2 (u^\dual \cdot x) \cdot y }{u^\dual \cdot u^\dual} \\ \\
&= \frac{ (u^\dual \cdot u^\dual - 2 u^\dual \cdot x) \cdot x + 2 (u^\dual \cdot x) \cdot y }{u^\dual \cdot u^\dual}
\end{align}

Développons le coefficient de $x$ :

$$u^\dual \cdot u^\dual - 2 u^\dual \cdot x = (x^\dual \cdot x - x^\dual \cdot y - y^\dual \cdot x + y^\dual \cdot y) - (2 x^\dual \cdot x - 2 y^\dual \cdot x)$$

Comme il s'agit d'une transformation unitaire, il est logique de demander que le produit scalaire soit conservé. On a donc $x^\dual \cdot x = y^\dual \cdot y$. Si on impose de plus que $x^\dual \cdot y$ soit réel, on a :

$$x^\dual \cdot y = \conjugue(x^\dual \cdot y) = y^\dual \cdot x$$

et :

$$u^\dual \cdot u^\dual - 2 u^\dual \cdot x = (2 x^\dual \cdot x - 2 y^\dual \cdot x) - (2 x^\dual \cdot x - 2 y^\dual \cdot x) = 0$$

On a alors :

$$U \cdot x = \gamma \cdot y$$

où :

$$\gamma = \frac{2 u^\dual \cdot x}{u^\dual \cdot u^\dual} \in \setC$$

Le vecteur $U \cdot x$ est donc identique à $y$ à un scalaire près.


*** Base canonique

Un cas particulier intéressant est celui où $y$ est proportionnel au $i^{eme}$ vecteur de la base canonique :

$$y = \alpha \cdot \canonique_i$$

avec $\alpha \in \setC$. Si $x_i = \composante_i x$, on a alors :

$$y^\dual \cdot x = \conjaccent{\alpha} \cdot x_i$$

  - Si $x_i = 0$, on a $y^\dual \cdot x = 0 \in \setR$. Il suffit alors de choisir $\alpha$ pour que le produit scalaire soit conservé :

$$y^\dual \cdot y = \abs{\alpha}^2 = x^\dual \cdot x$$

On peut donc prendre :

$$\alpha = \sqrt{x^\dual \cdot x} \in \setR$$

  - Considérons à présent le cas où $x_i \ne 0$. Si on prend $\alpha = \lambda \cdot x_i$ où $\lambda \in \setR$ est quelconque, on a :

$$y^\dual \cdot x = \lambda \cdot \conjaccent{x}_i \cdot x_i = \lambda \cdot \abs{x_i}^2 \in \setR$$

Comme on exige que la norme soit conservée, il faut de plus que :

$$y^\dual \cdot y = \lambda^2 \cdot \abs{x_i}^2 = x^\dual \cdot x$$

On doit donc avoir $\lambda^2 = x^\dual \cdot x / \abs{x_i}^2$ et :

$$\alpha = \lambda \cdot x_i = \sqrt{ \frac{ x^\dual \cdot x }{ \abs{x_i}^2 } } \cdot x_i$$


Il ne nous reste alors plus qu'à poser $u = x - \alpha \cdot \canonique_i$ et $U = \matunitaire(u)$ pour obtenir :

$$U \cdot x = \gamma \cdot \canonique_i$$

pour un certain $\gamma \in \setC$.


** Décomposition de Householder

Soit une matrice $A$ de taille $(m,n)$ et le partitionnement en colonnes :

$$A = [x \ x_2 \ ... \ x_n]$$

Soit $H_1$ la matrice de Householder qui permet de transformer $x$ en $\gamma \cdot \canonique_1$, pour un certain $\gamma_1 \in \setC$. On a alors :

#+BEGIN_CENTER
\(
H_1 \cdot A =
\begin{Matrix}{cc}
\gamma_1 & \hdots \\
0 & A^{(n - 1)}
\end{Matrix}
\)
#+END_CENTER

On peut répéter le même processus avec $A^{(n-1)}$ et la matrice de Householder $H^{(n-1)}$ correspondante. Si on pose alors :

#+BEGIN_CENTER
\(
H_2 =
\begin{Matrix}{cc}
1 & 0 \\
0 & H^{(n-1)}
\end{Matrix}
=
\begin{Matrix}{cc}
I_1 & 0 \\
0 & H^{(n-1)}
\end{Matrix}
\)
#+END_CENTER

il vient :

#+BEGIN_CENTER
\(
H_2 \cdot H_1 \cdot A =
\begin{Matrix}{ccc}
\gamma_1 & \hdots & \hdots \\
0 & \gamma_2 & \hdots \\
0 & 0 & H^{(n-2)}
\end{Matrix}
\)
#+END_CENTER

On peut répéter le processus $p = \min\{m,n\}$ fois en utilisant à l'étape $k + 1$ :

#+BEGIN_CENTER
\(
H_{k + 1} =
\begin{Matrix}{cc}
I_k & 0 \\
0 & H^{(n-k)}
\end{Matrix}
\)
#+END_CENTER

Posons :

$$H = H_p \cdot ... \cdot H_1$$

Si $m \strictinferieur n$, on obtient à la fin du processus :

#+BEGIN_CENTER
\(
H \cdot A = R =
\begin{Matrix}{cccc}
\gamma_1 & \hdots & \hdots & \hdots \\
0 & \ddots & \hdots & \hdots \\
0 & 0 & \gamma_m & \hdots
\end{Matrix}
\)
#+END_CENTER

Si $m = n$, on obtient à la fin du processus :

#+BEGIN_CENTER
\(
H \cdot A = R =
\begin{Matrix}{ccc}
\gamma_1 & \hdots & \hdots \\
0 & \ddots & \hdots \\
0 & 0 & \gamma_m
\end{Matrix}
\)
#+END_CENTER

Si $m \strictsuperieur n$, on obtient à la fin du processus :

#+BEGIN_CENTER
\(
H \cdot A = R =
\begin{Matrix}{ccc}
\gamma_1 & \hdots & \hdots \\
0 & \ddots & \hdots \\
0 & 0 & \gamma_m \\
0 & 0 & 0
\end{Matrix}
\)
#+END_CENTER

On voit que dans tous les cas la matrice $R$ est triangulaire supérieure. Posons :

$$Q = H^{-1} = H_1 \cdot ... \cdot H_p = H^\dual$$

On a alors la décomposition :

$$A = Q \cdot R$$

où $Q$ est une matrice unitaire et $R$ une matrice triangulaire supérieure. On note cette décomposition :

$$(Q,R) = \householder(A)$$
