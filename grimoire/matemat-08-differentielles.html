<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN"
"http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" lang="fr" xml:lang="fr">
<head>
<!-- 2023-05-10 mer 16:44 -->
<meta http-equiv="Content-Type" content="text/html;charset=utf-8" />
<meta name="viewport" content="width=device-width, initial-scale=1" />
<title>Eclats de vers : Matemat 08 : Différentielles</title>
<meta name="author" content="chimay" />
<meta name="generator" content="Org Mode" />
<style>
  .title  { text-align: center;
             margin-bottom: .2em; }
  .subtitle { text-align: center;
              font-size: medium;
              font-weight: bold;
              margin-top:0; }
  .todo   { font-family: monospace; color: red; }
  .done   { font-family: monospace; color: green; }
  .priority { font-family: monospace; color: orange; }
  .tag    { background-color: #eee; font-family: monospace;
            padding: 2px; font-size: 80%; font-weight: normal; }
  .timestamp { color: #bebebe; }
  .timestamp-kwd { color: #5f9ea0; }
  .org-right  { margin-left: auto; margin-right: 0px;  text-align: right; }
  .org-left   { margin-left: 0px;  margin-right: auto; text-align: left; }
  .org-center { margin-left: auto; margin-right: auto; text-align: center; }
  .underline { text-decoration: underline; }
  #postamble p, #preamble p { font-size: 90%; margin: .2em; }
  p.verse { margin-left: 3%; }
  pre {
    border: 1px solid #e6e6e6;
    border-radius: 3px;
    background-color: #f2f2f2;
    padding: 8pt;
    font-family: monospace;
    overflow: auto;
    margin: 1.2em;
  }
  pre.src {
    position: relative;
    overflow: auto;
  }
  pre.src:before {
    display: none;
    position: absolute;
    top: -8px;
    right: 12px;
    padding: 3px;
    color: #555;
    background-color: #f2f2f299;
  }
  pre.src:hover:before { display: inline; margin-top: 14px;}
  /* Languages per Org manual */
  pre.src-asymptote:before { content: 'Asymptote'; }
  pre.src-awk:before { content: 'Awk'; }
  pre.src-authinfo::before { content: 'Authinfo'; }
  pre.src-C:before { content: 'C'; }
  /* pre.src-C++ doesn't work in CSS */
  pre.src-clojure:before { content: 'Clojure'; }
  pre.src-css:before { content: 'CSS'; }
  pre.src-D:before { content: 'D'; }
  pre.src-ditaa:before { content: 'ditaa'; }
  pre.src-dot:before { content: 'Graphviz'; }
  pre.src-calc:before { content: 'Emacs Calc'; }
  pre.src-emacs-lisp:before { content: 'Emacs Lisp'; }
  pre.src-fortran:before { content: 'Fortran'; }
  pre.src-gnuplot:before { content: 'gnuplot'; }
  pre.src-haskell:before { content: 'Haskell'; }
  pre.src-hledger:before { content: 'hledger'; }
  pre.src-java:before { content: 'Java'; }
  pre.src-js:before { content: 'Javascript'; }
  pre.src-latex:before { content: 'LaTeX'; }
  pre.src-ledger:before { content: 'Ledger'; }
  pre.src-lisp:before { content: 'Lisp'; }
  pre.src-lilypond:before { content: 'Lilypond'; }
  pre.src-lua:before { content: 'Lua'; }
  pre.src-matlab:before { content: 'MATLAB'; }
  pre.src-mscgen:before { content: 'Mscgen'; }
  pre.src-ocaml:before { content: 'Objective Caml'; }
  pre.src-octave:before { content: 'Octave'; }
  pre.src-org:before { content: 'Org mode'; }
  pre.src-oz:before { content: 'OZ'; }
  pre.src-plantuml:before { content: 'Plantuml'; }
  pre.src-processing:before { content: 'Processing.js'; }
  pre.src-python:before { content: 'Python'; }
  pre.src-R:before { content: 'R'; }
  pre.src-ruby:before { content: 'Ruby'; }
  pre.src-sass:before { content: 'Sass'; }
  pre.src-scheme:before { content: 'Scheme'; }
  pre.src-screen:before { content: 'Gnu Screen'; }
  pre.src-sed:before { content: 'Sed'; }
  pre.src-sh:before { content: 'shell'; }
  pre.src-sql:before { content: 'SQL'; }
  pre.src-sqlite:before { content: 'SQLite'; }
  /* additional languages in org.el's org-babel-load-languages alist */
  pre.src-forth:before { content: 'Forth'; }
  pre.src-io:before { content: 'IO'; }
  pre.src-J:before { content: 'J'; }
  pre.src-makefile:before { content: 'Makefile'; }
  pre.src-maxima:before { content: 'Maxima'; }
  pre.src-perl:before { content: 'Perl'; }
  pre.src-picolisp:before { content: 'Pico Lisp'; }
  pre.src-scala:before { content: 'Scala'; }
  pre.src-shell:before { content: 'Shell Script'; }
  pre.src-ebnf2ps:before { content: 'ebfn2ps'; }
  /* additional language identifiers per "defun org-babel-execute"
       in ob-*.el */
  pre.src-cpp:before  { content: 'C++'; }
  pre.src-abc:before  { content: 'ABC'; }
  pre.src-coq:before  { content: 'Coq'; }
  pre.src-groovy:before  { content: 'Groovy'; }
  /* additional language identifiers from org-babel-shell-names in
     ob-shell.el: ob-shell is the only babel language using a lambda to put
     the execution function name together. */
  pre.src-bash:before  { content: 'bash'; }
  pre.src-csh:before  { content: 'csh'; }
  pre.src-ash:before  { content: 'ash'; }
  pre.src-dash:before  { content: 'dash'; }
  pre.src-ksh:before  { content: 'ksh'; }
  pre.src-mksh:before  { content: 'mksh'; }
  pre.src-posh:before  { content: 'posh'; }
  /* Additional Emacs modes also supported by the LaTeX listings package */
  pre.src-ada:before { content: 'Ada'; }
  pre.src-asm:before { content: 'Assembler'; }
  pre.src-caml:before { content: 'Caml'; }
  pre.src-delphi:before { content: 'Delphi'; }
  pre.src-html:before { content: 'HTML'; }
  pre.src-idl:before { content: 'IDL'; }
  pre.src-mercury:before { content: 'Mercury'; }
  pre.src-metapost:before { content: 'MetaPost'; }
  pre.src-modula-2:before { content: 'Modula-2'; }
  pre.src-pascal:before { content: 'Pascal'; }
  pre.src-ps:before { content: 'PostScript'; }
  pre.src-prolog:before { content: 'Prolog'; }
  pre.src-simula:before { content: 'Simula'; }
  pre.src-tcl:before { content: 'tcl'; }
  pre.src-tex:before { content: 'TeX'; }
  pre.src-plain-tex:before { content: 'Plain TeX'; }
  pre.src-verilog:before { content: 'Verilog'; }
  pre.src-vhdl:before { content: 'VHDL'; }
  pre.src-xml:before { content: 'XML'; }
  pre.src-nxml:before { content: 'XML'; }
  /* add a generic configuration mode; LaTeX export needs an additional
     (add-to-list 'org-latex-listings-langs '(conf " ")) in .emacs */
  pre.src-conf:before { content: 'Configuration File'; }

  table { border-collapse:collapse; }
  caption.t-above { caption-side: top; }
  caption.t-bottom { caption-side: bottom; }
  td, th { vertical-align:top;  }
  th.org-right  { text-align: center;  }
  th.org-left   { text-align: center;   }
  th.org-center { text-align: center; }
  td.org-right  { text-align: right;  }
  td.org-left   { text-align: left;   }
  td.org-center { text-align: center; }
  dt { font-weight: bold; }
  .footpara { display: inline; }
  .footdef  { margin-bottom: 1em; }
  .figure { padding: 1em; }
  .figure p { text-align: center; }
  .equation-container {
    display: table;
    text-align: center;
    width: 100%;
  }
  .equation {
    vertical-align: middle;
  }
  .equation-label {
    display: table-cell;
    text-align: right;
    vertical-align: middle;
  }
  .inlinetask {
    padding: 10px;
    border: 2px solid gray;
    margin: 10px;
    background: #ffffcc;
  }
  #org-div-home-and-up
   { text-align: right; font-size: 70%; white-space: nowrap; }
  textarea { overflow-x: auto; }
  .linenr { font-size: smaller }
  .code-highlighted { background-color: #ffff00; }
  .org-info-js_info-navigation { border-style: none; }
  #org-info-js_console-label
    { font-size: 10px; font-weight: bold; white-space: nowrap; }
  .org-info-js_search-highlight
    { background-color: #ffff00; color: #000000; font-weight: bold; }
  .org-svg { }
</style>
<link rel="stylesheet" type="text/css" href="../style/defaut.css" />
<script type="text/x-mathjax-config">
    MathJax.Hub.Config({
        displayAlign: "center",
        displayIndent: "0em",

        "HTML-CSS": { scale: 100,
                        linebreaks: { automatic: "false" },
                        webFont: "TeX"
                       },
        SVG: {scale: 100,
              linebreaks: { automatic: "false" },
              font: "TeX"},
        NativeMML: {scale: 100},
        TeX: { equationNumbers: {autoNumber: "AMS"},
               MultLineWidth: "85%",
               TagSide: "right",
               TagIndent: ".8em"
             }
});
</script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.0/MathJax.js?config=TeX-AMS_HTML"></script>
</head>
<body>
<div id="content" class="content">
<h1 class="title">Eclats de vers : Matemat 08 : Différentielles</h1>
<p>
<a href="index.html">Index des Grimoires</a>
</p>

<p>
<a href="../index.html">Retour à l’accueil</a>
</p>

<div id="table-of-contents" role="doc-toc">
<h2>Table des matières</h2>
<div id="text-table-of-contents" role="doc-toc">
<ul>
<li><a href="#org465a916">1. Différentielles</a></li>
<li><a href="#org71501d8">2. Dérivées</a></li>
<li><a href="#orgd7d675b">3. Différentielles et polynômes</a></li>
<li><a href="#orge1bad48">4. Dérivées des puissances</a></li>
<li><a href="#orge9d8032">5. Différentielles et matrices</a></li>
<li><a href="#org15bf4bb">6. Résolution d'équations</a></li>
</ul>
</div>
</div>

<p>
\( \newcommand{\parentheses}[1]{\left(#1\right)}
\newcommand{\crochets}[1]{\left[#1\right]}
\newcommand{\accolades}[1]{\left\{#1\right\}}
\newcommand{\ensemble}[1]{\left\{#1\right\}}
\newcommand{\identite}{\mathrm{Id}}
\newcommand{\indicatrice}{\boldsymbol{\delta}}
\newcommand{\dirac}{\delta}
\newcommand{\moinsun}{{-1}}
\newcommand{\inverse}{\ddagger}
\newcommand{\pinverse}{\dagger}
\newcommand{\topologie}{\mathfrak{T}}
\newcommand{\ferme}{\mathfrak{F}}
\newcommand{\img}{\mathbf{i}}
\newcommand{\binome}[2]{
\left\{ \begin{array}{c}
#1 \\
#2 \\
\end{array} \right\}
}
\newcommand{\canonique}{\mathfrak{c}}
\newcommand{\tenseuridentite}{\boldsymbol{\mathcal{I}}}
\newcommand{\permutation}{\boldsymbol{\epsilon}}
\newcommand{\matriceZero}{\mathfrak{0}}
\newcommand{\matriceUn}{\mathfrak{1}}
\newcommand{\christoffel}[2]{
\left\{ \begin{array}{c}
#1 \\
#2 \\
\end{array} \right\}
}
\newcommand{\lagrangien}{\mathfrak{L}}
\newcommand{\sousens}{\mathfrak{P}}
\newcommand{\partition}{\mathrm{Partition}}
\newcommand{\tribu}{\mathrm{Tribu}}
\newcommand{\topologies}{\mathrm{Topo}}
\newcommand{\setB}{\mathbb{B}}
\newcommand{\setN}{\mathbb{N}}
\newcommand{\setZ}{\mathbb{Z}}
\newcommand{\setQ}{\mathbb{Q}}
\newcommand{\setR}{\mathbb{R}}
\newcommand{\setC}{\mathbb{C}}
\newcommand{\corps}{\mathbb{K}}
\newcommand{\boule}{\mathfrak{B}}
\newcommand{\intervalleouvert}[2]{\relax \ ] #1 , #2 [ \ \relax}
\newcommand{\intervallesemiouvertgauche}[2]{\relax \ ] #1 , #2 ]}
\newcommand{\intervallesemiouvertdroite}[2]{[ #1 , #2 [ \ \relax}
\newcommand{\fonction}{\mathbb{F}}
\newcommand{\bijection}{\mathrm{Bij}}
\newcommand{\polynome}{\mathrm{Poly}}
\newcommand{\lineaire}{\mathrm{Lin}}
\newcommand{\continue}{\mathrm{Cont}}
\newcommand{\homeomorphisme}{\mathrm{Hom}}
\newcommand{\etagee}{\mathrm{Etagee}}
\newcommand{\lebesgue}{\mathrm{Leb}}
\newcommand{\lipschitz}{\mathrm{Lip}}
\newcommand{\suitek}{\mathrm{Suite}}
\newcommand{\matrice}{\mathbb{M}}
\newcommand{\krylov}{\mathrm{Krylov}}
\newcommand{\tenseur}{\mathbb{T}}
\newcommand{\essentiel}{\mathfrak{E}}
\newcommand{\relation}{\mathrm{Rel}}
\newcommand{\strictinferieur}{\ < \ }
\newcommand{\strictsuperieur}{\ > \ }
\newcommand{\ensinferieur}{\eqslantless}
\newcommand{\enssuperieur}{\eqslantgtr}
\newcommand{\esssuperieur}{\gtrsim}
\newcommand{\essinferieur}{\lesssim}
\newcommand{\essegal}{\eqsim}
\newcommand{\union}{\ \cup \ }
\newcommand{\intersection}{\ \cap \ }
\newcommand{\opera}{\divideontimes}
\newcommand{\autreaddition}{\boxplus}
\newcommand{\autremultiplication}{\circledast}
\newcommand{\commutateur}[2]{\left[ #1 , #2 \right]}
\newcommand{\convolution}{\circledcirc}
\newcommand{\correlation}{\ \natural \ }
\newcommand{\diventiere}{\div}
\newcommand{\modulo}{\bmod}
\newcommand{\pgcd}{pgcd}
\newcommand{\ppcm}{ppcm}
\newcommand{\produitscalaire}[2]{\left\langle #1 \left|\right\relax #2 \right\rangle}
\newcommand{\scalaire}[2]{\left\langle #1 \| #2 \right\rangle}
\newcommand{\braket}[3]{\left\langle #1 \right| #2 \left| #3 \right\rangle}
\newcommand{\orthogonal}{\bot}
\newcommand{\forme}[2]{\left\langle #1 , #2 \right\rangle}
\newcommand{\biforme}[3]{\left\langle #1 , #2 , #3 \right\rangle}
\newcommand{\contraction}[3]{\left\langle #1 \odot #3 \right\rangle_{#2}}
\newcommand{\dblecont}[5]{\left\langle #1 \right| #3 \left| #5 \right\rangle_{#2,#4}}
\newcommand{\major}{major}
\newcommand{\minor}{minor}
\newcommand{\maxim}{maxim}
\newcommand{\minim}{minim}
\newcommand{\argument}{arg}
\newcommand{\argmin}{arg\ min}
\newcommand{\argmax}{arg\ max}
\newcommand{\supessentiel}{ess\ sup}
\newcommand{\infessentiel}{ess\ inf}
\newcommand{\dual}{\star}
\newcommand{\distance}{\mathfrak{dist}}
\newcommand{\norme}[1]{\left\| #1 \right\|}
\newcommand{\normetrois}[1]{\left|\left\| #1 \right\|\right|}
\newcommand{\adh}{adh}
\newcommand{\interieur}{int}
\newcommand{\frontiere}{\partial}
\newcommand{\image}{im}
\newcommand{\domaine}{dom}
\newcommand{\noyau}{ker}
\newcommand{\support}{supp}
\newcommand{\signe}{sign}
\newcommand{\abs}[1]{\left| #1 \right|}
\newcommand{\unsur}[1]{\frac{1}{#1}}
\newcommand{\arrondisup}[1]{\lceil #1 \rceil}
\newcommand{\arrondiinf}[1]{\lfloor #1 \rfloor}
\newcommand{\conjugue}{conj}
\newcommand{\conjaccent}[1]{\overline{#1}}
\newcommand{\division}{division}
\newcommand{\difference}{\boldsymbol{\Delta}}
\newcommand{\differentielle}[2]{\mathfrak{D}^{#1}_{#2}}
\newcommand{\OD}[2]{\frac{d #1}{d #2}}
\newcommand{\OOD}[2]{\frac{d^2 #1}{d #2^2}}
\newcommand{\NOD}[3]{\frac{d^{#3} #1}{d #2^{#3}}}
\newcommand{\deriveepartielle}[2]{\frac{\partial #1}{\partial #2}}
\newcommand{\dblederiveepartielle}[2]{\frac{\partial^2 #1}{\partial #2 \partial #2}}
\newcommand{\dfdxdy}[3]{\frac{\partial^2 #1}{\partial #2 \partial #3}}
\newcommand{\dfdxdx}[2]{\frac{\partial^2 #1}{\partial #2^2}}
\newcommand{\gradient}{\mathbf{\nabla}}
\newcommand{\combilin}[1]{\mathrm{span}\{ #1 \}}
\newcommand{\trace}{tr}
\newcommand{\proba}{\mathbb{P}}
\newcommand{\probaof}[1]{\mathbb{P}\left[#1\right]}
\newcommand{\esperof}[1]{\mathbb{E}\left[#1\right]}
\newcommand{\cov}[2]{\mathrm{cov} \left( #1 , #2 \right) }
\newcommand{\var}[1]{\mathrm{var} \left( #1 \right) }
\newcommand{\rand}{\mathrm{rand}}
\newcommand{\variation}[1]{\left\langle #1 \right\rangle}
\newcommand{\composante}{comp}
\newcommand{\bloc}{bloc}
\newcommand{\ligne}{ligne}
\newcommand{\colonne}{colonne}
\newcommand{\diagonale}{diag}
\newcommand{\matelementaire}{\mathrm{Elem}}
\newcommand{\matpermutation}{permut}
\newcommand{\matunitaire}{\mathrm{Unitaire}}
\newcommand{\gaussjordan}{\mathrm{GaussJordan}}
\newcommand{\householder}{\mathrm{Householder}}
\newcommand{\rang}{rang}
\newcommand{\schur}{\mathrm{Schur}}
\newcommand{\singuliere}{\mathrm{DVS}}
\newcommand{\convexe}{\mathrm{Convexe}}
\newcommand{\petito}[1]{o\left(#1\right)}
\newcommand{\grando}[1]{O\left(#1\right)} \)
</p>

<div id="outline-container-org465a916" class="outline-2">
<h2 id="org465a916"><span class="section-number-2">1.</span> Différentielles</h2>
<div class="outline-text-2" id="text-1">
<div id="text-table-of-contents" role="doc-toc">
<ul>
<li><a href="#org24120a8">1.1. Dépendances</a></li>
<li><a href="#org4fce723">1.2. Définition</a></li>
<li><a href="#org1499ee3">1.3. Continuité</a></li>
<li><a href="#org230e526">1.4. Dérivées partielles</a></li>
<li><a href="#orgd67bd2c">1.5. Dérivées et limites</a></li>
<li><a href="#org8af3247">1.6. Représentation matricielle</a></li>
<li><a href="#org290256d">1.7. Dérivées ordinaires</a></li>
<li><a href="#org95fc80e">1.8. L'application dérivée</a></li>
<li><a href="#orged91681">1.9. Hessienne</a></li>
<li><a href="#orgc7bab30">1.10. Dérivée d'ordre \(k\)</a></li>
<li><a href="#org5a28573">1.11. Fonctions à intégrale continue</a></li>
<li><a href="#org50d379b">1.12. Différentiabilité uniforme</a></li>
</ul>
</div>

<p>
\label{chap:differentielles}
</p>
</div>


<div id="outline-container-org24120a8" class="outline-3">
<h3 id="org24120a8"><span class="section-number-3">1.1.</span> Dépendances</h3>
<div class="outline-text-3" id="text-1-1">
<ul class="org-ul">
<li>Chapitre \ref{chap:lineaire} : Les fonctions linéaires</li>
</ul>
</div>
</div>


<div id="outline-container-org4fce723" class="outline-3">
<h3 id="org4fce723"><span class="section-number-3">1.2.</span> Définition</h3>
<div class="outline-text-3" id="text-1-2">
<p>
Soit les espaces vectoriels \(\Omega, F\) sur \(\corps\).
</p>

<p>
L'idée à la base de la notion de différentielles est de linéariser localement une fonction \(f : \Omega \mapsto F\) autour d'un point \(a \in \Omega\). Pour tout \(h \in \Omega\) suffisamment petit, on veut donc avoir :
</p>

<p>
\[f(a + h) - f(a) \approx \differentielle{f}{a}(h)\]
</p>

<p>
où \(\differentielle{f}{a}\) est une application linéaire de \(\Omega\) vers \(F\). On suppose que la norme existe de \(\differentielle{f}{a}\) existe, de sorte que nous puissions écrire :
</p>

<p>
\[\norme{ \differentielle{f}{a}(h) } \le \norme{ \differentielle{f}{a} } \cdot \norme{h}\]
</p>

<p>
On voit que la norme de la différentielle tend plus vite que \(h\) vers \(0\).
On demande que la norme de l'erreur donnée par :
</p>

<p>
\[E(h) = f(a + h) - f(a) - \differentielle{f}{a}(h)\]
</p>

<p>
devienne négligeable par rapport à :
</p>

<p>
\[\norme{ \differentielle{f}{a} } \cdot \norme{h}\]
</p>

<p>
lorsque \(h\) tend vers \(0\). Comme la norme de la différentielle ne varie pas, il nous suffit d'imposer que :
</p>

<p>
\[\lim_{ \substack{ h \to 0 \\ h \ne 0 } } \frac{ \norme{E(h)} }{ \norme{h} } = 0\]
</p>

<p>
Si ces conditions sont vérifiées, on dit que \(f\) est différentiable en \(a\) et que \(\differentielle{f}{a}\) est la différentielle de \(f\) en \(a\). On a alors :
</p>

<p>
\[f(a + h) - f(a) = \differentielle{f}{a}(h) + E(h)\]
</p>
</div>


<div id="outline-container-org9d28a82" class="outline-4">
<h4 id="org9d28a82"><span class="section-number-4">1.2.1.</span> Notation</h4>
<div class="outline-text-4" id="text-1-2-1">
<p>
Tout au long de ce chapitre, nous notons :
</p>

<div class="org-center">
<p>
\(
\lim_{h \to 0} = \lim_{ \substack{ h \to 0 \\ h \ne 0 } } \\
\lim_{b \to a} = \lim_{ \substack{ b \to a \\ b \ne a } }
\)
</p>
</div>
</div>
</div>
</div>


<div id="outline-container-org1499ee3" class="outline-3">
<h3 id="org1499ee3"><span class="section-number-3">1.3.</span> Continuité</h3>
<div class="outline-text-3" id="text-1-3">
<p>
Si \(a\) est différentiable en \(a\), elle est forcément continue en \(a\). En effet, la différentielle est bien continue :
</p>

<p>
\[\norme{ \differentielle{f}{a}(0) } \le \norme{ \differentielle{f}{a} } \cdot \norme{0} = 0\]
</p>

<p>
Par définition, pour tout \(\epsilon \strictsuperieur 0\), on peut trouver \(\delta \strictsuperieur 0\) tel que :
</p>

<p>
\[\frac{ \norme{E(h)} }{ \norme{h} } \le \epsilon\]
</p>

<p>
c'est-à-dire :
</p>

<p>
\[\norme{E(h)} \le \epsilon \cdot \norme{h}\]
</p>

<p>
On en déduit que l'erreur est continue en \(h = 0\). On a donc :
</p>

<p>
\[\lim_{h \to 0} \norme{E(h)} = 0\]
</p>

<p>
L'expression de \(f(a + h)\) peut donc s'écrire :
</p>

<p>
\[\lim_{h \to 0} f(a + h) = f(a) + \lim_{h \to 0} \left[ \differentielle{f}{a}(h) + E(h) \right] = f(a)\]
</p>
</div>
</div>


<div id="outline-container-org230e526" class="outline-3">
<h3 id="org230e526"><span class="section-number-3">1.4.</span> Dérivées partielles</h3>
<div class="outline-text-3" id="text-1-4">
<p>
Nous allons à présent voir comment obtenir les composantes de la différentielle dans le cas d'espaces de dimensions finies. Nous disposons donc d'une base \((\varpi_1,...,\varpi_n)\) de \(\Omega\) et d'une base \((\phi_1,...,\phi_m)\) de \(F\). Nous introduisons les composantes \(f_1,...,f_m : \Omega \mapsto F\) de \(f\) telles que :
</p>

<p>
\[f(x) = \sum_{i = 1}^m f_i(x) \cdot \phi_i\]
</p>

<p>
pour tout \(x \in \Omega\). Nous procédons de même pour l'erreur :
</p>

<p>
\[E(h) = \sum_{i = 1}^m E_i(h) \cdot \phi_i\]
</p>

<p>
pour tout \(h \in \Omega\). Nous allons également utiliser les coordonnées \(h_1,...,h_n \in \corps\) de \(h\) :
</p>

<p>
\[h = \sum_{i = 1}^n h_i \cdot \varpi_i\]
</p>

<p>
Par linéarité :
</p>

<p>
\[f(a + h) - f(a) = \sum_{j = 1}^n \differentielle{f}{a}(\varpi_j) \cdot h_j + E(h)\]
</p>

<p>
Mais on peut trouver des \(\Delta_{ij} \in \corps\) tels que :
</p>

<p>
\[\differentielle{f}{a}(\varpi_j) = \sum_{i = 1}^m \Delta_{ij} \cdot \phi_i\]
</p>

<p>
Injectons les expressions des composantes dans \(F\). On obtient :
</p>

<p>
\[\sum_i \phi_i \cdot \left[ f_i(a + h) - f_i(a) - E_i(h) - \sum_{j = 1}^n \Delta_{ij} \cdot h_j \right] = 0\]
</p>

<p>
Par indépendance linéaire des \(\phi_i\), on a alors :
</p>

<p>
\[f_i(a + h) - f_i(a) = \sum_{j = 1}^n \Delta_{ij} \cdot h_j + E_i(h)\]
</p>

<p>
pour tout \(i \in \setZ(1,m)\). On nomme \(\Delta_{ij}\) la dérivée partielle de \(f_i\) par rapport à \(\varpi_j\). On la note :
</p>

<p>
\[\partial_j f_i(a) = \deriveepartielle{f_i}{x_j}(a) = \Delta_{ij}\]
</p>
</div>


<div id="outline-container-org13aa2f3" class="outline-4">
<h4 id="org13aa2f3"><span class="section-number-4">1.4.1.</span> Attention</h4>
<div class="outline-text-4" id="text-1-4-1">
<p>
Ne pas confondre la frontière \(\frontiere A\) d'un ensemble \(A\) avec la dérivée \(\partial f\) d'une fonction \(f\).
</p>
</div>
</div>
</div>


<div id="outline-container-orgd67bd2c" class="outline-3">
<h3 id="orgd67bd2c"><span class="section-number-3">1.5.</span> Dérivées et limites</h3>
<div class="outline-text-3" id="text-1-5">
<p>
Soit \(\lambda \in \corps\). Si l'on choisit \(h = \lambda \ \varpi_j\), on a \(h_k = \lambda \ \indicatrice_{jk}\) et :
</p>

\begin{align}
f_i(a + \lambda \ \varpi_j) - f_i(a) &= \sum_{k = 1}^n \partial_k f_i(a) \cdot \lambda \cdot \indicatrice_{jk} +  E_i(h) \\
&= \lambda \cdot \partial_j f_i(a)  +  E_i(h)
\end{align}

<p>
En divisant l'équation ci-dessus par \(\lambda\) puis en faisant tendre \(\lambda\) vers 0, on obtient :
</p>

<p>
\[\partial_j f_i(a) = \lim_{\lambda \to 0} \left[ \frac{f_i(a + \lambda \ \varpi_j) - f_i(a)}{\lambda} - \frac{E_i(h)}{\lambda} \right]\]
</p>

<p>
Comme l'erreur doit converger plus vite vers zéro que la norme \(\norme{h} = \lambda\), la limite du second terme du membre de droite s'annule et on a :
</p>

<p>
\[\partial_j f_i(a) = \lim_{\lambda \to 0} \frac{f_i(a + \lambda \ \varpi_j) - f_i(a)}{\lambda}\]
</p>

<p>
Ce qui montre que la dérivée partielle \(\partial_j f_i\) est la variation de \(f_i\) obtenue lorsqu'on fait varier la \(j^{ème}\) variable (celle correspondant à \(\varpi_j\)).
</p>
</div>
</div>


<div id="outline-container-org8af3247" class="outline-3">
<h3 id="org8af3247"><span class="section-number-3">1.6.</span> Représentation matricielle</h3>
<div class="outline-text-3" id="text-1-6">
<p>
On associe à la différentielle \(\differentielle{f}{a}\) la matrice Jacobienne \(\partial f(a)\) de \(f\) en \(a\) définie par :
</p>

<p>
\[\partial f(a) = \Big( \partial_j f_i(a) \Big)_{i,j}\]
</p>

<p>
On peut alors écrire la linéarisation de \(f\) sous forme de produit matriciel :
</p>

<p>
\[f(a + h) - f(a) = \partial f(a) \cdot h + E(h)\]
</p>

<p>
où \(f,h,E\) sont les vecteurs colonnes associés aux grandeurs du même nom.
</p>
</div>


<div id="outline-container-org859638b" class="outline-4">
<h4 id="org859638b"><span class="section-number-4">1.6.1.</span> Vecteurs associés</h4>
<div class="outline-text-4" id="text-1-6-1">
<p>
On dispose de \(n\) vecteurs représentant chacun la dérivée des composantes de \(f\) par rapport à la \(j^{ème}\) variable :
</p>

<p>
\[\partial_j f(a) = \Big( \partial_j f_i(a) \Big)_i\]
</p>

<p>
et de \(m\) vecteurs représentant chacun les dérivées de la \(i^{ème}\) composante de \(f\) :
</p>

<p>
\[\partial f_i(a) = \Big( \partial_j f_i(a) \Big)_j\]
</p>
</div>
</div>


<div id="outline-container-org1718d40" class="outline-4">
<h4 id="org1718d40"><span class="section-number-4">1.6.2.</span> Notation</h4>
<div class="outline-text-4" id="text-1-6-2">
<p>
Dans le cas où la variable porte un nom par défaut, comme par exemple :
</p>

<p>
\[f : x \mapsto f(x)\]
</p>

<p>
on note aussi :
</p>

<p>
\[\deriveepartielle{f}{x}(a) = \partial f(a)\]
</p>
</div>


<div id="outline-container-org3825685" class="outline-5">
<h5 id="org3825685"><span class="section-number-5">1.6.2.1.</span> Plusieurs sous-variables</h5>
<div class="outline-text-5" id="text-1-6-2-1">
<p>
Lorsque plusieurs sous variables portent un nom par défaut, comme par exemple :
</p>

<p>
\[f : (x,y) \mapsto f(x,y)\]
</p>

<p>
on note aussi :
</p>

<p>
\[\deriveepartielle{f}{x}(a) = \partial_x f(a)\]
</p>

<p>
pour la Jacobienne par rapport aux variables \(x = (x_1,...,x_s)\) et :
</p>

<p>
\[\deriveepartielle{f}{y}(a) = \partial_y f(a)\]
</p>

<p>
pour la Jacobienne par rapport aux variables \(y = (y_1,...,y_t)\)
</p>
</div>
</div>


<div id="outline-container-orgd88176e" class="outline-5">
<h5 id="orgd88176e"><span class="section-number-5">1.6.2.2.</span> Symbolique</h5>
<div class="outline-text-5" id="text-1-6-2-2">
<p>
On a aussi les notations symboliques :
</p>

<div class="org-center">
<p>
\(
df = \deriveepartielle{f}{x^T} \ dx \\
df_i = \sum_j \deriveepartielle{f_i}{x_j} \ dx_j
\)
</p>
</div>

<p>
où \(df\) représente une petite variation de \(f\) suite à une petite variation \(dx\) de \(x\).
</p>

<p>
On utilise parfois la transposée de la Jacobienne :
</p>

<p>
\[\Big[ \partial f(x) \Big]^T = \deriveepartielle{f^T}{x} = \left( \deriveepartielle{f}{x^T} \right)^T\]
</p>
</div>
</div>
</div>


<div id="outline-container-org68dd326" class="outline-4">
<h4 id="org68dd326"><span class="section-number-4">1.6.3.</span> Appellation</h4>
<div class="outline-text-4" id="text-1-6-3">
<p>
Pour des fonctions du type \(f : \setR^n \mapsto \setR\), la Jacobienne se réduit à un vecteur matriciel. On dit alors que \(\partial f\) est le gradient de \(f\).
</p>
</div>
</div>
</div>


<div id="outline-container-org290256d" class="outline-3">
<h3 id="org290256d"><span class="section-number-3">1.7.</span> Dérivées ordinaires</h3>
<div class="outline-text-3" id="text-1-7">
<p>
Dans le cas où \(m = n = 1\), il n'y a qu'une dérivée partielle, \(\partial_1 f_1\), que l'on appelle alors dérivée ordinaire. On a les équivalences :
</p>

<p>
\[\lambda \ \phi_1 \quad \Leftrightarrow \quad \lambda \quad \Leftrightarrow \quad \lambda \ \epsilon_1\]
</p>

<p>
On peut considérer \(\Omega\) et \(F\) comme équivalents à \(\corps\) et se restreindre à des fonctions \(f : \corps \mapsto \corps\) sans perte de généralité. La dérivée ordinaire est alors simplement :
</p>

<p>
\[\partial f(a) = \lim_{\lambda \to 0} \frac{f(a + \lambda) - f(a)}{\lambda}\]
</p>
</div>


<div id="outline-container-org5b2091d" class="outline-4">
<h4 id="org5b2091d"><span class="section-number-4">1.7.1.</span> Notation</h4>
<div class="outline-text-4" id="text-1-7-1">
<p>
On note aussi :
</p>

<p>
\[\OD{f}{x}(a) = \lim_{\lambda \to 0} \frac{f(a + \lambda) - f(a)}{\lambda}\]
</p>

<p>
On a alors :
</p>

<p>
\[f(a + h) = f(a) + \OD{f}{x}(a) \cdot h + E(h)\]
</p>

<p>
et :
</p>

<p>
\[\lim_{\lambda \to 0} \frac{E(\lambda)}{\lambda} = \lim_{\lambda \to 0} \left[ \frac{f(a + \lambda) - f(a)}{\lambda} - \OD{f}{x}(a) \right] = 0\]
</p>
</div>
</div>


<div id="outline-container-orgd5ace49" class="outline-4">
<h4 id="orgd5ace49"><span class="section-number-4">1.7.2.</span> Définition équivalente</h4>
<div class="outline-text-4" id="text-1-7-2">
<p>
Si on pose \(b = a + \lambda\), on voit que \(\lambda = b - a\) et que la convergence \(h \to 0\) est équivalente à \(b \to a\). On a donc :
</p>

<p>
\[\OD{f}{x}(a) = \lim_{b \to a} \frac{f(b) - f(a)}{b - a}\]
</p>
</div>
</div>
</div>


<div id="outline-container-org95fc80e" class="outline-3">
<h3 id="org95fc80e"><span class="section-number-3">1.8.</span> L'application dérivée</h3>
<div class="outline-text-3" id="text-1-8">
<p>
Si \(f\) est différentiable en tout vecteur \(a\) de \(A \subseteq \Omega\), on dit que \(f\) est différentiable sur \(A\). On peut alors définir une application dérivée \(\partial f : A \mapsto \matrice(\corps,m,n)\) définie par :
</p>

<p>
\[\partial f : a \mapsto \partial f(a)\]
</p>

<p>
pour tout \(a \in A\). Si cette nouvelle application \(\partial f\) est également continue, on dit que \(f\) est continûment différentiable. On note \(\continue^1(A,F)\) l'ensemble des fonctions continûment différentiables de \(A\) vers \(F\).
</p>
</div>
</div>


<div id="outline-container-orged91681" class="outline-3">
<h3 id="orged91681"><span class="section-number-3">1.9.</span> Hessienne</h3>
<div class="outline-text-3" id="text-1-9">
<p>
Soit \(f : \Omega \mapsto \corps\) avec \(\Omega\) de dimension finie \(n\). Supposons que \(f\) est différentiable sur \(A \subseteq \Omega\). La dimension de \(\corps\) sur \(\corps\) étant \(1\), la Jacobienne \(\partial f(a)\) se réduit à un vecteur matriciel de composantes \(\partial_i f\). Si la dérivée \(\partial f : A \mapsto \corps^n\) est elle-même différentiable, on nomme l'application définie par :
</p>

<p>
\[\partial^2 f = \partial \left( \partial f \right)\]
</p>

<p>
la dérivée seconde de \(f\). Il s'agit d'une fonction qui transforme un élément de \(A\) en un « vecteur matriciel de vecteurs matriciels » appartenant à \(\matrice(\corps^n,n,1)\). On peut assimiler cet objet à une matrice équivalente de taille \((n,n)\) dont les composantes sont des éléments de \(\corps\). En définitive, nous avons \(\partial^2 f(a) \in \matrice(\corps,n,n)\) pour tout \(a \in A\). Cette matrice est appellée hessienne de \(f\) en \(a\). Ses composantes sont données par :
</p>

<p>
\[\partial_{ij}^2 f(a) = \partial_i \left( \partial_j f \right)(a)\]
</p>
</div>


<div id="outline-container-orgb840359" class="outline-4">
<h4 id="orgb840359"><span class="section-number-4">1.9.1.</span> Notation</h4>
<div class="outline-text-4" id="text-1-9-1">
<p>
On note aussi :
</p>

<p>
\[\dfdxdy{f}{x_i}{x_j}(a) = \partial_{ij}^2 f(a)\]
</p>

<p>
Lorsque \(i = j\), on note :
</p>

<p>
\[\dfdxdx{f}{x_i} = \dfdxdy{f}{x_i}{x_i} = \partial_{ii}^2 f(a)\]
</p>

<p>
En termes matriciels, cela donne :
</p>

<p>
\[\dblederiveepartielle{f}{x}(a) = \partial^2 f(a)\]
</p>
</div>
</div>


<div id="outline-container-org0543dd9" class="outline-4">
<h4 id="org0543dd9"><span class="section-number-4">1.9.2.</span> Dérivée ordinaire</h4>
<div class="outline-text-4" id="text-1-9-2">
<p>
Dans le cas où la dimension de \(\Omega\) est un \(1\), on peut l'assimiler à \(\corps\), on a alors une fonction \(f : \corps \mapsto \corps\) possédant une seule dérivée seconde, que l'on note :
</p>

<p>
\[\OOD{f}{t}(a) = \partial^2 f(a)\]
</p>
</div>
</div>


<div id="outline-container-org87e1e9e" class="outline-4">
<h4 id="org87e1e9e"><span class="section-number-4">1.9.3.</span> Continuité</h4>
<div class="outline-text-4" id="text-1-9-3">
<p>
On note \(\continue^2(A,\corps)\) l'ensemble des fonctions dont la dérivée seconde est continue sur \(A\).
</p>
</div>
</div>
</div>


<div id="outline-container-orgc7bab30" class="outline-3">
<h3 id="orgc7bab30"><span class="section-number-3">1.10.</span> Dérivée d'ordre \(k\)</h3>
<div class="outline-text-3" id="text-1-10">
<p>
Soit \(f : \corps \mapsto \corps\) et \(k \in \setN\). Pour autant que la fonction \(f\) soit suffisamment dérivable, on définit par récurrence la fonction \(\partial^k f : \corps \mapsto \corps\) :
</p>

\begin{align}
\partial^0 f &= f \\
\partial^k f &= \partial \big( \partial^{k-1} f \big)
\end{align}

<p>
La dérivée d'ordre \(k\)  de \(f\) est donc la fonction obtenue lorsqu'on applique un nombre \(k\) de fois l'opérateur de dérivation \(\partial\) à la fonction \(f\) :
</p>

<p>
\[\partial^k f = (\partial \circ ... \circ \partial)(f)\]
</p>
</div>


<div id="outline-container-orgec4370a" class="outline-4">
<h4 id="orgec4370a"><span class="section-number-4">1.10.1.</span> Ensembles</h4>
<div class="outline-text-4" id="text-1-10-1">
<p>
On note \(\continue^k(A,\corps)\) l'ensemble des fonctions \(f : A \mapsto \corps\) dont la dérivée d'ordre \(k\) :
</p>

<p>
\[\partial^k f : A \mapsto \corps\]
</p>

<p>
existe et est continue sur \(A\).
</p>
</div>


<div id="outline-container-orge2a3ed8" class="outline-5">
<h5 id="orge2a3ed8"><span class="section-number-5">1.10.1.1.</span> Infini</h5>
<div class="outline-text-5" id="text-1-10-1-1">
<p>
Si la dérivée \(\partial^k f\) existe pour tout \(k \in \setN\), on dit que \(f\) est indéfiniment dérivable. On note \(\continue^\infty(A,\corps)\) l'ensemble des fonctions indéfiniment dérivables.
</p>
</div>
</div>


<div id="outline-container-org1cdb8d0" class="outline-5">
<h5 id="org1cdb8d0"><span class="section-number-5">1.10.1.2.</span> Ordre \(0\)</h5>
<div class="outline-text-5" id="text-1-10-1-2">
<p>
On voit que \(\continue^0 = \continue\).
</p>
</div>
</div>
</div>


<div id="outline-container-org014934e" class="outline-4">
<h4 id="org014934e"><span class="section-number-4">1.10.2.</span> Homéomorphisme</h4>
<div class="outline-text-4" id="text-1-10-2">
<p>
On note \(\homeomorphisme^k(A,\corps)\) l'ensemble des bijections \(f\) de \(\continue^k\) telles que la fonction \(f^{-1}\) soit aussi dans \(\continue^k\).
</p>
</div>
</div>


<div id="outline-container-org36694d2" class="outline-4">
<h4 id="org36694d2"><span class="section-number-4">1.10.3.</span> Notations</h4>
<div class="outline-text-4" id="text-1-10-3">
<p>
Pour \(\alpha = (\alpha_1, \alpha_2, ..., \alpha_n)\), on écrit également :
</p>

<div class="org-center">
<p>
\(
\partial^\alpha f = \partial^{\alpha_1 ... \alpha_n} f =
\partial_1^{\alpha_1} ... \partial_n^{\alpha_n} f
\)
</p>
</div>
</div>
</div>
</div>


<div id="outline-container-org5a28573" class="outline-3">
<h3 id="org5a28573"><span class="section-number-3">1.11.</span> Fonctions à intégrale continue</h3>
<div class="outline-text-3" id="text-1-11">
<p>
Soit une fonction \(u : A \mapsto B\). Si la fonction \(v\) définie par :
</p>

<p>
\[v(x) = \int_a^x u(x) d\mu(x)\]
</p>

<p>
est continue, on dit que \(u\) est à intégrale continue. On note \(\continue_{\mu}^{-1}(A,B)\) l'ensemble des fontions à intégrale continue.
</p>
</div>
</div>


<div id="outline-container-org50d379b" class="outline-3">
<h3 id="org50d379b"><span class="section-number-3">1.12.</span> Différentiabilité uniforme</h3>
<div class="outline-text-3" id="text-1-12">
<p>
On dit qu'une fonction \(f\) est uniformément différentiable sur \(A\) si pour tout \(\epsilon \strictsuperieur 0\), on peut trouver un \(\delta \strictsuperieur 0\) tel que :
</p>

<p>
\[\abs{f(s) - f(t) - \partial f(t) \cdot (s - t)} \le \epsilon \cdot \abs{s - t}\]
</p>

<p>
quel que soit \(s,t \in A\) vérifiant \(\abs{s - t} \le \delta\).
</p>
</div>
</div>
</div>


<div id="outline-container-org71501d8" class="outline-2">
<h2 id="org71501d8"><span class="section-number-2">2.</span> Dérivées</h2>
<div class="outline-text-2" id="text-2">
<div id="text-table-of-contents" role="doc-toc">
<ul>
<li><a href="#orge1ea229">2.1. Dépendances</a></li>
<li><a href="#org16861e5">2.2. Fonction constante</a></li>
<li><a href="#org195736f">2.3. Identité</a></li>
<li><a href="#org3d9a611">2.4. Composition de fonctions</a></li>
<li><a href="#orgb822f90">2.5. Inverse fonctionnel</a></li>
<li><a href="#org31c8d83">2.6. Addition</a></li>
<li><a href="#orged9bd71">2.7. Produit scalaire</a></li>
<li><a href="#org8492a18">2.8. Inverse multiplicatif</a></li>
<li><a href="#org43934dc">2.9. Fraction</a></li>
<li><a href="#orgf8dbebc">2.10. Dérivée d'une limite</a></li>
</ul>
</div>

<p>
\label{chap:derivee}
</p>
</div>


<div id="outline-container-orge1ea229" class="outline-3">
<h3 id="orge1ea229"><span class="section-number-3">2.1.</span> Dépendances</h3>
<div class="outline-text-3" id="text-2-1">
<ul class="org-ul">
<li>Chapitre \ref{chap:differ} : Les différentielles</li>
</ul>
</div>
</div>


<div id="outline-container-org16861e5" class="outline-3">
<h3 id="org16861e5"><span class="section-number-3">2.2.</span> Fonction constante</h3>
<div class="outline-text-3" id="text-2-2">
<p>
Si \(f\) est constante, on peut trouver \(c \in F\) tel que :
</p>

<p>
\[f(x) = c\]
</p>

<p>
pour tout \(x \in \Omega\). On a alors :
</p>

<p>
\[\partial_j f(a) = \lim_{\lambda \to 0} \frac{c - c}{\lambda} = 0\]
</p>

<p>
Par conséquent :
</p>

<p>
\[\partial f(a) = 0\]
</p>
</div>
</div>


<div id="outline-container-org195736f" class="outline-3">
<h3 id="org195736f"><span class="section-number-3">2.3.</span> Identité</h3>
<div class="outline-text-3" id="text-2-3">
<p>
Considérons le cas particulier où \(m = n\) et où \(f = \identite\). Lorsque \(i = j\), nous avons :
</p>

<p>
\[\partial_i f_i(a) = \lim_{\lambda \to 0} \frac{x_i + \lambda - x_i}{\lambda} = \lim_{\lambda \to 0} \frac{\lambda}{\lambda} = 1\]
</p>

<p>
Lorsque \(i \ne j\), on a par contre :
</p>

<p>
\[\partial_j f_i(a) = \lim_{\lambda \to 0} \frac{x_i - x_i}{\lambda} = \lim_{\lambda \to 0} \frac{0}{\lambda} = 0\]
</p>

<p>
On en conclut que :
</p>

<p>
\[\partial f(a) = \partial \identite(a) = ( \indicatrice_{ij} )_{i,j} = I\]
</p>

<p>
La Jacobienne de la fonction identité est la matrice identité.
</p>
</div>
</div>


<div id="outline-container-org3d9a611" class="outline-3">
<h3 id="org3d9a611"><span class="section-number-3">2.4.</span> Composition de fonctions</h3>
<div class="outline-text-3" id="text-2-4">
<p>
Nous allons nous intéresser à présent au moyen d'obtenir la dérivée d'une composée de fonctions \(f : E \mapsto F\) et \(g : F \mapsto G\). Supposons que \(f\) soit différentiable en \(a\) et que \(g\) soit différentiable en \(b = f(a)\). On a alors :
</p>

<div class="org-center">
<p>
\(
df(a) = f(a + da) - f(a) = \partial f(a) \cdot da + E_f(da) \\
dg(b) = g(b + db) - g(b) = \partial g(b) \cdot db + E_g(db)
\)
</p>
</div>

<p>
Choisissons en particulier \(db = f(a + da) - f(a)\). On a alors :
</p>

\begin{align}
dg(b) &= g(f(a + da)) - g(f(a)) \\
&= (g \circ f)(a + da) - (g \circ f)(a) \\
&= d(g \circ f)(a)
\end{align}

<p>
Mais d'un autre coté :
</p>

\begin{align}
dg(b) &= \partial g(f(a)) \cdot df(a) + E_g(df(a)) \\
&= \partial g(f(a)) \cdot \left( \partial f(a) \cdot da + E_f(da) \right) + E_g(df(a)) \\
&= \partial g(f(a)) \cdot \partial f(a) \cdot da + E_f(da) \cdot da + E_g(df(a)) \\
\end{align}

<p>
Il est aisé de vérifier que :
</p>

<p>
\[\lim_{h \to 0} \frac{\partial g(f(a)) \cdot E_f(da)  + E_g(df(a))}{\norme{h}} = 0\]
</p>

<p>
puisque \(\partial g(f(a))\) ne dépend pas de \(da\). On a donc montré que \(g \circ f\) est différentiable en \(a\) et que :
</p>

<p>
\[\partial (g \circ f)(a) =  \partial g(f(a)) \cdot \partial f(a) = (\partial g \circ f)(a) \cdot \partial f(a)\]
</p>

<p>
La dérivée d'une composée de fonctions est donc tout simplement le produit des Jacobiennes.
</p>
</div>


<div id="outline-container-org60eea07" class="outline-4">
<h4 id="org60eea07"><span class="section-number-4">2.4.1.</span> Notation</h4>
<div class="outline-text-4" id="text-2-4-1">
<p>
Soit le schéma fonctionnel :
</p>

<p>
\[(x_1,...,x_n) \mapsto (y_1,...,y_m) \mapsto (z_1,...,z_p)\]
</p>

<p>
On note aussi :
</p>

<div class="org-center">
<p>
\(
\deriveepartielle{z_i}{x_j} = \sum_k \deriveepartielle{z_i}{y_k} \cdot \deriveepartielle{y_k}{x_j} \\ \\
\deriveepartielle{z}{x^T} = \deriveepartielle{z}{y^T} \cdot \deriveepartielle{y}{x^T}
\)
</p>
</div>

<p>
Pour \(z : t \mapsto (x_1(t),x_2(t),...,x_n(t))\), on a également :
</p>

<p>
\[\OD{z}{t} = \sum_k \deriveepartielle{z}{x_k} \cdot \OD{x_k}{t}\]
</p>

<p>
Si \(x,y,z \in \corps\), on a encore :
</p>

<p>
\[\OD{z}{x} = \OD{z}{y} \cdot \OD{y}{x}\]
</p>
</div>
</div>
</div>


<div id="outline-container-orgb822f90" class="outline-3">
<h3 id="orgb822f90"><span class="section-number-3">2.5.</span> Inverse fonctionnel</h3>
<div class="outline-text-3" id="text-2-5">
<p>
En considérant le cas particulier \(g = f^{-1}\), on a \(g \circ f = \identite\). Choisissons un vecteur \(a\) où \(f\) est différentiable et posons \(b = f(a)\). On a :
</p>

<p>
\[\partial f^{-1}(b) \cdot \partial f(a) = \partial \identite(a) = I\]
</p>

<p>
La jacobienne de l'inverse d'une fonction est donc l'inverse matriciel de sa jacobienne :
</p>

<p>
\[\partial f^{-1}(b) = \left[ \partial f(a) \right]^{-1}\]
</p>
</div>


<div id="outline-container-org8bbae10" class="outline-4">
<h4 id="org8bbae10"><span class="section-number-4">2.5.1.</span> Notation</h4>
<div class="outline-text-4" id="text-2-5-1">
<p>
On note aussi :
</p>

<p>
\[\deriveepartielle{y}{x}  = \left(\deriveepartielle{x}{y}\right)^{-1}\]
</p>

<p>
ou, pour des fonctions \(y : \corps \mapsto \corps\) :
</p>

<p>
\[\OD{y}{x}  = \left(\OD{x}{y}\right)^{-1}\]
</p>
</div>
</div>
</div>


<div id="outline-container-org31c8d83" class="outline-3">
<h3 id="org31c8d83"><span class="section-number-3">2.6.</span> Addition</h3>
<div class="outline-text-3" id="text-2-6">
<p>
Si \(f,g\) sont différentiables en \(a\), on a :
</p>

<div class="org-center">
<p>
\(
f(a + h) - f(a) = \partial f(a) \cdot h + E_f(h) \\
g(a + h) - g(a) = \partial g(a) \cdot h + E_g(h)
\)
</p>
</div>

<p>
En additionnant les équations ci-dessus, on obtient :
</p>

<div class="org-center">
<p>
\(
\left[ f(a + h) + g(a + h) \right] - \left[ f(a) + g(a) \right]
= [ \partial f(a) + \partial g(a) ] \cdot h \\
\qquad \qquad + E_f(h) + E_g(h)
\)
</p>
</div>

<p>
Il est clair que :
</p>

<p>
\[\lim_{h \to 0} \frac{\norme{E_f(h) + E_g(h)}}{\norme{h}} = 0\]
</p>

<p>
La fonction \(f + g\) est donc différentiable en \(a\) et :
</p>

<p>
\[\partial (f + g)(a) = \partial f(a) + \partial g(a)\]
</p>

<p>
On peut montrer, cette fois en soustrayant les deux équations que :
</p>

<p>
\[\partial (f - g)(a) = \partial f(a) - \partial g(a)\]
</p>
</div>
</div>


<div id="outline-container-orged9bd71" class="outline-3">
<h3 id="orged9bd71"><span class="section-number-3">2.7.</span> Produit scalaire</h3>
<div class="outline-text-3" id="text-2-7">
<p>
Considérons deux fonctions \(f,g\) différentiables en \(a\) :
</p>

<div class="org-center">
<p>
\(
f(a + h) = f(a) + \partial f(a) \cdot h + E_f(h) \\
g(a + h) = g(a) + \partial g(a) \cdot h + E_g(h) \\
\)
</p>
</div>

<p>
Leur produit scalaire s'écrit :
</p>

<div class="org-center">
<p>
\(
\scalaire{f(a + h)}{g(a + h)} = \scalaire{f(a)}{g(a)} + \scalaire{f(a)}{\partial g(a) \cdot h} + \scalaire{\partial f(a) \cdot h}{g(a)} \\
\qquad \qquad \qquad + E_{f \cdot g}(h)
\)
</p>
</div>

<p>
où :
</p>

<div class="org-center">
<p>
\(
E_{f \cdot g}(h) = \scalaire{E_f(h)}{E_g(h)} + \scalaire{E_f(h)}{g(a)} + \scalaire{E_f(h)}{\partial g(a)} + \\
\qquad \qquad \scalaire{f(a)}{E_g(h)} + \scalaire{\partial f(a)}{E_g(h)}
\)
</p>
</div>

<p>
On a donc :
</p>

<p>
\[\lim_{h \to 0} \frac{\norme{E_{f \cdot g}(h)}}{\norme{h}} = 0\]
</p>

<p>
ce qui nous montre que la différentielle du produit scalaire s'écrit :
</p>

<p>
\[\differentielle{ \scalaire{f}{g} }{a}(h) = \scalaire{f(a)}{\partial g(a) \cdot h} + \scalaire{\partial f(a) \cdot h}{g(a)}\]
</p>

<p>
En terme de composantes, on a :
</p>

<p>
\[\differentielle{ \scalaire{f}{g} }{a}(h) = \sum_{j = 1}^n \varpi_j \cdot h_j \cdot \sum_{i = 1}^m \left[ f_i(a) \cdot \partial_j g_i(a) + \partial_j f_i(a) \cdot g_i(a) \right]\]
</p>

<p>
La représentation matricielle s'écrit donc :
</p>

<p>
\[\partial \scalaire{f}{g}(a) = \left[ \partial g(a) \right]^T \cdot f(a) + \left[ \partial f(a) \right]^T \cdot g(a)\]
</p>
</div>


<div id="outline-container-org67e348f" class="outline-4">
<h4 id="org67e348f"><span class="section-number-4">2.7.1.</span> Dérivée ordinaire</h4>
<div class="outline-text-4" id="text-2-7-1">
<p>
Dans le cas où \(m = n = 1\), cette expression se simplifie en :
</p>

<p>
\[\OD{}{x}(f \cdot g)(a) = f(a) \cdot \OD{g}{x}(a) +  \OD{f}{x}(a) \cdot g(a)\]
</p>
</div>
</div>


<div id="outline-container-orgf9e2f5f" class="outline-4">
<h4 id="orgf9e2f5f"><span class="section-number-4">2.7.2.</span> Constante</h4>
<div class="outline-text-4" id="text-2-7-2">
<p>
Si une des deux fonctions est constante, soit \(g(x) = c\) pour tout \(x\), on a :
</p>

<p>
\[\partial g(x) = 0\]
</p>

<p>
et :
</p>

<p>
\[\OD{}{x}(f \cdot c)(a) = f(a) \cdot 0 +  \OD{f}{x}(a) \cdot c = c \cdot \OD{f}{x}(a)\]
</p>
</div>
</div>


<div id="outline-container-org32252af" class="outline-4">
<h4 id="org32252af"><span class="section-number-4">2.7.3.</span> Notation</h4>
<div class="outline-text-4" id="text-2-7-3">
<p>
On note aussi :
</p>

<p>
\[d(f \cdot g) = df \cdot g + f \cdot dg\]
</p>
</div>
</div>
</div>


<div id="outline-container-org8492a18" class="outline-3">
<h3 id="org8492a18"><span class="section-number-3">2.8.</span> Inverse multiplicatif</h3>
<div class="outline-text-3" id="text-2-8">
<p>
Soit les fonctions \(f,g : \corps \mapsto \corps\) reliées par l'équation :
</p>

<p>
\[f \cdot g = 1\]
</p>

<p>
En dérivant, on obtient :
</p>

<p>
\[\OD{f}{x} \cdot g + f \cdot \OD{g}{x} = \OD{1}{x} = 0\]
</p>

<p>
Donc, si \(g \ne 0\), on a :
</p>

<p>
\[\OD{f}{x} = - \frac{f}{g} \cdot \OD{g}{x}\]
</p>

<p>
Mais comme \(f(x) = 1/g(x)\), cela nous donne :
</p>

<p>
\[\OD{}{x}\left(\unsur{g}\right)(x) = - \unsur{g(x)^2} \cdot \OD{g}{x}(x)\]
</p>
</div>
</div>


<div id="outline-container-org43934dc" class="outline-3">
<h3 id="org43934dc"><span class="section-number-3">2.9.</span> Fraction</h3>
<div class="outline-text-3" id="text-2-9">
<p>
En appliquant les résultats précédents, on obtient :
</p>

<p>
\[\OD{}{x}\left( \frac{f}{g} \right)(x) = \OD{}{x}\left( f \cdot \unsur{g} \right)(x) = \OD{f}{x}(x) \cdot g(x) - \frac{f(x)}{g(x)^2} \cdot \OD{g}{x}(x)\]
</p>

<p>
et finalement :
</p>

<p>
\[\OD{}{x}\left( \frac{f}{g} \right)(x) = \frac{\OD{f}{x}(x) \cdot g(x) - f(x) \cdot \OD{g}{x}}{g(x)^2}\]
</p>
</div>
</div>


<div id="outline-container-orgf8dbebc" class="outline-3">
<h3 id="orgf8dbebc"><span class="section-number-3">2.10.</span> Dérivée d'une limite</h3>
<div class="outline-text-3" id="text-2-10">
<p>
Soit la suite de fonctions :
</p>

<p>
\[F = \{ f_n \in \setR^\setR : n \in \setN \}\]
</p>

<p>
convergeant en tout point \(x \in \setR\) vers une fonction \(f : \setR \mapsto \setR\) :
</p>

<p>
\[\lim_{n \to \infty} f_n(x) = f(x)\]
</p>

<p>
Si la fonction \(f\) est différentiable en \(a \in \setR\), on a :
</p>

\begin{align}
\partial f(a) &= \lim_{h \to 0} \unsur{h} \big[f(a + h) - f(a)\big] \\
&= \lim_{h \to 0} \unsur{h} \crochets{\lim_{n \to \infty} f_n(a + h) - \lim_{n \to \infty} f_n(a)} \\
&= \lim_{h \to 0} \lim_{n \to \infty} \frac{f_n(a + h) - f_n(a)}{h}
\end{align}
</div>
</div>
</div>


<div id="outline-container-orgd7d675b" class="outline-2">
<h2 id="orgd7d675b"><span class="section-number-2">3.</span> Différentielles et polynômes</h2>
<div class="outline-text-2" id="text-3">
<div id="text-table-of-contents" role="doc-toc">
<ul>
<li><a href="#orga97ad2c">3.1. Dépendances</a></li>
<li><a href="#orgc946527">3.2. Polynômes</a></li>
</ul>
</div>

<p>
\label{chap:diffpoly}
</p>
</div>


<div id="outline-container-orga97ad2c" class="outline-3">
<h3 id="orga97ad2c"><span class="section-number-3">3.1.</span> Dépendances</h3>
<div class="outline-text-3" id="text-3-1">
<ul class="org-ul">
<li>Chapitre \ref{chap:differ} : Les différentielles</li>
</ul>
</div>
</div>


<div id="outline-container-orgc946527" class="outline-3">
<h3 id="orgc946527"><span class="section-number-3">3.2.</span> Polynômes</h3>
<div class="outline-text-3" id="text-3-2">
<p>
Soit \(n \in \setN\). Nous allons analyser la différentiabilité du monôme \(\mu : \setR \mapsto \setR\) défini par :
</p>

<p>
\[\mu : t \mapsto t^n\]
</p>

<p>
pour tout \(t \in \setR\). La formule de factorisation nous donne :
</p>

<p>
\[s^n - t^n = (s - t) \sum_{i = 0}^{n - 1} s^i \cdot t^{n - 1 - i}\]
</p>

<p>
On a donc :
</p>

<p>
\[\frac{s^n - t^n}{s - t} = \sum_{i = 0}^{n - 1} s^i \cdot t^{n - 1 - i}\]
</p>

<p>
Passant à la limite \(s \to t\), on obtient :
</p>

<p>
\[\lim_{ s \to t} \frac{s^n - t^n}{s - t} = \sum_{i = 0}^{n - 1} t^i \cdot t^{n - 1 - i} = \sum_{i = 0}^{n - 1} t^{n - 1} = n \cdot t^{n - 1}\]
</p>

<p>
On en conclut que la dérivée existe sur \(\setR\) et que :
</p>

<p>
\[\OD{}{t} (t^n) = \lim_{ s \to t} \frac{s^n - t^n}{s - t} = n \cdot t^{n - 1}\]
</p>

<p>
La dérivée d'une combinaison linéaire étant identique à la combinaison linéaire des dérivées (voir dérivée d'une somme et la multiplication par une constante), on en conclut que tous les polynômes sont dérivables sur \(\setR\).
</p>
</div>


<div id="outline-container-org984a1c8" class="outline-4">
<h4 id="org984a1c8"><span class="section-number-4">3.2.1.</span> Uniformité</h4>
<div class="outline-text-4" id="text-3-2-1">
<p>
Choisissons \(\alpha,\beta \in \setR\) avec \(\alpha \le \beta\) et analysons la différentiabilité sur l'intervalle \([\alpha,\beta]\). Posons :
</p>

<p>
\[e(s,t) = \frac{s^n - t^n}{s - t} - \OD{}{t}(t^n)\]
</p>

<p>
Si \(n = 1\), on a :
</p>

<p>
\[e(s,t) = \frac{s - t}{s - t} - 1 = 0\]
</p>

<p>
Le monôme de degré \(1\) est donc uniformément différentiable. Considérons à présent le cas où \(n \ge 2\). Le passage à la limite nous montre que :
</p>

<p>
\[\OD{}{t} (t^n) = \sum_{i = 0}^{n - 1} t^i \cdot t^{n - 1 - i}\]
</p>

<p>
En utilisant les propriétés des sommes, on obtient :
</p>

\begin{align}
e(s,t) &= \sum_{i = 0}^{n - 1} s^i \ t^{n - 1 - i} - \sum_{i = 0}^{n - 1} t^i \ t^{n - 1 - i} \\
&= \sum_{i = 0}^{n - 1} (s^i - t^i) \ t^{n - 1 - i}
\end{align}

<p>
En factorisant tous les \(s^i - t^i\), on a alors :
</p>

<p>
\[e(s,t) = \sum_{i = 0}^{n - 1} t^{n - 1 - i} \ (s - t) \ \sum_{k = 0}^{i - 1} s^k \ t^{i - 1 - k}\]
</p>

<p>
et comme \(s - t\) ne dépend pas de \(i\) :
</p>

<p>
\[e(s,t) = (s - t) \sum_{i = 0}^{n - 1} t^{n - 1 - i} \ \sum_{k = 0}^{i - 1} s^k \ t^{i - 1 - k}\]
</p>

<p>
Si on pose \(M = \max \{ \abs{\alpha} , \abs{\beta} \}\), on a clairement \(\abs{s}, \abs{t} \le M\). On peut alors trouver la borne supérieure :
</p>

\begin{align}
\abs{e(s,t)} &\le \abs{s - t} \sum_{i = 0}^{n - 1} M^{n - 1 - i} \ \sum_{k = 0}^{i - 1} M^{i - 1} \\
&\le \abs{s - t} \sum_{i = 0}^{n - 1} M^{n - 1 - i} \ i \ M^{i - 1} \\
&\le \abs{s - t} \ M^{n - 2} \ \sum_{i = 0}^{n - 1} i \\
&\le \unsur{2} \ \abs{s - t} \ M^{n - 2} \ (n - 1) \ n
\end{align}

<p>
Fixons à présent \(\epsilon \strictsuperieur 0\). Il suffit de prendre :
</p>

<p>
\[\abs{s - t} \le \delta \le \frac{ 2 \epsilon}{ M^{n - 2} \cdot (n - 1) \cdot n }\]
</p>

<p>
pour avoir :
</p>

<p>
\[\abs{e(s,t)} \le \frac{ M^{n - 2} \cdot (n - 1) \cdot n \cdot \delta }{2} \le \epsilon\]
</p>

<p>
Comme on a :
</p>

<p>
\[\mu(s) - \mu(t) - \partial \mu(t) = s^n - t^n - n \cdot t^{n - 1} = e(s,t) \cdot (s - t)\]
</p>

<p>
on dispose de la borne supérieure :
</p>

<p>
\[\abs{\mu(s) - \mu(t) - \partial \mu(t)} \le \abs{e(s,t)} \cdot \abs{s - t} \le \epsilon \cdot \abs{s - t}\]
</p>

<p>
Comme le choix de \(\delta\) ne dépend ni de \(s\) ni de \(t\), le monôme \(\mu\) est uniformément différentiable sur \([\alpha,\beta]\).
</p>

<p>
On généralise aisément à un polynôme quelconque :
</p>

<p>
\[p(x) = \sum_{i = 0}^n a_i \cdot x^i\]
</p>

<p>
en constatant que :
</p>

<p>
\[\abs{p(s) - p(t) - \partial p(t) \cdot (s - t)} \le \abs{s - t} \sum_{i = 0}^n \abs{a_i} \cdot \abs{e_i(s,t)}\]
</p>

<p>
où \(e_i\) est l'erreur obtenue avec le monôme de degré \(i\). Mais comme on peut trouver des \(\delta_k\) tels que :
</p>

<p>
\[\abs{e_i(s,t)} \le \frac{\epsilon}{\sum_j \abs{a_j}}\]
</p>

<p>
il suffit de choisir \(\delta = \min \{ \delta_0, \delta_1, ..., \delta_n \}\) pour avoir :
</p>

<p>
\[\abs{p(s) - p(t) - \partial p(t) \cdot (s - t)} \le \abs{s - t} \cdot \epsilon \cdot \frac{ \sum_i \abs{a_i} }{ \sum_j \abs{a_j} } = \abs{s - t} \cdot \epsilon\]
</p>

<p>
Tout polynôme est uniformément différentiable sur des intervalles de la forme \([\alpha,\beta]\). Cette généralisation montre aussi que toute combinaison linéaire de fonctions uniformément différentiables est uniformément différentiable.
</p>
</div>
</div>
</div>
</div>


<div id="outline-container-orge1bad48" class="outline-2">
<h2 id="orge1bad48"><span class="section-number-2">4.</span> Dérivées des puissances</h2>
<div class="outline-text-2" id="text-4">
<div id="text-table-of-contents" role="doc-toc">
<ul>
<li><a href="#org5fef8d1">4.1. Introduction</a></li>
<li><a href="#org44583b9">4.2. L'inverse multiplicatif</a></li>
<li><a href="#org8edb4c4">4.3. Puissances négatives</a></li>
<li><a href="#org8300785">4.4. Racines</a></li>
<li><a href="#orgd9a4021">4.5. Puissances fractionnaires</a></li>
<li><a href="#org49ccae6">4.6. Puissances réelles</a></li>
</ul>
</div>
</div>


<div id="outline-container-org5fef8d1" class="outline-3">
<h3 id="org5fef8d1"><span class="section-number-3">4.1.</span> Introduction</h3>
<div class="outline-text-3" id="text-4-1">
<p>
Nous allons évaluer les dérivées des fonctions \(f : x \mapsto x^\alpha\) où \(x,\alpha \in \setR\).
</p>
</div>
</div>


<div id="outline-container-org44583b9" class="outline-3">
<h3 id="org44583b9"><span class="section-number-3">4.2.</span> L'inverse multiplicatif</h3>
<div class="outline-text-3" id="text-4-2">
<p>
Commençons par :
</p>

<div class="org-center">
<p>
\(
x \cdot y = 1 \\
y = \unsur{x} = x^{-1}
\)
</p>
</div>

<p>
On en déduit que :
</p>

<p>
\[x \cdot dy + y \cdot dx = d(1) = 0\]
</p>

<p>
ce qui nous donne :
</p>

<p>
\[\OD{}{x}\left( \unsur{x} \right) = \OD{y}{x} = -\frac{y}{x} = -\frac{1}{x^2}\]
</p>
</div>
</div>


<div id="outline-container-org8edb4c4" class="outline-3">
<h3 id="org8edb4c4"><span class="section-number-3">4.3.</span> Puissances négatives</h3>
<div class="outline-text-3" id="text-4-3">
<p>
Considérons la relation :
</p>

<p>
\[y = x^{-n}\]
</p>

<p>
où \(n\in\setN\). On définit la variable intermédiaire \(z\) telle que :
</p>

<div class="org-center">
<p>
\(
z = x^{-1} \\
y = z^n
\)
</p>
</div>

<p>
On en déduit :
</p>

\begin{align}
\OD{y}{x} = \OD{y}{z} \cdot \OD{z}{x} &= n \cdot z^{n - 1} \cdot \left( -\unsur{x^2} \right) \\
&= - n \cdot x^{1 - n} \cdot x^{-2} \\
&= (-n) \cdot x^{-n - 1}
\end{align}
</div>
</div>


<div id="outline-container-org8300785" class="outline-3">
<h3 id="org8300785"><span class="section-number-3">4.4.</span> Racines</h3>
<div class="outline-text-3" id="text-4-4">
<p>
Toujours pour \(n \in \setN\), considérons la relation :
</p>

<p>
\[y = x^n \qquad \Leftrigfhtarrow\qquad x = y^{1/n}\]
</p>

<p>
Posons \(\alpha = 1/n\). On a :
</p>

<p>
\[\OD{y}{x} = n \cdot x^{n - 1} = n \cdot y \cdot y^{-\alpha}\]
</p>

<p>
Donc :
</p>

<p>
\[\OD{x}{y} = \alpha \cdot y^{\alpha-1}\]
</p>
</div>
</div>


<div id="outline-container-orgd9a4021" class="outline-3">
<h3 id="orgd9a4021"><span class="section-number-3">4.5.</span> Puissances fractionnaires</h3>
<div class="outline-text-3" id="text-4-5">
<p>
Choisissons à présent :
</p>

<p>
\[y = x^{m/n}\]
</p>

<p>
où \(m\in\setZ\) et \(n\in\setN\). Définissons la variable intermédiaire :
</p>

<p>
\[z = x^m\]
</p>

<p>
On a alors :
</p>

<p>
\[y = z^{1/n}\]
</p>

<p>
Posons \(\alpha = m/n\). La dérivée s'écrit :
</p>

\begin{align}
\OD{y}{x} = \OD{y}{z} \cdot \OD{z}{x} &= \unsur{n} \cdot z^{\unsur{n} - 1} \cdot m \cdot x^{m - 1} \\
&= \alpha \cdot x^{\alpha - m} \cdot x^{m - 1} \\
&= \alpha \cdot x^{\alpha - 1}
\end{align}

<p>
On a donc :
</p>

<p>
\[\OD{}{x}\left( x^\alpha \right) = \alpha \cdot x^{\alpha-1}\]
</p>
</div>
</div>


<div id="outline-container-org49ccae6" class="outline-3">
<h3 id="org49ccae6"><span class="section-number-3">4.6.</span> Puissances réelles</h3>
<div class="outline-text-3" id="text-4-6">
<p>
Par passage à la limite, on obtient :
</p>

<p>
\[\OD{}{x}\left( x^\alpha \right) = \alpha \cdot x^{\alpha-1}\]
</p>

<p>
pour tout \(\alpha \in \setR\).
</p>
</div>
</div>
</div>


<div id="outline-container-orge9d8032" class="outline-2">
<h2 id="orge9d8032"><span class="section-number-2">5.</span> Différentielles et matrices</h2>
<div class="outline-text-2" id="text-5">
<div id="text-table-of-contents" role="doc-toc">
<ul>
<li><a href="#org9b63ecb">5.1. Applications linéaires</a></li>
<li><a href="#orgb562f5a">5.2. Formes linéaires</a></li>
<li><a href="#org29bf113">5.3. Formes bilinéaires</a></li>
<li><a href="#orgfc6da1e">5.4. Formes quadratiques</a></li>
<li><a href="#org4a1320d">5.5. Produit matriciel</a></li>
<li><a href="#orgacffd1a">5.6. Matrice inverse</a></li>
</ul>
</div>

<p>
\label{chap:diffmatr}
</p>
</div>


<div id="outline-container-org9b63ecb" class="outline-3">
<h3 id="org9b63ecb"><span class="section-number-3">5.1.</span> Applications linéaires</h3>
<div class="outline-text-3" id="text-5-1">
<p>
Soit \(A \in \matrice(\setR,m,n)\). Considérons la fonction \(\mathcal{A} : \setR^m \times \setR^n \mapsto \setR\) définie par :
</p>

<p>
\[\mathcal{A}(x) = A \cdot x\]
</p>

<p>
En terme de composantes, on a :
</p>

<p>
\[\mathcal{A}_i(x) = \sum_j A_{ij} \cdot x_j\]
</p>

<p>
Les dérivées s'écrivent :
</p>

<p>
\[\deriveepartielle{\mathcal{A}_i}{x_k}(x) = A_{ik}\]
</p>

<p>
On a donc :
</p>

<p>
\[\deriveepartielle{}{x}(A \cdot x) = A\]
</p>
</div>
</div>


<div id="outline-container-orgb562f5a" class="outline-3">
<h3 id="orgb562f5a"><span class="section-number-3">5.2.</span> Formes linéaires</h3>
<div class="outline-text-3" id="text-5-2">
<p>
Soit \(u \in \setR^n\). Considérons la forme linéaire \(\varphi : \setR^n \mapsto \setR\) définie par :
</p>

<p>
\[\varphi(x) = x^\dual \cdot u\]
</p>

<p>
En terme de composantes, on a :
</p>

<p>
\[\varphi(x) = \sum_i x_i \cdot u_i\]
</p>

<p>
Les dérivées s'écrivent :
</p>

<p>
\[\deriveepartielle{\varphi}{x_k}(x) = u_i\]
</p>

<p>
On a donc :
</p>

<p>
\[\deriveepartielle{}{x}(x^\dual \cdot u) = u\]
</p>

<p>
Comme \(u^\dual \cdot x = x^\dual \cdot u\), on a aussi :
</p>

<p>
\[\deriveepartielle{}{x}(u^\dual \cdot x) = u\]
</p>
</div>
</div>


<div id="outline-container-org29bf113" class="outline-3">
<h3 id="org29bf113"><span class="section-number-3">5.3.</span> Formes bilinéaires</h3>
<div class="outline-text-3" id="text-5-3">
<p>
Soit à présent \(A \in \matrice(\setR,m,n)\). Considérons la forme bilinéaire \(\vartheta : \setR^m \times \setR^n \mapsto \setR\) définie par :
</p>

<p>
\[\vartheta(x,y) = x^\dual \cdot A \cdot y\]
</p>

<p>
En terme de composantes, on a :
</p>

<p>
\[\vartheta(x,y) = \sum_{i,j} x_i \cdot A_{ij} \cdot y_j\]
</p>

<p>
Les dérivées s'écrivent :
</p>

<div class="org-center">
<p>
\(
\deriveepartielle{\vartheta}{x_k}(x,y) = \sum_j A_{kj} \cdot y_j \\ \\
\deriveepartielle{\vartheta}{y_k}(x,y) = \sum_i x_i \cdot A_{ik}
\)
</p>
</div>

<p>
On a donc :
</p>

<div class="org-center">
<p>
\(
\deriveepartielle{}{x}(x^\dual \cdot A \cdot y) = A \cdot y \\ \\
\deriveepartielle{}{y}(x^\dual \cdot A \cdot y) = A^\dual \cdot x
\)
</p>
</div>

<p>
Les dérivées secondes s'en déduisent alors :
</p>

<div class="org-center">
<p>
\(
\dfdxdy{}{x}{x}(x^\dual \cdot A \cdot y) = 0 \\ \\
\dfdxdy{}{y}{y}(x^\dual \cdot A \cdot y) = 0 \\ \\
\dfdxdy{}{y}{x}(x^\dual \cdot A \cdot y) = A
\)
</p>
</div>
</div>
</div>


<div id="outline-container-orgfc6da1e" class="outline-3">
<h3 id="orgfc6da1e"><span class="section-number-3">5.4.</span> Formes quadratiques</h3>
<div class="outline-text-3" id="text-5-4">
<p>
Soit à présent \(A \in \matrice(\setR,n,n)\). Considérons la forme quadratique \(\mathcal{Q} : \setR^m \times \setR^n \mapsto \setR\) définie par :
</p>

<p>
\[\mathcal{Q}(x) = x^\dual \cdot A \cdot x\]
</p>

<p>
En terme de composantes, on a :
</p>

<p>
\[\mathcal{Q}(x) = \sum_{i,j} x_i \cdot A_{ij} \cdot x_j\]
</p>

<p>
Les dérivées s'écrivent :
</p>

\begin{align}
\deriveepartielle{\mathcal{Q}}{x_k}(x) &= \sum_j A_{kj} \cdot x_j + \sum_i x_i \cdot A_{ik} \\
&= \sum_i (A_{ki} + A_{ik}) \cdot x_i
\end{align}

<p>
On a donc :
</p>

<p>
\[\deriveepartielle{}{x}(x^\dual \cdot A \cdot x) = (A + A^\dual) \cdot x\]
</p>

<p>
La dérivée seconde est alors immédiate :
</p>

<p>
\[\dfdxdy{}{x}{x}(x^\dual \cdot A \cdot x) = A + A^\dual\]
</p>

<p>
Un cas fréquent est celui d'une matrice \(H\) hermitienne (\(H^\dual = H\)). On a alors :
</p>

<p>
\[\deriveepartielle{}{x}(x^\dual \cdot H \cdot x) = 2 \cdot H \cdot x\]
</p>

<p>
et :
</p>

<p>
\[\dfdxdy{}{x}{x}(x^\dual \cdot H \cdot x) = 2 \cdot H\]
</p>
</div>
</div>


<div id="outline-container-org4a1320d" class="outline-3">
<h3 id="org4a1320d"><span class="section-number-3">5.5.</span> Produit matriciel</h3>
<div class="outline-text-3" id="text-5-5">
<p>
Soient les fonctions \(A : \setR \mapsto \matrice(\setR,m,n)\) et \(B : \setR \mapsto \matrice(\setR,n,p)\) qui, à chaque réel \(t\), associent des matrices de composantes réelles. Nous allons tenter de trouver une expression de la dérivée du produit matriciel \(A \cdot B\). Les propriétés des différentielles nous permettent d'écrire :
</p>

<p>
\[\partial \sum_{k = 1}^n A_{ik}(t) \cdot B_{kj}(t) = \sum_{k = 1}^n \partial A_{ik}(t) \cdot B_{kj}(t) + \sum_{k=1}^n A_{ik}(t) \cdot \partial B_{kj}(t)\]
</p>

<p>
On a donc :
</p>

<p>
\[\partial (A \cdot B) = \partial A \cdot B + A \cdot \partial B\]
</p>

<p>
ou, symboliquement :
</p>

<p>
\[d(A \cdot B) = dA \cdot B + A \cdot dB\]
</p>
</div>
</div>


<div id="outline-container-orgacffd1a" class="outline-3">
<h3 id="orgacffd1a"><span class="section-number-3">5.6.</span> Matrice inverse</h3>
<div class="outline-text-3" id="text-5-6">
<p>
Considérons le cas où \(A\) est carrée (\(m = n\)). En dérivant la relation \(A \cdot A^{-1} = I\), on obtient :
</p>

<p>
\[0 = dI = d(A \cdot A^{-1}) = dA \cdot A^{-1} + A \cdot dA^{-1}\]
</p>

<p>
et donc :
</p>

<p>
\[d(A^{-1}) = - A^{-1} \cdot dA \cdot A^{-1}\]
</p>
</div>
</div>
</div>


<div id="outline-container-org15bf4bb" class="outline-2">
<h2 id="org15bf4bb"><span class="section-number-2">6.</span> Résolution d'équations</h2>
<div class="outline-text-2" id="text-6">
<div id="text-table-of-contents" role="doc-toc">
<ul>
<li><a href="#org33c1c4f">6.1. Introduction</a></li>
<li><a href="#orgcfb357c">6.2. Newton-Raphson</a></li>
<li><a href="#org887faa2">6.3. Banach</a></li>
<li><a href="#orgbf814ac">6.4. Méthode hybride d'Aitken</a></li>
</ul>
</div>

<p>
\label{chap:resolu}
</p>
</div>


<div id="outline-container-org33c1c4f" class="outline-3">
<h3 id="org33c1c4f"><span class="section-number-3">6.1.</span> Introduction</h3>
<div class="outline-text-3" id="text-6-1">
<p>
Soit une fonction \(F : \setR^n \mapsto \setR^n\) et l'espace des solutions :
</p>

<p>
\[S = \{ s \in \setR^n : F(s) = 0 \} = \noyau F\]
</p>

<p>
Nous allons tenter d'obtenir itérativement une estimation d'un \(s \in S\). 0n part d'un certain \(x_0 \in \setR^n\) et on essaie à chaque itération :
</p>

<p>
\[x_{k + 1} = I(x_k) = x_k + \Delta_k\]
</p>

<p>
d'améliorer la qualité de notre estimation, c'est à dire de rapprocher \(F(x_{k + 1})\) de zéro. On espère que pour \(K\) assez grand, on aura :
</p>

<p>
\[F(x_K) \approx \lim_{k \to \infty} F(x_k) = 0\]
</p>

<p>
A moins que l'on ait déjà une vague idée d'une région \(X \subseteq \setR^n\) contenant une solution \(s \in S\), on choisit en général \(x_0 = 0\).
</p>
</div>
</div>


<div id="outline-container-orgcfb357c" class="outline-3">
<h3 id="orgcfb357c"><span class="section-number-3">6.2.</span> Newton-Raphson</h3>
<div class="outline-text-3" id="text-6-2">
<p>
On demande à chaque étape que le développement du premier ordre autour de \(x_k\) s'annule en \(x_{k + 1} = x_k + \Delta_k\). On impose donc :
</p>

<p>
\[F(x_{k + 1}) \approx F(x_k) + \partial F(x_k) \cdot \Delta_k \approx 0\]
</p>

<p>
On est amenés à résoudre le système :
</p>

<p>
\[\partial F(x_k) \cdot \Delta_k = - F(x_k)\]
</p>

<p>
Si la matrice \(\partial F(x_k)\) est inversible, on a :
</p>

<p>
\[\Delta_k = - \left[\partial F(x_k)\right]^{-1} \cdot F(x_k)\]
</p>

<p>
et :
</p>

<p>
\[x_{k + 1} = x_k - \left[\partial F(x_k)\right]^{-1} \cdot F(x_k)\]
</p>
</div>
</div>


<div id="outline-container-org887faa2" class="outline-3">
<h3 id="org887faa2"><span class="section-number-3">6.3.</span> Banach</h3>
<div class="outline-text-3" id="text-6-3">
<p>
On voit que la condition \(F(s) = 0\) est équivalente à \(F(s) + s = s\). On définit donc la fonction \(f : \setR^n \to \setR^n\) par :
</p>

<p>
\[f(x) = F(x) + x\]
</p>

<p>
pour tout \(x \in \setR^n\), et on cherche un \(s\) tel que \(f(s) = F(s) + s = s\).
Si \(f\) est contractante, on applique le théorème de Banach en itérant simplement par :
</p>

<p>
\[x_{k + 1} = f(x_k) = f^{k + 1}(x_0)\]
</p>

<p>
Pour \(K\) assez grand, on a alors :
</p>

<p>
\[x_K \approx \lim_{k \to \infty} x_k = s\]
</p>
</div>
</div>


<div id="outline-container-orgbf814ac" class="outline-3">
<h3 id="orgbf814ac"><span class="section-number-3">6.4.</span> Méthode hybride d'Aitken</h3>
<div class="outline-text-3" id="text-6-4">
<p>
Supposons à présent que \(F,f : \setR \to \setR\) avec \(f(x) = F(x) + x\) pour tout \(x \in \setR\). L'idée est d'utiliser simultanément les deux approches \(F(s) = 0\) et \(f(s) = F(s) + s = s\). Soit :
</p>

<p>
\[F'(x) = \OD{F}{x}(x)\]
</p>

<p>
Supposons qu'au bout de \(k\) itérations on ait obtenu \(x_k\) comme approximation de la solution \(s\). On commence par effectuer deux $f$-itérations en partant de \(x_k\) :
</p>

\begin{align}
u_0 &= x_k \\
u_1 &= f(u_0) \\
u_2 &= f(u_1)
\end{align}

<p>
Les valeurs de \(F\) s'écrivent alors :
</p>

<div class="org-center">
<p>
\(
F(u_0) = f(u_0) - u_0 = u_1 - u_0 \\
F(u_1) = f(u_1) - u_1 = u_2 - u_1
\)
</p>
</div>

<p>
On se sert ensuite du développement :
</p>

<p>
\[F(u_0) \approx F(u_1) + F'(u_1) \cdot (u_0 - u_1)\]
</p>

<p>
pour approximer \(F'\) :
</p>

<p>
\[F'(u_1) \approx \frac{F(u_0) - F(u_1)}{u_0 - u_1} = \frac{F(u_1) - F(u_0)}{u_1 - u_0}\]
</p>

<p>
On a donc :
</p>

<p>
\[F'(u_1) \approx \frac{(u_2 - u1) - (u_1 - u_0)}{u_1 - u_0} = \frac{(u_2 - 2 u_1 + u_0)}{u_1 - u_0}\]
</p>

<p>
On applique ensuite l'itération de Newton-Raphson :
</p>

<p>
\[x_{k + 1} = x_{k} - \frac{F(x_k)}{F'(x_k)}\]
</p>

<p>
en remplaçant la dérivée \(F'\) par son approximation :
</p>

<p>
\[x_{k+1} = x_k - \frac{(u_1 - u_0)^2}{u_2 - 2 u_1 + u_0}\]
</p>

<p>
On espère que la suite des \(x_k\) ainsi définie converge plus vite vers la solution \(s\) que la suite des \(f^k(x_0)\).
</p>
</div>
</div>
</div>
</div>
<div id="postamble" class="status">
<p class="author">Auteur: chimay</p>
<p class="date">Created: 2023-05-10 mer 16:44</p>
<p class="validation"><a href="https://validator.w3.org/check?uri=referer">Validate</a></p>
</div>
</body>
</html>
