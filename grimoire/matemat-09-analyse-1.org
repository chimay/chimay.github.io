
#+STARTUP: showall

#+TITLE: Eclats de vers : Matemat 09 : Analyse - 1
#+AUTHOR: chimay
#+EMAIL: or du val chez gé courriel commercial
#+LANGUAGE: fr
#+LINK_HOME: file:../index.html
#+LINK_UP: file:index.html
#+HTML_HEAD: <link rel="stylesheet" type="text/css" href="../style/defaut.css" />

#+OPTIONS: H:6
#+OPTIONS: toc:nil

#+TAGS: noexport(n)

[[file:index.org][Index des Grimoires]]

#+INCLUDE: "../include/navigan-1.org"

#+TOC: headlines 1

#+INCLUDE: "../include/commandes-tex.org"

* Théorème de Rolle

#+TOC: headlines 1 local

\label{chap:intediff}


** Dépendances

  - Chapitre \ref{chap:differ} : Les différentielles
  - Chapitre \ref{chap:integral} : Les intégrales


** Extrema locaux

Soit $f : A \mapsto \setR$ avec $A \subseteq \setR^n$. Supposons que $f$ soit différentiable et atteigne un minimum local en $a \in \interieur A$. On a :

#+BEGIN_CENTER
\(
f(a + h) - f(a) = \differentielle{f}{a}(h) + E(h) \\
f(a - h) - f(a) = - \differentielle{f}{a}(h) + E(-h)
\)
#+END_CENTER

Fixons $\epsilon \strictsuperieur 0$. On peut trouver $\delta_1 \strictsuperieur 0$ tel que :

$$\abs{E(h)} \le \epsilon \cdot \abs{h}$$

pour tout $h \in \boule(0,\delta_1)$. On peut aussi trouver $\delta_2 \strictsuperieur 0$ tel que :

$$f(a) \le f(a + h)$$

pour tout $h \in \boule(0,\delta_2)$. Posons $\delta = \min \{ \delta_1,\delta_2 \}$ et choisissons $h \in \boule(0,\delta)$. On a également $-h \in \boule(0,\delta)$. Donc :

#+BEGIN_CENTER
\(
\differentielle{f}{a}(h) = f(a + h) - f(a) - E(h) \ge - E(h) \ge - \epsilon \cdot \norme{-h} \\
\differentielle{f}{a}(h) = f(a) - f(a - h) + E(-h) \le E(-h) \le \epsilon \cdot \norme{h}
\)
#+END_CENTER

On en conclut que :

$$\abs{\differentielle{f}{a}(h)} \le \epsilon \cdot \norme{h}$$

Posons $\gamma = \delta / 2$ et remarquons que l'ensemble de norme fixe $N = \{ h \in \setR^n : \norme{h} = \gamma \}$ est inclus dans $\boule(0,\delta)$. Les propriétés des applications linéaires nous disent que :

$$\norme{\differentielle{f}{a}} = \sup \left\{ \unsur{\gamma} \norme{\differentielle{f}{a}(h)} : h \in N \right\}$$

Or, la borne nous dit que :

$$\unsur{\gamma} \abs{\differentielle{f}{a}(h)} \le \epsilon$$

quel que soit $\epsilon \strictsuperieur 0$ et $h \in N$. Donc :

$$\norme{\differentielle{f}{a}} = 0$$

ce qui implique que :

$$\differentielle{f}{a} = 0$$

La différentielle s'annule donc en un minimum local. On montre de la même manière que la différentielle s'annule en un maximum local.


*** La Jacobienne

La Jacobienne étant la représentation matricielle de la différentielle, elle s'annule également aux extrema locaux.


** Théorème de Rolle

Soit $f \in \continue^1([a,b],\setR)$ avec $f(a) = f(b)$. Comme $f$ est continue, il existe $\lambda,\sigma \in [a,b]$ tels que :

#+BEGIN_CENTER
\(
f(\lambda) = \min f([a,b]) \\
f(\sigma) = \max f([a,b])
\)
#+END_CENTER


*** Configurations

Plusieurs cas peuvent se présenter :


  - $\lambda \strictinferieur f(a) = f(b) \strictinferieur \sigma$ : dans ce cas, la fonction atteint ses deux bornes à l'intérieur de l'intervalle :

$$\{\lambda,\sigma\} \subseteq \intervalleouvert{a}{b}$$

Comme les extrema sont aussi des extrema locaux, on a :

$$\partial f(\lambda) = \partial f(\sigma) = 0$$

  - $\lambda \strictinferieur f(a) = f(b) = \sigma$ : dans ce cas, la fonction atteint son minimum à l'intérieur de l'intervalle :

$$\lambda \in \intervalleouvert{a}{b}$$

et on a :

$$\partial f(\lambda) = 0$$

  - $\lambda = f(a) = f(b) \strictinferieur \sigma$ : dans ce cas, la fonction atteint son maximum à l'intérieur de l'intervalle :

$$\sigma \in \intervalleouvert{a}{b}$$

et on a :

$$\partial f(\sigma) = 0$$

  - $\lambda = f(a) = f(b) = \sigma$ : dans ce cas, on a :

$$\lambda \le f(x) \le \sigma = \lambda$$

pour tout $x \in [a,b]$, et donc :

$$f(x) = \lambda$$

La fonction $f$ est constante et $\partial f = 0$. On peut donc prendre n'importe quel $c \in \intervalleouvert{a}{b}$, on aura :

$$\partial f(c) = 0$$



*** Conclusion

Dans tous les cas, on a au moins un $c \in \intervalleouvert{a}{b}$ tel que :

$$\partial f(c) = 0$$


** Théorème des accroissements finis

Soit $f \in \continue^1([a,b],\setR)$ et la fonction $g$ associée définie par :

$$g(x) = f(x) - \frac{f(b) - f(a)}{b - a} \cdot (x - a)$$

pour tout $x \in [a,b]$. Comme $g(a) = g(b) = f(a)$, on peut trouver un $c \in \intervalleouvert{a}{b}$ tel que :

$$0 = \partial g(c) = \partial f(c) - \frac{f(b) - f(a)}{b - a}$$

On a donc :

$$\partial f(c) = \frac{f(b)-f(a)}{b-a}$$

On vient ainsi de démontrer le théorème des accroissements finis.


*** Dimension $n$

On peut généraliser ce théorème à une fonction $f \in \continue^1(\setR^m,\setR^n)$. Soit $u,v \in \setR^m$. On considére le segment $[u,v]$ et la fonction associée $\phi : [0,1] \mapsto \setR^m$ définie par :

$$\phi(t) = u + t \cdot (v - u)$$

pour tout $t \in [0,1]$. On pose alors :

$$g(t) = (f \circ \phi)(t) = f( u + t \cdot (v - u))$$

pour tout $t \in [0,1]$. En appliquant le résultat précédent aux composantes $g_i$ sur l'intervalle $[0,1]$, on obtient un $s \in \intervalleouvert{0}{1}$ tel que :

$$\partial g_i(s) = \frac{g_i(1) - g_i(0)}{1 - 0} = g_i(1) - g_i(0) = f_i(v) - f_i(u)$$

En appliquant la formule permettant d'évaluer la dérivée d'une composition de fonctions, on obtient :

$$\partial g_i(s) = \sum_j \partial_j f_i(u + s \cdot (v - u)) \cdot (v_j - u_j)$$

Utilisant la notation matricielle, on a donc :

$$f(v) - f(u) = \partial f(u + s \cdot (v - u)) \cdot (v - u)$$

Ce qui revient à dire qu'il existe un $w \in [u,v] \subseteq \setR^n$ tel que :

$$f(v) - f(u) = \partial f(w) \cdot (v - u)$$


** Théorème de Cauchy

Le théorème des accroissements finis nous donne un résultat sous la forme
symbolique :

$$\OD{f}{x} = \frac{\difference f}{\difference x}$$

Nous allons maintenant généraliser ce théorème, et obtenir le résultat :

$$\frac{df}{dg} = \frac{\difference f}{\difference g}$$

où $f,g \in \continue^1([a,b],\setR)$ et $a,b \in \setR$. Considérons à cette fin la
fonction $h$ définie par :

$$h(x) = [f(b) - f(a)] \cdot g(x) - f(x) \cdot [ g(b) - g(a) ]$$

On remarque que :

\begin{align}
h(a) &= f(b) \cdot g(a) - f(a) \cdot g(a) -  f(a)\cdot g(b) +  f(a)\cdot g(a) \\
&= f(b) \cdot g(a) - f(a) \cdot g(b) \\ \\
h(b) &= f(b) \cdot g(b) - f(a) \cdot g(b) -  f(b)\cdot g(b) +  f(b)\cdot g(a) \\
&= f(b) \cdot g(a) - f(a) \cdot g(b)
\end{align}

Donc :

$$h(a) = h(b)$$

Appliquant le théorème de Rolle à $h$, on peut donc trouver un $c \in \intervalleouvert{a}{b}$ tel que :

$$0 = \partial h(c) = [f(b) - f(a)] \cdot \partial g(c) - \partial f(c) \cdot [ g(b) - g(a) ]$$

On a donc :

$$\left[ f(b) - f(a) \right] \cdot \partial g(c) = \partial f(c) \cdot \left[ g(b) - g(a) \right]$$

Si $\partial f(c) \ne 0$ et $f(b) \ne f(a)$, on peut le mettre sous la forme :

$$\frac{\partial g(c)}{\partial f(c)} = \frac{g(b) - g(a)}{f(b) - f(a)}$$


** Théorème de l'Hospital

Soient $F,G$ deux fonctions continues sur $I=[\alpha,\beta]$ et dérivables
$I\setminus \{a\}$, avec $a \in \interieur\ I$. Supposons que les deux fonctions s'annulent en $a$ :

$$F(a) = G(a) = 0$$

Soit alors $h \ne 0$ tel que $b = a+h\in I\setminus \{a\}$. En appliquant le théorème de Cauchy à $F$ et $G$, on trouve un $t \in \intervalleouvert{0}{1}$ tel que :

$$[F(b) - F(a)] \cdot \partial G(a + t \cdot h) = [G(b) - G(a)] \cdot \partial F(a + t \cdot h)$$

Mais comme $F$ et $G$ s'annulent en $a$, on a :

$$F(b) \cdot \partial G(a + t \cdot h) = G(b) \cdot \partial F(a + t \cdot h)$$

Si de plus $\partial G$ ne s'annule pas sur $I$, on peut écrire :

$$\frac{ \partial F(a + t \cdot h) }{ \partial G(a + t \cdot h) } = \frac{F(b)}{F(a)}$$

On voit en faisant tendre $h$ vers $0$ que les limites, si elles existent, doivent être identiques. On a donc :

$$\lim_{x \to a} \frac{F(x)}{G(x)} =  \lim_{x \to a} \frac{\partial F(a)}{\partial G(a)}$$


** Uniformité

Nous allons à présent montrer que toute fonction continument différentiable sur un intervalle de la forme $[\alpha,\beta]$ y est uniformément différentiable.

Soit une fonction $f \in \continue^1([\alpha,\beta],\setR)$. Comme la dérivée $\partial f$ est continue sur $[\alpha,\beta]$, elle y est uniformément continue. Fixons $\epsilon \strictsuperieur 0$. On peut donc trouver un $\delta \strictsuperieur 0$ tel que :

$$\abs{\partial f(s) - \partial f(t)} \le \epsilon$$

pour tout $s,t \in [\alpha,\beta]$ vérifiant $\abs{s - t} \le \delta$. Si $s = t$, on a bien évidemment :

$$\abs{f(t) - f(t) - \partial f(t) \cdot (t - t)} = 0 \le \epsilon \cdot (t - t) = 0$$

Considérons à présent le cas $s \ne t$. Nous pouvons supposer sans perte de généralité que $s \strictinferieur t$. Le théorème des accroissements finis nous dit qu'on peut trouver un $\gamma \in ]s,t[$ tel que :

$$\partial f(\gamma) = \frac{f(t) - f(s)}{t - s}$$

On a donc :

$$\frac{f(t) - f(s)}{t - s} - \partial f(s) = \partial f(\gamma) - \partial f(s)$$

Mais comme $\abs{\gamma - s} \le \abs{t - s} \le \delta$, on a $\abs{\partial f(\gamma) - \partial f(s)} \le \epsilon$ et :

$$\abs{\frac{f(t) - f(s)}{t - s} - \partial f(s)} \le \epsilon$$

On a donc bien :

$$\abs{f(t) - f(s) - \partial f(s) \cdot (t - s)} \le \epsilon \cdot \abs{t - s}$$

pour tout $s,t \in [\alpha,\beta]$ vérifiant $\abs{s - t} \le \delta$.


*** Remarque

Le théorème {\em n'est pas} applicable aux autres types d'intervalles. Cela ne marche pas sur $]\alpha,\beta[$ par exemple.



* Théorème fondamental

#+TOC: headlines 1 local

\label{chap:fonda}


** Dépendances

  - Chapitre \ref{chap:differ} : Les différentielles
  - Chapitre \ref{chap:integral} : Les intégrales


** Dérivée de l'intégrale

\label{sec:derivee_integrale}

Soit une fonction $f \in \continue([\alpha,\beta],\setR)$, un réel $a \in [\alpha,\beta]$ et la fonction intégrale associée $I : [\alpha,\beta] \mapsto \setR$ définie par :

$$I(x) = \int_a^x f(s) \ ds$$

pour tout $x \in [\alpha,\beta]$. On constate que :

$$I(a) = \int_a^a f(s) \ ds = \int_{ \{a\} } f(s) \ ds$$

La mesure de Lebesgue du singleton $\{a\}$ étant nulle, l'intégrale s'annule et on a :

$$I(a) = 0$$

Nous allons chercher à évaluer la dérivée de la fonction intégrale $I$ en un point quelconque $b \in [\alpha,\beta]$.

Nous utilisons dans la suite la notation abrégée :

$$\int_x^y = \int_x^y f(s) \ ds$$

Par additivité, on a :

$$\int_a^{b + h} = \int_a^b + \int_b^{b + h}$$

c'est-à-dire :

$$I(b + h) = I(b) + \int_b^{b + h}$$

pour tout réel $h$ tel que $b + h \in [\alpha,\beta]$. Quelle est la valeur de l'intégrale sur $[b, b + h]$ ? Fixons $\epsilon \strictsuperieur 0$. Par continuité de $f$, on peut choisir $\delta > 0$ tel que :

$$\abs{f(b + s) - f(b)} \le \epsilon$$

pour tout $s$ vérifiant $\abs{s} \le \delta$. Cela n'est possible que si :

$$f(b) - \epsilon \le f(b + s) \le f(b) + \epsilon$$

On a donc :

#+BEGIN_CENTER
\(
\sup \Big\{ f(b + s) : \abs{s} \le \delta \Big\} \le f(b) + \epsilon \\
\inf \Big\{ f(b + s) : \abs{s} \le \delta \Big\} \ge f(b) - \epsilon
\)
#+END_CENTER

Si nous choisissons $h \in (0,\delta)$, l'intégrale peut donc être majorée et minorée par :

$$\int_b^{b + h} \le (f(b) + \epsilon) \cdot \mu_L([a, a + h]) = (f(b) + \epsilon) \cdot h$$

et :

$$\int_b^{b + h} \ge (f(b) - \epsilon) \cdot \mu_L([a, a + h]) = (f(b) - \epsilon) \cdot h$$

Nous disposons donc des inégalités :

$$(f(b) - \epsilon) \cdot h \le \int_b^{b + h} \le (f(b) + \epsilon) \cdot h$$

Autrement dit :

$$f(b) - \epsilon \le \unsur{h} \int_b^{b + h} \le f(b) + \epsilon$$

D'un autre coté, on a :

$$\unsur{h} \int_b^{b + h} f(s) \ ds = \frac{I(b + h) - I(b)}{h}$$

Passons à la limite $\delta \to 0$. On a alors $h \to 0$ et :

$$f(b) - \epsilon \le \lim_{h \to 0} \unsur{h} \int_b^{b + h} \le f(b) + \epsilon$$

Ces inégalités devant être valables pour tout $\epsilon \strictsuperieur 0$, on a forcément :

$$\lim_{h \to 0} \frac{I(b + h) - I(b)}{h} = f(b)$$

On en conclut que $I$ est dérivable et que :

$$\OD{I}{x}(b) = \lim_{h \to 0} \frac{I(b + h) - I(b)}{h} = f(b)$$

Autrement dit :

$$\OD{}{x} \int_a^x f(s) \ ds = f(x)$$


** Intégrale de la dérivée

\label{sec:integrale_derivee}

Soit $\alpha,\beta \in \setR$ avec $\alpha \le \beta$. Soit la fonction $F \in \continue^1([\alpha,\beta],\setR)$ et sa dérivée continue :

$$f = \partial F = \OD{F}{s}$$

Comme $F$ est continument différentiable sur $[\alpha,\beta]$, elle y est uniformément différentiable. De même, $f$ est continue sur $[\alpha,\beta]$. Elle y est donc uniformément continue. Nous allons tenter d'évaluer l'intégrale :

$$\int_a^b f(s) \ ds = \int_a^b \OD{F}{x}(s) \ ds$$

avec $a,b \in [\alpha,\beta]$ et $a \le b$.

Nous utilisons dans la suite la notation abrégée :

$$\int_x^y = \int_x^y f(s) \ ds$$


*** L'idée

L'idée intuitive est que :

$$\difference F = \sum_i \difference F_i = \sum_i \frac{ \difference F_i }{ \difference x_i } \cdot \difference x_i$$

En passant à la limite $\difference x_i \to 0$, on soupçonne alors le résultat suivant :

$$\difference F = \int_a^b \OD{F}{x} \ dx$$


*** La réalisation

Fixons $\epsilon \strictsuperieur 0$. Comme $f$ est uniformément continue, nous savons qu'il existe $\vartheta \strictsuperieur 0$ tel que :

$$\abs{f(x + h) - f(x)} \le \epsilon$$

pour tout $x,h$ vérifiant $x,x + h \in [\alpha,\beta]$ et $\abs{h} \le \vartheta$. On en déduit que :

#+BEGIN_CENTER
\(
\sup_{\xi \in [x - \vartheta , x + \vartheta]} f(\xi) \le f(x) + \epsilon \\
\inf_{\xi \in [x - \vartheta , x + \vartheta]} f(\xi) \ge f(x) - \epsilon
\)
#+END_CENTER

On a donc les bornes pour l'intégrale :

$$(f(x) - \epsilon) \cdot h \le \int_x^{x + h} \le (f(x) + \epsilon) \cdot h$$

Comme $F$ est uniformément différentiable, nous pouvons trouver $\varpi \strictsuperieur 0$ tel que :

$$\abs{F(x + h) - F(x) - f(x) \cdot h} \le \epsilon \cdot h$$

pour tout $x,h$ vérifiant $x,x + h \in [\alpha,\beta]$ et $\abs{h} \le \varpi$. On en déduit que :

#+BEGIN_CENTER
\(
F(x + h) - F(x) - f(x) \cdot h \le \epsilon \cdot h \\
f(x) \cdot h - (F(x + h) - F(x)) \le \epsilon \cdot h
\)
#+END_CENTER

En considérant ces deux inégalités par rapport au centre $f(x) \cdot h$, on obtient :

$$F(x + h) - F(x) - \epsilon \cdot h \le f(x) \cdot h \le F(x + h) - F(x) + \epsilon \cdot h$$

En soustrayant ou en ajoutant $\epsilon \cdot h$ à ces inégalités, on a :

\begin{align}
F(x + h) - F(x) - 2 \epsilon \cdot h &\le f(x) \cdot h - \epsilon \cdot h \le& F(x + h) - F(x) \\
F(x + h) - F(x) &\le f(x) \cdot h + \epsilon \cdot h \le& F(x + h) - F(x) + 2 \epsilon \cdot h
\end{align}

Si $\abs{h} \le \min \{ \vartheta , \varpi \}$, nous avons de nouvelles bornes pour l'intégrale :

$$F(x + h) - F(x) - 2 \epsilon \cdot h \le \int_x^{x + h} \le F(x + h) - F(x) + 2 \epsilon \cdot h$$

Choisissons à présent $n \in \setN$ tel que :

$$\abs{ \frac{b - a}{n} } \le \min \{ \vartheta , \varpi \}$$

Posons $h = (b - a)/n$ et définissons la série :

$$x_i = a + i \cdot h$$

On a alors $a = x_0 \le x_1 \le ... \le x_n = b$. Les propriétés des sommes nous disent que :

$$\sum_{i = 1}^n (F(x_i) - F(x_{i - 1})) = F(x_n) - F(x_0) = F(b) - F(a)$$

D'un autre coté, on a clairement :

$$\sum_{i = 1}^n \int_{ x_{i - 1} }^{x_i} = \int_a^b$$

Si nous appliquons les bornes précédentes avec $x = x_{i - 1}$, nous avons $x + h = x_i$ et :

$$F(x_i) - F(x_{i - 1}) - 2 \epsilon \cdot h \le \int_{ x_{i - 1} }^{x_i} \le F(x_i) - F(x_{i - 1}) + 2 \epsilon \cdot h$$

En sommant sur $i = 1,2,...,n$, nous obtenons par conséquent :

$$F(b) - F(a) - 2 \epsilon \cdot h \cdot n \le \int_a^b \le F(b) - F(a) + 2 \epsilon \cdot h \cdot n$$

Mais comme $h \cdot n = b - a$, cela devient :

$$F(b) - F(a) - 2 \epsilon \cdot (b - a) \le \int_a^b \le F(b) - F(a) + 2 \epsilon \cdot (b - a)$$

Ces bornes devant être satisfaites pour tout $\epsilon \strictsuperieur 0$, on en déduit que :

$$\int_a^b = F(b) - F(a)$$

On a donc finalement :

$$\int_a^b f(s) \ ds = \int_a^b \OD{F}{s}(s) \ ds = F(b) - F(a)$$


*** Primitive

Cette relation permet de calculer l'intégrale d'une fonction continue $f : t \mapsto f(t)$ lorsqu'on connaît une fonction $F$ vérifiant :

$$\OD{F}{t} = f$$

On appelle « primitive » de $f$ une telle fonction $F$.


*** Notation

On note aussi :

$$\difference F = \int dF$$


** Polynômes

On sait que :

$$\OD{}{t}\big(t^n\big) = n \cdot t^{n - 1}$$

Comme $n$ est constante, on peut le réécrire :

$$\OD{}{t}\left( \frac{t^n}{n} \right) = t^{n - 1}$$

ou, en posant $m = n - 1$ :

$$\OD{}{t}\left( \frac{t^{m + 1}}{m + 1} \right) = t^m$$

L'intégrale s'écrit donc :

$$\int_a^b t^m \ dt = \frac{ b^{m + 1} - a^{m + 1} }{m + 1}$$

On a en particulier :

$$\int_0^x t^m \ dt = \frac{ x^{m + 1} }{m + 1}$$


*** Exemples

$$\int_0^x t \ dt = \frac{ x^2 }{2}$$

$$\int_0^x t^2 \ dt = \frac{ x^3 }{3}$$


** Valeur moyenne


*** Accroissements finis

Soit des réels distincts $a,b$ vérifiant $a \strictinferieur b$, la fonction $f \in \continue([a,b],\setR)$ et la fonction $F : [a,b] \mapsto \setR$ définie par :

$$F(x) = \int_a^x f(t) \ dt$$

pour tout $x \in [a,b]$. Comme $F \in \continue^1([a,b],\setR)$, le théorème des accroissements finis nous dit qu'on peut trouver un $c \in \intervalleouvert{a}{b}$ tel que :

$$\partial F(c) = \frac{F(b) - F(a)}{b - a}$$

On sait que :

$$\partial F(c) = f(c)$$

On a aussi :

$$F(b) = \int_a^b f(t) \ dt$$

et :

$$F(a) = \int_a^a f(t) \ dt = \int_{ \{ a \} } f(t) \ dt$$

La mesure de Lebesgue du singleton $\{a\}$ étant nulle, l'intégrale s'annule et on a :

$$F(a) = 0$$

On a donc $F(b) - F(a) = F(b)$ et :

$$f(c) = \unsur{b - a} \int_a^b f(t) \ dt$$


*** Théorème de Cauchy

Soit des réels distincts $a,b$ vérifiant $a \strictinferieur b$, les fonctions $f,g \in \continue([a,b],\setR)$ et les fonction $F,G : [a,b] \mapsto \setR$ définies par :

\begin{align}
F(x) &= \int_a^x f(t) \ dt \\
G(x) &= \int_a^x g(t) \ dt
\end{align}

pour tout $x \in [a,b]$. Comme $F,G \in \continue^1([a,b],\setR)$, le théorème de Cauchy nous dit qu'on peut trouver un $c \in \intervalleouvert{a}{b}$ tel que :

$$\partial F(c) \cdot \big[G(b) - G(a)\big] = \big[F(b) - F(a)\big] \cdot \partial G(c)$$

On sait que :

#+BEGIN_CENTER
\(
\partial F(c) = f(c) \\
\partial G(c) = g(c)
\)
#+END_CENTER

On a aussi :

\begin{align}
F(b) &= \int_a^b f(t) \ dt \\
G(b) &= \int_a^b g(t) \ dt
\end{align}

et :

\begin{align}
F(a) &= \int_a^a f(t) \ dt = 0 \\
G(a) &= \int_a^a g(t) \ dt = 0
\end{align}

On en conclut que :

$$f(c) \ \int_a^b g(t) \ dt = g(c) \ \int_a^b f(t) \ dt$$

Si l'intégrale de $g$ et $g(c)$ sont non nuls, on peut mettre cette relation sous la forme :

$$\frac{f(c)}{g(c)} = \frac{ \int_a^b f(t) \ dt }{ \int_a^b g(t) \ dt }$$


** Intégration par parties

Soient $f,g \in \continue^1(\setR,\setR)$. On se rappelle que :

$$\partial (f \cdot g) = \partial f \cdot g + f \cdot \partial g$$

et comme on a :

$$\int_a^b \partial (f \cdot g) \ dx = f(b) \cdot g(b) - f(a) \cdot g(a)$$

on obtient la formule d'intégration par parties :

$$\int_a^b f(x) \cdot \partial g(x) \ dx = (f \cdot g)(b) - (f \cdot g)(a) - \int_a^b \partial f(x) \cdot g(x) \ dx$$


*** Stieltjes

Le résultat est également valable lorsqu'on utilise les mesures de Stieltjes associées à $f$ et $g$ :

$$\int_a^b f(x) \cdot dg(x) = \difference (f \cdot g) - \int_a^b g(x) \cdot df(x)$$


*** Dérivée constante

On considère le cas particulier où $\partial g = 1$. Une exemple de fonction $g$ vérifiant cette propriété est simplement $g = \identite$. On a donc $g(x) = x$ et :

$$\int_a^b f(x) \ dx = \int_a^b f(x) \cdot 1 \ dx = \int_a^b f(x) \cdot \partial g(x) \ dx$$

L'intégration par parties nous donne :

$$\int_a^b f(x) \ dx = f(b) \cdot b - f(a) \cdot a - \int_a^b \partial f(x) \cdot x \ dx$$


** Changement de variable

Considérons une fonction $f \in \continue(\setR,\setR)$ et un changement de variable $x = \varphi(s)$ où $\varphi \in \homeomorphisme^1(\setR,\setR)$. Soit la mesure de lebesgue $\mu([\alpha,\beta]) = \beta - \alpha$.


*** L'idée

$$\sum_i f_i \cdot \difference x_i = \sum_i f_i \cdot \frac{\difference x_i}{\difference s_i} \cdot \difference s_i$$

On devrait donc avoir par passage à la limite :

$$\int f \ dx = \int f \ \OD{x}{s} \ ds$$


*** La réalisation

On applique le même procédé qu'à la section \ref{sec:integrale_derivee}. Si $x$ est proche de $y$, on a :

$$\int_x^y \approx f(x) \cdot (y - x)$$

Posant $s = \varphi^{-1}(x)$ et $t = \varphi^{-1}(y)$, on a aussi :

$$y - x = \varphi(t) - \varphi(s) = \OD{\varphi}{s}(s) \cdot (t - s) + e(\abs{s - t})$$

où $e$ converge plus vite que $s - t$ vers $0$. On en conclut que :

$$\int_x^y \approx (f \circ \varphi)(s) \cdot \OD{\varphi}{s}(s) \cdot (t - s)$$

On remarque que le second membre est une approximation de l'intégrale de la fonction :

$$F(s) = (f \circ \varphi)(s) \cdot \OD{\varphi}{s}(s)$$

sur l'intervalle $[s,t] = [\varphi^{-1}(x),\varphi^{-1}(y)]$. Il ne nous reste plus qu'à sommer sur tous les petits intervalles $[x_{i - 1},x_i]$ et à passer à la limite $h = x_i - x_{i - 1} \to 0$ pour obtenir :

$$\int_a^b f(x) \ dx = \int_{\varphi^{-1}(a)}^{\varphi^{-1}(b)} (f \circ \varphi)(s) \cdot \OD{\varphi}{s}(s) \ ds$$
