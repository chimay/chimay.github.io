
#+STARTUP: showall

#+TITLE: Eclats de vers : Matemat 09 : Analyse - 2
#+AUTHOR: chimay
#+EMAIL: or du val chez gé courriel commercial
#+LANGUAGE: fr
#+LINK_HOME: file:../index.html
#+LINK_UP: file:index.html
#+HTML_HEAD: <link rel="stylesheet" type="text/css" href="../style/defaut.css" />

#+OPTIONS: H:6
#+OPTIONS: toc:nil

#+TAGS: noexport(n)

[[file:index.org][Index des Grimoires]]

#+INCLUDE: "../include/navigan-1.org"

#+TOC: headlines 1

#+INCLUDE: "../include/commandes-tex.org"

* Développements de Taylor

#+TOC: headlines 1 local


** Polynômes de Taylor

Considérons un polynôme $p : \setR \mapsto \setR$ de degré $n$ défini par :

$$p(x) = \sum_{i = 0}^n \gamma_i \cdot x^i$$

pour tout $x \in \setR$. Calculons ses dérivées :

\begin{align}
\partial p(x) &= \sum_{i = 1}^n \gamma_i \cdot i \cdot x^{i - 1} \\
\partial^2 p(x) &= \sum_{i = 2}^n \gamma_i \cdot i \cdot (i - 1) \cdot x^{i - 2} \\
\vdots \\
\partial^k p(x) &= \sum_{i = k}^n \gamma_i \cdot \frac{i !}{(i - k) !} \cdot x^{i - k} \\
\vdots \\
\partial^n p(x) &= n! \cdot \gamma_n
\end{align}

Lorsqu'on évalue ces dérivées en $0$, seuls les termes en $x^{k - k} = 1$ ne s'annulent pas. On obtient donc :

$$\partial^k p(0) = \frac{k !}{0 !} \cdot \gamma_k = k ! \cdot \gamma_k$$

ce qui nous donne l'expression des coefficients de $p$ en fonction de ses dérivées en $0$ :

$$\gamma_k = \unsur{k !} \cdot \partial^k p(0)$$

Le polynôme peut donc se réécrire :

$$p(x) = \sum_{i = 0}^n \unsur{i !} \cdot \partial^i p(0) \cdot x^i$$

Cette expression est appelée développement de Taylor de $p$ autour de $0$.


*** Généralisation

Soit $a \in \setR$. La fonction $r$ définie par :

$$r(t) = p(t + a) = \sum_{i = 0}^n \gamma_i \cdot (t + a)^i$$

pour tout $t \in \setR$ est clairement un polynôme de degré $n$. On a $r(0) = p(a)$ et plus généralement :

$$\partial^i r(0) = \partial^i p(a)$$

pout tout $i \ge 0$. Le développement de Taylor de $r$ autour de $0$ s'écrit :

$$r(t) = \sum_{i = 0}^n \unsur{i !} \cdot \partial^i r(0) \cdot t^i$$

ou encore :

$$r(t) = \sum_{i = 0}^n \unsur{i !} \cdot \partial^i p(a) \cdot t^i$$

En posant $x = t + a$, on a $t = x - a$ et :

$$p(x) = p(t + a) = r(t)$$

Le développement devient :

$$p(x) = \sum_{i = 0}^n \unsur{i !} \cdot \partial^i p(a) \cdot (x - a)^i$$

Cette expression est nommée développement de Taylor de $p$ autour de $a$.


** Opérateur de Taylor

Soit $\alpha, \beta \in \setR$ avec $\alpha \strictinferieur \beta$, une fonction $f \in \continue^N([\alpha,\beta],\setR)$ et $a \in [\alpha,\beta]$. Par analogie avec le développement de Taylor des polynômes, on définit l'opérateur de Taylor $T_a^N$ par :

$$T_a^N(f)(x) = \sum_{k = 0}^N \unsur{k !} \cdot \partial^k f(a) \cdot (x - a)^k$$

pour tout $x \in [\alpha,\beta]$.


*** Erreur

L'erreur $E_a^N$ de l'opérateur $T_a^N$ est donnée par :

$$E_a^N(f)(x) = f(x) - T_a^N(f)(x)$$

pour tout $x \in [\alpha,\beta]$.


*** Polynômes

Si $p$ est un polynôme de degré $N$, on a bien entendu $T_a^N(p) = p$ pour tout $a \in \setR$ et $E_a^N(p) = 0$.


** Forme intégrale


*** Premier ordre

Soit $\alpha, \beta \in \setR$ avec $\alpha \strictinferieur \beta$ et la fonction $f \in \continue^2([\alpha,\beta],\setR)$. Le théorème fondamental nous dit que :

$$\int_a^x \partial f(t) \ dt = f(x) - f(a)$$

pour tout $a,x \in [\alpha,\beta]$. Appliquant le même théorème à la dérivée $\partial f$, on a aussi :

$$\int_a^x \partial^2 f(t) \ dt = \partial f(x) - \partial f(a)$$


**** Intégration par parties

Soit $u = \partial f$ et $v = \identite$. on a :

$$\int_a^x u(x) \ \partial v(x) \ dx = \int_a^x \partial f(t) \cdot 1 \ dt = \int_a^x \partial f(t) \ dt$$

L'intégration par parties nous donne :

$$\int_a^x u(x) \ \partial v(x) \ dx = v(x) \ u(x) - v(a) \ u(a) - \int_a^x v(t) \ \partial u(t) \ dt$$

En tenant compte des définitions de $u$ et $v$, on obtient :

$$\int_a^x \partial f(t) \ dt = x \ \partial f(x) - a \ \partial f(a) - \int_a^x t \ \partial^2 f(t) \ dt$$


Appliquons le théorème fondamental au membre de gauche :

$$f(x) - f(a) = x \ \partial f(x) - a \ \partial f(a) - \int_a^x t \ \partial^2 f(t) \ dt$$

ou encore :

$$f(x) = f(a) + x \ \partial f(x) - a \ \partial f(a) - \int_a^x t \ \partial^2 f(t) \ dt$$

En multipliant la relation :

$$\partial f(x) - \partial f(a) = \int_a^x \partial^2 f(t) \ dt$$

par $x$, on arrive au résultat :

$$x \ \partial f(x) = x \ \partial f(a) + \int_a^x x \ \partial^2 f(t) \ dt$$

L'expression de $f(x)$ devient alors :

$$f(x) = f(a) + x \ \partial f(a) + \int_a^x x \ \partial^2 f(t) \ dt - a \ \partial f(a) - \int_a^x t \ \partial^2 f(t) \ dt$$

et finalement :

$$f(x) = f(a) + (x - a) \cdot \partial f(a) + \int_a^x (x - t) \cdot \partial^2 f(t) \ dt$$

Le membre de droite est appelé développement de Taylor du premier ordre de $f$ sous forme intégrale.


*** Second ordre

Soit $f \in \continue^3([\alpha,\beta],\setR)$. Comme $\continue^3 \subseteq \continue^2$, $f$ admet un développement de Taylor du premier ordre sous forme intégrale. Nous allons intégrer par parties le terme :

$$\int_a^x (x - t) \cdot \partial^2 f(t) \ dt$$

On sait que :

$$\OD{}{ŧ} \left[ \unsur{2} (x - t)^2 \right] = (x - t) \cdot (-1) = - (x - t)$$

Posons $u = \partial^2 f$ et :

$$v : t \mapsto \unsur{2} (x - t)^2$$

On a :

$$\int_a^x \partial v(t) \ u(t) \ dt = - \int_a^x (x - t) \ \partial^2 f(t) \ dt$$

et :

$$\int_a^x v(t) \ \partial u(t) \ dt = \unsur{2} \int_a^x (x - t)^2 \ \partial^3 f(t) \ dt$$

Enfin :

\begin{align}
\int_a^x \partial (v \cdot u)(t) \ dt &= \unsur{2} \ (x - x)^2 \ \partial^2 f(x) - \unsur{2} \ (x - a)^2 \ \partial^2 f(a) \\
&= 0 - \unsur{2} \ (x - a)^2 \ \partial^2 f(a) \\
&= - \unsur{2} \ (x - a)^2 \ \partial^2 f(a)
\end{align}

On en conclut que :

$$- \int_a^x (x - t) \ \partial^2 f(t) \ dt = - \unsur{2} \ (x - a)^2 \ \partial^2 f(a) - \unsur{2} \int_a^x (x - t)^2 \ \partial^3 f(t) \ dt$$

ou encore :

$$\int_a^x (x - t) \ \partial^2 f(t) \ dt = \unsur{2} \ (x - a)^2 \ \partial^2 f(a) + \unsur{2} \int_a^x (x - t)^2 \ \partial^3 f(t) \ dt$$

Le développement du premier ordre peut dont se réécrire :

$$f(x) = f(a) + (x - a) \ \partial f(a) + \unsur{2} \ (x - a)^2 \ \partial^2 f(a) + \unsur{2} \int_a^x (x - t)^2 \ \partial^3 f(t) \ dt$$

Le membre de droite est appelé développement du second ordre de $f$ sous forme intégrale.


*** Ordre $N$

Soit $f \in \continue^{N + 1}([\alpha,\beta],\setR)$. On montre en intégrant par parties que :

$$\int_a^x (x - t)^{k - 1} \ \partial^k f(t) \ dt = \unsur{k} \ (x - a)^k \ \partial^k f(a) + \unsur{k} \int_a^x (x - t)^k \ \partial^{k + 1} f(t) \ dt$$

pour tout $k \in \setZ[2,N]$. On en déduit par récurrence le développement de Taylor d'ordre $N$ de $f$ sous forme intégrale :

$$f(x) = \sum_{k = 0}^N \unsur{k !} \cdot \partial^k f(a) \cdot (x - a)^k + \unsur{N !} \int_a^x (x - t)^N \ \partial^{N + 1} f(t) \ dt$$


** Erreur

On a :

$$E_a^N(f)(x) = f(x) - T_a^N(f)(x) = \unsur{N !} \int_a^x (x - t)^N \ \partial^{N + 1} f(t) \ dt$$

En appliquant le théorème de Cauchy entre $a$ et $x$ aux fonctions $F,G$ définies par :

\begin{align}
F(z) &= \int_a^z (x - t)^N \ \partial^{N + 1} f(t) \ dt \\
G(z) &= \int_a^z (x - t)^N \ dt
\end{align}

pour tout $z \in [\alpha,\beta]$, on voit que l'on peut trouver un $c \in \intervalleouvert{a}{x}$ si $a \strictinferieur x$ ou un $c \in \intervalleouvert{x}{a}$ si $x \strictinferieur a$ tel que :

$$(x - c)^N \ F(x) = (x - c)^N \ \partial^{N + 1} f(c) \ G(x)$$

ou encore :

$$\partial^{N + 1} f(c) \ G(x) = F(x)$$

Comme :

\begin{align}
G(x) = \int_a^x (x - t)^N \ dt &= - \big[ (x - x)^{N + 1} - (x - a)^{N + 1} \big] / (N + 1) \\
&= - \big[ 0 - (x - a)^{N + 1} \big] / (N + 1) \\
&= (x - a)^{N + 1} / (N + 1)
\end{align}

on a :

$$\partial^{N + 1} f(c) \ \frac{ (x - a)^{N + 1} }{N + 1} = F(x) = \int_a^x (x - t)^N \ \partial^{N + 1} f(t) \ dt$$

On en déduit que :

$$E_a^N(f)(x) = \partial^{N + 1} f(c) \ \frac{ (x - a)^{N + 1} }{(N + 1) !}$$


** Forme différentielle

Soit une fonction $f \in \continue^{N+1}([\alpha,\beta],\setR)$ et $a,x \in [\alpha,\beta]$. On définit la fonction $F : [\alpha,\beta] \mapsto \setR$ par :

\begin{align}
F(t) &= \sum_{k = 0}^N \unsur{k !} \cdot \partial^k f(t) \cdot (x - t)^k \\
&= f(t) + \partial f(t) \ (x - t) + \partial^2 f(t) \ \frac{(x - t)^2}{2} + ...
\end{align}

pour tout $t \in [\alpha,\beta]$. On a :

$$F(x) = f(x) + \partial f(x) \ (x - x) + \partial^2 f(x) \frac{(x - x)^2}{2} + ... = f(x) + 0 = f(x)$$

et :

$$F(a) = f(a) + \partial f(a) \ (x - a) + \partial^2 f(a) \frac{(x - a)^2}{2} + ... = T_a^N(f)(x)$$

La dérivée de $F$ s'écrit :

#+BEGIN_CENTER
\(
\partial F(t) = \partial f(t) + \big[ \partial f(t) \ (-1) + \partial^2 f(t) \ (x - t) \big] \\
+ \left[ - \partial^2 f(t) \ (x - t) + \partial^3 f(t) \ \frac{(x-t)^2}{2} \right] \\
... \\
+ \left[ - \partial^{N - 1} f(t) \ \frac{(x - t)^{N - 2}}{(N - 2) !} + \partial^N f(t) \ \frac{(x-t)^{N - 1}}{(N - 1) !} \right] \\
+ \left[ - \partial^N f(t) \ \frac{(x-t)^{N - 1}}{(N - 1) !} + \partial^{N + 1} f(t) \ \frac{(x-t)^N}{N !} \right]
\)
#+END_CENTER

On voit que tous les termes s'annulent sauf le dernier, et :

$$\partial F(t) = \partial^{N + 1} f(t) \ \frac{(x-t)^N}{N !}$$

Soit $G \in \continue^1([\alpha,\beta],\setR)$. On peut appliquer le théorème de Cauchy à $F$ et $G$ entre $a$ et $x$. On dispose alors d'un $c \in \intervalleouver{a}{x}$ si $a \strictinferieur x$ ou d'un $c \in \intervalleouvert{x}{a}$ si $x \strictinferieur a$ tel que :

$$\partial F(c) \ \big[G(x) - G(a)\big] = \big[F(x) - F(a)\big] \ \partial G(c)$$

On a :

$$F(x) - F(a) = f(x) - T_a^N(f)(x) = E_a^N(f)(x)$$

On en conclut que :

$$E_a^N(f)(x) \ \partial G(c) = \partial^{N + 1} f(c) \ \frac{(x-c)^N}{N !} \ \big[G(x) - G(a)\big]$$


*** Forme de Lagrange

Soit le choix :

$$G : t \mapsto (x - t)^{N + 1}$$

on a :

$$G(x) = (x - x)^{N + 1} = 0$$

et :

$$G(a) = (x - a)^{N + 1}$$

La dérivée s'écrit :

$$\partial G(t) = - (N  + 1) \ (x - t)^N$$

La relation de Cauchy devient :

$$- E_a^N(f)(x) \ (N  + 1) \ (x - c)^N = - \partial^{N + 1} f(c) \ \frac{(x-c)^N}{N !} \ (x - a)^{N + 1}$$

On a donc l'expression de l'erreur :

$$E_a^N(f)(x) = \partial^{N + 1} f(c) \ \frac{(x - a)^{N + 1}}{(N + 1) !}$$


*** Forme de Cauchy

Soit le choix :

$$G : t \mapsto t - a$$

on a :

$$G(x) = x - a$$

et :

$$G(a) = a - a = 0$$

La dérivée s'écrit :

$$\partial G(t) = 1$$

La relation de Cauchy devient :

$$E_a^N(f)(x) = \partial^{N + 1} f(c) \ \frac{(x-c)^N}{N !} \ (x - a)$$


** Borne

Soit $f \in \continue^{N + 1}([\alpha,\beta],\setR)$. Comme $\partial^{N+1} f$ est continue, sa norme $\norme{.}_\infty$ sur $[\alpha,\beta]$ est finie et on a :

$$\abs{E_a^N(f)(x)} \le \norme{\partial^{N + 1} f}_\infty \ \frac{ \abs{x - a}^{N + 1} }{(N + 1) !}$$

On peut majorer cette expression en constatant que :

$$\abs{x - a} \le \abs{\beta - \alpha}$$

La borne de l'erreur devient alors :

$$\abs{E_a^N(f)(x)} \le \norme{\partial^{N + 1} f}_\infty \ \frac{ \abs{\beta - \alpha}^{N + 1} }{(N + 1) !}$$

Le membre de droite ne dépendant pas de $x$, on a :

$$\norme{E_a^N(f)}_\infty \le \norme{\partial^{N + 1} f}_\infty \ \frac{ \abs{\beta - \alpha}^{N + 1} }{(N + 1) !}$$


** Convergence

Soit $f \in \continue^\infty([\alpha,\beta],\setR)$. Si on peut trouver un $\sigma \in \setR$ tel que :

$$\norme{\partial^n f}_\infty \le \sigma$$

pour tout $n \in \setN$, on a :

$$\norme{E_a^N(f)}_\infty \le \sigma \ \frac{ \abs{\beta - \alpha}^{N + 1} }{(N + 1) !}$$

On en conclut que :

$$0 \le \lim_{N \to \infty} \norme{E_a^N(f)}_\infty \le \sigma \ \lim_{N \to \infty} \frac{ \abs{\beta - \alpha}^{N + 1} }{(N + 1) !} = 0$$

L'erreur converge vers zéro quand $N$ tend vers l'infini :

$$\lim_{N \to \infty} \norme{E_a^N(f)}_\infty = 0$$


** Dimension $n$


*** Premier ordre

Soit $\Omega \subseteq \setR^n$, la fonction $f \in \continue^1(\Omega,\setR)$ et les vecteurs $u,v \in \setR^n$ tels que le segment $[u,v]$ est inclus dans $\Omega$. On définit la fonction $\lambda : [0,1] \mapsto \setR^n$ associée au segment $[u,v]$ par :

$$\lambda(s) = u + s \cdot (v - u)$$

pour tout $s \in [0,1]$, ainsi que la fonction $\varphi = f \circ \lambda$ qui vérifie :

$$\varphi(s) = (f \circ \lambda)(s) = f(u + s \cdot (v - u))$$

pour tout $s \in [0,1]$. On pose :

$$h = v - u$$

On a :

$$\varphi(0) = f(u)$$

La dérivée s'écrit :

$$\partial \varphi(s) = \sum_i \partial_i f(u + s \cdot h) \cdot h_i$$

ou, en utilisant la notation matricielle :

$$\partial \varphi(s) = \partial f(u + s \cdot h) \cdot h$$

On a la valeur particulière :

$$\partial \varphi(0) = \partial f(u) \cdot h$$

La dérivée seconde s'écrit :

$$\partial^2 \varphi(s) = \sum_{i,j} h_j \cdot \partial^2_{ji} f(u + s \cdot h) \cdot h_i$$

ou, en utilisant la notation matricielle :

$$\partial^2 \varphi(s) = h^\dual \cdot \partial^2 f(u + s \cdot h) \cdot h$$

Le développement du premier ordre de $\varphi$ autour de $0$ s'écrit donc :

$$\varphi(s) = f(u) + s \cdot \partial f(u) \cdot h + E_u^1(s,h)$$

avec :

$$E_u^1(s,h) = h^\dual \cdot \partial^2 f(u + c \cdot h) \cdot h \cdot \frac{(c - 0)^2}{2} =  h^\dual \cdot \partial^2 f(u + c \cdot h) \cdot h \cdot \frac{c^2}{2}$$

pour un certain $c \in \intervalleouvert{0}{s}$. Mais comme :

$$\varphi(1) = f(u + h) = f(v)$$

on en déduit le développement de $f$ :

$$f(v) = f(u) + \partial f(u) \cdot (v - u) + \mathcal{E}_u^1(h)$$

avec :

$$\mathcal{E}_u^1(h) = h^\dual \cdot \partial^2 f(u + c \cdot h) \cdot h \cdot \frac{c^2}{2}$$

pour un certain $c \in \intervalleouvert{0}{1}$.


**** Borne

Soit :

$$M^2 = \max_{i,j} \norme{\partial^2_{ij} f}_\infty$$

On a :

$$\abs{\mathcal{E}_u^1(h)} \le \unsur{2} \cdot n^2 \cdot M^2 \cdot \norme{h}^2$$


*** Second ordre

Soit $f \in \continue^3(\Omega,\setR)$. Avec les mêmes notations que précédemment, on a :

$$\partial^2 \varphi(0) = h^\dual \cdot \partial^2 f(u) \cdot h$$

La dérivée tierce de $\varphi$ s'écrit :

$$\partial^3 \varphi(s) = \sum_{i,j,k} \partial^3_{kji} f(u + s \cdot h) \cdot h_i \cdot h_j \cdot h_k$$

ou, en utilisant la notation tensorielle :

$$\partial^3 \varphi(s) = \partial^3 f(u + s \cdot h) : h \otimes h \otimes h$$

Le développement  du second ordre de $\varphi$ autour de $0$ s'écrit :

$$\varphi(s) = f(u) + s \ \partial f(u) \cdot h + \frac{s^2}{2} \ h^\dual \cdot \partial^2 f(u) \cdot h + E_u^2(s,h)$$

avec :

$$E_u^2(s,h) = \partial^3 f(u + c \cdot h) : h \otimes h \otimes h \cdot \frac{c^3}{6}$$

pour un certain $c \in \intervalleouvert{0}{s}$. Mais comme :

$$\varphi(1) = f(u + h) = f(v)$$

on en déduit le développement de $f$ :

$$f(v) = f(u) + \partial f(u) \cdot h + h^\dual \cdot \partial^2 f(u) \cdot h + \mathcal{E}_u^2(h)$$

avec :

$$\mathcal{E}_u^2(h) = \partial^3 f(u + c \cdot h) : h \otimes h \otimes h \cdot \frac{c^3}{6}$$

pour un certain $c \in \intervalleouvert{0}{1}$.


**** Borne

Soit :

$$M^3 = \max_{i,j,k} \norme{\partial^3_{ijk} f}_\infty$$

On a :

$$\abs{\mathcal{E}_u^2(h)} \le \unsur{6} \cdot n^3 \cdot M^3 \cdot \norme{h}^3$$


** Notation

Soit la fonction $E : \Omega \subseteq \setR^m \mapsto \setR^n$, la fonction $b : \setR \mapsto \setR$ et le vecteur $h \in \Omega$. On note $E \sim \petito{b(h)}$, ou on dit que $E$ est en $\petito{b(h)}$, pour signifier que :

$$\lim_{h \to 0} \frac{ \norme{E(h)} }{b(\norme{h})} = 0$$

On note $E \sim \grando{b(h)}$, ou on dit que $E$ est en $\grando{b(h)}$, pour signifier qu'il existe $M \in \setR$ tel que :

$$\norme{E(h)} \le M \cdot b(\norme{h})$$

pour tout $h \in \Omega$.


*** Puissance

Une famille de fonction souvent employée est la puissance :

$$b_k : x \mapsto x^k$$

pour un certain $k \in \setN$. On a alors $\petito{h^k}$ si :

$$\lim_{h \to 0} \frac{ \norme{E(h)} }{\norme{h}^k} = 0$$

et $\grando{h^k}$ si :

$$\norme{E(h)} \le M \cdot \norme{h}^k$$


*** Relation

Si $E \sim \grando{h^k}$, on a :

$$0 \le \lim_{h \to 0} \frac{\norme{E(h)}}{\norme{h}^{k - 1}} \le \lim_{h \to 0} \frac{M \ \norme{h}^k}{\norme{h}^{k - 1}} = 0$$

d'où :

$$\lim_{h \to 0} \frac{\norme{E(h)}}{\norme{h}^{k - 1}} = 0$$

et $E \sim \petito{h^{k - 1}}$.


*** Cas particulier

Le $\grando{1}$ implique une erreur bornée en valeur absolue, le $\petito{1}$ implique la continuité et le $\petito{h}$ la différentiabilité.


*** Développement de Taylor

Pour toute fonction $f \in \continue^{N + 1}(\Omega, \setR^n)$, l'erreur $E_a^N(f)$ du développement de Taylor d'ordre $N$ est en $\grando{h^{N+1}}$.

** Extrapolation de Richardson

Supposons qu'une fonction $v$ nous donne une approximation de $V$ respectant :

$$v(h) \approx V + C \cdot h^m + O(h^{m+1})$$

pour un certain $C \in \setR$ et pour tout $h \in [0,R] \subseteq \setR$. L'entier $m$ est appelé l'ordre de l'approximation. Supposons que l'on dispose de deux estimations de $V_1 = v(h)$ et $V_2 = v(h/k)$. On a alors :

#+BEGIN_CENTER
\(
V_1 = v(h) = V + C \cdot h^m + O(h^{m+1}) \\
V_2 = v\left(h/k\right) = V + C \cdot \left(\frac{h}{k}\right)^m
+ O(h^{m+1})
\)
#+END_CENTER

On se sert de la première équation pour obtenir une expression de $C \cdot h^m$ :

$$C \cdot h^m = V_1 - V + O(h^{m+1})$$

Posons :

$$r = \unsur{k^m}$$

On a alors :

$$V_2 = V + r \cdot C \cdot h^m + O(h^{m+1}) = V + r \cdot (V_1 - V) + O(h^{m+1})$$

On en conclut que :

$$(1 - r) \cdot V = V_2 - r \cdot V_1 + O(h^{m+1})$$

Ce qui nous donne l'approximation :

$$V = \frac{V_2 - r \cdot V_1}{1 - r} + O(h^{m+1})$$

Cette approximation est plus précise, car l'erreur n'est plus en $O(h^m)$ mais en $O(h^{m + 1})$. On appelle cette technique l'extrapolation de Richardson.


*** Cas particulier

Un cas particulier intéressant est celui où l'approximation est d'ordre $1$ et où $k = 2$. On a alors :

$$V = 2 V_2 - V_1 + O(h^2) = V_2 + (V_2 - V_1) + O(h^2)$$

ce qui revient à faire l'approximation $V - V_2 \approx V_2 - V_1$.


* Développements d'Hadamard

#+TOC: headlines 1 local

\label{chap:fonda}


** Dépendances

  - Chapitre \ref{chap:differ} : Les différentielles
  - Chapitre \ref{chap:integral} : Les intégrales


** Lemme de Hadamard

Soit la fonction $f : \setR^m \to \setR^n$ et les vecteurs $u,v \in \setR^m$. On définit la fonction $\lambda : [0,1] \mapsto \setR^m$ associée au segment $[u,v] \subseteq \setR^m$ par :

$$\lambda(s) = u + s \cdot (v - u)$$

pour tout $s \in [0,1]$. On a bien entendu $\lambda(0) = u$ et $\lambda(1) = v$. On définit également la fonction $\varphi = f \circ \lambda$ qui vérifie :

$$\varphi(s) = (f \circ \lambda)(s) = f(u + s \cdot (v - u))$$

pour tout $t \in [0,1]$. On voit que $\varphi(0) = f(u)$ et $\varphi(1) = f(v)$. Donc, en termes de composantes dans $\setR^n$, on a :

$$f_i(v) - f_i(u) = \varphi_i(1) - \varphi_i(0) = \int_0^1 \OD{\varphi_i}{s}(s) \ ds$$

où $i \in \{1,2,...,n\}$.

Voyons quelle est la forme de la dérivée :

\begin{align}
\OD{\varphi_i}{s}(s) &= \sum_j \partial_j f_i(u + s \cdot (v - u)) \cdot \partial \lambda_j(s) \\
&= \sum_j \partial_j f_i(u + s \cdot (v - u)) \cdot (v_j - u_j)
\end{align}

où $j \in \{1,2,...,m\}$. Si nous définissons :

$$G_{ij}(u,v) = \int_0^1 \partial_j f_i(u + s \cdot (v - u)) \ ds$$

nous obtenons alors l'expression de la variation :

$$f_i(v) - f_i(u) = \sum_j G_{ij}(u,v) \cdot (v_j - u_j)$$

En termes matriciels :

$$G(u,v) = \big[G_{ij}(u,v)\big]_{i,j} = \left[ \int_0^1 \partial_j f_i(u + s \cdot (v - u)) \ ds \right]_{i,j}

est donc l'intégrale de la Jacobienne :

$$G(u,v) = \int_0^1 \partial f(u + s \cdot (v - u)) \ ds$$

et :

$$f(v) - f(u) = G(u,v) \cdot (v - u)$$


** Développement du second ordre

Soit la fonction $f \in \continue^2(\setR^n,\setR)$ et les vecteurs $a,h \in \setR^n$. On définit la fonction $\lambda : [0,1] \mapsto \setR^n$ associée au segment $[a, a + h]$ :

$$\lambda(s) = a + s \cdot h$$

pour tout $s \in [0,1]$. Le lemme de Hadamard nous dit que :

$$f(a + h) - f(a) = \int_0^1 \partial f(a + s \cdot h) \cdot h \ ds$$

Par définition de la dérivée seconde, on a :

$$\partial_i f(a + s \cdot h) = \partial_i f(a) + \sum_j \partial_{ji}^2 f(a) \cdot h_j \cdot s + e_i(s \cdot h)$$

où l'erreur $e$ vérifie :

$$\lim_{h \to 0} \frac{ \norme{e(h)} }{ \norme{h} } = 0$$

L'intégrale s'écrit alors :

$$f(a + h) - f(a) = \sum_i \int_0^1 \left[ \partial_i f(a) + \sum_j \partial_{ji}^2 f(a) \cdot h_j \cdot s + e_i(s \cdot h) \right]  \cdot h_i \ ds$$

La grandeur $\partial_i f(a) \cdot h_i$ ne dépendant pas de $s$, on a :

$$\int_0^1 \partial_i f(a) \cdot h_i \ ds = \partial_i f(a) \cdot h_i \cdot (1 - 0) = \partial_i f(a) \cdot h_i$$

D'un autre coté, comme $s^2/2$ est une primitive de $s$, on a :

$$\int_0^1 s \ ds = \unsur{2} \cdot (1^2 - 0^2) = \unsur{2}$$

et donc :

$$\int_0^1 \partial_{ji}^2 f(a) \cdot h_j \cdot h_i \cdot s \ ds = \unsur{2} \partial_{ji}^2 f(a) \cdot h_j \cdot h_i$$

Posons :

$$\mathcal{E}_2(h) = \sum_i \int_0^1 e_i(s \cdot h) \cdot h_i \ ds$$

On a alors :

$$f(a + h) - f(a) = \sum_i \partial_i f(a) \cdot h_i + \unsur{2} \sum_{i,j} h_j \cdot \partial_{ji}^2 f(a) \cdot h_i + \mathcal{E}_2(h)$$

En termes matriciels, cette expression fait intervenir la Jacobienne et la Hessienne :

$$f(a + h) - f(a) = \partial f(a) \cdot h + \unsur{2} \ h^\dual \cdot \partial^2 f(a) \cdot h + \mathcal{E}_2(h)$$


*** Comportement de l'erreur

Nous savons que, pour toute précision $\epsilon \strictsuperieur 0$, nous pouvons trouver un $\delta \strictsuperieur 0$ tel que :

$$\frac{\norme{e(h)}}{\norme{h}} \le \epsilon$$

pour tout $h$ vérifiant $\norme{h} \le \delta$. Comme $\abs{e_i} \le \norme{e}$ et $\abs{h_i} \le \norme{h}$, on a alors :

\begin{align}
\abs{\mathcal{E}_2(h)} &\le \sum_i \abs{\int_0^1 e_i(s \cdot h) \cdot h_i \ ds} \\
&\le n \cdot \epsilon \cdot \norme{h}^2
\end{align}

L'erreur décroît donc plus vite que $\norme{h}^2$ :

$$\lim_{h \to 0} \frac{ \abs{\mathcal{E}_2(h)} }{ \norme{h}^2 } = 0$$


*** Dérivées ordinaires

Lorsque $n = 1$, le développement est simplement :

$$f(a + h) = f(a) + \OD{f}{x}(a) \cdot h + \OOD{f}{x}(a) \cdot \frac{h^2}{2} + \mathcal{E}_2(h)$$

On constate qu'il est analogue au développement de Taylor d'ordre deux autour de $a$.


** Développement du troisième ordre

Soit la fonction $f \in \continue^3(\setR^n,\setR)$ et les vecteurs $a,h \in \setR^n$. En évaluant le développement du second ordre de chaque $\partial_i f$, on a :

$$\partial_i f(a + s \cdot h) = \partial_i f(a) + \sum_j \partial_{ji} f(a) \cdot h_j \cdot s + \sum_{j,k} h_k \cdot \partial_{kji}^3 f(a) \cdot h_j \cdot \frac{s^2}{2} + e_i(h)$$

où $e \sim \petito{h^2}$. En intégrant, nous obtenons une estimation de la variation de $f$ :

$$f(a + h) - f(a) = \sum_i \int_0^1 \partial_i f(a + s \cdot h) \cdot h_i \ ds$$

Posons :

\begin{align}
I_1(h) &= \sum_i \int_0^1 \partial_i f(a) \cdot h_i \ ds \\
I_2(h) &= \sum_{i,j} \int_0^1 h_j \cdot \partial_{ji} f(a) \cdot h_i \cdot s \ ds \\
I_3(h) &= \unsur{2} \sum_{i,j,k} \int_0^1 h_k \cdot \partial_{kji}^3 f(a) \cdot h_j \cdot h_i \cdot s^2 \ ds \\
\mathcal{E}_3(h) &= \sum_i \int_0^1 e_i(h) \cdot h_i \ ds
\end{align}

Comme $s^3/3$ est une primitive de $s^2$, on a :

$$\int_0^1 s^2 \ ds = \unsur{3} \cdot (1^3 - 0^3) = \unsur{3}$$

Les intégrales s'écrivent donc :

\begin{align}
I_1(h) &= \sum_i \partial_i f(a) \cdot h_i \\
I_2(h) &= \unsur{2} \sum_{i,j} h_i \cdot \partial_{ji} f(a) \cdot h_j \\
I_3(h) &= \unsur{6} \sum_{i,j,k} \partial_{kji}^3 f(a) \cdot h_i \cdot h_j \cdot h_k
\end{align}

et la variation de $f$ est donnée par :

$$f(a + h) - f(a) = I_1(h) + I_2(h) + I_3(h) + \mathcal{E}_3(h)$$

En terme de notations tensorielles, on peut l'écrire symboliquement :

$$f(a + h) - f(a) = \partial f(a) \cdot h + \unsur{2} h^\dual \cdot \partial^2 f(a) \cdot h + \unsur{6} \contraction{\partial^3 f(a)}{3}{h \otimes h \otimes h}$$


*** Comportement de l'erreur

Nous savons que, pour toute précision $\epsilon \strictsuperieur 0$, nous pouvons trouver un $\delta \strictsuperieur 0$ tel que :

$$\frac{\norme{e(h)}}{\norme{h}^2} \le \epsilon$$

pour tout $h$ vérifiant $\norme{h} \le \delta$. Comme $\abs{e_i(h)} \le \norme{e(h)}$ et $\abs{h_i} \le \norme{h}$, on a :

\begin{align}
\abs{\mathcal{E}_3(h)} &\le \sum_i \abs{\int_0^1 e_i(h) \cdot h_i \ ds} \\
&\le n \cdot \epsilon \cdot \norme{h}^3
\end{align}

L'erreur $\abs{\mathcal{E}_3(h)}$ est donc en $\petito{h^3}$.


*** Dérivées ordinaires

Lorsque $n = 1$, le développement est simplement :

$$f(a + h) = f(a) + \OD{f}{x}(a) \cdot h + \OOD{f}{x}(a) \cdot \frac{h^2}{2} + \NOD{f}{x}{3} \cdot \frac{h^3}{6} + \mathcal{E}_3(h)$$

On constate qu'il est analogue au développement de Taylor d'ordre trois autour de $a$.
