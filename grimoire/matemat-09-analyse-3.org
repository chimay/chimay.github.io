
#+STARTUP: showall

#+TITLE: Eclats de vers : Matemat 09 : Analyse - 3
#+AUTHOR: chimay
#+EMAIL: or du val chez gé courriel commercial
#+LANGUAGE: fr
#+LINK_HOME: file:../index.html
#+LINK_UP: file:index.html
#+HTML_HEAD: <link rel="stylesheet" type="text/css" href="../style/defaut.css" />

#+OPTIONS: H:6
#+OPTIONS: toc:nil

#+TAGS: noexport(n)

[[file:index.org][Index des Grimoires]]

#+INCLUDE: "../include/navigan-1.org"

#+TOC: headlines 1

#+INCLUDE: "../include/commandes-tex.org"

* Symétrie

#+TOC: headlines 1 local

Soit la fonction $f \in \continue^2(\setR^2,\setR)$. Posons $\partial_x = \partial_1$ et $\partial_y = \partial_2$. Nous allons tenter d'évaluer les dérivées secondes $\partial_{xy} = \partial_{12}$ et $\partial_{yx} = \partial_{21}$. On note :

\begin{align}
\varphi_{11} &=& \varphi(x,y) \\
\varphi_{21} &=& \varphi(x + h, y) \\
\varphi_{12} &=& \varphi(x, y + h) \\
\varphi_{22} &=& \varphi(x + h, y + h)
\end{align}

où $\varphi = f$ ou une de ses dérivées. Comme $\partial_{xy} = \partial_x \partial_y$ et $\partial_{yx} = \partial_y \partial_x$, on a par définition :

#+BEGIN_CENTER
\(
\Delta_{xy} = \partial_y f_{21} - \partial_y f_{11} = \partial_{xy} f_{11} \cdot h + o(h) \\
\Delta_{yx} = \partial_x f_{12} - \partial_x f_{11} = \partial_{yx} f_{11} \cdot h + o(h)
\)
#+END_CENTER

Multiplié par $h$, cela devient :

#+BEGIN_CENTER
\(
\Delta_{xy} \cdot h = \partial_{xy} f_{11} \cdot h^2 + o(h^2) \\
\Delta_{yx} \cdot h = \partial_{yx} f_{11} \cdot h^2 + o(h^2)
\)
#+END_CENTER

On dispose également des développements d'ordre deux :

#+BEGIN_CENTER
\(
f_{22} - f_{21} = \partial_y f_{21} \cdot h + \partial_{yy} f_{21} \cdot \frac{h^2}{2} + o(h^2) \\
f_{12} - f_{11} = \partial_y f_{11} \cdot h + \partial_{yy} f_{11} \cdot \frac{h^2}{2} + o(h^2) \\
f_{22} - f_{12} = \partial_x f_{12} \cdot h + \partial_{xx} f_{12} \cdot \frac{h^2}{2} + o(h^2) \\
f_{12} - f_{11} = \partial_x f_{11} \cdot h + \partial_{xx} f_{11} \cdot \frac{h^2}{2} + o(h^2)
\)
#+END_CENTER

On en conclut que :

#+BEGIN_CENTER
\(
\Delta_{xy} \cdot h = D + \Delta_{yy} + o(h^2) \\
\Delta_{yx} \cdot h = D + \Delta_{xx} + o(h^2)
\)
#+END_CENTER

où l'on a posé :

#+BEGIN_CENTER
\(
D = f_{22} - f_{21} - f_{12} + f_{11} \\
\Delta_{yy} = (\partial_{yy} f_{11} - \partial_{yy} f_{21}) \cdot h^2 \\
\Delta_{xx} = (\partial_{xx} f_{11} -  \partial_{xx} f_{12}) \cdot h^2
\)
#+END_CENTER

Par continuité de $\partial_{xx} f$ et de $\partial_{yy} f$, on a :

#+BEGIN_CENTER
\(
\lim_{h \to 0} \frac{\Delta_{yy}}{h^2} = \lim_{h \to 0} (\partial_{yy} f_{11} - \partial_{yy} f_{21}) = 0 \\
\lim_{h \to 0} \frac{\Delta_{xx}}{h^2} = \lim_{h \to 0} (\partial_{xx} f_{11} -  \partial_{xx} f_{12}) = 0
\)
#+END_CENTER

On en conclut que $\Delta_{xx}, \Delta_{yy} \sim o(h^2)$. Comme la somme de deux erreurs en $o(h^2)$ donne également une erreur en $o(h^2)$, on a :

#+BEGIN_CENTER
\(
\Delta_{xy} \cdot h = D + o(h^2) + o(h^2) = D + o(h^2) \\
\Delta_{yx} \cdot h = D + o(h^2) + o(h^2) = D + o(h^2)
\)
#+END_CENTER

et :

#+BEGIN_CENTER
\(
\partial_{xy} f_{11} \cdot h^2 = D + o(h^2) \\
\partial_{yx} f_{11} \cdot h^2 = D + o(h^2)
\)
#+END_CENTER

On en conclut que la différence $\partial_{xy} f_{11} - \partial_{yx} f_{11} \sim o(1)$ tend vers $0$ avec $h$, ce qui n'est possible que si :

$$\partial_{xy} f_{11} = \partial_{yx} f_{11}$$

Nous avons donc prouvé que :

$$\partial_{xy} f(x,y) = \partial_{yx} f(x,y)$$


*** Généralisation

On peut bien entendu généraliser à une fonction $f \in \continue^2(\setR^n,\setR)$. On a alors :

$$\partial_{ij} f = \partial_{ji} f$$

où $i,j \in \{1,2,...,n\}$. Si $H = \partial^2 f$, on écrit aussi ce résultat sous la forme :

$$H^\dual = H$$


** Dérivation par rapport à un paramètre

Nous allons a présent examiner ce qu'il se passe lorsque les bornes de l'intervalle d'intégration ($a,b : \setR \mapsto \setR$) et la fonction à intégrer ($f : \setR \times \setR \mapsto \setR$) varient par rapport à un paramètre. Soit la fonction $I : \setR \mapsto \setR$ définie par :

$$I(t) = \int_{a(t)}^{b(t)} f(s,t) \ ds$$

Pour une valeur donnée de $t$, posons :

$$\phi_t(s) = f(s,t)$$

L'intégrale de $\phi_t$ peut s'évaluer si nous connaissons une primitive $\psi_t$ telle que :

$$\OD{\psi_t}{s}(s) = \phi_t(s)$$

Mais cette expression consiste à évaluer la variation de $\psi$ lorsque $s$ varie, $t$ étant fixé. Cela revient donc à une dérivée partielle par rapport à $s$. Donc, si nous connaissons une fonction $F$ telle que :

$$\deriveepartielle{F}{s}(s,t) = f(s,t)$$

nous pouvons réécrire l'intégrale :

$$\int_{a(t)}^{b(t)} f(s,t) \ ds = F(b(t),t) - F(a(t),t)$$

Il ne nous reste plus alors qu'à évaluer la dérivée de $I$ par rapport à $t$ en utilisant la règle des compositions de fonctions :

$$\OD{I}{t}(t) = \deriveepartielle{F}{s}(b(t),t) \cdot \OD{b}{t}(t) + \deriveepartielle{F}{t}(b(t),t) - \deriveepartielle{F}{s}(a(t),t) \cdot \OD{b}{t}(t) - \deriveepartielle{F}{t}(a(t),t)$$

Si $F \in \continue^2(\setR^2,\setR)$, la symétrie des dérivées secondes nous permet d'écrire :

$$\deriveepartielle{F}{t} = \deriveepartielle{}{t} \left[ \deriveepartielle{f}{s} \right] = \deriveepartielle{}{s} \left[ \deriveepartielle{f}{t} \right]$$

La dérivée partielle de $F$ par rapport à $t$ est donc une primitive de la dérivée partielle de $f$ par rapport à $t$. On en déduit que :

$$\int_\alpha^\beta \deriveepartielle{f}{t}(s,t) \ ds = \deriveepartielle{F}{t}(\beta,t) - \deriveepartielle{F}{t}(\alpha,t)$$

pour tout $\alpha,\beta \in \setR$. Pour un $t$ fixé quelconque, on peut poser $\alpha = a(t)$ et $\beta = b(t)$. Il vient alors :

$$\OD{I}{t}(t) = \deriveepartielle{F}{s}(b(t),t) \cdot \OD{b}{t}(t) - \deriveepartielle{F}{s}(a(t),t) \cdot \OD{b}{t}(t) + \int_{a(t)}^{b(t)} \deriveepartielle{f}{t}(s,t) \ ds$$


** Différences finies

Soit une fonction $f : \setR^2 \mapsto \setR^m$ deux fois continument dérivable. Nous allons voir comment évaluer des approximations des dérivées premières et secondes de $f$. On note $\partial_x = \partial_1$ et $\partial_y = \partial_2$. On choisit les réels $x,y$ et la variation non nulle $h \in \setR$.


*** Dérivées premières

Soustrayons les développements d'ordre deux :

#+BEGIN_CENTER
\(
f(x + h,y) \approx f(x,y) + h \cdot \partial_x f(x,y) + \frac{h^2}{2} \cdot \partial^2 f(x,y) \\
f(x - h,y) \approx f(x,y) - h \cdot \partial_x f(x,y) + \frac{h^2}{2} \cdot \partial^2 f(x,y)
\)
#+END_CENTER

On obtient :

$$f(x + h,y) - f(x - h,y) \approx 2 h \cdot \partial_x f(x,y)$$

et donc :

$$\partial_x f(x,y) \approx \frac{f(x + h, y) - f(x - h, y)}{2 h}$$

L'erreur est en $o(h) = o(h^2)/h$. En procédant de même avec $y$, on obtient :

$$\partial_y f(x,y) \approx \frac{f(x, y + h) - f(x, y - h)}{2 h}$$


*** Dérivées secondes

On additionne cette fois les mêmes développements et on obtient :

$$f(x + h,y) + f(x - h,y) \approx 2 f(x,y) + \frac{h^2}{2} \cdot \partial^2 f(x,y) + o(h^2)$$

et donc :

$$\partial_{xx}^2 f(x,y) \approx \frac{f(x + h, y) - 2 f(x,y) + f(x - h, y)}{h^2}$$

L'erreur est en $o(1) = o(h^2)/h^2$, et donc aussi petite que l'on veut pourvu que $h \ne 0$ soit suffisamment petit. En procédant de même avec $y$, on obtient :

$$\partial_{yy}^2 f(x,y) \approx \frac{f(x, y + h) - 2 f(x,y) + f(x, y - h)}{h^2}$$

On vérifie également en évaluant les développements en $(x \pm h, y \pm h)$ que :

$$\partial_{xy}^2 f(x,y) \approx \frac{f(x + h, y + h) - f(x + h, y - h) - f(x - h, y + h) + f(x - h, y - h)}{h^2}$$

La dernière dérivée seconde s'évalue approximativement par $\partial_{yx}^2 f(x,y) = \partial_{xy}^2 f(x,y)$.


*** Généralisation

Nous allons voir comment généraliser ces résultats aux dérivées $\partial_{ij}$ d'une fonction $F : \setR^n \mapsto \setR^m$. Soit $u \in \setR^n$ et les vecteurs de la base canonique $\canonique_i \in \setR^n$. On définit les fonctions $f_{ij} : \setR^n \mapsto \setR^m$ par :

$$f_{ij}(x,y) = F(u + x \cdot \canonique_i + y \cdot \canonique_j)$$

On a clairement :

\begin{align}
\partial_i F(u) &=& \partial_x f_{ij}(0,0) \\
\partial_{ii} F(u) &=& \partial_{xx} f_{ij}(0,0) \\
\partial_{ij} F(u) &=& \partial_{xy} f_{ij}(0,0) \\
\partial_{jj} F(u) &=& \partial_{yy} f_{ij}(0,0)
\end{align}

Il suffit donc d'utiliser les méthodes d'approximations des dérivées de $f_{ij}$ pour approximer les dérivées de $F$.


* Distributions

#+TOC: headlines 1 local

\label{chap:distribu}


** Dépendances

  - Chapitre \ref{chap:relation} : Les fonctions
  - Chapitre \ref{chap:lineaire} : Les fonctions linéaires


** Formes et fonctions

On peut toujours associer une forme linéaire $\varphi$ à une fonction intégrable quelconque $\hat{\varphi}$ en définissant :

$$\forme{\varphi}{u} = \int_A u(x) \cdot \hat{\varphi}(x) \ dx$$

Inversément, on ne pourra pas toujours trouver une fonction $\hat{\varphi}$ correspondant à une forme linéaire $\varphi$ donnée. On définira malgré tout l'intégrale généralisée en notant :

$$\int_A u(x)  \cdot \varphi(x) \ dx = \forme{\varphi}{u}$$

où il ne faut pas perdre de vue que $\varphi$ n'est pas nécessairement une fonction.


** Formes et mesures

Soit $u : A \mapsto \setR$. A toute mesure $\mu$, on peut associer une forme linéaire $\hat{\mu}$ par :

$$\forme{ \hat{\mu} }{u} = \int_A u(x) \ d\mu(x)$$

Inversément, à toute forme linéaire $\hat{\mu}$, on peut associer une fonction
$\mu : \sousens(\setR) \mapsto \setR$ par :

$$\mu(A) = \forme{ \hat{\mu} }{\indicatrice_A}$$

Toutefois, rien ne garantit que la fonction $\mu$ ainsi définie est une mesure. En particulier, rien ne garantit qu'elle soit positive.


** Fonction et forme bilinéaire

A toute fonction $\hat{K} : A \times B \mapsto F$, on peut associer une forme bilinéaire $K$ par :

$$\biforme{u}{K}{v} = \int_{A \times B} u(x) \cdot K(x,y) \cdot v(y) \ d\mu(x) \ d\nu(y)$$

pour toutes fonctions $u,v : A \mapsto B$. Inversément, à toute forme bilinéaire $K$, on peut associer une intégrale généralisée en notant :

$$\int_{A \times B} u(x) \cdot K(x,y) \cdot v(y) \ d\mu(x) \ d\nu(y) = \biforme{u}{K}{v}$$


** Définition

Nous nous intéressons ici au cas où l'espace vectoriel $E$ est un ensemble de fonctions intégrables : $E = \mathcal{F} \subseteq \lebesgue^2(\setR,\setR)$. Les limites à l'infini doivent alors forcément s'annuler

$$\lim_{x \to +\infty} u(x) = \lim_{x \to -\infty} u(x) = 0$$

pour tout $u \in \mathcal{F}$.


** Delta de Dirac

La distribution $\dirac \in F^\dual$ de Dirac est définie par :

$$\forme{\dirac}{u} = u(0)$$

pour tout $u \in F$. Elle correspond bien sûr à l'intégrale :

$$\int_\setR \dirac(x) \cdot u(x) \ dx = u(0)$$

On remarque que :

$$\int_{A^2} \dirac(\xi - x) \cdot K(\xi,\eta) \cdot \dirac(\eta - y) \ d\mu(\xi) \ d\nu(\eta) = K(x,y)$$


** Dérivée

En intégrant par parties, on a :

#+BEGIN_CENTER
\(
\int_{\setR} \OD{u}{x}(x) \cdot v(x) \ dx =
\lim_{a \to +\infty} \left[ u(a) \cdot v(a) - u(-a) \cdot v(-a) \right]
- \int_{\setR} u(x) \cdot \OD{v}{x}(x) \ dx
\)
#+END_CENTER

mais comme les limites à l'infini s'annulent, cette expression se réduit à :

$$\int_{\setR} \OD{u}{x}(x) \cdot v(x) \ dx = - \int_{\setR} u(x) \cdot \OD{v}{x}(x) \ dx$$

Par extension, on définit la dérivée $\OD{u}{x}$ d'une distribution $u$ par :

$$\forme{\OD{u}{x}}{v} = - \forme{u}{\OD{v}{x}}$$

pour tout $v\in F$.


*** Échelon

Comme application, considérons la fonction échelon $e_+$ :

#+BEGIN_CENTER
\(
e_+(x) = \indicatrice_{[0,+\infty)} =
\begin{cases}
1 & \mbox{si  } t \ge 0 \\
0 & \mbox{si  } t < 0
\end{cases}
\)
#+END_CENTER

Pour tout $v\in F$, on a :

\begin{align}
\forme{\OD{e_+}{x}}{v} &=& - \forme{e_+}{\OD{v}{x}} \\
&=& - \int_0^{+\infty} \OD{v}{x}(x) dx
\end{align}

Appliquons à présent le théorème fondamental. Il vient :

$$\forme{\OD{e_+}{x}}{v} = - \left[\lim_{x \to +\infty} v(x) - v(0)\right] = v(0)$$

On en déduit que :

$$\OD{e_+}{x} = \dirac$$

au sens des distributions.


** Dilatation

Soit $d_a$ l'opérateur de dilatation :

$$d_a(u)(x) = u(a \cdot x)$$

où $a > 0$ est un réel strictement positif.

Le changement de variable $\xi = a \cdot x$ nous donne $d\xi = a \ dx$ et donc :

$$\int_{\setR} \hat{u}(a \ x) \ v(x) \ dx = \unsur{a} \int_{\setR} \hat{u}(\xi) \ v\left( \xi/a \right) \ d\xi$$

On définit donc l'extension de cet opérateur aux distributions par :

$$\forme{d_a(u)}{v} = \unsur{a} \forme{u}{d_{1/a}(v)}$$


** Réflexion

L'opérateur de réflexion $r$ se définit par :

$$r(u)(x) = u(-x)$$

Le changement de variable $\xi = -x$ nous donne $d\xi = -dx$ et donc :

\begin{align}
\int_{\setR} \hat{u}(-x) \ v(x) \ dx &=& \lim_{a \to +\infty}\int_{-a}^a \hat{u}(-x) \ v(x) \ dx \\
&=& \lim_{a \to +\infty} - \int_a^{-a} \hat{u}(\xi) \ v(-\xi) \ d\xi \\
&=& \lim_{a \to +\infty} \int^{-a}_a \hat{u}(\xi) \ v(-\xi) \ d\xi
\end{align}

On définit donc l'extension de cet opérateur aux distributions par :

$$\forme{r(u)}{v} = \forme{u}{r(v)}$$


** Translation

L'opérateur de translation $t_a$ est défini par :

$$t_a(u)(x) = u(x - a)$$

Le changement de variable $\xi = x - a$ nous donne $d\xi = dx$ et donc :

$$\int_{\setR} \hat{u}(x-a) v(x) dx = \int_{\setR} \hat{u}(\xi) v(\xi+a) d\xi$$

On définit donc les extensions de cet opérateur aux distributions par :

$$\forme{t_a(u)}{v} = \forme{u}{t_{-a}(v)}$$


** Convolution

Les intégrales unidimensionnelles permettent de définir l'opérateur de convolution
$\convolution$. Soit deux fonctions $u, v : \setR \mapsto \setR$, leur convolution
est une fonction $u \convolution v : \setR \mapsto \setR$ définie par :

$$(u \convolution v)(t) = \int_{-\infty}^{+\infty} u(t-s) \ v(s) \ ds$$

pour tout $t \in \setR$.


*** Dirac

En utilisant les résultats ci-dessus, on arrive facilement à :

$$\int_{\setR} u(x) \ \dirac(x-a) \ dx = u(a)$$

Comme :

$$\int_{\setR} u(x) \ \dirac(-x) \ dx = \int_{\setR} u(-x) \ \dirac(x) \ dx = u(0)$$

on en déduit que $\dirac(-x) = \dirac(x)$ et :

$$\int_{\setR} \dirac(x-y) \ u(y)  \ dy = u(x)$$

c'est-à-dire :

$$\dirac \convolution u = u$$

La distribution de Dirac est neutre pour le produit de convolution.
On peut montrer que ce neutre est unique.


** Corrélation

Les intégrales unidimensionnelles permettent de définir l'opérateur de corrélation
$\correlation$. Soit deux fonctions $u, v : \setR \mapsto \setR$, leur corrélation
est une fonction $u \correlation v : \setR \mapsto \setR$ définie par :

$$(u \correlation v)(t) = \int_{-\infty}^{+\infty} u(s+t) \ v(s) \ ds$$

pour tout $t \in \setR$.


* Formes différentielles

#+TOC: headlines 1 local

\label{chap:formedif}


** Dépendances

  - Chapitre \ref{chap:differ} : Les différentielles
  - Chapitre \ref{chap:integral} : Les intégrales


** Intégrale d'un tenseur

Soit $(\canonique_1,\canonique_2,...,\canonique_n)$ la bace canonique de $\setR^n$ et la fonction tensorielle $T : A \mapsto \tenseur_m(\setR^n)$ qui, à chaque $x \in A$ associe un tenseur $T(x)$ de la forme :

$$T(x) = \sum_{i,j,...,p} t_{ij...p}(x) \cdot \canonique_i \otimes \canonique_j \otimes ... \otimes \canonique_p$$

L'intégrale de cette fonction est définie par :

$$\int_A T(x) \ d\mu(x) = \sum_{i,j,...,p} I_{ij...p} \cdot \canonique_i \otimes \canonique_j \otimes ... \otimes \canonique_p$$

où chaque coordonnée $I_{ij...p}$ est l'intégrale de la coordonnée correspondante de $T$ :

$$I_{ij...p} = \int_A t_{ij...p}(x) \ d\mu(x)$$


** Produit extérieur

Soit $d\mu = dx = dx_1 \ ... \ dx_n$ la mesure de Lebesgue sur $\setR^n$. On sait que $dx$ représente la mesure de l'élément de volume $[x_1,x_1 + dx_1] \times ... \times [x_n,x_n + dx_N]$. Etant donné que nous avons construit le produit extérieur pour représenter (au signe près) des mesures de surfaces et de volumes, il est tout à fait naturel de le faire intervenir dans une mesure de Lebesgue. Soit la base canonique $(e_1,e_2,...,e_n)$ de $\setR^n$ et les vecteurs :

$$\delta_i = dx_i \cdot e_i$$

où les $dx_i$ sont bien évidemment des scalaires. Si nous évaluons le produit extérieur des ces vecteurs, nous obtenons :

$$\delta_1 \wedge \delta_2 \wedge ... \wedge \delta_n = \sum_{i,j,...,k} \permutation_{ij...k} \cdot dx_1 \cdot \delta_{1i} \cdot dx_2 \cdot \delta_{2j} \hdots \cdot dx_n \cdot \delta_{nk}$$

Le seul terme ne s'annulant pas étant $\permutation_{1,2,...,n} = 1$, on a finalement :

$$\delta_1 \wedge \delta_2 \wedge ... \wedge \delta_n = dx_1 \cdot dx_2 \cdot ... \cdot dx_n = dx$$

Cette constatation nous amène à définir une mesure plus générale. Considérons à présent des vecteurs infinitésimaux $\upsilon_1,\upsilon_2,...,\upsilon_n \in \setR^n$, c'est à dire des vecteurs dont la norme tendra vers zéro dans l'intégrale. Afin de garantir la positivité de la mesure, nous définissons :

$$du = \abs{\upsilon_1 \wedge \upsilon_2 \wedge ... \wedge \upsilon_n}$$


** Tenseur différentiel

Il est même possible de définir des tenseurs différentiels $dU$ en choisissant $m \le n$ et en posant :

$$dU = \upsilon_1 \wedge \upsilon_2 \wedge ... \wedge \upsilon_m$$

Il est clair que $dU \in \tenseur_{n - m}(\setR^n)$. On nomme ce type de tenseur une forme différentielle.


** Paramétrisation

Soit $m,n \in \setN$ avec $m \le n$ et la fonction $\phi : U \subseteq \setR^m \mapsto \setR^n$, dérivable et inversible. Le but est de paramétrer $x$ sur $\phi(U)$ par la relation $x = \phi(u)$ pour tout $u \in U$. Nous utilisons la base canonique $(e_1,e_2,...,e_m)$ de $\setR^m$ et nous posons :

$$\delta_i = \phi(u + du_i \ e_i) - \phi(u) = \partial_i \phi(u) \ du_i$$

Sur $\phi(U)$, on utilise le tenseur différentiel :

$$dX = \delta_1 \wedge \delta_2 \wedge ... \wedge \delta_m$$

La linéarité du produit extérieur nous permet d'ecrire :

\begin{align}
dX &=& \partial_1 \phi(u) \wedge \partial_2 \phi(u) \wedge ... \wedge \partial_m \phi(u) \ du_1 \ du_2 \ ... \ du_m \\
&=& \partial_1 \phi(u) \wedge \partial_2 \phi(u) \wedge ... \wedge \partial_m \phi(u) \ du
\end{align}

On définit le tenseur $W \in \tenseur_{n - m}(\setR^n)$ associé à $dX$ par :

$$W(u) = \partial_1 \phi(u) \wedge \partial_2 \phi(u) \wedge ... \wedge \partial_m \phi(u)$$

Deux cas peuvent alors se présenter.


*** Fonction tensorielle

On peut évaluer l'intégrale d'une fonction tensorielle $f : \setR^n \mapsto \tenseur_p(\setR^n)$ en utilisant la contraction maximale avec $dX$. Comme on a l'équivalence $x \in \phi(U) \leftrightarrow u \in U$, on a alors :

$$\int_{\phi(U)} f(x) : dX = \int_U (f\circ\phi)(u) : W(u) \ du$$

Dans le cas particulier où $p = n - m$, on obtiendra un scalaire.


*** Fonction scalaire

On peut évaluer l'intégrale d'une fonction scalaire $f : \setR^n \mapsto \setR$ en utilisant la norme de $dX$. On a alors $dx = \norme{dX}$ et :

$$\int_{\phi(U)} f(x) dx = \int_U (f\circ\phi)(u) \cdot \norme{W(u)} \ du$$


*** Pavé

Un cas particulier important est celui où $U = [\alpha_1,\beta_1] \times .. \times [\alpha_m,\beta_m]$ pour certains $\alpha_i,\beta_i \in \setR$. On a alors :

$$\int_{\phi(U)} \sim \int_{\alpha_1}^{\beta_1} du_1 \int_{\alpha_2}^{\beta_2} du_2 \ ... \int_{\alpha_m}^{\beta_m} du_m$$


** Changement de variable

Nous considérons à présent le cas où $m = n$. Nous utilisons la base canonique $(e_1,e_2,...,e_n)$ de $\setR^n$ et nous posons de nouveau :

$$\delta_i = \phi(u + du_i \ e_i) - \phi(u) = \partial_i \phi(u) \ du_i$$

On utilise la mesure :

$$dx = \abs{\delta_1 \wedge \delta_2 \wedge ... \wedge \delta_n}$$

On a alors :

\begin{align}
dx &=& \abs{\partial_1 \phi(u) \wedge \partial_2 \phi(u) \wedge ... \wedge \partial_n \phi(u) \ du} \\
&=& \abs{\sum_{i,j,...,k} \permutation_{ij...k} \cdot \partial_1 \phi_i(u) \cdot \partial_2 \phi_j(u) \cdot \ \hdots \ \cdot \partial_n \phi_k(u)} \ du \\
&=& \abs{\det \partial \phi(u)} \ du
\end{align}

On voit donc apparaître le déterminant de la Jacobienne de $\phi$. Comme on a l'équivalence $x \in A \leftrightarrow u \in \phi^{-1}(A)$, le changement de variable peut s'écrire :

$$\int_A f(x) \ dx = \int_{\phi^{-1}(A)} (f\circ\phi)(u) \cdot \abs{ \det \partial \phi(u) } \ du$$


*** Pavé

Un cas particulier important est celui où $\phi^{-1}(A) = [\alpha_1,\beta_1] \times .. \times [\alpha_n,\beta_n]$ pour certains $\alpha_i,\beta_i \in \setR$. On a alors :

$$\int_A \sim \int_{\alpha_1}^{\beta_1} du_1 \int_{\alpha_2}^{\beta_2} du_2 \ ... \int_{\alpha_n}^{\beta_n} du_n$$


** Intégrales de ligne vectorielles

Soit une fonction continue $\gamma : [a,b] \to \setR^n$ définissant la courbe $\Lambda = \gamma([a,b])$.  L'intégrale de ligne d'une fonction $f : \setR^n \mapsto \setR^n$ sur cette courbe est l'intégrale de la contraction d'ordre $1$ de $f$ avec $d\gamma$, qui revient ici au produit scalaire du vecteur $f(x) \in \Lambda$ par le vecteur $\partial \gamma(t)$. On a donc :

$$\int_\Lambda f\cdot d\Lambda = \int_a^b \scalaire{(f \circ \gamma)(t)}{\partial \gamma(t) } dt$$


** Intégrales de ligne scalaires

Dans le cas d'une fonction $g : \setR^n \mapsto \setR$, on utilise comme mesure la longueur $\norme{\partial \gamma(t)}$ de chaque petit segment $d\Lambda$. On a alors :

$$\int_\Lambda g \ d\Lambda = \int_a^b (g \circ \gamma)(t) \cdot \norme{\partial \gamma(t)} \ dt$$


** Contour fermé

Si $\gamma(a) = \gamma(b)$, on dit que le contour fermé, et on note en général :

$$\oint_\Lambda = \int_\Lambda$$


** Intégrales de surface vectorielles

Soit $f : \setR^n \mapsto \setR^n$, $\sigma : A \subseteq \setR^{n - 1} \mapsto \setR^n$ et la surface $\Theta = \sigma(A)$. On définit les vecteurs :

$$\delta_i = \deriveepartielle{\sigma}{u_i} du_i$$

pour $i = 1, ..., n - 1$. L'intégrale de surface est simplement la contraction d'ordre $1$ :

$$\int_\Theta f \cdot d\Theta = \int_A \scalaire{(f \circ \sigma)(u)}{ \delta_1 \wedge ... \wedge \delta_{n-1} }$$

qui nous donne un scalaire. Dans le cas particulier où $n = 3$ et où $A = [U_1,U_2] \times [V_1,V_2]$, on a :

$$\int_\Theta f \cdot d\Theta = \int_{U_1}^{U_2} du \ \int_{V_1}^{V_2} (f \circ \sigma)(u,v) \cdot \left( \deriveepartielle{\sigma}{u}(u,v) \wedge \deriveepartielle{\sigma}{v}(u,v) \right) \ dv$$


** Intégrales de surface scalaires

Soit $f : \setR^n \mapsto \setR^n$, $\sigma : A \subseteq \setR^{n - 1} \mapsto \setR^n$ et la surface $\Theta = \sigma(A)$. On définit les vecteurs :

$$\delta_i = \deriveepartielle{\sigma}{u_i} du_i$$

pour $i = 1, ..., n - 1$. Utilisant comme mesure la norme du produit extérieur des $\delta_i$, on obtient :

$$\int_\Theta f \ d\Theta = \int_A (f \circ \sigma)(u) \cdot \norme{ \delta_1 \wedge ... \wedge \delta_{n-1} }$$

Dans le cas particulier où $n = 3$ et où $A = [U_1,U_2] \times [V_1,V_2]$, on a :

$$\int_\Theta f \ d\Theta = \int_{U_1}^{U_2} du \ \int_{V_1}^{V_2} (f \circ \sigma)(u,v) \cdot \norme{ \deriveepartielle{\sigma}{u}(u,v) \wedge \deriveepartielle{\sigma}{v}(u,v) } \ dv$$


** Intégrale de flux

Soit $A \subseteq \setR^n$ et la fonction $a : \setR^n \to \setR$ telle que :

$$A = \{ x \in \setR^n : a(x) \le 0 \}$$

On s'arrange de plus pour avoir $a$ constante sur la frontière :

$$\partial A = \{ x \in \setR^n : a(x) = 0 \}$$

On introduit le vecteur normal :

$$n = \unsur{\norme{\deriveepartielle{a}{x}}} \cdot \deriveepartielle{a}{x}$$

L'intégrale du flux sortant de la fonction $f : \setR^n \to \setR^n$ est alors donnée par :

$$\int_{\partial A} \scalaire{f}{n} \ d\mu$$


** Différentielle

Soit une fonction $f : \setR^n \mapsto \setR$, les vecteurs infinitésimaux $\delta_1,...,\delta_{n - 1} \in \setR^n$ et la forme différentielle :

$$\omega = f \cdot \delta_1 \wedge \delta_2 \wedge ... \wedge \delta_{n-1}$$

Si $f$ est différentiable, on définit la différentielle de $\omega$ par :

$$d\omega = \sum_i \deriveepartielle{f}{x_i} \cdot \kappa_i \wedge \delta_1 \wedge \delta_2 \wedge ... \wedge \delta_{n-1}$$

où :

$$\kappa_i = dx_i \cdot e_i$$

On note aussi symboliquement :

$$d\omega = df \wedge dx_1 \wedge ... \wedge dx_{n-1}$$

On peut montrer sous certaines conditions que l'intégrale
sur la frontière de $A$ est alors donnée par :

$$\int_{\partial A} \omega = \int_A d\omega$$


** Théorème de Stokes

Soit $f,g : \setR^2 \mapsto \setR$ et les vecteurs infinitésimaux :

#+BEGIN_CENTER
\(
\delta x = e_1 \ dx \\
\delta y = e_2 \ dy
\)
#+END_CENTER

Considérons la forme différentielle :

$$\omega = f \delta x + g \delta y$$

Si les fonctions sont différentiables, on a alors :

$$d\omega = \deriveepartielle{f}{x} \delta x \wedge \delta x +  \deriveepartielle{f}{y} \delta y \wedge \delta x + \deriveepartielle{g}{x} \delta x \wedge \delta y +  \deriveepartielle{g}{y} \delta y \wedge \delta y$$

Mais comme :

#+BEGIN_CENTER
\(
\delta x \wedge \delta x = \delta y \wedge \delta y = 0 \\
\delta y \wedge \delta x = - \delta x \wedge \delta y
\)
#+END_CENTER

il vient :

$$d\omega =  \left( \deriveepartielle{g}{x} - \deriveepartielle{f}{y} \right) \delta x \wedge \delta y$$

En intégrant, on obtient alors :

$$\int_{\partial A} (f \ \delta x + g \ \delta y)  = \int_A \left( \deriveepartielle{g}{x} - \deriveepartielle{f}{y} \right) dx \wedge dy$$

Mais comme nous somme dans la base canonique, on a $\delta x \wedge \delta y = dx \ dy$ et :

$$\int_{\partial A} (f \ \delta x + g \ \delta y)  = \int_A \left( \deriveepartielle{g}{x} - \deriveepartielle{f}{y} \right) \ dx \ dy$$
