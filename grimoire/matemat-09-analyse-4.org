
#+STARTUP: showall

#+TITLE: Eclats de vers : Matemat 09 : Analyse - 4
#+AUTHOR: chimay
#+EMAIL: or du val chez gé courriel commercial
#+LANGUAGE: fr
#+LINK_HOME: file:../index.html
#+LINK_UP: file:index.html
#+HTML_HEAD: <link rel="stylesheet" type="text/css" href="../style/defaut.css" />

#+OPTIONS: H:6
#+OPTIONS: toc:nil

#+TAGS: noexport(n)

[[file:index.org][Index des Grimoires]]

#+INCLUDE: "../include/navigan-1.org"

#+TOC: headlines 1

#+INCLUDE: "../include/commandes-tex.org"

* Géométrie différentielle

#+TOC: headlines 1 local

\label{chap:geometri}


** Dépendances

  - Chapitre \ref{chap:vecteur} : Les vecteurs
  - Chapitre \ref{chap:ps} : Les produits scalaires
  - Chapitre \ref{chap:tenseur} : Les tenseurs


** Indices covariants et contravariants

Les indices inférieurs (le $i$ des vecteurs $a_i^j$ par exemple) des
tenseurs sont appelés /indices covariants/.

Les indices supérieurs (le $j$ des vecteurs $a_i^j$ par exemple) des
tenseurs sont appelés /indices contravariants/.

Ne pas confondre ces /indices supérieurs/ contravariants , très
utilisés en calcul tensoriel, avec les puissances ! Dans le contexte
des tenseurs, une éventuelle puissance d'un scalaire $\theta_i^j$
serait notée au besoin par :

$$\big( \theta_j^i \big)^m = \theta_j^i \cdot ... \cdot \theta_j^i$$


** Coordonnées curvilignes

Soit l'espace vectoriel $E = \setR^n$ sur $\setR$. Les coordonnées curvilignes sont basées sur la notion de position $r$, exprimée comme une fonction de certains paramètres $x \in \setR^n$ que nous appelons « coordonnées » de $r$ :

$$r = \rho(x)$$

où $\rho : \setR^n \to \setR^n$. Nous envisageons également le cas du changement de variable. La position dépend alors d'un autre jeu de coordonnées $y \in \setR^n$ :

$$r = \sigma(y)$$

où $\sigma : \setR^n \to \setR^n$. Nous définissons les vecteurs fondamentaux $e_i$ et $e^i$ au moyen de ces fonctions :

#+BEGIN_CENTER
\(
e_i(x) = \deriveepartielle{\rho}{x^i}(x) \\
e^i(y) = \deriveepartielle{\sigma}{y_i}(y)
\)
#+END_CENTER

de telle sorte que :

$$dr = \sum_i e_i \ dx^i  = \sum_i e^i \ dy_i$$

Nous supposons que $(e_1, ..., e_n)$ et $(e^1, ..., e^n)$ sont des bases de $E$.


*** Courbe

Dans le cas où $x$ et $y$ ne dépendent que d'un paramètre $t \in \setR$, on a :

$$\OD{r}{t} = \sum_i e_i \ \OD{x^i}{t}  = \sum_i e^i \ \OD{y_i}{t}$$

On dit alors que la position $r$ décrit une courbe.


** Changement de variable

Si les fonctions $\rho$ et $\sigma$ sont inversibles, on a :

#+BEGIN_CENTER
\(
x = \rho^{-1}(r) = (\rho^{-1} \circ \sigma)(y) = \phi(y) \\
y = \sigma^{-1}(r) = (\sigma^{-1} \circ \rho)(x) = \psi(x)
\)
#+END_CENTER

où nous avons implicitement défini $\phi = \rho^{-1} \circ \sigma$ et $\psi = \sigma^{-1} \circ \rho$. Nous notons $\deriveepartielle{x^i}{y_j}$ et $\deriveepartielle{y_i}{x^j}$ les coordonnées des dérivées de $\phi$ et $\psi$ suivant les bases formées par les $e_i$ et les $e^i$ :

#+BEGIN_CENTER
\(
\deriveepartielle{\phi}{y_j} = \sum_i \deriveepartielle{x^i}{y_j} \ e_i \\
\deriveepartielle{\psi}{x^j} = \sum_i \deriveepartielle{y_i}{x^j} \ e^i
\)
#+END_CENTER

La composition des dérivées nous donne les relations :

#+BEGIN_CENTER
\(
e_i = \sum_j \deriveepartielle{r}{y_j} \ \deriveepartielle{y_j}{x^i} =  \sum_j \deriveepartielle{y_j}{x^i} \ e^j \\
e^i = \sum_j \deriveepartielle{r}{x^j} \ \deriveepartielle{x^j}{y_i} =  \sum_j \deriveepartielle{x^j}{y_i} \ e_j
\)
#+END_CENTER

qui nous permettent de relier les $e_i$ aux $e^j$ et inversément.


** Produit scalaire

Les produits intérieurs entre vecteurs de base se notent habituellement :

#+BEGIN_CENTER
\(
g_{ij} = \scalaire{e_i}{e_j} \\
g_i^j = \scalaire{e_i}{e^j} \\
g^{ij} = \scalaire{e^i}{e^j}
\)
#+END_CENTER

Il est clair d'après les propriétés de symétrie de ce produit que :

#+BEGIN_CENTER
\(
g_{ij} = g_{ji} \\
g^{ij} = g^{ji} \\
g_i^j = g_j^i
\)
#+END_CENTER

Le produit scalaire de deux vecteurs $a,b\in E$ définis par :

#+BEGIN_CENTER
\(
a = \sum_i a^i \ e_i = \sum_i a_i \ e^i \\
b = \sum_i b^i \ e_i = \sum_i b_i \ e^i
\)
#+END_CENTER

peut s'écrire indifféremment comme :

#+BEGIN_CENTER
\(
\scalaire{a}{b} = \sum_{i,j} g_{ij} \ a^i \ b^j \\
\scalaire{a}{b} = \sum_{i,j} g^{ij} \ a_i \ b_j \\
\scalaire{a}{b} = \sum_{i,j} g_j^i \ a_i \ b^j \\
\scalaire{a}{b} = \sum_{i,j} g_i^j \ a^i \ b_j
\)
#+END_CENTER

Et en particulier, la longeur $ds$ d'un changement de position $dr$ vérifie :

$$(ds)^2 = \scalaire{dr}{dr} = \sum_{i,j} g_{ij} \ dx^i \ dx^j = \sum_{i,j} g^{ij} \ dy_i \ dy_j$$

De plus, les relations entre les vecteurs $e_i$ et les vecteurs $e^i$ permettent
de déduire, en utilisant la linéarité du produit scalaire :

#+BEGIN_CENTER
\(
g_{ij} = \sum_k \deriveepartielle{y_k}{x^i} \ g_j^k = \sum_{k,l} \deriveepartielle{y_k}{x^i} \ \deriveepartielle{y_l}{x^j} \ g^{kl} \\
g^{ij} = \sum_k \deriveepartielle{x_k}{y^i} \ g_k^j = \sum_{k,l} \deriveepartielle{x^k}{y_i} \ \deriveepartielle{x^l}{y_j} \ g_{kl}
\)
#+END_CENTER


** Dérivées primales d'un vecteur

Nous allons à présent voir comment évolue un vecteur $a\in E$, que l'on note sous la forme :

$$a = \sum_i a^i \ e_i$$

où les coordonnées $a^i\in\setR$ tout comme les vecteurs de base $e_i$ dépendent des coordonnées
$x^i$. La règle de dérivation d'un produit nous donne :

$$da = \sum_i da^i \ e_i + \sum_k a^k \ de_k$$

La différentielle $da^i$ s'obtient directement :

$$da^i = \sum_j \deriveepartielle{a^i}{x^j} \ dx^j$$

On peut suivre la même règle avec $de_i$ :

$$de_k = \sum_j \deriveepartielle{e_k}{x^j} \ dx^j$$

Les symboles de Christoffel $\christoffel{i}{kj}$ sont définis comme
les coordonnées de $\deriveepartielle{e_k}{x^j}$ suivant la base $(e_1, ..., e_n)$ :

$$\deriveepartielle{e_k}{x^j} = \sum_i \christoffel{i}{kj} \ e_i$$

Notons que comme :

$$\deriveepartielle{e_k}{x^j} = \dfdxdy{r}{x^j}{x^k} = \dfdxdy{r}{x^k}{x^j} = \deriveepartielle{e_j}{x^k}$$

on a la symétrie :

$$\christoffel{i}{kj} = \christoffel{i}{jk}$$

On peut évaluer ces symboles si on connait par exemple les valeurs des :

\begin{align}
\scalaire{e^i}{ \deriveepartielle{e_k}{x^j} } &= \sum_m \christoffel{m}{kj} \ \scalaire{e^i}{e_m} \\
&= \sum_m \christoffel{m}{kj} \ g^i_m
\end{align}

On a alors, pour chaque choix de $k,j$ un système linéaire à résoudre. Il suffit d'inverser
la matrice $G = (g^i_m)_{i,m}$ pour obtenir les valeurs des symboles.

La dérivation d'un vecteur $a\in E$ s'écrit alors :

$$da = \sum_{i,j} e_i \ dx^j \ \left[ \deriveepartielle{a^i}{x^j} + \sum_k \christoffel{i}{kj} \ a^k  \right]$$

On définit les coordonnées :

$$\gradient_j a^i = \deriveepartielle{a^i}{x^j} + \sum_k \christoffel{i}{kj} \ a^k$$

Dans le cas où les coordonnées dépendent d'un paramètre $t\in\setR$, on a :

\begin{align}
\OD{a}{t} &= \sum_{i,j} e_i \ \OD{x^j}{t} \ \left[ \deriveepartielle{a^i}{x^j} + \sum_k \christoffel{i}{kj} \ a^k  \right] \\
&= \sum_{i} e_i \ \left[ \OD{a^i}{t} + \sum_{j,k} \christoffel{i}{kj} \ a^k \ \OD{x^j}{t} \right]
\end{align}


*** Dérivée seconde et géodésique

Considérons le cas :

$$a = \OD{r}{t} = \sum_i e_i \ \OD{x^i}{t}$$

Les coordonnées de $a$ sont clairement $a^i = \OD{x^i}{t}$ et la dérivée seconde :

$$\OOD{r}{t} = \OD{}{t}\OD{r}{t} = \OD{a}{t}$$

s'écrit :

$$\OOD{r}{t} = \sum_{i} e_i \ \left[ \OOD{x^i}{t} + \sum_{j,k} \christoffel{i}{kj} \ \OD{x^k}{t}\ \OD{x^j}{t} \right]$$

Les courbes $x^i = x^i(t)$ vérifiant $\OOD{r}{t} = 0$ sont appelées des géodésiques.


** Dérivées duales d'un vecteur

Nous allons recommencer le même processus, écrivant cette fois  $a\in E$ sous la forme :

$$a = \sum_i a_i \ e^i$$

Les coordonnées $a_i\in\setR$ tout comme les vecteurs de base $e^i$ dépendent des coordonnées
$y_i$. En suivant la même méthode que ci-dessus, on obtient :

$$da = \sum_{i,j} e^i \ dy_j \left[ \deriveepartielle{a_i}{y_j} + \sum_k \christoffel{kj}{i} \ a_k  \right]$$


où l'on a introduit de nouveaux symboles de Christoffel, définis par :

$$\deriveepartielle{e^k}{y_j} = \sum_i \christoffel{kj}{i} \ e^i$$

Ces nouveaux symboles présentent la symétrie :

$$\christoffel{kj}{i} = \christoffel{jk}{i}$$


** Dérivées d'un tenseur

On étend simplement la notion de dérivée aux tenseurs en appliquant
la formule :

$$d(a \otimes b) = da \otimes b + a \otimes db$$

où $a$ et $b$ sont deux tenseurs d'ordre quelconque.
Par exemple, pour le tenseur :

$$T = \sum_{i,j} T^i_j \ e_i \otimes e^j$$

on a :

$$dT = \sum_{i,j} \left[ dT^i_j \ e_i \otimes e^j + T^i_j \ de_i \otimes e^j + T^i_j \ e_i \otimes de^j \right]$$

qui devient, en introduisant les symboles de Christoffel :

$$dT = \sum_{i,j} e_i \otimes e^j \left[ dT^i_j + \sum_{k,m} \christoffel{i}{mk} \ T^m_j \ dx^k + \sum_{k,m} \christoffel{mk}{j} \ T^i_m \ dy_k \right]$$


** Produit scalaire et symboles de Christoffel

Lorsqu'on différentie les $g_{ij}$, on obtient :

\begin{align}
dg_{ij} &= \scalaire{de_i}{e_j} + \scalaire{e_i}{de_j} \\
&= \sum_{k,l} \christoffel{k}{il} \ g_{kj} \ dx^l + \sum_{k,l} \christoffel{k}{jl} \ g_{ik} \ dx^l
\end{align}

On en déduit que :

$$\deriveepartielle{g_{ij}}{x^l} = \sum_k \christoffel{k}{il} \ g_{kj} + \sum_k \christoffel{k}{jl} \ g_{ik}$$

Définissons :

$$\gamma_{ijl} = \sum_k \christoffel{k}{il} \ g_{kj}$$

Les propriétés de symétrie des symboles de Christoffel nous montrent que :

$$\gamma_{ijl} = \gamma_{ilj}$$

Et comme (changement de l'indice $l$ en $k$) :

$$\deriveepartielle{g_{ij}}{x^k} = \gamma_{ijk} + \gamma_{jik}$$

On en déduit :

\begin{align}
\deriveepartielle{g_{ij}}{x^k} + \deriveepartielle{g_{jk}}{x^i} - \deriveepartielle{g_{ki}}{x^j}
&= 2 \ \gamma_{jik} \\
&= 2 \sum_l \christoffel{l}{jk} \ g_{li}
\end{align}

AFAIRE : LA FIN DU CHAPITRE EST A DÉBROUILLONNER


** Bases biorthonormées


*** produit scalaire

Nous considérons tout au long de cette section le cas particulier où les bases
sont biorthonormées, c'est-à-dire :

$$g_i^j = \indicatrice_i^j$$

On déduit des relations liant les $g^{ij},g_{ij}$ aux $g_i^j$ que :

#+BEGIN_CENTER
\(
g_{ik} g^{kj} = \sum_{k,l,m} \deriveepartielle{y_l}{x^i}\deriveepartielle{x^m}{y_k} g_k^l g_m^j \\
g_{ik} g^{kj} = \sum_{k,l,m} \deriveepartielle{y_l}{x^i}\deriveepartielle{x^m}{y_k} \indicatrice_k^l \indicatrice_m^j \\
g_{ik} g^{kj} = \sum_{k} \deriveepartielle{y_k}{x^i}\deriveepartielle{x^j}{y_k} \\
g_{ik} g^{kj} = \deriveepartielle{x^i}{x^j} = \indicatrice_i^j
\)
#+END_CENTER

On aurait de même :

$$g^{ik} g_{kj} = \indicatrice_j^i$$


*** Coordonnées

Les coordonnées d'un tenseur de la forme :

$$T = \sum_{i,j,k,l} T_{i...j}^{k...l} e^i \otimes ... \otimes e^j \otimes e_k \otimes ... \otimes e_l$$

où il y a $m$ indices $i...j$ et $n$ indices $k...l$ s'obtiennent facilement
en utilisant la contraction double :

$$T_{i...j}^{k...l} = \dblecont{e_j \otimes ... \otimes e_i}{m}{T}{n}{e^l \otimes ... \otimes e^k}$$


** Dérivées des changements de variable

#+BEGIN_CENTER
\(
\deriveepartielle{x^i}{y_j} = \scalaire{e_i}{ \deriveepartielle{\phi}{y_j} } \\
\deriveepartielle{y_i}{x^j} = \scalaire{e^i}{ \deriveepartielle{\psi}{x^j} }
\)
#+END_CENTER


*** Christoffel

Tenant compte de cette identité, l'équation reliant les symboles de Christoffel
aux produits scalaires devient :

$$\christoffel{m}{jk} = \frac{1}{2}\sum_i g^{im}\left[\deriveepartielle{g_{ij}}{x^k} + \deriveepartielle{g_{jk}}{x^i} - \deriveepartielle{g_{ki}}{x^j}\right]$$


*** Dérivée d'un vecteur

La relation :

$$d\scalaire{e^i}{e_j} = d\indicatrice_i^j = 0$$

nous conduit à :

#+BEGIN_CENTER
\(
\scalaire{de^i}{e_j}+\scalaire{e^i}{de_j} = 0 \\
\sum_{k,l} \christoffel{ik}{l} \scalaire{e^l}{e_j} dy_k +
\sum_{k,l} \christoffel{l}{jk} \scalaire{e^i}{e_l} dx^k = 0 \\
\sum_k \christoffel{ik}{l} dy_k = - \sum_k \christoffel{l}{jk} dx^m
\)
#+END_CENTER

Par ailleurs :

$$da_i = \deriveepartielle{a_i}{y_j} dy_j = \deriveepartielle{a_i}{x^j} dx^j$$

On peut donc réexprimer la dérivée duale comme :

$$da = \sum_{i,j} e^i dx^j \left[ \deriveepartielle{a_i}{x^j} - \sum_k \christoffel{k}{ij} a_k \right]$$


*** Gradient

On peut également définir le gradient d'un vecteur par :

$$\gradient a = \sum_{i,j} \gradient_j a^i e_i \otimes e^j$$

de telle sorte que l'on ait :

$$da = \scalaire{\gradient a}{dr} = \gradient a \cdot dr$$



*** Dérivée d'un tenseur

$$dT = \sum_{i,j,k} e_i \otimes e^j dx^k \left[ \deriveepartielle{T^i_j}{x^k} + \sum_m \christoffel{i}{mk} T^m_j - \sum_m \christoffel{m}{jk} T^i_m \right]$$

On définit alors les coordonnées :

$$\gradient_k T^i_j = \deriveepartielle{T^i_j}{x^k} + \sum_m \christoffel{i}{mk} T^m_j - \sum_m \christoffel{m}{jk} T^i_m$$


*** Tenseur de courbure

Appliquons la formule de dérivation des coordonnées d'un tenseur dans le cas particulier où :

$$T^i_j = \gradient_j a^i$$

On a :

#+BEGIN_CENTER
\(
\deriveepartielle{T^i_j}{x^k} = \dfdxdy{a^i}{x^j}{x^k}
+ \sum_m \christoffel{i}{jm} \deriveepartielle{a^m}{x^k}
+ \sum_m \deriveepartielle{}{x^k}\christoffel{i}{jm} a^m \\ \\

\sum_l \christoffel{i}{kl} T^l_j =
\sum_l \christoffel{i}{kl} \deriveepartielle{a^i}{x^j} +
\sum_{l,m} \christoffel{i}{kl} \christoffel{l}{jm} a^m \\ \\

-\sum_l \christoffel{l}{jk} T^i_l =
-\sum_l \christoffel{l}{jk} \deriveepartielle{a^i}{x^l} -
\sum_{l,m} \christoffel{l}{jk} \christoffel{i}{lm} a^m
\)
#+END_CENTER

La somme de tous ces termes vaut $\gradient_k T^i_j = \gradient_k \gradient_j a^i$.
En interchangeant les indices $j$ et $k$, on obtient $\gradient_j \gradient_k a^i$.
On en déduit, en utilisant les propriétés de symétrie que :

$$\gradient_k \gradient_j a^i - \gradient_j \gradient_k a^i = \sum_m R^i_{m,kj} a^m$$

où les $R_{...}^{...}$ sont définis par :

$$R^i_{m,kj} = \deriveepartielle{}{x^k}\christoffel{i}{jm} - \deriveepartielle{}{x^j}\christoffel{i}{km} + \sum_l \christoffel{i}{kl}\christoffel{l}{jm} - \sum_l \christoffel{i}{jl}\christoffel{l}{km}$$

Ce sont les coordonnées du tenseur de courbure de Riemann-Christoffel.


* L'espace vectoriel des polynômes

#+TOC: headlines 1 local

\label{chap:vectpoly}


** Introduction

AFAIRE : ARRANGER LE CHAPITRE

Il est clair d'après la définition des polynômes que les espaces
$\mathcal{P}_n$ sont des espaces vectoriels pour l'ensemble des
scalaires $S=\setR$ et que :

$$\mathcal{P}_n = \ev{\mu_0,\mu_1,...,\mu_n}$$

Nous allons montrer que $(\mu_0,\mu_1,...,\mu_n)$ forme
une base de $\mathcal{P}_n$. Pour cela, il nous reste à prouver
l'indépendance linéaire des $\mu_i$ :

$$\sum_{i=0}^n a_i \mu_i = 0 \quad\Rightarrow\quad a_0 = a_1 = ... = a_n = 0$$

c'est-à-dire :

$$\sum_{i=0}^n a_i x^i = 0 \quad\forall x \in\setR \quad\Rightarrow\quad a_0 = a_1 = ... = a_n = 0$$

Nous allons le montrer par récurrence.

Comme $\mu_0=1$ on a évidemment :

$$a_0 1 = 0 \quad\Rightarrow\quad a_0 = 0$$

et la thèse est vraie pour $n=0$. Supposons à présent qu'elle
soit vraie pour $n-1$. Choisissons $p\in\mathcal{P}_n$ :

$$p(x) = \sum_{i=0}^n a_i x^i$$

et supposons que $p(x) = 0$ pour tout $x\in\setR$. Comme $p(0)=0$,
on a :

$$a_0 = 0$$

donc :

$$p(x) = \sum_{i=1}^n a_i x^i = x q(x) = 0$$

où l'on à définit $q\in\mathcal{P}_{n-1}$ par :

$$q(x) = \sum_{i=1}^n a_i x^{i-1}$$

Il est clair que, pour tout $x\ne 0$, $q(x) = 0$. Mais comme les polynômes
sont des fonctions continues, on a :

$$q(0) = \lim_{ \substack{ x \rightarrow 0 \\ x \ne 0 } } q(x) = 0$$

Donc $q$ s'annule également en $0$. On en conclut que $q(x)$ est
nul pour tout $x\in\setR$. Par l'hypothèse de récurrence, les
coefficients de ce polynôme sont tous nuls :

$$a_1 = a_2 = ... = a_n = 0$$

Rassemblant les résultats, il vient :

$$a_0 = a_1 = ... = a_n = 0$$

et $(\mu_0,\mu_1,...,\mu_n)$ forme bien une base de $\mathcal{P}_n$.


** Polynômes orthogonaux

Nous allons à présent voir comment construire des suites de polynômes
orthogonaux pour le produit scalaire :

$$\scalaire{p}{q} = \int_a^b p(x) q(x) d\mu(x)$$

ou, lorsque c'est possible :

$$\scalaire{p}{q} = \int_a^b p(x) q(x) w(x) dx$$


*** Récurrence

On pourrait bien entendu partir de la suite de la base canonique
de monômes $(1,x,x^2,...,x^n)$ et l'orthogonaliser en utilisant le
procédé de Gram-Schmidt, mais on peut arriver à un algorithme plus
rapide en utilisant les propriétés des polynômes. Soit $(\phi_n)_n$
une suite de polynômes orthonormés, où $\phi_i$ est de degré $i$.
On a donc :

$$\scalaire{\phi_m}{\phi_n} = \int_A \phi_m(x) \phi_n(x) d\mu(x) = \delta_{mn}$$

Supposons que $(\phi_0,...,\phi_n)$ forme une base de $\mathcal{P}_n$.
On peut vérifier que $(\phi_0,...,\phi_n,x\phi_n)$ forme une base de
$\mathcal{P}_{n+1}$. On peut donc représenter $\phi_{n+1}$ comme :

$$\phi_{n+1}(x) = a_n x\phi_n(x) + b_n \phi_n(x) + c_n \phi_{n-1}(x) + \sum_{i=0}^{n-2} d_i \phi_i(x)$$

Soit $i \in \{0, ..., n-2\}$. La condition d'orthogonalité de
$\phi_{n+1}$ avec $\phi_i$ s'écrit :

$$\scalaire{\phi_i}{\phi_{n+1}} = a_n \scalaire{\phi_i}{x\phi_n} + b_n \scalaire{\phi_i}{\phi_n} + c_n \scalaire{\phi_i}{\phi_{n-1}} + \sum_{j=0}^{n-2} d_j \scalaire{\phi_i}{\phi_j} = 0$$

L'orthogonalité implique que :

#+BEGIN_CENTER
\(
\scalaire{\phi_i}{\phi_n} = \scalaire{\phi_i}{\phi_{n-1}} = 0 \\
\scalaire{\phi_i}{\phi_j} = \delta_{ij}
\)
#+END_CENTER

On a aussi :

$$\scalaire{\phi_i}{x\phi_n} = \int_A x \phi_i(x) \phi_n(x) d\mu(x) = \scalaire{x\phi_i}{\phi_n}$$

Mais comme $\phi_i$ est de degré $i$, $x\phi_i$ est de degré $i+1$
et on peut l'exprimer comme :

$$x \phi_i = \sum_{j=0}^{i+1} \alpha_i \phi_j$$

Le produit scalaire devient alors :

$$\scalaire{\phi_i}{x\phi_n} = \sum_{j=0}^{i+1} \alpha_i \scalaire{\phi_j}{\phi_n} = 0$$

puisque $j \le i+1 < n$. On en conclut que :

$$\scalaire{\phi_i}{\phi_{n+1}} = \sum_{j=0}^{n-2} d_j \delta_{ij} = d_i = 0$$

Les conditions :

#+BEGIN_CENTER
\(
\scalaire{\phi_{n+1}}{\phi_n} = 0 \\
\scalaire{\phi_{n+1}}{\phi_{n-1}} = 0
\)
#+END_CENTER

impliquent respectivement que :

#+BEGIN_CENTER
\(
b_n = -a_n\scalaire{\phi_n}{x\phi_n} \\
c_n = -a_n\scalaire{\phi_{n-1}}{x\phi_n}
\)
#+END_CENTER

La condition de normalisation :

$$\scalaire{\phi_{n+1}}{\phi_{n+1}} = a_n \scalaire{x\phi_n}{\phi_{n+1}} = 1$$

nous donne alors la valeur de $a_n$ :

#+BEGIN_CENTER
\(
a_n^2 \scalaire{x\phi_n}{x\phi_n}} -
a_n^2 \scalaire{x\phi_n}{\phi_n}^2 -
a_n^2 \scalaire{x\phi_n}{\phi_{n-1}}^2 = 1 \\
a_n = \left[\scalaire{x\phi_n}{x\phi_n} -
\scalaire{x\phi_n}{\phi_n}^2 -
\scalaire{x\phi_n}{\phi_{n-1}}^2\right]^{-1/2}
\)
#+END_CENTER

On voit donc que le choix du produit scalaire détermine :

#+BEGIN_CENTER
\(
\phi_0 = \unsur{\sqrt{\scalaire{1}{1}}} \\
\phi_1 = a_1 (x - \scalaire{\phi_0}{x} \phi_0)
\)
#+END_CENTER

ainsi que toute la suite de polynômes.


*** Approximation

Soit une suite de polynômes orthonormaux $(\phi_0,...\phi_n)$
pour le produit scalaire :

$$\scalaire{u}{v} = \int_A u(x) v(x) d\mu(x)$$

Nous cherchons l'approximation de $u$ :

$$w(x) = \sum_{i=0}^n w_i \phi_i(x)$$

qui minimise l'erreur au sens intégral :

$$\scalaire{u-w}{u-w} = \int_A [u(x)-w(x)]^2 d\mu(x)$$

sur $\mathcal{P}_n$. Imposant que la dérivée par rapport aux $w_i$ soit nulle, on
obtient :

$$2 \int_A \phi_i(x) [u(x)-w(x)] d\mu(x) = 0$$

Mais comme :

$$w_i = \int_A \phi_i(x) w(x) d\mu(x)$$

on obtient :

$$w_i = \int_A \phi_i(x) u(x) d\mu(x) = \scalaire{\phi_i}{u}$$

Ce qui n'a rien d'étonnant au vu  des résultats du chapitre \ref{chap:vector}.
On peut vérifier facilement que la hessienne de l'erreur par rapport aux $w_i$
est bien positive. L'approximation ainsi définie :

#+BEGIN_CENTER
\(
w(x) = \sum_{i=0}^n \phi_i(x) \int_A \phi_i(y) u(y) d\mu(y) \\
w(x) = \sum_{i=0}^n \int_A \phi_i(x) \phi_i(y) u(y) d\mu(y)
\)
#+END_CENTER

minimise donc bien l'erreur sur l'ensemble des polynômes de degré $n$.


*** Intégration de Gauss

Soit une suite de polynômes orthonormaux $(\phi_0,...\phi_n)$
pour le produit scalaire :

$$\scalaire{u}{v} = \int_A u(x) v(x) d\mu(x)$$

Considérons la formule d'intégration :

$$I(f) = \sum_{i=0}^n w_i f(x_i)$$

supposée approximer l'intégrale :

$$\langle f \rangle = \scalaire{f}{1} = \int_A f(x) d\mu(x)$$

Fixons les points $x_0 < x_1 < ... < x_n$ et imposons que
la formule soit exacte pour $\phi_0,...,\phi_n$. On a :

$$\langle \phi_k \rangle = \sum_{i=0}^n w_i \phi_k(x_i)$$

où $k = 0,1,...,n$. Définissant les matrices et vecteurs :

#+BEGIN_CENTER
\(
\varphi = (\langle \phi_k \rangle)_k \\
W  = (w_i)_i \\
\Phi = \left(\phi_i(x_j)\right)_{i,j}
\)
#+END_CENTER

ces conditions se ramènent à :

$$\Phi W = \varphi$$

Si la matrice $\Phi(n+1,n+1)$ est inversible, on a alors :

$$W = \Phi^{-1} \varphi$$

La formule est alors valable pour tout polynôme de $\mathcal{P}_n$.
Notons que

$$\langle \phi_k \rangle = \unsur{\phi_0} \scalaire{\phi_k}{\phi_0}$$

s'annule pour tout $k\ne 0$. Si les racines de $\phi_{n+1}$ sont
toutes distinctes, on peut choisir les $x_i$ tels que :

$$\phi_{n+1}(x_i) = 0$$

On a alors :

$$\langle \phi_{n+1} \rangle = I(\phi_{n+1}) = 0$$

et la formule devient valable sur $\mathcal{P}_{n+1}$. Mieux,
considérons un polynôme $p$ de degré $n+m+1$ où $m \ge 0$ et sa
division euclidienne par $\phi_{n+1}$. On a :

$$p(x) = q(x) \phi_{n+1}(x) + r(x)$$

Comme $q$ est de degré $m$, on a :

$$q = \sum_{i=0}^m q_i \phi_i$$

Si $m \le n$, on a donc :

$$\langle q \phi_{n+1} \rangle = \sum_{i=0}^n q_i \scalaire{\phi_i}{\phi_{n+1}} = 0$$

et :

$$\langle p \rangle = \langle r \rangle = I(r)$$

puisque $r$ est de degré $n$ au plus. Comme $\phi_{n+1}$ s'annule en
les $x_i$, on a aussi ;

$$I(p) = I(r)$$

Rassemblant tout ces résultats, on obtient :

$$\int_A f(x) d\mu(x) = \sum_{i=0}^n w_i f(x_i)$$

pour tout polynôme $f\in\mathcal{P}_{2n+1}$. En pratique, on utilise
ces formules d'intégration pour des fonctions qui ne sont pas forcément
des polynômes.


** Legendre

Les polynômes de Legendre sont orthogonaux pour le produit scalaire :

$$\int_{-1}^1 P_n(x) P_m(x) dx = \frac{2}{2 n + 1} \delta_{mn}$$

Ils obéissent à la récurrence :

#+BEGIN_CENTER
\(
P_0(x) = 1 \\
P_1(x) = x \\
(n+1) P_{n+1}(x) = (2 n + 1) x P_n(x) - n P_{n-1}(x)
\)
#+END_CENTER


** Interpolation

Un problème d'interpolation consiste à trouver les coefficients :
$a_i\in\setR$ tels que la fonction :

$$u = \sum_{i=1}^n a_i u_i$$

où les $u_i$ sont des polynômes de degré $n$, vérifie :

$$\form{\phi_i}{u} = y_i$$

pour tout $i=1,2,...,n$, où les $\phi_i$ sont des formes linéaires de $\mathcal{P}_N^D$
et les $y_i$ des réels donnés.

On utilise couramment des bases biorthogonales :

$$\form{\phi_i}{u_j} = \delta_{ij}$$

et on a alors simplement :

$$a_i = \form{\phi_i}{u}$$

L'exemple le plus courant est :

#+BEGIN_CENTER
\(
\form{\phi_i}{u} = u(x_i) \\
y_i = f(x_i)
\)
#+END_CENTER

pour une certaine fonction $f$ à interpoler. Les conditions ci-dessus
se résument alors à l'égalité de $f$ et de $u$ en un nombre fini de points :

$$u(x_i) = f(x_i)$$

On rencontre parfois aussi le cas :

#+BEGIN_CENTER
\(
\form{\phi_i}{u} = \OD{u}{x}(x_i) \\
y_i = \OD{f}{x}(x_i)
\)
#+END_CENTER


*** Lagrange

Les polynômes de Lagrange $\Lambda_i$ sont biorthogonaux aux formes :

$$\form{\phi_i}{u} = u(x_i)$$

On a donc :

$$\form{\phi_j}{\Lambda_i} = \Lambda_i(x_j) = \delta_{ij}$$

Le polynôme $\Lambda_i$ doit donc s'annuler en tout les points $x_j$,
où $j \ne i$. On peut donc le factoriser comme :

$$\Lambda_i(x) = A_i \prod_{j \in E_i} (x-x_j) = A_i P_i(x)$$

où $E_i = \{ 1,2,...,n \} \setminus \{i\}$. Mais comme $\Lambda_i(x_i) = 1$, on a :

$$A_i = \unsur{P_i(x_i)}$$

et :

$$\Lambda_i(x) = \prod_{j \in E_i} \frac{(x-x_j)}{(x_i - x_j)}$$

Donc si on souhaite construire un polynôme :

$$w(x) = \sum_{i=1}^{n} u_i \Lambda_i(x)$$

qui interpole $u$ en les $x_i$ :

$$u(x_i) = w(x_i)$$

pour tout $i = 1,2,...,n$, il faut et il suffit de prendre :

$$u_i = \form{\phi_i}{u} = u(x_i)$$


*** Newton

L'interpolation de Newton utilise des polynômes construit récursivement à partir
des polynômes de degré inférieur. Soit $f$ la fonction à interpoler, $p_{i,j}$ le polynôme
de degré $j-i$ :

$$p_{ij}(x) = \sum_{j=0}^{j-i} a_k x^k$$

tels que :

$$p_{ij}(x_k) = f(x_k)$$

pour tous $k\in\{i,i+1,...,j\}$. On voit que si $i=j$, on a :

$$p_{ii} = f(x_i)$$

Pour $i < j$, on peut construire les $p_{i,j}$ par récurrence. On vérifie
que :

$$p_{ij}(x) = \frac{(x-x_i)p_{i+1,j}(x)-(x-x_j)p_{i,j-1}(x)}{x_j-x_i}$$

satisfait bien aux conditions d'interpolation ci-dessus.
