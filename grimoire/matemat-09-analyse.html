<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN"
"http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" lang="fr" xml:lang="fr">
<head>
<!-- 2025-10-20 lun 17:55 -->
<meta http-equiv="Content-Type" content="text/html;charset=utf-8" />
<meta name="viewport" content="width=device-width, initial-scale=1" />
<title>Eclats de vers : Matemat 09 : Analyse</title>
<meta name="author" content="chimay" />
<meta name="generator" content="Org Mode" />
<style type="text/css">
  .title  { text-align: center;
             margin-bottom: .2em; }
  .subtitle { text-align: center;
              font-size: medium;
              font-weight: bold;
              margin-top:0; }
  .todo   { font-family: monospace; color: red; }
  .done   { font-family: monospace; color: green; }
  .priority { font-family: monospace; color: orange; }
  .tag    { background-color: #eee; font-family: monospace;
            padding: 2px; font-size: 80%; font-weight: normal; }
  .timestamp { color: #bebebe; }
  .timestamp-kwd { color: #5f9ea0; }
  .org-right  { margin-left: auto; margin-right: 0px;  text-align: right; }
  .org-left   { margin-left: 0px;  margin-right: auto; text-align: left; }
  .org-center { margin-left: auto; margin-right: auto; text-align: center; }
  .underline { text-decoration: underline; }
  #postamble p, #preamble p { font-size: 90%; margin: .2em; }
  p.verse { margin-left: 3%; }
  pre {
    border: 1px solid #e6e6e6;
    border-radius: 3px;
    background-color: #f2f2f2;
    padding: 8pt;
    font-family: monospace;
    overflow: auto;
    margin: 1.2em;
  }
  pre.src {
    position: relative;
    overflow: auto;
  }
  pre.src:before {
    display: none;
    position: absolute;
    top: -8px;
    right: 12px;
    padding: 3px;
    color: #555;
    background-color: #f2f2f299;
  }
  pre.src:hover:before { display: inline; margin-top: 14px;}
  /* Languages per Org manual */
  pre.src-asymptote:before { content: 'Asymptote'; }
  pre.src-awk:before { content: 'Awk'; }
  pre.src-authinfo::before { content: 'Authinfo'; }
  pre.src-C:before { content: 'C'; }
  /* pre.src-C++ doesn't work in CSS */
  pre.src-clojure:before { content: 'Clojure'; }
  pre.src-css:before { content: 'CSS'; }
  pre.src-D:before { content: 'D'; }
  pre.src-ditaa:before { content: 'ditaa'; }
  pre.src-dot:before { content: 'Graphviz'; }
  pre.src-calc:before { content: 'Emacs Calc'; }
  pre.src-emacs-lisp:before { content: 'Emacs Lisp'; }
  pre.src-fortran:before { content: 'Fortran'; }
  pre.src-gnuplot:before { content: 'gnuplot'; }
  pre.src-haskell:before { content: 'Haskell'; }
  pre.src-hledger:before { content: 'hledger'; }
  pre.src-java:before { content: 'Java'; }
  pre.src-js:before { content: 'Javascript'; }
  pre.src-latex:before { content: 'LaTeX'; }
  pre.src-ledger:before { content: 'Ledger'; }
  pre.src-lisp:before { content: 'Lisp'; }
  pre.src-lilypond:before { content: 'Lilypond'; }
  pre.src-lua:before { content: 'Lua'; }
  pre.src-matlab:before { content: 'MATLAB'; }
  pre.src-mscgen:before { content: 'Mscgen'; }
  pre.src-ocaml:before { content: 'Objective Caml'; }
  pre.src-octave:before { content: 'Octave'; }
  pre.src-org:before { content: 'Org mode'; }
  pre.src-oz:before { content: 'OZ'; }
  pre.src-plantuml:before { content: 'Plantuml'; }
  pre.src-processing:before { content: 'Processing.js'; }
  pre.src-python:before { content: 'Python'; }
  pre.src-R:before { content: 'R'; }
  pre.src-ruby:before { content: 'Ruby'; }
  pre.src-sass:before { content: 'Sass'; }
  pre.src-scheme:before { content: 'Scheme'; }
  pre.src-screen:before { content: 'Gnu Screen'; }
  pre.src-sed:before { content: 'Sed'; }
  pre.src-sh:before { content: 'shell'; }
  pre.src-sql:before { content: 'SQL'; }
  pre.src-sqlite:before { content: 'SQLite'; }
  /* additional languages in org.el's org-babel-load-languages alist */
  pre.src-forth:before { content: 'Forth'; }
  pre.src-io:before { content: 'IO'; }
  pre.src-J:before { content: 'J'; }
  pre.src-makefile:before { content: 'Makefile'; }
  pre.src-maxima:before { content: 'Maxima'; }
  pre.src-perl:before { content: 'Perl'; }
  pre.src-picolisp:before { content: 'Pico Lisp'; }
  pre.src-scala:before { content: 'Scala'; }
  pre.src-shell:before { content: 'Shell Script'; }
  pre.src-ebnf2ps:before { content: 'ebfn2ps'; }
  /* additional language identifiers per "defun org-babel-execute"
       in ob-*.el */
  pre.src-cpp:before  { content: 'C++'; }
  pre.src-abc:before  { content: 'ABC'; }
  pre.src-coq:before  { content: 'Coq'; }
  pre.src-groovy:before  { content: 'Groovy'; }
  /* additional language identifiers from org-babel-shell-names in
     ob-shell.el: ob-shell is the only babel language using a lambda to put
     the execution function name together. */
  pre.src-bash:before  { content: 'bash'; }
  pre.src-csh:before  { content: 'csh'; }
  pre.src-ash:before  { content: 'ash'; }
  pre.src-dash:before  { content: 'dash'; }
  pre.src-ksh:before  { content: 'ksh'; }
  pre.src-mksh:before  { content: 'mksh'; }
  pre.src-posh:before  { content: 'posh'; }
  /* Additional Emacs modes also supported by the LaTeX listings package */
  pre.src-ada:before { content: 'Ada'; }
  pre.src-asm:before { content: 'Assembler'; }
  pre.src-caml:before { content: 'Caml'; }
  pre.src-delphi:before { content: 'Delphi'; }
  pre.src-html:before { content: 'HTML'; }
  pre.src-idl:before { content: 'IDL'; }
  pre.src-mercury:before { content: 'Mercury'; }
  pre.src-metapost:before { content: 'MetaPost'; }
  pre.src-modula-2:before { content: 'Modula-2'; }
  pre.src-pascal:before { content: 'Pascal'; }
  pre.src-ps:before { content: 'PostScript'; }
  pre.src-prolog:before { content: 'Prolog'; }
  pre.src-simula:before { content: 'Simula'; }
  pre.src-tcl:before { content: 'tcl'; }
  pre.src-tex:before { content: 'TeX'; }
  pre.src-plain-tex:before { content: 'Plain TeX'; }
  pre.src-verilog:before { content: 'Verilog'; }
  pre.src-vhdl:before { content: 'VHDL'; }
  pre.src-xml:before { content: 'XML'; }
  pre.src-nxml:before { content: 'XML'; }
  /* add a generic configuration mode; LaTeX export needs an additional
     (add-to-list 'org-latex-listings-langs '(conf " ")) in .emacs */
  pre.src-conf:before { content: 'Configuration File'; }

  table { border-collapse:collapse; }
  caption.t-above { caption-side: top; }
  caption.t-bottom { caption-side: bottom; }
  td, th { vertical-align:top;  }
  th.org-right  { text-align: center;  }
  th.org-left   { text-align: center;   }
  th.org-center { text-align: center; }
  td.org-right  { text-align: right;  }
  td.org-left   { text-align: left;   }
  td.org-center { text-align: center; }
  dt { font-weight: bold; }
  .footpara { display: inline; }
  .footdef  { margin-bottom: 1em; }
  .figure { padding: 1em; }
  .figure p { text-align: center; }
  .equation-container {
    display: table;
    text-align: center;
    width: 100%;
  }
  .equation {
    vertical-align: middle;
  }
  .equation-label {
    display: table-cell;
    text-align: right;
    vertical-align: middle;
  }
  .inlinetask {
    padding: 10px;
    border: 2px solid gray;
    margin: 10px;
    background: #ffffcc;
  }
  #org-div-home-and-up
   { text-align: right; font-size: 70%; white-space: nowrap; }
  textarea { overflow-x: auto; }
  .linenr { font-size: smaller }
  .code-highlighted { background-color: #ffff00; }
  .org-info-js_info-navigation { border-style: none; }
  #org-info-js_console-label
    { font-size: 10px; font-weight: bold; white-space: nowrap; }
  .org-info-js_search-highlight
    { background-color: #ffff00; color: #000000; font-weight: bold; }
  .org-svg { }
</style>
<link rel="stylesheet" type="text/css" href="../style/defaut.css" />
<script>
  window.MathJax = {
    tex: {
      ams: {
        multlineWidth: '85%'
      },
      tags: 'ams',
      tagSide: 'right',
      tagIndent: '.8em'
    },
    chtml: {
      scale: 1.0,
      displayAlign: 'center',
      displayIndent: '0em'
    },
    svg: {
      scale: 1.0,
      displayAlign: 'center',
      displayIndent: '0em'
    },
    output: {
      font: 'mathjax-modern',
      displayOverflow: 'overflow'
    }
  };
</script>

<script
  id="MathJax-script"
  async
  src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js">
</script>
</head>
<body>
<div id="content" class="content">
<h1 class="title">Eclats de vers : Matemat 09 : Analyse</h1>
<p>
<a href="index.html">Index des Grimoires</a>
</p>

<p>
<a href="../index.html">Retour à l’accueil</a>
</p>

<div id="table-of-contents" role="doc-toc">
<h2>Table des matières</h2>
<div id="text-table-of-contents" role="doc-toc">
<ul>
<li><a href="#orgeb183e7">1. Théorème de Rolle</a></li>
<li><a href="#orgb24d4c4">2. Théorème fondamental</a></li>
<li><a href="#orge0053f7">3. Développements de Taylor</a></li>
<li><a href="#orgd915616">4. Développements d'Hadamard</a></li>
<li><a href="#org4a38a8f">5. Symétrie</a></li>
<li><a href="#org8395fa0">6. Distributions</a></li>
<li><a href="#org26746a3">7. Formes différentielles</a></li>
<li><a href="#orgca17a42">8. Géométrie différentielle</a></li>
<li><a href="#org33dc391">9. L'espace vectoriel des polynômes</a></li>
</ul>
</div>
</div>

<p>
\(
\newcommand{\parentheses}[1]{\left(#1\right)}
\newcommand{\crochets}[1]{\left[#1\right]}
\newcommand{\accolades}[1]{\left\{#1\right\}}
\newcommand{\ensemble}[1]{\left\{#1\right\}}
\newcommand{\identite}{\mathrm{Id}}
\newcommand{\indicatrice}{\boldsymbol{\delta}}
\newcommand{\dirac}{\delta}
\newcommand{\moinsun}{{-1}}
\newcommand{\inverse}{\ddagger}
\newcommand{\pinverse}{\dagger}
\newcommand{\topologie}{\mathfrak{T}}
\newcommand{\ferme}{\mathfrak{F}}
\newcommand{\img}{\mathbf{i}}
\newcommand{\binome}[2]{
\left\{ \begin{array}{c}
#1 \\
#2 \\
\end{array} \right\}
}
\newcommand{\canonique}{\mathfrak{c}}
\newcommand{\tenseuridentite}{\boldsymbol{\mathcal{I}}}
\newcommand{\permutation}{\boldsymbol{\epsilon}}
\newcommand{\matriceZero}{\mathfrak{0}}
\newcommand{\matriceUn}{\mathfrak{1}}
\newcommand{\christoffel}[2]{
\left\{ \begin{array}{c}
#1 \\
#2 \\
\end{array} \right\}
}
\newcommand{\lagrangien}{\mathfrak{L}}
\newcommand{\sousens}{\mathfrak{P}}
\newcommand{\partition}{\mathrm{Partition}}
\newcommand{\tribu}{\mathrm{Tribu}}
\newcommand{\topologies}{\mathrm{Topo}}
\newcommand{\setB}{\mathbb{B}}
\newcommand{\setN}{\mathbb{N}}
\newcommand{\setZ}{\mathbb{Z}}
\newcommand{\setQ}{\mathbb{Q}}
\newcommand{\setR}{\mathbb{R}}
\newcommand{\setC}{\mathbb{C}}
\newcommand{\corps}{\mathbb{K}}
\newcommand{\boule}{\mathfrak{B}}
\newcommand{\intervalleouvert}[2]{\left] #1 , #2 \right[}
\newcommand{\intervallesemiouvertgauche}[2]{ \left] #1 , #2 \right]}
\newcommand{\intervallesemiouvertdroite}[2]{\left[ #1 , #2 \right[ }
\newcommand{\fonction}{\mathbb{F}}
\newcommand{\bijection}{\mathrm{Bij}}
\newcommand{\polynome}{\mathrm{Poly}}
\newcommand{\lineaire}{\mathrm{Lin}}
\newcommand{\continue}{\mathrm{Cont}}
\newcommand{\homeomorphisme}{\mathrm{Hom}}
\newcommand{\etagee}{\mathrm{Etagee}}
\newcommand{\lebesgue}{\mathrm{Leb}}
\newcommand{\lipschitz}{\mathrm{Lip}}
\newcommand{\suitek}{\mathrm{Suite}}
\newcommand{\matrice}{\mathbb{M}}
\newcommand{\krylov}{\mathrm{Krylov}}
\newcommand{\tenseur}{\mathbb{T}}
\newcommand{\essentiel}{\mathfrak{E}}
\newcommand{\relation}{\mathrm{Rel}}
\DeclareMathOperator*{\strictinferieur}{\ < \ }
\DeclareMathOperator*{\strictsuperieur}{\ > \ }
\DeclareMathOperator*{\ensinferieur}{\eqslantless}
\DeclareMathOperator*{\enssuperieur}{\eqslantgtr}
\DeclareMathOperator*{\esssuperieur}{\gtrsim}
\DeclareMathOperator*{\essinferieur}{\lesssim}
\newcommand{\essegal}{\eqsim}
\newcommand{\union}{\ \cup \ }
\newcommand{\intersection}{\ \cap \ }
\newcommand{\opera}{\divideontimes}
\newcommand{\autreaddition}{\boxplus}
\newcommand{\autremultiplication}{\circledast}
\newcommand{\commutateur}[2]{\left[ #1 , #2 \right]}
\newcommand{\convolution}{\circledcirc}
\newcommand{\correlation}{\ \natural \ }
\newcommand{\diventiere}{\div}
\newcommand{\modulo}{\bmod}
\DeclareMathOperator*{\pgcd}{pgcd}
\DeclareMathOperator*{\ppcm}{ppcm}
\newcommand{\produitscalaire}[2]{\left\langle #1 \vert #2 \right\rangle}
\newcommand{\scalaire}[2]{\left\langle #1 \| #2 \right\rangle}
\newcommand{\braket}[3]{\left\langle #1 \vert #2 \vert #3 \right\rangle}
\newcommand{\orthogonal}{\bot}
\newcommand{\forme}[2]{\left\langle #1 , #2 \right\rangle}
\newcommand{\biforme}[3]{\left\langle #1 , #2 , #3 \right\rangle}
\newcommand{\contraction}[3]{\left\langle #1 \odot #3 \right\rangle_{#2}}
\newcommand{\dblecont}[5]{\left\langle #1 \vert #3 \vert #5 \right\rangle_{#2,#4}}
\DeclareMathOperator*{\major}{major}
\DeclareMathOperator*{\minor}{minor}
\DeclareMathOperator*{\maxim}{maxim}
\DeclareMathOperator*{\minim}{minim}
\DeclareMathOperator*{\argument}{arg}
\DeclareMathOperator*{\argmin}{arg\ min}
\DeclareMathOperator*{\argmax}{arg\ max}
\DeclareMathOperator*{\supessentiel}{ess\ sup}
\DeclareMathOperator*{\infessentiel}{ess\ inf}
\newcommand{\dual}{\star}
\newcommand{\distance}{\mathfrak{dist}}
\newcommand{\norme}[1]{\left\| #1 \right\|}
\newcommand{\normetrois}[1]{\left|\left\| #1 \right\|\right|}
\DeclareMathOperator*{\adh}{adh}
\DeclareMathOperator*{\interieur}{int}
\newcommand{\frontiere}{\partial}
\DeclareMathOperator*{\image}{im}
\DeclareMathOperator*{\domaine}{dom}
\DeclareMathOperator*{\noyau}{ker}
\DeclareMathOperator*{\support}{supp}
\DeclareMathOperator*{\signe}{sign}
\newcommand{\abs}[1]{\left| #1 \right|}
\newcommand{\unsur}[1]{\frac{1}{#1}}
\newcommand{\arrondisup}[1]{\lceil #1 \rceil}
\newcommand{\arrondiinf}[1]{\lfloor #1 \rfloor}
\DeclareMathOperator*{\conjugue}{conj}
\newcommand{\conjaccent}[1]{\overline{#1}}
\DeclareMathOperator*{\division}{division}
\newcommand{\difference}{\boldsymbol{\Delta}}
\newcommand{\differentielle}[2]{\mathfrak{D}^{#1}_{#2}}
\newcommand{\OD}[2]{\frac{d #1}{d #2}}
\newcommand{\OOD}[2]{\frac{d^2 #1}{d #2^2}}
\newcommand{\NOD}[3]{\frac{d^{#3} #1}{d #2^{#3}}}
\newcommand{\deriveepartielle}[2]{\frac{\partial #1}{\partial #2}}
\newcommand{\PD}[2]{\frac{\partial #1}{\partial #2}}
\newcommand{\dblederiveepartielle}[2]{\frac{\partial^2 #1}{\partial #2 \partial #2}}
\newcommand{\dfdxdy}[3]{\frac{\partial^2 #1}{\partial #2 \partial #3}}
\newcommand{\dfdxdx}[2]{\frac{\partial^2 #1}{\partial #2^2}}
\newcommand{\gradient}{\mathbf{\nabla}}
\newcommand{\combilin}[1]{\mathrm{span}\{ #1 \}}
\DeclareMathOperator*{\trace}{tr}
\newcommand{\proba}{\mathbb{P}}
\newcommand{\probaof}[1]{\mathbb{P}\left[#1\right]}
\newcommand{\esperof}[1]{\mathbb{E}\left[#1\right]}
\newcommand{\cov}[2]{\mathrm{cov} \left( #1 , #2 \right) }
\newcommand{\var}[1]{\mathrm{var} \left( #1 \right) }
\newcommand{\rand}{\mathrm{rand}}
\newcommand{\variation}[1]{\left\langle #1 \right\rangle}
\DeclareMathOperator*{\composante}{comp}
\DeclareMathOperator*{\bloc}{bloc}
\DeclareMathOperator*{\ligne}{ligne}
\DeclareMathOperator*{\colonne}{colonne}
\DeclareMathOperator*{\diagonale}{diag}
\newcommand{\matelementaire}{\mathrm{Elem}}
\DeclareMathOperator*{\matpermutation}{permut}
\newcommand{\matunitaire}{\mathrm{Unitaire}}
\newcommand{\gaussjordan}{\mathrm{GaussJordan}}
\newcommand{\householder}{\mathrm{Householder}}
\DeclareMathOperator*{\rang}{rang}
\newcommand{\schur}{\mathrm{Schur}}
\newcommand{\singuliere}{\mathrm{DVS}}
\newcommand{\convexe}{\mathrm{Convexe}}
\newcommand{\petito}[1]{o\left(#1\right)}
\newcommand{\grando}[1]{O\left(#1\right)}
\)
</p>

<p>
\(
\newenvironment{Eqts}
{ \begin{equation*} \begin{gathered} }
{ \end{gathered} \end{equation*} }
\newenvironment{Matrix}
{\left[ \begin{array}}
{\end{array} \right]}
\)
</p>
<div id="outline-container-orgeb183e7" class="outline-2">
<h2 id="orgeb183e7"><span class="section-number-2">1.</span> Théorème de Rolle</h2>
<div class="outline-text-2" id="text-1">
<div id="text-table-of-contents-1" role="doc-toc">
<ul>
<li><a href="#orgc37cade">1.1. Dépendances</a></li>
<li><a href="#org4fea3e7">1.2. Extrema locaux</a></li>
<li><a href="#orgc37c773">1.3. Théorème de Rolle</a></li>
<li><a href="#orgaabb58a">1.4. Théorème des accroissements finis</a></li>
<li><a href="#org7841af9">1.5. Théorème de Cauchy</a></li>
<li><a href="#orge97f0ea">1.6. Théorème de l'Hospital</a></li>
<li><a href="#orgc8e6445">1.7. Uniformité</a></li>
</ul>
</div>

<p>
\label{chap:intediff}
</p>
</div>
<div id="outline-container-orgc37cade" class="outline-3">
<h3 id="orgc37cade"><span class="section-number-3">1.1.</span> Dépendances</h3>
<div class="outline-text-3" id="text-1-1">
<ul class="org-ul">
<li>Chapitre \ref{chap:differ} : Les différentielles</li>
<li>Chapitre \ref{chap:integral} : Les intégrales</li>
</ul>
</div>
</div>
<div id="outline-container-org4fea3e7" class="outline-3">
<h3 id="org4fea3e7"><span class="section-number-3">1.2.</span> Extrema locaux</h3>
<div class="outline-text-3" id="text-1-2">
<p>
Soit \(f : A \mapsto \setR\) avec \(A \subseteq \setR^n\). Supposons que \(f\) soit différentiable et atteigne un minimum local en \(a \in \interieur A\). On a :
</p>

<div class="org-center">
<p>
\(
f(a + h) - f(a) = \differentielle{f}{a}(h) + E(h) \)
</p>

<p>
\(
f(a - h) - f(a) = - \differentielle{f}{a}(h) + E(-h)
\)
</p>
</div>

<p>
Fixons \(\epsilon \strictsuperieur 0\). On peut trouver \(\delta_1 \strictsuperieur 0\) tel que :
</p>

<p>
\[\abs{E(h)} \le \epsilon \cdot \abs{h}\]
</p>

<p>
pour tout \(h \in \boule(0,\delta_1)\). On peut aussi trouver \(\delta_2 \strictsuperieur 0\) tel que :
</p>

<p>
\[f(a) \le f(a + h)\]
</p>

<p>
pour tout \(h \in \boule(0,\delta_2)\). Posons \(\delta = \min \{ \delta_1,\delta_2 \}\) et choisissons \(h \in \boule(0,\delta)\). On a également \(-h \in \boule(0,\delta)\). Donc :
</p>

<div class="org-center">
<p>
\(
\differentielle{f}{a}(h) = f(a + h) - f(a) - E(h) \ge - E(h) \ge - \epsilon \cdot \norme{-h} \)
</p>

<p>
\(
\differentielle{f}{a}(h) = f(a) - f(a - h) + E(-h) \le E(-h) \le \epsilon \cdot \norme{h}
\)
</p>
</div>

<p>
On en conclut que :
</p>

<p>
\[\abs{\differentielle{f}{a}(h)} \le \epsilon \cdot \norme{h}\]
</p>

<p>
Posons \(\gamma = \delta / 2\) et remarquons que l'ensemble de norme fixe \(N = \{ h \in \setR^n : \norme{h} = \gamma \}\) est inclus dans \(\boule(0,\delta)\). Les propriétés des applications linéaires nous disent que :
</p>

<p>
\[\norme{\differentielle{f}{a}} = \sup \left\{ \unsur{\gamma} \norme{\differentielle{f}{a}(h)} : h \in N \right\}\]
</p>

<p>
Or, la borne nous dit que :
</p>

<p>
\[\unsur{\gamma} \abs{\differentielle{f}{a}(h)} \le \epsilon\]
</p>

<p>
quel que soit \(\epsilon \strictsuperieur 0\) et \(h \in N\). Donc :
</p>

<p>
\[\norme{\differentielle{f}{a}} = 0\]
</p>

<p>
ce qui implique que :
</p>

<p>
\[\differentielle{f}{a} = 0\]
</p>

<p>
La différentielle s'annule donc en un minimum local. On montre de la même manière que la différentielle s'annule en un maximum local.
</p>
</div>
<div id="outline-container-org0469f09" class="outline-4">
<h4 id="org0469f09"><span class="section-number-4">1.2.1.</span> La Jacobienne</h4>
<div class="outline-text-4" id="text-1-2-1">
<p>
La Jacobienne étant la représentation matricielle de la différentielle, elle s'annule également aux extrema locaux.
</p>
</div>
</div>
</div>
<div id="outline-container-orgc37c773" class="outline-3">
<h3 id="orgc37c773"><span class="section-number-3">1.3.</span> Théorème de Rolle</h3>
<div class="outline-text-3" id="text-1-3">
<p>
Soit \(f \in \continue^1([a,b],\setR)\) avec \(f(a) = f(b)\). Comme \(f\) est continue, il existe \(\lambda,\sigma \in [a,b]\) tels que :
</p>

<div class="org-center">
<p>
\(
f(\lambda) = \min f([a,b]) \)
</p>

<p>
\(
f(\sigma) = \max f([a,b])
\)
</p>
</div>
</div>
<div id="outline-container-orga486640" class="outline-4">
<h4 id="orga486640"><span class="section-number-4">1.3.1.</span> Configurations</h4>
<div class="outline-text-4" id="text-1-3-1">
<p>
Plusieurs cas peuvent se présenter :
</p>


<ul class="org-ul">
<li>\(\lambda \strictinferieur f(a) = f(b) \strictinferieur \sigma\) : dans ce cas, la fonction atteint ses deux bornes à l'intérieur de l'intervalle :</li>
</ul>

<p>
\[\{\lambda,\sigma\} \subseteq \intervalleouvert{a}{b}\]
</p>

<p>
Comme les extrema sont aussi des extrema locaux, on a :
</p>

<p>
\[\partial f(\lambda) = \partial f(\sigma) = 0\]
</p>

<ul class="org-ul">
<li>\(\lambda \strictinferieur f(a) = f(b) = \sigma\) : dans ce cas, la fonction atteint son minimum à l'intérieur de l'intervalle :</li>
</ul>

<p>
\[\lambda \in \intervalleouvert{a}{b}\]
</p>

<p>
et on a :
</p>

<p>
\[\partial f(\lambda) = 0\]
</p>

<ul class="org-ul">
<li>\(\lambda = f(a) = f(b) \strictinferieur \sigma\) : dans ce cas, la fonction atteint son maximum à l'intérieur de l'intervalle :</li>
</ul>

<p>
\[\sigma \in \intervalleouvert{a}{b}\]
</p>

<p>
et on a :
</p>

<p>
\[\partial f(\sigma) = 0\]
</p>

<ul class="org-ul">
<li>\(\lambda = f(a) = f(b) = \sigma\) : dans ce cas, on a :</li>
</ul>

<p>
\[\lambda \le f(x) \le \sigma = \lambda\]
</p>

<p>
pour tout \(x \in [a,b]\), et donc :
</p>

<p>
\[f(x) = \lambda\]
</p>

<p>
La fonction \(f\) est constante et \(\partial f = 0\). On peut donc prendre n'importe quel \(c \in \intervalleouvert{a}{b}\), on aura :
</p>

<p>
\[\partial f(c) = 0\]
</p>
</div>
</div>
<div id="outline-container-org37c0394" class="outline-4">
<h4 id="org37c0394"><span class="section-number-4">1.3.2.</span> Conclusion</h4>
<div class="outline-text-4" id="text-1-3-2">
<p>
Dans tous les cas, on a au moins un \(c \in \intervalleouvert{a}{b}\) tel que :
</p>

<p>
\[\partial f(c) = 0\]
</p>
</div>
</div>
</div>
<div id="outline-container-orgaabb58a" class="outline-3">
<h3 id="orgaabb58a"><span class="section-number-3">1.4.</span> Théorème des accroissements finis</h3>
<div class="outline-text-3" id="text-1-4">
<p>
Soit \(f \in \continue^1([a,b],\setR)\) et la fonction \(g\) associée définie par :
</p>

<p>
\[g(x) = f(x) - \frac{f(b) - f(a)}{b - a} \cdot (x - a)\]
</p>

<p>
pour tout \(x \in [a,b]\). Comme \(g(a) = g(b) = f(a)\), on peut trouver un \(c \in \intervalleouvert{a}{b}\) tel que :
</p>

<p>
\[0 = \partial g(c) = \partial f(c) - \frac{f(b) - f(a)}{b - a}\]
</p>

<p>
On a donc :
</p>

<p>
\[\partial f(c) = \frac{f(b)-f(a)}{b-a}\]
</p>

<p>
On vient ainsi de démontrer le théorème des accroissements finis.
</p>
</div>
<div id="outline-container-org35cfd71" class="outline-4">
<h4 id="org35cfd71"><span class="section-number-4">1.4.1.</span> Dimension \(n\)</h4>
<div class="outline-text-4" id="text-1-4-1">
<p>
On peut généraliser ce théorème à une fonction \(f \in \continue^1(\setR^m,\setR^n)\). Soit \(u,v \in \setR^m\). On considére le segment \([u,v]\) et la fonction associée \(\phi : [0,1] \mapsto \setR^m\) définie par :
</p>

<p>
\[\phi(t) = u + t \cdot (v - u)\]
</p>

<p>
pour tout \(t \in [0,1]\). On pose alors :
</p>

<p>
\[g(t) = (f \circ \phi)(t) = f( u + t \cdot (v - u))\]
</p>

<p>
pour tout \(t \in [0,1]\). En appliquant le résultat précédent aux composantes \(g_i\) sur l'intervalle \([0,1]\), on obtient un \(s \in \intervalleouvert{0}{1}\) tel que :
</p>

<p>
\[\partial g_i(s) = \frac{g_i(1) - g_i(0)}{1 - 0} = g_i(1) - g_i(0) = f_i(v) - f_i(u)\]
</p>

<p>
En appliquant la formule permettant d'évaluer la dérivée d'une composition de fonctions, on obtient :
</p>

<p>
\[\partial g_i(s) = \sum_j \partial_j f_i(u + s \cdot (v - u)) \cdot (v_j - u_j)\]
</p>

<p>
Utilisant la notation matricielle, on a donc :
</p>

<p>
\[f(v) - f(u) = \partial f(u + s \cdot (v - u)) \cdot (v - u)\]
</p>

<p>
Ce qui revient à dire qu'il existe un \(w \in [u,v] \subseteq \setR^n\) tel que :
</p>

<p>
\[f(v) - f(u) = \partial f(w) \cdot (v - u)\]
</p>
</div>
</div>
</div>
<div id="outline-container-org7841af9" class="outline-3">
<h3 id="org7841af9"><span class="section-number-3">1.5.</span> Théorème de Cauchy</h3>
<div class="outline-text-3" id="text-1-5">
<p>
Le théorème des accroissements finis nous donne un résultat sous la forme
symbolique :
</p>

<p>
\[\OD{f}{x} = \frac{\difference f}{\difference x}\]
</p>

<p>
Nous allons maintenant généraliser ce théorème, et obtenir le résultat :
</p>

<p>
\[\frac{df}{dg} = \frac{\difference f}{\difference g}\]
</p>

<p>
où \(f,g \in \continue^1([a,b],\setR)\) et \(a,b \in \setR\). Considérons à cette fin la
fonction \(h\) définie par :
</p>

<p>
\[h(x) = [f(b) - f(a)] \cdot g(x) - f(x) \cdot [ g(b) - g(a) ]\]
</p>

<p>
On remarque que :
</p>

\begin{align}
h(a) &= f(b) \cdot g(a) - f(a) \cdot g(a) -  f(a)\cdot g(b) +  f(a)\cdot g(a) \)

\(
&= f(b) \cdot g(a) - f(a) \cdot g(b) \\ \)

\(
h(b) &= f(b) \cdot g(b) - f(a) \cdot g(b) -  f(b)\cdot g(b) +  f(b)\cdot g(a) \)

\(
&= f(b) \cdot g(a) - f(a) \cdot g(b)
\end{align}

<p>
Donc :
</p>

<p>
\[h(a) = h(b)\]
</p>

<p>
Appliquant le théorème de Rolle à \(h\), on peut donc trouver un \(c \in \intervalleouvert{a}{b}\) tel que :
</p>

<p>
\[0 = \partial h(c) = [f(b) - f(a)] \cdot \partial g(c) - \partial f(c) \cdot [ g(b) - g(a) ]\]
</p>

<p>
On a donc :
</p>

<p>
\[\left[ f(b) - f(a) \right] \cdot \partial g(c) = \partial f(c) \cdot \left[ g(b) - g(a) \right]\]
</p>

<p>
Si \(\partial f(c) \ne 0\) et \(f(b) \ne f(a)\), on peut le mettre sous la forme :
</p>

<p>
\[\frac{\partial g(c)}{\partial f(c)} = \frac{g(b) - g(a)}{f(b) - f(a)}\]
</p>
</div>
</div>
<div id="outline-container-orge97f0ea" class="outline-3">
<h3 id="orge97f0ea"><span class="section-number-3">1.6.</span> Théorème de l'Hospital</h3>
<div class="outline-text-3" id="text-1-6">
<p>
Soient \(F,G\) deux fonctions continues sur \(I=[\alpha,\beta]\) et dérivables
\(I\setminus \{a\}\), avec \(a \in \interieur\ I\). Supposons que les deux fonctions s'annulent en \(a\) :
</p>

<p>
\[F(a) = G(a) = 0\]
</p>

<p>
Soit alors \(h \ne 0\) tel que \(b = a+h\in I\setminus \{a\}\). En appliquant le théorème de Cauchy à \(F\) et \(G\), on trouve un \(t \in \intervalleouvert{0}{1}\) tel que :
</p>

<p>
\[[F(b) - F(a)] \cdot \partial G(a + t \cdot h) = [G(b) - G(a)] \cdot \partial F(a + t \cdot h)\]
</p>

<p>
Mais comme \(F\) et \(G\) s'annulent en \(a\), on a :
</p>

<p>
\[F(b) \cdot \partial G(a + t \cdot h) = G(b) \cdot \partial F(a + t \cdot h)\]
</p>

<p>
Si de plus \(\partial G\) ne s'annule pas sur \(I\), on peut écrire :
</p>

<p>
\[\frac{ \partial F(a + t \cdot h) }{ \partial G(a + t \cdot h) } = \frac{F(b)}{F(a)}\]
</p>

<p>
On voit en faisant tendre \(h\) vers \(0\) que les limites, si elles existent, doivent être identiques. On a donc :
</p>

<p>
\[\lim_{x \to a} \frac{F(x)}{G(x)} =  \lim_{x \to a} \frac{\partial F(a)}{\partial G(a)}\]
</p>
</div>
</div>
<div id="outline-container-orgc8e6445" class="outline-3">
<h3 id="orgc8e6445"><span class="section-number-3">1.7.</span> Uniformité</h3>
<div class="outline-text-3" id="text-1-7">
<p>
Nous allons à présent montrer que toute fonction continument différentiable sur un intervalle de la forme \([\alpha,\beta]\) y est uniformément différentiable.
</p>

<p>
Soit une fonction \(f \in \continue^1([\alpha,\beta],\setR)\). Comme la dérivée \(\partial f\) est continue sur \([\alpha,\beta]\), elle y est uniformément continue. Fixons \(\epsilon \strictsuperieur 0\). On peut donc trouver un \(\delta \strictsuperieur 0\) tel que :
</p>

<p>
\[\abs{\partial f(s) - \partial f(t)} \le \epsilon\]
</p>

<p>
pour tout \(s,t \in [\alpha,\beta]\) vérifiant \(\abs{s - t} \le \delta\). Si \(s = t\), on a bien évidemment :
</p>

<p>
\[\abs{f(t) - f(t) - \partial f(t) \cdot (t - t)} = 0 \le \epsilon \cdot (t - t) = 0\]
</p>

<p>
Considérons à présent le cas \(s \ne t\). Nous pouvons supposer sans perte de généralité que \(s \strictinferieur t\). Le théorème des accroissements finis nous dit qu'on peut trouver un \(\gamma \in ]s,t[\) tel que :
</p>

<p>
\[\partial f(\gamma) = \frac{f(t) - f(s)}{t - s}\]
</p>

<p>
On a donc :
</p>

<p>
\[\frac{f(t) - f(s)}{t - s} - \partial f(s) = \partial f(\gamma) - \partial f(s)\]
</p>

<p>
Mais comme \(\abs{\gamma - s} \le \abs{t - s} \le \delta\), on a \(\abs{\partial f(\gamma) - \partial f(s)} \le \epsilon\) et :
</p>

<p>
\[\abs{\frac{f(t) - f(s)}{t - s} - \partial f(s)} \le \epsilon\]
</p>

<p>
On a donc bien :
</p>

<p>
\[\abs{f(t) - f(s) - \partial f(s) \cdot (t - s)} \le \epsilon \cdot \abs{t - s}\]
</p>

<p>
pour tout \(s,t \in [\alpha,\beta]\) vérifiant \(\abs{s - t} \le \delta\).
</p>
</div>
<div id="outline-container-org2c0c275" class="outline-4">
<h4 id="org2c0c275"><span class="section-number-4">1.7.1.</span> Remarque</h4>
<div class="outline-text-4" id="text-1-7-1">
<p>
Le théorème {\em n'est pas} applicable aux autres types d'intervalles. Cela ne marche pas sur \(]\alpha,\beta[\) par exemple.
</p>
</div>
</div>
</div>
</div>
<div id="outline-container-orgb24d4c4" class="outline-2">
<h2 id="orgb24d4c4"><span class="section-number-2">2.</span> Théorème fondamental</h2>
<div class="outline-text-2" id="text-2">
<div id="text-table-of-contents-2" role="doc-toc">
<ul>
<li><a href="#org88bd532">2.1. Dépendances</a></li>
<li><a href="#orga55c266">2.2. Dérivée de l'intégrale</a></li>
<li><a href="#orga0075d4">2.3. Intégrale de la dérivée</a></li>
<li><a href="#org8907d58">2.4. Polynômes</a></li>
<li><a href="#orgccbf1fb">2.5. Valeur moyenne</a></li>
<li><a href="#orgfebff26">2.6. Intégration par parties</a></li>
<li><a href="#org36ffb61">2.7. Changement de variable</a></li>
</ul>
</div>

<p>
\label{chap:fonda}
</p>
</div>
<div id="outline-container-org88bd532" class="outline-3">
<h3 id="org88bd532"><span class="section-number-3">2.1.</span> Dépendances</h3>
<div class="outline-text-3" id="text-2-1">
<ul class="org-ul">
<li>Chapitre \ref{chap:differ} : Les différentielles</li>
<li>Chapitre \ref{chap:integral} : Les intégrales</li>
</ul>
</div>
</div>
<div id="outline-container-orga55c266" class="outline-3">
<h3 id="orga55c266"><span class="section-number-3">2.2.</span> Dérivée de l'intégrale</h3>
<div class="outline-text-3" id="text-2-2">
<p>
\label{sec:derivee_integrale}
</p>

<p>
Soit une fonction \(f \in \continue([\alpha,\beta],\setR)\), un réel \(a \in [\alpha,\beta]\) et la fonction intégrale associée \(I : [\alpha,\beta] \mapsto \setR\) définie par :
</p>

<p>
\[I(x) = \int_a^x f(s) \ ds\]
</p>

<p>
pour tout \(x \in [\alpha,\beta]\). On constate que :
</p>

<p>
\[I(a) = \int_a^a f(s) \ ds = \int_{ \{a\} } f(s) \ ds\]
</p>

<p>
La mesure de Lebesgue du singleton \(\{a\}\) étant nulle, l'intégrale s'annule et on a :
</p>

<p>
\[I(a) = 0\]
</p>

<p>
Nous allons chercher à évaluer la dérivée de la fonction intégrale \(I\) en un point quelconque \(b \in [\alpha,\beta]\).
</p>

<p>
Nous utilisons dans la suite la notation abrégée :
</p>

<p>
\[\int_x^y = \int_x^y f(s) \ ds\]
</p>

<p>
Par additivité, on a :
</p>

<p>
\[\int_a^{b + h} = \int_a^b + \int_b^{b + h}\]
</p>

<p>
c'est-à-dire :
</p>

<p>
\[I(b + h) = I(b) + \int_b^{b + h}\]
</p>

<p>
pour tout réel \(h\) tel que \(b + h \in [\alpha,\beta]\). Quelle est la valeur de l'intégrale sur \([b, b + h]\) ? Fixons \(\epsilon \strictsuperieur 0\). Par continuité de \(f\), on peut choisir \(\delta > 0\) tel que :
</p>

<p>
\[\abs{f(b + s) - f(b)} \le \epsilon\]
</p>

<p>
pour tout \(s\) vérifiant \(\abs{s} \le \delta\). Cela n'est possible que si :
</p>

<p>
\[f(b) - \epsilon \le f(b + s) \le f(b) + \epsilon\]
</p>

<p>
On a donc :
</p>

<div class="org-center">
<p>
\(
\sup \Big\{ f(b + s) : \abs{s} \le \delta \Big\} \le f(b) + \epsilon \)
</p>

<p>
\(
\inf \Big\{ f(b + s) : \abs{s} \le \delta \Big\} \ge f(b) - \epsilon
\)
</p>
</div>

<p>
Si nous choisissons \(h \in (0,\delta)\), l'intégrale peut donc être majorée et minorée par :
</p>

<p>
\[\int_b^{b + h} \le (f(b) + \epsilon) \cdot \mu_L([a, a + h]) = (f(b) + \epsilon) \cdot h\]
</p>

<p>
et :
</p>

<p>
\[\int_b^{b + h} \ge (f(b) - \epsilon) \cdot \mu_L([a, a + h]) = (f(b) - \epsilon) \cdot h\]
</p>

<p>
Nous disposons donc des inégalités :
</p>

<p>
\[(f(b) - \epsilon) \cdot h \le \int_b^{b + h} \le (f(b) + \epsilon) \cdot h\]
</p>

<p>
Autrement dit :
</p>

<p>
\[f(b) - \epsilon \le \unsur{h} \int_b^{b + h} \le f(b) + \epsilon\]
</p>

<p>
D'un autre coté, on a :
</p>

<p>
\[\unsur{h} \int_b^{b + h} f(s) \ ds = \frac{I(b + h) - I(b)}{h}\]
</p>

<p>
Passons à la limite \(\delta \to 0\). On a alors \(h \to 0\) et :
</p>

<p>
\[f(b) - \epsilon \le \lim_{h \to 0} \unsur{h} \int_b^{b + h} \le f(b) + \epsilon\]
</p>

<p>
Ces inégalités devant être valables pour tout \(\epsilon \strictsuperieur 0\), on a forcément :
</p>

<p>
\[\lim_{h \to 0} \frac{I(b + h) - I(b)}{h} = f(b)\]
</p>

<p>
On en conclut que \(I\) est dérivable et que :
</p>

<p>
\[\OD{I}{x}(b) = \lim_{h \to 0} \frac{I(b + h) - I(b)}{h} = f(b)\]
</p>

<p>
Autrement dit :
</p>

<p>
\[\OD{}{x} \int_a^x f(s) \ ds = f(x)\]
</p>
</div>
</div>
<div id="outline-container-orga0075d4" class="outline-3">
<h3 id="orga0075d4"><span class="section-number-3">2.3.</span> Intégrale de la dérivée</h3>
<div class="outline-text-3" id="text-2-3">
<p>
\label{sec:integrale_derivee}
</p>

<p>
Soit \(\alpha,\beta \in \setR\) avec \(\alpha \le \beta\). Soit la fonction \(F \in \continue^1([\alpha,\beta],\setR)\) et sa dérivée continue :
</p>

<p>
\[f = \partial F = \OD{F}{s}\]
</p>

<p>
Comme \(F\) est continument différentiable sur \([\alpha,\beta]\), elle y est uniformément différentiable. De même, \(f\) est continue sur \([\alpha,\beta]\). Elle y est donc uniformément continue. Nous allons tenter d'évaluer l'intégrale :
</p>

<p>
\[\int_a^b f(s) \ ds = \int_a^b \OD{F}{x}(s) \ ds\]
</p>

<p>
avec \(a,b \in [\alpha,\beta]\) et \(a \le b\).
</p>

<p>
Nous utilisons dans la suite la notation abrégée :
</p>

<p>
\[\int_x^y = \int_x^y f(s) \ ds\]
</p>
</div>
<div id="outline-container-org3d22c31" class="outline-4">
<h4 id="org3d22c31"><span class="section-number-4">2.3.1.</span> L'idée</h4>
<div class="outline-text-4" id="text-2-3-1">
<p>
L'idée intuitive est que :
</p>

<p>
\[\difference F = \sum_i \difference F_i = \sum_i \frac{ \difference F_i }{ \difference x_i } \cdot \difference x_i\]
</p>

<p>
En passant à la limite \(\difference x_i \to 0\), on soupçonne alors le résultat suivant :
</p>

<p>
\[\difference F = \int_a^b \OD{F}{x} \ dx\]
</p>
</div>
</div>
<div id="outline-container-org3684e21" class="outline-4">
<h4 id="org3684e21"><span class="section-number-4">2.3.2.</span> La réalisation</h4>
<div class="outline-text-4" id="text-2-3-2">
<p>
Fixons \(\epsilon \strictsuperieur 0\). Comme \(f\) est uniformément continue, nous savons qu'il existe \(\vartheta \strictsuperieur 0\) tel que :
</p>

<p>
\[\abs{f(x + h) - f(x)} \le \epsilon\]
</p>

<p>
pour tout \(x,h\) vérifiant \(x,x + h \in [\alpha,\beta]\) et \(\abs{h} \le \vartheta\). On en déduit que :
</p>

<div class="org-center">
<p>
\(
\sup_{\xi \in [x - \vartheta , x + \vartheta]} f(\xi) \le f(x) + \epsilon \)
</p>

<p>
\(
\inf_{\xi \in [x - \vartheta , x + \vartheta]} f(\xi) \ge f(x) - \epsilon
\)
</p>
</div>

<p>
On a donc les bornes pour l'intégrale :
</p>

<p>
\[(f(x) - \epsilon) \cdot h \le \int_x^{x + h} \le (f(x) + \epsilon) \cdot h\]
</p>

<p>
Comme \(F\) est uniformément différentiable, nous pouvons trouver \(\varpi \strictsuperieur 0\) tel que :
</p>

<p>
\[\abs{F(x + h) - F(x) - f(x) \cdot h} \le \epsilon \cdot h\]
</p>

<p>
pour tout \(x,h\) vérifiant \(x,x + h \in [\alpha,\beta]\) et \(\abs{h} \le \varpi\). On en déduit que :
</p>

<div class="org-center">
<p>
\(
F(x + h) - F(x) - f(x) \cdot h \le \epsilon \cdot h \)
</p>

<p>
\(
f(x) \cdot h - (F(x + h) - F(x)) \le \epsilon \cdot h
\)
</p>
</div>

<p>
En considérant ces deux inégalités par rapport au centre \(f(x) \cdot h\), on obtient :
</p>

<p>
\[F(x + h) - F(x) - \epsilon \cdot h \le f(x) \cdot h \le F(x + h) - F(x) + \epsilon \cdot h\]
</p>

<p>
En soustrayant ou en ajoutant \(\epsilon \cdot h\) à ces inégalités, on a :
</p>

\begin{align}
F(x + h) - F(x) - 2 \epsilon \cdot h &\le f(x) \cdot h - \epsilon \cdot h \le& F(x + h) - F(x) \)

\(
F(x + h) - F(x) &\le f(x) \cdot h + \epsilon \cdot h \le& F(x + h) - F(x) + 2 \epsilon \cdot h
\end{align}

<p>
Si \(\abs{h} \le \min \{ \vartheta , \varpi \}\), nous avons de nouvelles bornes pour l'intégrale :
</p>

<p>
\[F(x + h) - F(x) - 2 \epsilon \cdot h \le \int_x^{x + h} \le F(x + h) - F(x) + 2 \epsilon \cdot h\]
</p>

<p>
Choisissons à présent \(n \in \setN\) tel que :
</p>

<p>
\[\abs{ \frac{b - a}{n} } \le \min \{ \vartheta , \varpi \}\]
</p>

<p>
Posons \(h = (b - a)/n\) et définissons la série :
</p>

<p>
\[x_i = a + i \cdot h\]
</p>

<p>
On a alors \(a = x_0 \le x_1 \le ... \le x_n = b\). Les propriétés des sommes nous disent que :
</p>

<p>
\[\sum_{i = 1}^n (F(x_i) - F(x_{i - 1})) = F(x_n) - F(x_0) = F(b) - F(a)\]
</p>

<p>
D'un autre coté, on a clairement :
</p>

<p>
\[\sum_{i = 1}^n \int_{ x_{i - 1} }^{x_i} = \int_a^b\]
</p>

<p>
Si nous appliquons les bornes précédentes avec \(x = x_{i - 1}\), nous avons \(x + h = x_i\) et :
</p>

<p>
\[F(x_i) - F(x_{i - 1}) - 2 \epsilon \cdot h \le \int_{ x_{i - 1} }^{x_i} \le F(x_i) - F(x_{i - 1}) + 2 \epsilon \cdot h\]
</p>

<p>
En sommant sur \(i = 1,2,...,n\), nous obtenons par conséquent :
</p>

<p>
\[F(b) - F(a) - 2 \epsilon \cdot h \cdot n \le \int_a^b \le F(b) - F(a) + 2 \epsilon \cdot h \cdot n\]
</p>

<p>
Mais comme \(h \cdot n = b - a\), cela devient :
</p>

<p>
\[F(b) - F(a) - 2 \epsilon \cdot (b - a) \le \int_a^b \le F(b) - F(a) + 2 \epsilon \cdot (b - a)\]
</p>

<p>
Ces bornes devant être satisfaites pour tout \(\epsilon \strictsuperieur 0\), on en déduit que :
</p>

<p>
\[\int_a^b = F(b) - F(a)\]
</p>

<p>
On a donc finalement :
</p>

<p>
\[\int_a^b f(s) \ ds = \int_a^b \OD{F}{s}(s) \ ds = F(b) - F(a)\]
</p>
</div>
</div>
<div id="outline-container-orgd8fd177" class="outline-4">
<h4 id="orgd8fd177"><span class="section-number-4">2.3.3.</span> Primitive</h4>
<div class="outline-text-4" id="text-2-3-3">
<p>
Cette relation permet de calculer l'intégrale d'une fonction continue \(f : t \mapsto f(t)\) lorsqu'on connaît une fonction \(F\) vérifiant :
</p>

<p>
\[\OD{F}{t} = f\]
</p>

<p>
On appelle « primitive » de \(f\) une telle fonction \(F\).
</p>
</div>
</div>
<div id="outline-container-org84040c5" class="outline-4">
<h4 id="org84040c5"><span class="section-number-4">2.3.4.</span> Notation</h4>
<div class="outline-text-4" id="text-2-3-4">
<p>
On note aussi :
</p>

<p>
\[\difference F = \int dF\]
</p>
</div>
</div>
</div>
<div id="outline-container-org8907d58" class="outline-3">
<h3 id="org8907d58"><span class="section-number-3">2.4.</span> Polynômes</h3>
<div class="outline-text-3" id="text-2-4">
<p>
On sait que :
</p>

<p>
\[\OD{}{t}\big(t^n\big) = n \cdot t^{n - 1}\]
</p>

<p>
Comme \(n\) est constante, on peut le réécrire :
</p>

<p>
\[\OD{}{t}\left( \frac{t^n}{n} \right) = t^{n - 1}\]
</p>

<p>
ou, en posant \(m = n - 1\) :
</p>

<p>
\[\OD{}{t}\left( \frac{t^{m + 1}}{m + 1} \right) = t^m\]
</p>

<p>
L'intégrale s'écrit donc :
</p>

<p>
\[\int_a^b t^m \ dt = \frac{ b^{m + 1} - a^{m + 1} }{m + 1}\]
</p>

<p>
On a en particulier :
</p>

<p>
\[\int_0^x t^m \ dt = \frac{ x^{m + 1} }{m + 1}\]
</p>
</div>
<div id="outline-container-org1695fe8" class="outline-4">
<h4 id="org1695fe8"><span class="section-number-4">2.4.1.</span> Exemples</h4>
<div class="outline-text-4" id="text-2-4-1">
<p>
\[\int_0^x t \ dt = \frac{ x^2 }{2}\]
</p>

<p>
\[\int_0^x t^2 \ dt = \frac{ x^3 }{3}\]
</p>
</div>
</div>
</div>
<div id="outline-container-orgccbf1fb" class="outline-3">
<h3 id="orgccbf1fb"><span class="section-number-3">2.5.</span> Valeur moyenne</h3>
<div class="outline-text-3" id="text-2-5">
</div>
<div id="outline-container-org077f671" class="outline-4">
<h4 id="org077f671"><span class="section-number-4">2.5.1.</span> Accroissements finis</h4>
<div class="outline-text-4" id="text-2-5-1">
<p>
Soit des réels distincts \(a,b\) vérifiant \(a \strictinferieur b\), la fonction \(f \in \continue([a,b],\setR)\) et la fonction \(F : [a,b] \mapsto \setR\) définie par :
</p>

<p>
\[F(x) = \int_a^x f(t) \ dt\]
</p>

<p>
pour tout \(x \in [a,b]\). Comme \(F \in \continue^1([a,b],\setR)\), le théorème des accroissements finis nous dit qu'on peut trouver un \(c \in \intervalleouvert{a}{b}\) tel que :
</p>

<p>
\[\partial F(c) = \frac{F(b) - F(a)}{b - a}\]
</p>

<p>
On sait que :
</p>

<p>
\[\partial F(c) = f(c)\]
</p>

<p>
On a aussi :
</p>

<p>
\[F(b) = \int_a^b f(t) \ dt\]
</p>

<p>
et :
</p>

<p>
\[F(a) = \int_a^a f(t) \ dt = \int_{ \{ a \} } f(t) \ dt\]
</p>

<p>
La mesure de Lebesgue du singleton \(\{a\}\) étant nulle, l'intégrale s'annule et on a :
</p>

<p>
\[F(a) = 0\]
</p>

<p>
On a donc \(F(b) - F(a) = F(b)\) et :
</p>

<p>
\[f(c) = \unsur{b - a} \int_a^b f(t) \ dt\]
</p>
</div>
</div>
<div id="outline-container-org8c2cdff" class="outline-4">
<h4 id="org8c2cdff"><span class="section-number-4">2.5.2.</span> Théorème de Cauchy</h4>
<div class="outline-text-4" id="text-2-5-2">
<p>
Soit des réels distincts \(a,b\) vérifiant \(a \strictinferieur b\), les fonctions \(f,g \in \continue([a,b],\setR)\) et les fonction \(F,G : [a,b] \mapsto \setR\) définies par :
</p>

\begin{align}
F(x) &= \int_a^x f(t) \ dt \)

\(
G(x) &= \int_a^x g(t) \ dt
\end{align}

<p>
pour tout \(x \in [a,b]\). Comme \(F,G \in \continue^1([a,b],\setR)\), le théorème de Cauchy nous dit qu'on peut trouver un \(c \in \intervalleouvert{a}{b}\) tel que :
</p>

<p>
\[\partial F(c) \cdot \big[G(b) - G(a)\big] = \big[F(b) - F(a)\big] \cdot \partial G(c)\]
</p>

<p>
On sait que :
</p>

<div class="org-center">
<p>
\(
\partial F(c) = f(c) \)
</p>

<p>
\(
\partial G(c) = g(c)
\)
</p>
</div>

<p>
On a aussi :
</p>

\begin{align}
F(b) &= \int_a^b f(t) \ dt \)

\(
G(b) &= \int_a^b g(t) \ dt
\end{align}

<p>
et :
</p>

\begin{align}
F(a) &= \int_a^a f(t) \ dt = 0 \)

\(
G(a) &= \int_a^a g(t) \ dt = 0
\end{align}

<p>
On en conclut que :
</p>

<p>
\[f(c) \ \int_a^b g(t) \ dt = g(c) \ \int_a^b f(t) \ dt\]
</p>

<p>
Si l'intégrale de \(g\) et \(g(c)\) sont non nuls, on peut mettre cette relation sous la forme :
</p>

<p>
\[\frac{f(c)}{g(c)} = \frac{ \int_a^b f(t) \ dt }{ \int_a^b g(t) \ dt }\]
</p>
</div>
</div>
</div>
<div id="outline-container-orgfebff26" class="outline-3">
<h3 id="orgfebff26"><span class="section-number-3">2.6.</span> Intégration par parties</h3>
<div class="outline-text-3" id="text-2-6">
<p>
Soient \(f,g \in \continue^1(\setR,\setR)\). On se rappelle que :
</p>

<p>
\[\partial (f \cdot g) = \partial f \cdot g + f \cdot \partial g\]
</p>

<p>
et comme on a :
</p>

<p>
\[\int_a^b \partial (f \cdot g) \ dx = f(b) \cdot g(b) - f(a) \cdot g(a)\]
</p>

<p>
on obtient la formule d'intégration par parties :
</p>

<p>
\[\int_a^b f(x) \cdot \partial g(x) \ dx = (f \cdot g)(b) - (f \cdot g)(a) - \int_a^b \partial f(x) \cdot g(x) \ dx\]
</p>
</div>
<div id="outline-container-org920433b" class="outline-4">
<h4 id="org920433b"><span class="section-number-4">2.6.1.</span> Stieltjes</h4>
<div class="outline-text-4" id="text-2-6-1">
<p>
Le résultat est également valable lorsqu'on utilise les mesures de Stieltjes associées à \(f\) et \(g\) :
</p>

<p>
\[\int_a^b f(x) \cdot dg(x) = \difference (f \cdot g) - \int_a^b g(x) \cdot df(x)\]
</p>
</div>
</div>
<div id="outline-container-org3f74cd0" class="outline-4">
<h4 id="org3f74cd0"><span class="section-number-4">2.6.2.</span> Dérivée constante</h4>
<div class="outline-text-4" id="text-2-6-2">
<p>
On considère le cas particulier où \(\partial g = 1\). Une exemple de fonction \(g\) vérifiant cette propriété est simplement \(g = \identite\). On a donc \(g(x) = x\) et :
</p>

<p>
\[\int_a^b f(x) \ dx = \int_a^b f(x) \cdot 1 \ dx = \int_a^b f(x) \cdot \partial g(x) \ dx\]
</p>

<p>
L'intégration par parties nous donne :
</p>

<p>
\[\int_a^b f(x) \ dx = f(b) \cdot b - f(a) \cdot a - \int_a^b \partial f(x) \cdot x \ dx\]
</p>
</div>
</div>
</div>
<div id="outline-container-org36ffb61" class="outline-3">
<h3 id="org36ffb61"><span class="section-number-3">2.7.</span> Changement de variable</h3>
<div class="outline-text-3" id="text-2-7">
<p>
Considérons une fonction \(f \in \continue(\setR,\setR)\) et un changement de variable \(x = \varphi(s)\) où \(\varphi \in \homeomorphisme^1(\setR,\setR)\). Soit la mesure de lebesgue \(\mu([\alpha,\beta]) = \beta - \alpha\).
</p>
</div>
<div id="outline-container-orgfe13f9c" class="outline-4">
<h4 id="orgfe13f9c"><span class="section-number-4">2.7.1.</span> L'idée</h4>
<div class="outline-text-4" id="text-2-7-1">
<p>
\[\sum_i f_i \cdot \difference x_i = \sum_i f_i \cdot \frac{\difference x_i}{\difference s_i} \cdot \difference s_i\]
</p>

<p>
On devrait donc avoir par passage à la limite :
</p>

<p>
\[\int f \ dx = \int f \ \OD{x}{s} \ ds\]
</p>
</div>
</div>
<div id="outline-container-org4565d53" class="outline-4">
<h4 id="org4565d53"><span class="section-number-4">2.7.2.</span> La réalisation</h4>
<div class="outline-text-4" id="text-2-7-2">
<p>
On applique le même procédé qu'à la section \ref{sec:integrale_derivee}. Si \(x\) est proche de \(y\), on a :
</p>

<p>
\[\int_x^y \approx f(x) \cdot (y - x)\]
</p>

<p>
Posant \(s = \varphi^{-1}(x)\) et \(t = \varphi^{-1}(y)\), on a aussi :
</p>

<p>
\[y - x = \varphi(t) - \varphi(s) = \OD{\varphi}{s}(s) \cdot (t - s) + e(\abs{s - t})\]
</p>

<p>
où \(e\) converge plus vite que \(s - t\) vers \(0\). On en conclut que :
</p>

<p>
\[\int_x^y \approx (f \circ \varphi)(s) \cdot \OD{\varphi}{s}(s) \cdot (t - s)\]
</p>

<p>
On remarque que le second membre est une approximation de l'intégrale de la fonction :
</p>

<p>
\[F(s) = (f \circ \varphi)(s) \cdot \OD{\varphi}{s}(s)\]
</p>

<p>
sur l'intervalle \([s,t] = [\varphi^{-1}(x),\varphi^{-1}(y)]\). Il ne nous reste plus qu'à sommer sur tous les petits intervalles \([x_{i - 1},x_i]\) et à passer à la limite \(h = x_i - x_{i - 1} \to 0\) pour obtenir :
</p>

<p>
\[\int_a^b f(x) \ dx = \int_{\varphi^{-1}(a)}^{\varphi^{-1}(b)} (f \circ \varphi)(s) \cdot \OD{\varphi}{s}(s) \ ds\]
</p>
</div>
</div>
</div>
</div>
<div id="outline-container-orge0053f7" class="outline-2">
<h2 id="orge0053f7"><span class="section-number-2">3.</span> Développements de Taylor</h2>
<div class="outline-text-2" id="text-3">
<div id="text-table-of-contents-3" role="doc-toc">
<ul>
<li><a href="#org85cf13e">3.1. Polynômes de Taylor</a></li>
<li><a href="#org53a1ec7">3.2. Opérateur de Taylor</a></li>
<li><a href="#org0c7568b">3.3. Forme intégrale</a></li>
<li><a href="#org7a2f2a9">3.4. Erreur</a></li>
<li><a href="#orgfbfaa00">3.5. Forme différentielle</a></li>
<li><a href="#org28e0d70">3.6. Borne</a></li>
<li><a href="#orge9cbea0">3.7. Convergence</a></li>
<li><a href="#org9ff4683">3.8. Dimension \(n\)</a></li>
<li><a href="#org4d7367b">3.9. Notation</a></li>
<li><a href="#org76cdca7">3.10. Extrapolation de Richardson</a></li>
</ul>
</div>
</div>
<div id="outline-container-org85cf13e" class="outline-3">
<h3 id="org85cf13e"><span class="section-number-3">3.1.</span> Polynômes de Taylor</h3>
<div class="outline-text-3" id="text-3-1">
<p>
Considérons un polynôme \(p : \setR \mapsto \setR\) de degré \(n\) défini par :
</p>

<p>
\[p(x) = \sum_{i = 0}^n \gamma_i \cdot x^i\]
</p>

<p>
pour tout \(x \in \setR\). Calculons ses dérivées :
</p>

\begin{align}
\partial p(x) &= \sum_{i = 1}^n \gamma_i \cdot i \cdot x^{i - 1} \)

\(
\partial^2 p(x) &= \sum_{i = 2}^n \gamma_i \cdot i \cdot (i - 1) \cdot x^{i - 2} \)

\(
\vdots \)

\(
\partial^k p(x) &= \sum_{i = k}^n \gamma_i \cdot \frac{i !}{(i - k) !} \cdot x^{i - k} \)

\(
\vdots \)

\(
\partial^n p(x) &= n! \cdot \gamma_n
\end{align}

<p>
Lorsqu'on évalue ces dérivées en \(0\), seuls les termes en \(x^{k - k} = 1\) ne s'annulent pas. On obtient donc :
</p>

<p>
\[\partial^k p(0) = \frac{k !}{0 !} \cdot \gamma_k = k ! \cdot \gamma_k\]
</p>

<p>
ce qui nous donne l'expression des coefficients de \(p\) en fonction de ses dérivées en \(0\) :
</p>

<p>
\[\gamma_k = \unsur{k !} \cdot \partial^k p(0)\]
</p>

<p>
Le polynôme peut donc se réécrire :
</p>

<p>
\[p(x) = \sum_{i = 0}^n \unsur{i !} \cdot \partial^i p(0) \cdot x^i\]
</p>

<p>
Cette expression est appelée développement de Taylor de \(p\) autour de \(0\).
</p>
</div>
<div id="outline-container-orgd9b59c3" class="outline-4">
<h4 id="orgd9b59c3"><span class="section-number-4">3.1.1.</span> Généralisation</h4>
<div class="outline-text-4" id="text-3-1-1">
<p>
Soit \(a \in \setR\). La fonction \(r\) définie par :
</p>

<p>
\[r(t) = p(t + a) = \sum_{i = 0}^n \gamma_i \cdot (t + a)^i\]
</p>

<p>
pour tout \(t \in \setR\) est clairement un polynôme de degré \(n\). On a \(r(0) = p(a)\) et plus généralement :
</p>

<p>
\[\partial^i r(0) = \partial^i p(a)\]
</p>

<p>
pout tout \(i \ge 0\). Le développement de Taylor de \(r\) autour de \(0\) s'écrit :
</p>

<p>
\[r(t) = \sum_{i = 0}^n \unsur{i !} \cdot \partial^i r(0) \cdot t^i\]
</p>

<p>
ou encore :
</p>

<p>
\[r(t) = \sum_{i = 0}^n \unsur{i !} \cdot \partial^i p(a) \cdot t^i\]
</p>

<p>
En posant \(x = t + a\), on a \(t = x - a\) et :
</p>

<p>
\[p(x) = p(t + a) = r(t)\]
</p>

<p>
Le développement devient :
</p>

<p>
\[p(x) = \sum_{i = 0}^n \unsur{i !} \cdot \partial^i p(a) \cdot (x - a)^i\]
</p>

<p>
Cette expression est nommée développement de Taylor de \(p\) autour de \(a\).
</p>
</div>
</div>
</div>
<div id="outline-container-org53a1ec7" class="outline-3">
<h3 id="org53a1ec7"><span class="section-number-3">3.2.</span> Opérateur de Taylor</h3>
<div class="outline-text-3" id="text-3-2">
<p>
Soit \(\alpha, \beta \in \setR\) avec \(\alpha \strictinferieur \beta\), une fonction \(f \in \continue^N([\alpha,\beta],\setR)\) et \(a \in [\alpha,\beta]\). Par analogie avec le développement de Taylor des polynômes, on définit l'opérateur de Taylor \(T_a^N\) par :
</p>

<p>
\[T_a^N(f)(x) = \sum_{k = 0}^N \unsur{k !} \cdot \partial^k f(a) \cdot (x - a)^k\]
</p>

<p>
pour tout \(x \in [\alpha,\beta]\).
</p>
</div>
<div id="outline-container-orga452cef" class="outline-4">
<h4 id="orga452cef"><span class="section-number-4">3.2.1.</span> Erreur</h4>
<div class="outline-text-4" id="text-3-2-1">
<p>
L'erreur \(E_a^N\) de l'opérateur \(T_a^N\) est donnée par :
</p>

<p>
\[E_a^N(f)(x) = f(x) - T_a^N(f)(x)\]
</p>

<p>
pour tout \(x \in [\alpha,\beta]\).
</p>
</div>
</div>
<div id="outline-container-orgb66641e" class="outline-4">
<h4 id="orgb66641e"><span class="section-number-4">3.2.2.</span> Polynômes</h4>
<div class="outline-text-4" id="text-3-2-2">
<p>
Si \(p\) est un polynôme de degré \(N\), on a bien entendu \(T_a^N(p) = p\) pour tout \(a \in \setR\) et \(E_a^N(p) = 0\).
</p>
</div>
</div>
</div>
<div id="outline-container-org0c7568b" class="outline-3">
<h3 id="org0c7568b"><span class="section-number-3">3.3.</span> Forme intégrale</h3>
<div class="outline-text-3" id="text-3-3">
</div>
<div id="outline-container-org94cc5ec" class="outline-4">
<h4 id="org94cc5ec"><span class="section-number-4">3.3.1.</span> Premier ordre</h4>
<div class="outline-text-4" id="text-3-3-1">
<p>
Soit \(\alpha, \beta \in \setR\) avec \(\alpha \strictinferieur \beta\) et la fonction \(f \in \continue^2([\alpha,\beta],\setR)\). Le théorème fondamental nous dit que :
</p>

<p>
\[\int_a^x \partial f(t) \ dt = f(x) - f(a)\]
</p>

<p>
pour tout \(a,x \in [\alpha,\beta]\). Appliquant le même théorème à la dérivée \(\partial f\), on a aussi :
</p>

<p>
\[\int_a^x \partial^2 f(t) \ dt = \partial f(x) - \partial f(a)\]
</p>
</div>
<div id="outline-container-orgc66af79" class="outline-5">
<h5 id="orgc66af79"><span class="section-number-5">3.3.1.1.</span> Intégration par parties</h5>
<div class="outline-text-5" id="text-3-3-1-1">
<p>
Soit \(u = \partial f\) et \(v = \identite\). on a :
</p>

<p>
\[\int_a^x u(x) \ \partial v(x) \ dx = \int_a^x \partial f(t) \cdot 1 \ dt = \int_a^x \partial f(t) \ dt\]
</p>

<p>
L'intégration par parties nous donne :
</p>

<p>
\[\int_a^x u(x) \ \partial v(x) \ dx = v(x) \ u(x) - v(a) \ u(a) - \int_a^x v(t) \ \partial u(t) \ dt\]
</p>

<p>
En tenant compte des définitions de \(u\) et \(v\), on obtient :
</p>

<p>
\[\int_a^x \partial f(t) \ dt = x \ \partial f(x) - a \ \partial f(a) - \int_a^x t \ \partial^2 f(t) \ dt\]
</p>


<p>
Appliquons le théorème fondamental au membre de gauche :
</p>

<p>
\[f(x) - f(a) = x \ \partial f(x) - a \ \partial f(a) - \int_a^x t \ \partial^2 f(t) \ dt\]
</p>

<p>
ou encore :
</p>

<p>
\[f(x) = f(a) + x \ \partial f(x) - a \ \partial f(a) - \int_a^x t \ \partial^2 f(t) \ dt\]
</p>

<p>
En multipliant la relation :
</p>

<p>
\[\partial f(x) - \partial f(a) = \int_a^x \partial^2 f(t) \ dt\]
</p>

<p>
par \(x\), on arrive au résultat :
</p>

<p>
\[x \ \partial f(x) = x \ \partial f(a) + \int_a^x x \ \partial^2 f(t) \ dt\]
</p>

<p>
L'expression de \(f(x)\) devient alors :
</p>

<p>
\[f(x) = f(a) + x \ \partial f(a) + \int_a^x x \ \partial^2 f(t) \ dt - a \ \partial f(a) - \int_a^x t \ \partial^2 f(t) \ dt\]
</p>

<p>
et finalement :
</p>

<p>
\[f(x) = f(a) + (x - a) \cdot \partial f(a) + \int_a^x (x - t) \cdot \partial^2 f(t) \ dt\]
</p>

<p>
Le membre de droite est appelé développement de Taylor du premier ordre de \(f\) sous forme intégrale.
</p>
</div>
</div>
</div>
<div id="outline-container-orgfa07972" class="outline-4">
<h4 id="orgfa07972"><span class="section-number-4">3.3.2.</span> Second ordre</h4>
<div class="outline-text-4" id="text-3-3-2">
<p>
Soit \(f \in \continue^3([\alpha,\beta],\setR)\). Comme \(\continue^3 \subseteq \continue^2\), \(f\) admet un développement de Taylor du premier ordre sous forme intégrale. Nous allons intégrer par parties le terme :
</p>

<p>
\[\int_a^x (x - t) \cdot \partial^2 f(t) \ dt\]
</p>

<p>
On sait que :
</p>

<p>
\[\OD{}{ŧ} \left[ \unsur{2} (x - t)^2 \right] = (x - t) \cdot (-1) = - (x - t)\]
</p>

<p>
Posons \(u = \partial^2 f\) et :
</p>

<p>
\[v : t \mapsto \unsur{2} (x - t)^2\]
</p>

<p>
On a :
</p>

<p>
\[\int_a^x \partial v(t) \ u(t) \ dt = - \int_a^x (x - t) \ \partial^2 f(t) \ dt\]
</p>

<p>
et :
</p>

<p>
\[\int_a^x v(t) \ \partial u(t) \ dt = \unsur{2} \int_a^x (x - t)^2 \ \partial^3 f(t) \ dt\]
</p>

<p>
Enfin :
</p>

\begin{align}
\int_a^x \partial (v \cdot u)(t) \ dt &= \unsur{2} \ (x - x)^2 \ \partial^2 f(x) - \unsur{2} \ (x - a)^2 \ \partial^2 f(a) \)

\(
&= 0 - \unsur{2} \ (x - a)^2 \ \partial^2 f(a) \)

\(
&= - \unsur{2} \ (x - a)^2 \ \partial^2 f(a)
\end{align}

<p>
On en conclut que :
</p>

<p>
\[- \int_a^x (x - t) \ \partial^2 f(t) \ dt = - \unsur{2} \ (x - a)^2 \ \partial^2 f(a) - \unsur{2} \int_a^x (x - t)^2 \ \partial^3 f(t) \ dt\]
</p>

<p>
ou encore :
</p>

<p>
\[\int_a^x (x - t) \ \partial^2 f(t) \ dt = \unsur{2} \ (x - a)^2 \ \partial^2 f(a) + \unsur{2} \int_a^x (x - t)^2 \ \partial^3 f(t) \ dt\]
</p>

<p>
Le développement du premier ordre peut dont se réécrire :
</p>

<p>
\[f(x) = f(a) + (x - a) \ \partial f(a) + \unsur{2} \ (x - a)^2 \ \partial^2 f(a) + \unsur{2} \int_a^x (x - t)^2 \ \partial^3 f(t) \ dt\]
</p>

<p>
Le membre de droite est appelé développement du second ordre de \(f\) sous forme intégrale.
</p>
</div>
</div>
<div id="outline-container-org4e3ede2" class="outline-4">
<h4 id="org4e3ede2"><span class="section-number-4">3.3.3.</span> Ordre \(N\)</h4>
<div class="outline-text-4" id="text-3-3-3">
<p>
Soit \(f \in \continue^{N + 1}([\alpha,\beta],\setR)\). On montre en intégrant par parties que :
</p>

<p>
\[\int_a^x (x - t)^{k - 1} \ \partial^k f(t) \ dt = \unsur{k} \ (x - a)^k \ \partial^k f(a) + \unsur{k} \int_a^x (x - t)^k \ \partial^{k + 1} f(t) \ dt\]
</p>

<p>
pour tout \(k \in \setZ[2,N]\). On en déduit par récurrence le développement de Taylor d'ordre \(N\) de \(f\) sous forme intégrale :
</p>

<p>
\[f(x) = \sum_{k = 0}^N \unsur{k !} \cdot \partial^k f(a) \cdot (x - a)^k + \unsur{N !} \int_a^x (x - t)^N \ \partial^{N + 1} f(t) \ dt\]
</p>
</div>
</div>
</div>
<div id="outline-container-org7a2f2a9" class="outline-3">
<h3 id="org7a2f2a9"><span class="section-number-3">3.4.</span> Erreur</h3>
<div class="outline-text-3" id="text-3-4">
<p>
On a :
</p>

<p>
\[E_a^N(f)(x) = f(x) - T_a^N(f)(x) = \unsur{N !} \int_a^x (x - t)^N \ \partial^{N + 1} f(t) \ dt\]
</p>

<p>
En appliquant le théorème de Cauchy entre \(a\) et \(x\) aux fonctions \(F,G\) définies par :
</p>

\begin{align}
F(z) &= \int_a^z (x - t)^N \ \partial^{N + 1} f(t) \ dt \)

\(
G(z) &= \int_a^z (x - t)^N \ dt
\end{align}

<p>
pour tout \(z \in [\alpha,\beta]\), on voit que l'on peut trouver un \(c \in \intervalleouvert{a}{x}\) si \(a \strictinferieur x\) ou un \(c \in \intervalleouvert{x}{a}\) si \(x \strictinferieur a\) tel que :
</p>

<p>
\[(x - c)^N \ F(x) = (x - c)^N \ \partial^{N + 1} f(c) \ G(x)\]
</p>

<p>
ou encore :
</p>

<p>
\[\partial^{N + 1} f(c) \ G(x) = F(x)\]
</p>

<p>
Comme :
</p>

\begin{align}
G(x) = \int_a^x (x - t)^N \ dt &= - \big[ (x - x)^{N + 1} - (x - a)^{N + 1} \big] / (N + 1) \)

\(
&= - \big[ 0 - (x - a)^{N + 1} \big] / (N + 1) \)

\(
&= (x - a)^{N + 1} / (N + 1)
\end{align}

<p>
on a :
</p>

<p>
\[\partial^{N + 1} f(c) \ \frac{ (x - a)^{N + 1} }{N + 1} = F(x) = \int_a^x (x - t)^N \ \partial^{N + 1} f(t) \ dt\]
</p>

<p>
On en déduit que :
</p>

<p>
\[E_a^N(f)(x) = \partial^{N + 1} f(c) \ \frac{ (x - a)^{N + 1} }{(N + 1) !}\]
</p>
</div>
</div>
<div id="outline-container-orgfbfaa00" class="outline-3">
<h3 id="orgfbfaa00"><span class="section-number-3">3.5.</span> Forme différentielle</h3>
<div class="outline-text-3" id="text-3-5">
<p>
Soit une fonction \(f \in \continue^{N+1}([\alpha,\beta],\setR)\) et \(a,x \in [\alpha,\beta]\). On définit la fonction \(F : [\alpha,\beta] \mapsto \setR\) par :
</p>

\begin{align}
F(t) &= \sum_{k = 0}^N \unsur{k !} \cdot \partial^k f(t) \cdot (x - t)^k \)

\(
&= f(t) + \partial f(t) \ (x - t) + \partial^2 f(t) \ \frac{(x - t)^2}{2} + ...
\end{align}

<p>
pour tout \(t \in [\alpha,\beta]\). On a :
</p>

<p>
\[F(x) = f(x) + \partial f(x) \ (x - x) + \partial^2 f(x) \frac{(x - x)^2}{2} + ... = f(x) + 0 = f(x)\]
</p>

<p>
et :
</p>

<p>
\[F(a) = f(a) + \partial f(a) \ (x - a) + \partial^2 f(a) \frac{(x - a)^2}{2} + ... = T_a^N(f)(x)\]
</p>

<p>
La dérivée de \(F\) s'écrit :
</p>

<div class="org-center">
<p>
\(
\partial F(t) = \partial f(t) + \big[ \partial f(t) \ (-1) + \partial^2 f(t) \ (x - t) \big] \)
</p>

<p>
\(
</p>
<ul class="org-ul">
<li>\left[ - &part;<sup>2</sup> f(t) \ (x - t) + &part;<sup>3</sup> f(t) \ \frac{(x-t)^2}{2} \right] \)</li>
</ul>

<p>
\(
... \)
</p>

<p>
\(
</p>
<ul class="org-ul">
<li>\left[ - &part;<sup>N - 1</sup> f(t) \ \frac{(x - t)<sup>N - 2</sup>}{(N - 2) !} + &part;<sup>N</sup> f(t) \ \frac{(x-t)<sup>N - 1</sup>}{(N - 1) !} \right] \)</li>
</ul>

<p>
\(
</p>
<ul class="org-ul">
<li>\left[ - &part;<sup>N</sup> f(t) \ \frac{(x-t)<sup>N - 1</sup>}{(N - 1) !} + &part;<sup>N + 1</sup> f(t) \ \frac{(x-t)^N}{N !} \right]</li>
</ul>
<p>
\)
</p>
</div>

<p>
On voit que tous les termes s'annulent sauf le dernier, et :
</p>

<p>
\[\partial F(t) = \partial^{N + 1} f(t) \ \frac{(x-t)^N}{N !}\]
</p>

<p>
Soit \(G \in \continue^1([\alpha,\beta],\setR)\). On peut appliquer le théorème de Cauchy à \(F\) et \(G\) entre \(a\) et \(x\). On dispose alors d'un \(c \in \intervalleouver{a}{x}\) si \(a \strictinferieur x\) ou d'un \(c \in \intervalleouvert{x}{a}\) si \(x \strictinferieur a\) tel que :
</p>

<p>
\[\partial F(c) \ \big[G(x) - G(a)\big] = \big[F(x) - F(a)\big] \ \partial G(c)\]
</p>

<p>
On a :
</p>

<p>
\[F(x) - F(a) = f(x) - T_a^N(f)(x) = E_a^N(f)(x)\]
</p>

<p>
On en conclut que :
</p>

<p>
\[E_a^N(f)(x) \ \partial G(c) = \partial^{N + 1} f(c) \ \frac{(x-c)^N}{N !} \ \big[G(x) - G(a)\big]\]
</p>
</div>
<div id="outline-container-org47e1b17" class="outline-4">
<h4 id="org47e1b17"><span class="section-number-4">3.5.1.</span> Forme de Lagrange</h4>
<div class="outline-text-4" id="text-3-5-1">
<p>
Soit le choix :
</p>

<p>
\[G : t \mapsto (x - t)^{N + 1}\]
</p>

<p>
on a :
</p>

<p>
\[G(x) = (x - x)^{N + 1} = 0\]
</p>

<p>
et :
</p>

<p>
\[G(a) = (x - a)^{N + 1}\]
</p>

<p>
La dérivée s'écrit :
</p>

<p>
\[\partial G(t) = - (N  + 1) \ (x - t)^N\]
</p>

<p>
La relation de Cauchy devient :
</p>

<p>
\[- E_a^N(f)(x) \ (N  + 1) \ (x - c)^N = - \partial^{N + 1} f(c) \ \frac{(x-c)^N}{N !} \ (x - a)^{N + 1}\]
</p>

<p>
On a donc l'expression de l'erreur :
</p>

<p>
\[E_a^N(f)(x) = \partial^{N + 1} f(c) \ \frac{(x - a)^{N + 1}}{(N + 1) !}\]
</p>
</div>
</div>
<div id="outline-container-orgb323af4" class="outline-4">
<h4 id="orgb323af4"><span class="section-number-4">3.5.2.</span> Forme de Cauchy</h4>
<div class="outline-text-4" id="text-3-5-2">
<p>
Soit le choix :
</p>

<p>
\[G : t \mapsto t - a\]
</p>

<p>
on a :
</p>

<p>
\[G(x) = x - a\]
</p>

<p>
et :
</p>

<p>
\[G(a) = a - a = 0\]
</p>

<p>
La dérivée s'écrit :
</p>

<p>
\[\partial G(t) = 1\]
</p>

<p>
La relation de Cauchy devient :
</p>

<p>
\[E_a^N(f)(x) = \partial^{N + 1} f(c) \ \frac{(x-c)^N}{N !} \ (x - a)\]
</p>
</div>
</div>
</div>
<div id="outline-container-org28e0d70" class="outline-3">
<h3 id="org28e0d70"><span class="section-number-3">3.6.</span> Borne</h3>
<div class="outline-text-3" id="text-3-6">
<p>
Soit \(f \in \continue^{N + 1}([\alpha,\beta],\setR)\). Comme \(\partial^{N+1} f\) est continue, sa norme \(\norme{.}_\infty\) sur \([\alpha,\beta]\) est finie et on a :
</p>

<p>
\[\abs{E_a^N(f)(x)} \le \norme{\partial^{N + 1} f}_\infty \ \frac{ \abs{x - a}^{N + 1} }{(N + 1) !}\]
</p>

<p>
On peut majorer cette expression en constatant que :
</p>

<p>
\[\abs{x - a} \le \abs{\beta - \alpha}\]
</p>

<p>
La borne de l'erreur devient alors :
</p>

<p>
\[\abs{E_a^N(f)(x)} \le \norme{\partial^{N + 1} f}_\infty \ \frac{ \abs{\beta - \alpha}^{N + 1} }{(N + 1) !}\]
</p>

<p>
Le membre de droite ne dépendant pas de \(x\), on a :
</p>

<p>
\[\norme{E_a^N(f)}_\infty \le \norme{\partial^{N + 1} f}_\infty \ \frac{ \abs{\beta - \alpha}^{N + 1} }{(N + 1) !}\]
</p>
</div>
</div>
<div id="outline-container-orge9cbea0" class="outline-3">
<h3 id="orge9cbea0"><span class="section-number-3">3.7.</span> Convergence</h3>
<div class="outline-text-3" id="text-3-7">
<p>
Soit \(f \in \continue^\infty([\alpha,\beta],\setR)\). Si on peut trouver un \(\sigma \in \setR\) tel que :
</p>

<p>
\[\norme{\partial^n f}_\infty \le \sigma\]
</p>

<p>
pour tout \(n \in \setN\), on a :
</p>

<p>
\[\norme{E_a^N(f)}_\infty \le \sigma \ \frac{ \abs{\beta - \alpha}^{N + 1} }{(N + 1) !}\]
</p>

<p>
On en conclut que :
</p>

<p>
\[0 \le \lim_{N \to \infty} \norme{E_a^N(f)}_\infty \le \sigma \ \lim_{N \to \infty} \frac{ \abs{\beta - \alpha}^{N + 1} }{(N + 1) !} = 0\]
</p>

<p>
L'erreur converge vers zéro quand \(N\) tend vers l'infini :
</p>

<p>
\[\lim_{N \to \infty} \norme{E_a^N(f)}_\infty = 0\]
</p>
</div>
</div>
<div id="outline-container-org9ff4683" class="outline-3">
<h3 id="org9ff4683"><span class="section-number-3">3.8.</span> Dimension \(n\)</h3>
<div class="outline-text-3" id="text-3-8">
</div>
<div id="outline-container-orgdf77872" class="outline-4">
<h4 id="orgdf77872"><span class="section-number-4">3.8.1.</span> Premier ordre</h4>
<div class="outline-text-4" id="text-3-8-1">
<p>
Soit \(\Omega \subseteq \setR^n\), la fonction \(f \in \continue^1(\Omega,\setR)\) et les vecteurs \(u,v \in \setR^n\) tels que le segment \([u,v]\) est inclus dans \(\Omega\). On définit la fonction \(\lambda : [0,1] \mapsto \setR^n\) associée au segment \([u,v]\) par :
</p>

<p>
\[\lambda(s) = u + s \cdot (v - u)\]
</p>

<p>
pour tout \(s \in [0,1]\), ainsi que la fonction \(\varphi = f \circ \lambda\) qui vérifie :
</p>

<p>
\[\varphi(s) = (f \circ \lambda)(s) = f(u + s \cdot (v - u))\]
</p>

<p>
pour tout \(s \in [0,1]\). On pose :
</p>

<p>
\[h = v - u\]
</p>

<p>
On a :
</p>

<p>
\[\varphi(0) = f(u)\]
</p>

<p>
La dérivée s'écrit :
</p>

<p>
\[\partial \varphi(s) = \sum_i \partial_i f(u + s \cdot h) \cdot h_i\]
</p>

<p>
ou, en utilisant la notation matricielle :
</p>

<p>
\[\partial \varphi(s) = \partial f(u + s \cdot h) \cdot h\]
</p>

<p>
On a la valeur particulière :
</p>

<p>
\[\partial \varphi(0) = \partial f(u) \cdot h\]
</p>

<p>
La dérivée seconde s'écrit :
</p>

<p>
\[\partial^2 \varphi(s) = \sum_{i,j} h_j \cdot \partial^2_{ji} f(u + s \cdot h) \cdot h_i\]
</p>

<p>
ou, en utilisant la notation matricielle :
</p>

<p>
\[\partial^2 \varphi(s) = h^\dual \cdot \partial^2 f(u + s \cdot h) \cdot h\]
</p>

<p>
Le développement du premier ordre de \(\varphi\) autour de \(0\) s'écrit donc :
</p>

<p>
\[\varphi(s) = f(u) + s \cdot \partial f(u) \cdot h + E_u^1(s,h)\]
</p>

<p>
avec :
</p>

<p>
\[E_u^1(s,h) = h^\dual \cdot \partial^2 f(u + c \cdot h) \cdot h \cdot \frac{(c - 0)^2}{2} =  h^\dual \cdot \partial^2 f(u + c \cdot h) \cdot h \cdot \frac{c^2}{2}\]
</p>

<p>
pour un certain \(c \in \intervalleouvert{0}{s}\). Mais comme :
</p>

<p>
\[\varphi(1) = f(u + h) = f(v)\]
</p>

<p>
on en déduit le développement de \(f\) :
</p>

<p>
\[f(v) = f(u) + \partial f(u) \cdot (v - u) + \mathcal{E}_u^1(h)\]
</p>

<p>
avec :
</p>

<p>
\[\mathcal{E}_u^1(h) = h^\dual \cdot \partial^2 f(u + c \cdot h) \cdot h \cdot \frac{c^2}{2}\]
</p>

<p>
pour un certain \(c \in \intervalleouvert{0}{1}\).
</p>
</div>
<div id="outline-container-org3c00971" class="outline-5">
<h5 id="org3c00971"><span class="section-number-5">3.8.1.1.</span> Borne</h5>
<div class="outline-text-5" id="text-3-8-1-1">
<p>
Soit :
</p>

<p>
\[M^2 = \max_{i,j} \norme{\partial^2_{ij} f}_\infty\]
</p>

<p>
On a :
</p>

<p>
\[\abs{\mathcal{E}_u^1(h)} \le \unsur{2} \cdot n^2 \cdot M^2 \cdot \norme{h}^2\]
</p>
</div>
</div>
</div>
<div id="outline-container-orgf7be0cf" class="outline-4">
<h4 id="orgf7be0cf"><span class="section-number-4">3.8.2.</span> Second ordre</h4>
<div class="outline-text-4" id="text-3-8-2">
<p>
Soit \(f \in \continue^3(\Omega,\setR)\). Avec les mêmes notations que précédemment, on a :
</p>

<p>
\[\partial^2 \varphi(0) = h^\dual \cdot \partial^2 f(u) \cdot h\]
</p>

<p>
La dérivée tierce de \(\varphi\) s'écrit :
</p>

<p>
\[\partial^3 \varphi(s) = \sum_{i,j,k} \partial^3_{kji} f(u + s \cdot h) \cdot h_i \cdot h_j \cdot h_k\]
</p>

<p>
ou, en utilisant la notation tensorielle :
</p>

<p>
\[\partial^3 \varphi(s) = \partial^3 f(u + s \cdot h) : h \otimes h \otimes h\]
</p>

<p>
Le développement  du second ordre de \(\varphi\) autour de \(0\) s'écrit :
</p>

<p>
\[\varphi(s) = f(u) + s \ \partial f(u) \cdot h + \frac{s^2}{2} \ h^\dual \cdot \partial^2 f(u) \cdot h + E_u^2(s,h)\]
</p>

<p>
avec :
</p>

<p>
\[E_u^2(s,h) = \partial^3 f(u + c \cdot h) : h \otimes h \otimes h \cdot \frac{c^3}{6}\]
</p>

<p>
pour un certain \(c \in \intervalleouvert{0}{s}\). Mais comme :
</p>

<p>
\[\varphi(1) = f(u + h) = f(v)\]
</p>

<p>
on en déduit le développement de \(f\) :
</p>

<p>
\[f(v) = f(u) + \partial f(u) \cdot h + h^\dual \cdot \partial^2 f(u) \cdot h + \mathcal{E}_u^2(h)\]
</p>

<p>
avec :
</p>

<p>
\[\mathcal{E}_u^2(h) = \partial^3 f(u + c \cdot h) : h \otimes h \otimes h \cdot \frac{c^3}{6}\]
</p>

<p>
pour un certain \(c \in \intervalleouvert{0}{1}\).
</p>
</div>
<div id="outline-container-org718d4cc" class="outline-5">
<h5 id="org718d4cc"><span class="section-number-5">3.8.2.1.</span> Borne</h5>
<div class="outline-text-5" id="text-3-8-2-1">
<p>
Soit :
</p>

<p>
\[M^3 = \max_{i,j,k} \norme{\partial^3_{ijk} f}_\infty\]
</p>

<p>
On a :
</p>

<p>
\[\abs{\mathcal{E}_u^2(h)} \le \unsur{6} \cdot n^3 \cdot M^3 \cdot \norme{h}^3\]
</p>
</div>
</div>
</div>
</div>
<div id="outline-container-org4d7367b" class="outline-3">
<h3 id="org4d7367b"><span class="section-number-3">3.9.</span> Notation</h3>
<div class="outline-text-3" id="text-3-9">
<p>
Soit la fonction \(E : \Omega \subseteq \setR^m \mapsto \setR^n\), la fonction \(b : \setR \mapsto \setR\) et le vecteur \(h \in \Omega\). On note \(E \sim \petito{b(h)}\), ou on dit que \(E\) est en \(\petito{b(h)}\), pour signifier que :
</p>

<p>
\[\lim_{h \to 0} \frac{ \norme{E(h)} }{b(\norme{h})} = 0\]
</p>

<p>
On note \(E \sim \grando{b(h)}\), ou on dit que \(E\) est en \(\grando{b(h)}\), pour signifier qu'il existe \(M \in \setR\) tel que :
</p>

<p>
\[\norme{E(h)} \le M \cdot b(\norme{h})\]
</p>

<p>
pour tout \(h \in \Omega\).
</p>
</div>
<div id="outline-container-org87a4ffd" class="outline-4">
<h4 id="org87a4ffd"><span class="section-number-4">3.9.1.</span> Puissance</h4>
<div class="outline-text-4" id="text-3-9-1">
<p>
Une famille de fonction souvent employée est la puissance :
</p>

<p>
\[b_k : x \mapsto x^k\]
</p>

<p>
pour un certain \(k \in \setN\). On a alors \(\petito{h^k}\) si :
</p>

<p>
\[\lim_{h \to 0} \frac{ \norme{E(h)} }{\norme{h}^k} = 0\]
</p>

<p>
et \(\grando{h^k}\) si :
</p>

<p>
\[\norme{E(h)} \le M \cdot \norme{h}^k\]
</p>
</div>
</div>
<div id="outline-container-org166983d" class="outline-4">
<h4 id="org166983d"><span class="section-number-4">3.9.2.</span> Relation</h4>
<div class="outline-text-4" id="text-3-9-2">
<p>
Si \(E \sim \grando{h^k}\), on a :
</p>

<p>
\[0 \le \lim_{h \to 0} \frac{\norme{E(h)}}{\norme{h}^{k - 1}} \le \lim_{h \to 0} \frac{M \ \norme{h}^k}{\norme{h}^{k - 1}} = 0\]
</p>

<p>
d'où :
</p>

<p>
\[\lim_{h \to 0} \frac{\norme{E(h)}}{\norme{h}^{k - 1}} = 0\]
</p>

<p>
et \(E \sim \petito{h^{k - 1}}\).
</p>
</div>
</div>
<div id="outline-container-org3a8669a" class="outline-4">
<h4 id="org3a8669a"><span class="section-number-4">3.9.3.</span> Cas particulier</h4>
<div class="outline-text-4" id="text-3-9-3">
<p>
Le \(\grando{1}\) implique une erreur bornée en valeur absolue, le \(\petito{1}\) implique la continuité et le \(\petito{h}\) la différentiabilité.
</p>
</div>
</div>
<div id="outline-container-org6b07b4c" class="outline-4">
<h4 id="org6b07b4c"><span class="section-number-4">3.9.4.</span> Développement de Taylor</h4>
<div class="outline-text-4" id="text-3-9-4">
<p>
Pour toute fonction \(f \in \continue^{N + 1}(\Omega, \setR^n)\), l'erreur \(E_a^N(f)\) du développement de Taylor d'ordre \(N\) est en \(\grando{h^{N+1}}\).
</p>
</div>
</div>
</div>
<div id="outline-container-org76cdca7" class="outline-3">
<h3 id="org76cdca7"><span class="section-number-3">3.10.</span> Extrapolation de Richardson</h3>
<div class="outline-text-3" id="text-3-10">
<p>
Supposons qu'une fonction \(v\) nous donne une approximation de \(V\) respectant :
</p>

<p>
\[v(h) \approx V + C \cdot h^m + O(h^{m+1})\]
</p>

<p>
pour un certain \(C \in \setR\) et pour tout \(h \in [0,R] \subseteq \setR\). L'entier \(m\) est appelé l'ordre de l'approximation. Supposons que l'on dispose de deux estimations de \(V_1 = v(h)\) et \(V_2 = v(h/k)\). On a alors :
</p>

<div class="org-center">
<p>
\(
V_1 = v(h) = V + C \cdot h^m + O(h^{m+1}) \)
</p>

<p>
\(
V<sub>2</sub> = v\left(h/k\right) = V + C &sdot; \left(\frac{h}{k}\right)<sup>m</sup>
</p>
<ul class="org-ul">
<li>O(h<sup>m+1</sup>)</li>
</ul>
<p>
\)
</p>
</div>

<p>
On se sert de la première équation pour obtenir une expression de \(C \cdot h^m\) :
</p>

<p>
\[C \cdot h^m = V_1 - V + O(h^{m+1})\]
</p>

<p>
Posons :
</p>

<p>
\[r = \unsur{k^m}\]
</p>

<p>
On a alors :
</p>

<p>
\[V_2 = V + r \cdot C \cdot h^m + O(h^{m+1}) = V + r \cdot (V_1 - V) + O(h^{m+1})\]
</p>

<p>
On en conclut que :
</p>

<p>
\[(1 - r) \cdot V = V_2 - r \cdot V_1 + O(h^{m+1})\]
</p>

<p>
Ce qui nous donne l'approximation :
</p>

<p>
\[V = \frac{V_2 - r \cdot V_1}{1 - r} + O(h^{m+1})\]
</p>

<p>
Cette approximation est plus précise, car l'erreur n'est plus en \(O(h^m)\) mais en \(O(h^{m + 1})\). On appelle cette technique l'extrapolation de Richardson.
</p>
</div>
<div id="outline-container-org3706887" class="outline-4">
<h4 id="org3706887"><span class="section-number-4">3.10.1.</span> Cas particulier</h4>
<div class="outline-text-4" id="text-3-10-1">
<p>
Un cas particulier intéressant est celui où l'approximation est d'ordre \(1\) et où \(k = 2\). On a alors :
</p>

<p>
\[V = 2 V_2 - V_1 + O(h^2) = V_2 + (V_2 - V_1) + O(h^2)\]
</p>

<p>
ce qui revient à faire l'approximation \(V - V_2 \approx V_2 - V_1\).
</p>
</div>
</div>
</div>
</div>
<div id="outline-container-orgd915616" class="outline-2">
<h2 id="orgd915616"><span class="section-number-2">4.</span> Développements d'Hadamard</h2>
<div class="outline-text-2" id="text-4">
<div id="text-table-of-contents-4" role="doc-toc">
<ul>
<li><a href="#orgb898d94">4.1. Dépendances</a></li>
<li><a href="#orgd35bc8b">4.2. Lemme de Hadamard</a></li>
<li><a href="#org792cdde">4.3. Développement du second ordre</a></li>
<li><a href="#org22caffa">4.4. Développement du troisième ordre</a></li>
</ul>
</div>

<p>
\label{chap:fonda}
</p>
</div>
<div id="outline-container-orgb898d94" class="outline-3">
<h3 id="orgb898d94"><span class="section-number-3">4.1.</span> Dépendances</h3>
<div class="outline-text-3" id="text-4-1">
<ul class="org-ul">
<li>Chapitre \ref{chap:differ} : Les différentielles</li>
<li>Chapitre \ref{chap:integral} : Les intégrales</li>
</ul>
</div>
</div>
<div id="outline-container-orgd35bc8b" class="outline-3">
<h3 id="orgd35bc8b"><span class="section-number-3">4.2.</span> Lemme de Hadamard</h3>
<div class="outline-text-3" id="text-4-2">
<p>
Soit la fonction \(f : \setR^m \to \setR^n\) et les vecteurs \(u,v \in \setR^m\). On définit la fonction \(\lambda : [0,1] \mapsto \setR^m\) associée au segment \([u,v] \subseteq \setR^m\) par :
</p>

<p>
\[\lambda(s) = u + s \cdot (v - u)\]
</p>

<p>
pour tout \(s \in [0,1]\). On a bien entendu \(\lambda(0) = u\) et \(\lambda(1) = v\). On définit également la fonction \(\varphi = f \circ \lambda\) qui vérifie :
</p>

<p>
\[\varphi(s) = (f \circ \lambda)(s) = f(u + s \cdot (v - u))\]
</p>

<p>
pour tout \(t \in [0,1]\). On voit que \(\varphi(0) = f(u)\) et \(\varphi(1) = f(v)\). Donc, en termes de composantes dans \(\setR^n\), on a :
</p>

<p>
\[f_i(v) - f_i(u) = \varphi_i(1) - \varphi_i(0) = \int_0^1 \OD{\varphi_i}{s}(s) \ ds\]
</p>

<p>
où \(i \in \{1,2,...,n\}\).
</p>

<p>
Voyons quelle est la forme de la dérivée :
</p>

\begin{align}
\OD{\varphi_i}{s}(s) &= \sum_j \partial_j f_i(u + s \cdot (v - u)) \cdot \partial \lambda_j(s) \)

\(
&= \sum_j \partial_j f_i(u + s \cdot (v - u)) \cdot (v_j - u_j)
\end{align}

<p>
où \(j \in \{1,2,...,m\}\). Si nous définissons :
</p>

<p>
\[G_{ij}(u,v) = \int_0^1 \partial_j f_i(u + s \cdot (v - u)) \ ds\]
</p>

<p>
nous obtenons alors l'expression de la variation :
</p>

<p>
\[f_i(v) - f_i(u) = \sum_j G_{ij}(u,v) \cdot (v_j - u_j)\]
</p>

<p>
En termes matriciels :
</p>

<p>
$$G(u,v) = \big[G<sub>ij</sub>(u,v)\big]<sub>i,j</sub> = \left[ \int_0^1 \partial_j f_i(u + s \cdot (v - u)) \ ds \right]<sub>i,j</sub>
</p>

<p>
est donc l'intégrale de la Jacobienne :
</p>

<p>
\[G(u,v) = \int_0^1 \partial f(u + s \cdot (v - u)) \ ds\]
</p>

<p>
et :
</p>

<p>
\[f(v) - f(u) = G(u,v) \cdot (v - u)\]
</p>
</div>
</div>
<div id="outline-container-org792cdde" class="outline-3">
<h3 id="org792cdde"><span class="section-number-3">4.3.</span> Développement du second ordre</h3>
<div class="outline-text-3" id="text-4-3">
<p>
Soit la fonction \(f \in \continue^2(\setR^n,\setR)\) et les vecteurs \(a,h \in \setR^n\). On définit la fonction \(\lambda : [0,1] \mapsto \setR^n\) associée au segment \([a, a + h]\) :
</p>

<p>
\[\lambda(s) = a + s \cdot h\]
</p>

<p>
pour tout \(s \in [0,1]\). Le lemme de Hadamard nous dit que :
</p>

<p>
\[f(a + h) - f(a) = \int_0^1 \partial f(a + s \cdot h) \cdot h \ ds\]
</p>

<p>
Par définition de la dérivée seconde, on a :
</p>

<p>
\[\partial_i f(a + s \cdot h) = \partial_i f(a) + \sum_j \partial_{ji}^2 f(a) \cdot h_j \cdot s + e_i(s \cdot h)\]
</p>

<p>
où l'erreur \(e\) vérifie :
</p>

<p>
\[\lim_{h \to 0} \frac{ \norme{e(h)} }{ \norme{h} } = 0\]
</p>

<p>
L'intégrale s'écrit alors :
</p>

<p>
\[f(a + h) - f(a) = \sum_i \int_0^1 \left[ \partial_i f(a) + \sum_j \partial_{ji}^2 f(a) \cdot h_j \cdot s + e_i(s \cdot h) \right]  \cdot h_i \ ds\]
</p>

<p>
La grandeur \(\partial_i f(a) \cdot h_i\) ne dépendant pas de \(s\), on a :
</p>

<p>
\[\int_0^1 \partial_i f(a) \cdot h_i \ ds = \partial_i f(a) \cdot h_i \cdot (1 - 0) = \partial_i f(a) \cdot h_i\]
</p>

<p>
D'un autre coté, comme \(s^2/2\) est une primitive de \(s\), on a :
</p>

<p>
\[\int_0^1 s \ ds = \unsur{2} \cdot (1^2 - 0^2) = \unsur{2}\]
</p>

<p>
et donc :
</p>

<p>
\[\int_0^1 \partial_{ji}^2 f(a) \cdot h_j \cdot h_i \cdot s \ ds = \unsur{2} \partial_{ji}^2 f(a) \cdot h_j \cdot h_i\]
</p>

<p>
Posons :
</p>

<p>
\[\mathcal{E}_2(h) = \sum_i \int_0^1 e_i(s \cdot h) \cdot h_i \ ds\]
</p>

<p>
On a alors :
</p>

<p>
\[f(a + h) - f(a) = \sum_i \partial_i f(a) \cdot h_i + \unsur{2} \sum_{i,j} h_j \cdot \partial_{ji}^2 f(a) \cdot h_i + \mathcal{E}_2(h)\]
</p>

<p>
En termes matriciels, cette expression fait intervenir la Jacobienne et la Hessienne :
</p>

<p>
\[f(a + h) - f(a) = \partial f(a) \cdot h + \unsur{2} \ h^\dual \cdot \partial^2 f(a) \cdot h + \mathcal{E}_2(h)\]
</p>
</div>
<div id="outline-container-org3a42864" class="outline-4">
<h4 id="org3a42864"><span class="section-number-4">4.3.1.</span> Comportement de l'erreur</h4>
<div class="outline-text-4" id="text-4-3-1">
<p>
Nous savons que, pour toute précision \(\epsilon \strictsuperieur 0\), nous pouvons trouver un \(\delta \strictsuperieur 0\) tel que :
</p>

<p>
\[\frac{\norme{e(h)}}{\norme{h}} \le \epsilon\]
</p>

<p>
pour tout \(h\) vérifiant \(\norme{h} \le \delta\). Comme \(\abs{e_i} \le \norme{e}\) et \(\abs{h_i} \le \norme{h}\), on a alors :
</p>

\begin{align}
\abs{\mathcal{E}_2(h)} &\le \sum_i \abs{\int_0^1 e_i(s \cdot h) \cdot h_i \ ds} \)

\(
&\le n \cdot \epsilon \cdot \norme{h}^2
\end{align}

<p>
L'erreur décroît donc plus vite que \(\norme{h}^2\) :
</p>

<p>
\[\lim_{h \to 0} \frac{ \abs{\mathcal{E}_2(h)} }{ \norme{h}^2 } = 0\]
</p>
</div>
</div>
<div id="outline-container-org292438b" class="outline-4">
<h4 id="org292438b"><span class="section-number-4">4.3.2.</span> Dérivées ordinaires</h4>
<div class="outline-text-4" id="text-4-3-2">
<p>
Lorsque \(n = 1\), le développement est simplement :
</p>

<p>
\[f(a + h) = f(a) + \OD{f}{x}(a) \cdot h + \OOD{f}{x}(a) \cdot \frac{h^2}{2} + \mathcal{E}_2(h)\]
</p>

<p>
On constate qu'il est analogue au développement de Taylor d'ordre deux autour de \(a\).
</p>
</div>
</div>
</div>
<div id="outline-container-org22caffa" class="outline-3">
<h3 id="org22caffa"><span class="section-number-3">4.4.</span> Développement du troisième ordre</h3>
<div class="outline-text-3" id="text-4-4">
<p>
Soit la fonction \(f \in \continue^3(\setR^n,\setR)\) et les vecteurs \(a,h \in \setR^n\). En évaluant le développement du second ordre de chaque \(\partial_i f\), on a :
</p>

<p>
\[\partial_i f(a + s \cdot h) = \partial_i f(a) + \sum_j \partial_{ji} f(a) \cdot h_j \cdot s + \sum_{j,k} h_k \cdot \partial_{kji}^3 f(a) \cdot h_j \cdot \frac{s^2}{2} + e_i(h)\]
</p>

<p>
où \(e \sim \petito{h^2}\). En intégrant, nous obtenons une estimation de la variation de \(f\) :
</p>

<p>
\[f(a + h) - f(a) = \sum_i \int_0^1 \partial_i f(a + s \cdot h) \cdot h_i \ ds\]
</p>

<p>
Posons :
</p>

\begin{align}
I_1(h) &= \sum_i \int_0^1 \partial_i f(a) \cdot h_i \ ds \)

\(
I_2(h) &= \sum_{i,j} \int_0^1 h_j \cdot \partial_{ji} f(a) \cdot h_i \cdot s \ ds \)

\(
I_3(h) &= \unsur{2} \sum_{i,j,k} \int_0^1 h_k \cdot \partial_{kji}^3 f(a) \cdot h_j \cdot h_i \cdot s^2 \ ds \)

\(
\mathcal{E}_3(h) &= \sum_i \int_0^1 e_i(h) \cdot h_i \ ds
\end{align}

<p>
Comme \(s^3/3\) est une primitive de \(s^2\), on a :
</p>

<p>
\[\int_0^1 s^2 \ ds = \unsur{3} \cdot (1^3 - 0^3) = \unsur{3}\]
</p>

<p>
Les intégrales s'écrivent donc :
</p>

\begin{align}
I_1(h) &= \sum_i \partial_i f(a) \cdot h_i \)

\(
I_2(h) &= \unsur{2} \sum_{i,j} h_i \cdot \partial_{ji} f(a) \cdot h_j \)

\(
I_3(h) &= \unsur{6} \sum_{i,j,k} \partial_{kji}^3 f(a) \cdot h_i \cdot h_j \cdot h_k
\end{align}

<p>
et la variation de \(f\) est donnée par :
</p>

<p>
\[f(a + h) - f(a) = I_1(h) + I_2(h) + I_3(h) + \mathcal{E}_3(h)\]
</p>

<p>
En terme de notations tensorielles, on peut l'écrire symboliquement :
</p>

<p>
\[f(a + h) - f(a) = \partial f(a) \cdot h + \unsur{2} h^\dual \cdot \partial^2 f(a) \cdot h + \unsur{6} \contraction{\partial^3 f(a)}{3}{h \otimes h \otimes h}\]
</p>
</div>
<div id="outline-container-org6990b7e" class="outline-4">
<h4 id="org6990b7e"><span class="section-number-4">4.4.1.</span> Comportement de l'erreur</h4>
<div class="outline-text-4" id="text-4-4-1">
<p>
Nous savons que, pour toute précision \(\epsilon \strictsuperieur 0\), nous pouvons trouver un \(\delta \strictsuperieur 0\) tel que :
</p>

<p>
\[\frac{\norme{e(h)}}{\norme{h}^2} \le \epsilon\]
</p>

<p>
pour tout \(h\) vérifiant \(\norme{h} \le \delta\). Comme \(\abs{e_i(h)} \le \norme{e(h)}\) et \(\abs{h_i} \le \norme{h}\), on a :
</p>

\begin{align}
\abs{\mathcal{E}_3(h)} &\le \sum_i \abs{\int_0^1 e_i(h) \cdot h_i \ ds} \)

\(
&\le n \cdot \epsilon \cdot \norme{h}^3
\end{align}

<p>
L'erreur \(\abs{\mathcal{E}_3(h)}\) est donc en \(\petito{h^3}\).
</p>
</div>
</div>
<div id="outline-container-org3f6534c" class="outline-4">
<h4 id="org3f6534c"><span class="section-number-4">4.4.2.</span> Dérivées ordinaires</h4>
<div class="outline-text-4" id="text-4-4-2">
<p>
Lorsque \(n = 1\), le développement est simplement :
</p>

<p>
\[f(a + h) = f(a) + \OD{f}{x}(a) \cdot h + \OOD{f}{x}(a) \cdot \frac{h^2}{2} + \NOD{f}{x}{3} \cdot \frac{h^3}{6} + \mathcal{E}_3(h)\]
</p>

<p>
On constate qu'il est analogue au développement de Taylor d'ordre trois autour de \(a\).
</p>
</div>
</div>
</div>
</div>
<div id="outline-container-org4a38a8f" class="outline-2">
<h2 id="org4a38a8f"><span class="section-number-2">5.</span> Symétrie</h2>
<div class="outline-text-2" id="text-5">
<div id="text-table-of-contents-5" role="doc-toc">
<ul>
<li><a href="#org2a85b68">5.1. Dérivation par rapport à un paramètre</a></li>
<li><a href="#orgbae3222">5.2. Différences finies</a></li>
</ul>
</div>

<p>
Soit la fonction \(f \in \continue^2(\setR^2,\setR)\). Posons \(\partial_x = \partial_1\) et \(\partial_y = \partial_2\). Nous allons tenter d'évaluer les dérivées secondes \(\partial_{xy} = \partial_{12}\) et \(\partial_{yx} = \partial_{21}\). On note :
</p>

\begin{align}
\varphi_{11} &= \varphi(x,y) \)

\(
\varphi_{21} &= \varphi(x + h, y) \)

\(
\varphi_{12} &= \varphi(x, y + h) \)

\(
\varphi_{22} &= \varphi(x + h, y + h)
\end{align}

<p>
où \(\varphi = f\) ou une de ses dérivées. Comme \(\partial_{xy} = \partial_x \partial_y\) et \(\partial_{yx} = \partial_y \partial_x\), on a par définition :
</p>

<div class="org-center">
<p>
\(
\Delta_{xy} = \partial_y f_{21} - \partial_y f_{11} = \partial_{xy} f_{11} \cdot h + o(h) \)
</p>

<p>
\(
\Delta_{yx} = \partial_x f_{12} - \partial_x f_{11} = \partial_{yx} f_{11} \cdot h + o(h)
\)
</p>
</div>

<p>
Multiplié par \(h\), cela devient :
</p>

<div class="org-center">
<p>
\(
\Delta_{xy} \cdot h = \partial_{xy} f_{11} \cdot h^2 + o(h^2) \)
</p>

<p>
\(
\Delta_{yx} \cdot h = \partial_{yx} f_{11} \cdot h^2 + o(h^2)
\)
</p>
</div>

<p>
On dispose également des développements d'ordre deux :
</p>

<div class="org-center">
<p>
\(
f_{22} - f_{21} = \partial_y f_{21} \cdot h + \partial_{yy} f_{21} \cdot \frac{h^2}{2} + o(h^2) \)
</p>

<p>
\(
f_{12} - f_{11} = \partial_y f_{11} \cdot h + \partial_{yy} f_{11} \cdot \frac{h^2}{2} + o(h^2) \)
</p>

<p>
\(
f_{22} - f_{12} = \partial_x f_{12} \cdot h + \partial_{xx} f_{12} \cdot \frac{h^2}{2} + o(h^2) \)
</p>

<p>
\(
f_{12} - f_{11} = \partial_x f_{11} \cdot h + \partial_{xx} f_{11} \cdot \frac{h^2}{2} + o(h^2)
\)
</p>
</div>

<p>
On en conclut que :
</p>

<div class="org-center">
<p>
\(
\Delta_{xy} \cdot h = D + \Delta_{yy} + o(h^2) \)
</p>

<p>
\(
\Delta_{yx} \cdot h = D + \Delta_{xx} + o(h^2)
\)
</p>
</div>

<p>
où l'on a posé :
</p>

<div class="org-center">
<p>
\(
D = f_{22} - f_{21} - f_{12} + f_{11} \)
</p>

<p>
\(
\Delta_{yy} = (\partial_{yy} f_{11} - \partial_{yy} f_{21}) \cdot h^2 \)
</p>

<p>
\(
\Delta_{xx} = (\partial_{xx} f_{11} -  \partial_{xx} f_{12}) \cdot h^2
\)
</p>
</div>

<p>
Par continuité de \(\partial_{xx} f\) et de \(\partial_{yy} f\), on a :
</p>

<div class="org-center">
<p>
\(
\lim_{h \to 0} \frac{\Delta_{yy}}{h^2} = \lim_{h \to 0} (\partial_{yy} f_{11} - \partial_{yy} f_{21}) = 0 \)
</p>

<p>
\(
\lim_{h \to 0} \frac{\Delta_{xx}}{h^2} = \lim_{h \to 0} (\partial_{xx} f_{11} -  \partial_{xx} f_{12}) = 0
\)
</p>
</div>

<p>
On en conclut que \(\Delta_{xx}, \Delta_{yy} \sim o(h^2)\). Comme la somme de deux erreurs en \(o(h^2)\) donne également une erreur en \(o(h^2)\), on a :
</p>

<div class="org-center">
<p>
\(
\Delta_{xy} \cdot h = D + o(h^2) + o(h^2) = D + o(h^2) \)
</p>

<p>
\(
\Delta_{yx} \cdot h = D + o(h^2) + o(h^2) = D + o(h^2)
\)
</p>
</div>

<p>
et :
</p>

<div class="org-center">
<p>
\(
\partial_{xy} f_{11} \cdot h^2 = D + o(h^2) \)
</p>

<p>
\(
\partial_{yx} f_{11} \cdot h^2 = D + o(h^2)
\)
</p>
</div>

<p>
On en conclut que la différence \(\partial_{xy} f_{11} - \partial_{yx} f_{11} \sim o(1)\) tend vers \(0\) avec \(h\), ce qui n'est possible que si :
</p>

<p>
\[\partial_{xy} f_{11} = \partial_{yx} f_{11}\]
</p>

<p>
Nous avons donc prouvé que :
</p>

<p>
\[\partial_{xy} f(x,y) = \partial_{yx} f(x,y)\]
</p>
</div>
<div id="outline-container-org836037c" class="outline-4">
<h4 id="org836037c"><span class="section-number-4">5.0.1.</span> Généralisation</h4>
<div class="outline-text-4" id="text-5-0-1">
<p>
On peut bien entendu généraliser à une fonction \(f \in \continue^2(\setR^n,\setR)\). On a alors :
</p>

<p>
\[\partial_{ij} f = \partial_{ji} f\]
</p>

<p>
où \(i,j \in \{1,2,...,n\}\). Si \(H = \partial^2 f\), on écrit aussi ce résultat sous la forme :
</p>

<p>
\[H^\dual = H\]
</p>
</div>
</div>
<div id="outline-container-org2a85b68" class="outline-3">
<h3 id="org2a85b68"><span class="section-number-3">5.1.</span> Dérivation par rapport à un paramètre</h3>
<div class="outline-text-3" id="text-5-1">
<p>
Nous allons a présent examiner ce qu'il se passe lorsque les bornes de l'intervalle d'intégration (\(a,b : \setR \mapsto \setR\)) et la fonction à intégrer (\(f : \setR \times \setR \mapsto \setR\)) varient par rapport à un paramètre. Soit la fonction \(I : \setR \mapsto \setR\) définie par :
</p>

<p>
\[I(t) = \int_{a(t)}^{b(t)} f(s,t) \ ds\]
</p>

<p>
Pour une valeur donnée de \(t\), posons :
</p>

<p>
\[\phi_t(s) = f(s,t)\]
</p>

<p>
L'intégrale de \(\phi_t\) peut s'évaluer si nous connaissons une primitive \(\psi_t\) telle que :
</p>

<p>
\[\OD{\psi_t}{s}(s) = \phi_t(s)\]
</p>

<p>
Mais cette expression consiste à évaluer la variation de \(\psi\) lorsque \(s\) varie, \(t\) étant fixé. Cela revient donc à une dérivée partielle par rapport à \(s\). Donc, si nous connaissons une fonction \(F\) telle que :
</p>

<p>
\[\deriveepartielle{F}{s}(s,t) = f(s,t)\]
</p>

<p>
nous pouvons réécrire l'intégrale :
</p>

<p>
\[\int_{a(t)}^{b(t)} f(s,t) \ ds = F(b(t),t) - F(a(t),t)\]
</p>

<p>
Il ne nous reste plus alors qu'à évaluer la dérivée de \(I\) par rapport à \(t\) en utilisant la règle des compositions de fonctions :
</p>

<p>
\[\OD{I}{t}(t) = \deriveepartielle{F}{s}(b(t),t) \cdot \OD{b}{t}(t) + \deriveepartielle{F}{t}(b(t),t) - \deriveepartielle{F}{s}(a(t),t) \cdot \OD{b}{t}(t) - \deriveepartielle{F}{t}(a(t),t)\]
</p>

<p>
Si \(F \in \continue^2(\setR^2,\setR)\), la symétrie des dérivées secondes nous permet d'écrire :
</p>

<p>
\[\deriveepartielle{F}{t} = \deriveepartielle{}{t} \left[ \deriveepartielle{f}{s} \right] = \deriveepartielle{}{s} \left[ \deriveepartielle{f}{t} \right]\]
</p>

<p>
La dérivée partielle de \(F\) par rapport à \(t\) est donc une primitive de la dérivée partielle de \(f\) par rapport à \(t\). On en déduit que :
</p>

<p>
\[\int_\alpha^\beta \deriveepartielle{f}{t}(s,t) \ ds = \deriveepartielle{F}{t}(\beta,t) - \deriveepartielle{F}{t}(\alpha,t)\]
</p>

<p>
pour tout \(\alpha,\beta \in \setR\). Pour un \(t\) fixé quelconque, on peut poser \(\alpha = a(t)\) et \(\beta = b(t)\). Il vient alors :
</p>

<p>
\[\OD{I}{t}(t) = \deriveepartielle{F}{s}(b(t),t) \cdot \OD{b}{t}(t) - \deriveepartielle{F}{s}(a(t),t) \cdot \OD{b}{t}(t) + \int_{a(t)}^{b(t)} \deriveepartielle{f}{t}(s,t) \ ds\]
</p>
</div>
</div>
<div id="outline-container-orgbae3222" class="outline-3">
<h3 id="orgbae3222"><span class="section-number-3">5.2.</span> Différences finies</h3>
<div class="outline-text-3" id="text-5-2">
<p>
Soit une fonction \(f : \setR^2 \mapsto \setR^m\) deux fois continument dérivable. Nous allons voir comment évaluer des approximations des dérivées premières et secondes de \(f\). On note \(\partial_x = \partial_1\) et \(\partial_y = \partial_2\). On choisit les réels \(x,y\) et la variation non nulle \(h \in \setR\).
</p>
</div>
<div id="outline-container-org16e43ef" class="outline-4">
<h4 id="org16e43ef"><span class="section-number-4">5.2.1.</span> Dérivées premières</h4>
<div class="outline-text-4" id="text-5-2-1">
<p>
Soustrayons les développements d'ordre deux :
</p>

<div class="org-center">
<p>
\(
f(x + h,y) \approx f(x,y) + h \cdot \partial_x f(x,y) + \frac{h^2}{2} \cdot \partial^2 f(x,y) \)
</p>

<p>
\(
f(x - h,y) \approx f(x,y) - h \cdot \partial_x f(x,y) + \frac{h^2}{2} \cdot \partial^2 f(x,y)
\)
</p>
</div>

<p>
On obtient :
</p>

<p>
\[f(x + h,y) - f(x - h,y) \approx 2 h \cdot \partial_x f(x,y)\]
</p>

<p>
et donc :
</p>

<p>
\[\partial_x f(x,y) \approx \frac{f(x + h, y) - f(x - h, y)}{2 h}\]
</p>

<p>
L'erreur est en \(o(h) = o(h^2)/h\). En procédant de même avec \(y\), on obtient :
</p>

<p>
\[\partial_y f(x,y) \approx \frac{f(x, y + h) - f(x, y - h)}{2 h}\]
</p>
</div>
</div>
<div id="outline-container-org193391f" class="outline-4">
<h4 id="org193391f"><span class="section-number-4">5.2.2.</span> Dérivées secondes</h4>
<div class="outline-text-4" id="text-5-2-2">
<p>
On additionne cette fois les mêmes développements et on obtient :
</p>

<p>
\[f(x + h,y) + f(x - h,y) \approx 2 f(x,y) + \frac{h^2}{2} \cdot \partial^2 f(x,y) + o(h^2)\]
</p>

<p>
et donc :
</p>

<p>
\[\partial_{xx}^2 f(x,y) \approx \frac{f(x + h, y) - 2 f(x,y) + f(x - h, y)}{h^2}\]
</p>

<p>
L'erreur est en \(o(1) = o(h^2)/h^2\), et donc aussi petite que l'on veut pourvu que \(h \ne 0\) soit suffisamment petit. En procédant de même avec \(y\), on obtient :
</p>

<p>
\[\partial_{yy}^2 f(x,y) \approx \frac{f(x, y + h) - 2 f(x,y) + f(x, y - h)}{h^2}\]
</p>

<p>
On vérifie également en évaluant les développements en \((x \pm h, y \pm h)\) que :
</p>

<p>
\[\partial_{xy}^2 f(x,y) \approx \frac{f(x + h, y + h) - f(x + h, y - h) - f(x - h, y + h) + f(x - h, y - h)}{h^2}\]
</p>

<p>
La dernière dérivée seconde s'évalue approximativement par \(\partial_{yx}^2 f(x,y) = \partial_{xy}^2 f(x,y)\).
</p>
</div>
</div>
<div id="outline-container-org5ce6b88" class="outline-4">
<h4 id="org5ce6b88"><span class="section-number-4">5.2.3.</span> Généralisation</h4>
<div class="outline-text-4" id="text-5-2-3">
<p>
Nous allons voir comment généraliser ces résultats aux dérivées \(\partial_{ij}\) d'une fonction \(F : \setR^n \mapsto \setR^m\). Soit \(u \in \setR^n\) et les vecteurs de la base canonique \(\canonique_i \in \setR^n\). On définit les fonctions \(f_{ij} : \setR^n \mapsto \setR^m\) par :
</p>

<p>
\[f_{ij}(x,y) = F(u + x \cdot \canonique_i + y \cdot \canonique_j)\]
</p>

<p>
On a clairement :
</p>

\begin{align}
\partial_i F(u) &= \partial_x f_{ij}(0,0) \)

\(
\partial_{ii} F(u) &= \partial_{xx} f_{ij}(0,0) \)

\(
\partial_{ij} F(u) &= \partial_{xy} f_{ij}(0,0) \)

\(
\partial_{jj} F(u) &= \partial_{yy} f_{ij}(0,0)
\end{align}

<p>
Il suffit donc d'utiliser les méthodes d'approximations des dérivées de \(f_{ij}\) pour approximer les dérivées de \(F\).
</p>
</div>
</div>
</div>
</div>
<div id="outline-container-org8395fa0" class="outline-2">
<h2 id="org8395fa0"><span class="section-number-2">6.</span> Distributions</h2>
<div class="outline-text-2" id="text-6">
<div id="text-table-of-contents-6" role="doc-toc">
<ul>
<li><a href="#org030b167">6.1. Dépendances</a></li>
<li><a href="#orgcd72598">6.2. Formes et fonctions</a></li>
<li><a href="#org610e24e">6.3. Formes et mesures</a></li>
<li><a href="#orgfdfdc28">6.4. Fonction et forme bilinéaire</a></li>
<li><a href="#orgc342593">6.5. Définition</a></li>
<li><a href="#org05c9554">6.6. Delta de Dirac</a></li>
<li><a href="#orgabe1c91">6.7. Dérivée</a></li>
<li><a href="#org2e6589f">6.8. Dilatation</a></li>
<li><a href="#org3d32b29">6.9. Réflexion</a></li>
<li><a href="#orga77afd8">6.10. Translation</a></li>
<li><a href="#org3ffbeca">6.11. Convolution</a></li>
<li><a href="#org63b6fa4">6.12. Corrélation</a></li>
</ul>
</div>

<p>
\label{chap:distribu}
</p>
</div>
<div id="outline-container-org030b167" class="outline-3">
<h3 id="org030b167"><span class="section-number-3">6.1.</span> Dépendances</h3>
<div class="outline-text-3" id="text-6-1">
<ul class="org-ul">
<li>Chapitre \ref{chap:relation} : Les fonctions</li>
<li>Chapitre \ref{chap:lineaire} : Les fonctions linéaires</li>
</ul>
</div>
</div>
<div id="outline-container-orgcd72598" class="outline-3">
<h3 id="orgcd72598"><span class="section-number-3">6.2.</span> Formes et fonctions</h3>
<div class="outline-text-3" id="text-6-2">
<p>
On peut toujours associer une forme linéaire \(\varphi\) à une fonction intégrable quelconque \(\hat{\varphi}\) en définissant :
</p>

<p>
\[\forme{\varphi}{u} = \int_A u(x) \cdot \hat{\varphi}(x) \ dx\]
</p>

<p>
Inversément, on ne pourra pas toujours trouver une fonction \(\hat{\varphi}\) correspondant à une forme linéaire \(\varphi\) donnée. On définira malgré tout l'intégrale généralisée en notant :
</p>

<p>
\[\int_A u(x)  \cdot \varphi(x) \ dx = \forme{\varphi}{u}\]
</p>

<p>
où il ne faut pas perdre de vue que \(\varphi\) n'est pas nécessairement une fonction.
</p>
</div>
</div>
<div id="outline-container-org610e24e" class="outline-3">
<h3 id="org610e24e"><span class="section-number-3">6.3.</span> Formes et mesures</h3>
<div class="outline-text-3" id="text-6-3">
<p>
Soit \(u : A \mapsto \setR\). A toute mesure \(\mu\), on peut associer une forme linéaire \(\hat{\mu}\) par :
</p>

<p>
\[\forme{ \hat{\mu} }{u} = \int_A u(x) \ d\mu(x)\]
</p>

<p>
Inversément, à toute forme linéaire \(\hat{\mu}\), on peut associer une fonction
\(\mu : \sousens(\setR) \mapsto \setR\) par :
</p>

<p>
\[\mu(A) = \forme{ \hat{\mu} }{\indicatrice_A}\]
</p>

<p>
Toutefois, rien ne garantit que la fonction \(\mu\) ainsi définie est une mesure. En particulier, rien ne garantit qu'elle soit positive.
</p>
</div>
</div>
<div id="outline-container-orgfdfdc28" class="outline-3">
<h3 id="orgfdfdc28"><span class="section-number-3">6.4.</span> Fonction et forme bilinéaire</h3>
<div class="outline-text-3" id="text-6-4">
<p>
A toute fonction \(\hat{K} : A \times B \mapsto F\), on peut associer une forme bilinéaire \(K\) par :
</p>

<p>
\[\biforme{u}{K}{v} = \int_{A \times B} u(x) \cdot K(x,y) \cdot v(y) \ d\mu(x) \ d\nu(y)\]
</p>

<p>
pour toutes fonctions \(u,v : A \mapsto B\). Inversément, à toute forme bilinéaire \(K\), on peut associer une intégrale généralisée en notant :
</p>

<p>
\[\int_{A \times B} u(x) \cdot K(x,y) \cdot v(y) \ d\mu(x) \ d\nu(y) = \biforme{u}{K}{v}\]
</p>
</div>
</div>
<div id="outline-container-orgc342593" class="outline-3">
<h3 id="orgc342593"><span class="section-number-3">6.5.</span> Définition</h3>
<div class="outline-text-3" id="text-6-5">
<p>
Nous nous intéressons ici au cas où l'espace vectoriel \(E\) est un ensemble de fonctions intégrables : \(E = \mathcal{F} \subseteq \lebesgue^2(\setR,\setR)\). Les limites à l'infini doivent alors forcément s'annuler
</p>

<p>
\[\lim_{x \to +\infty} u(x) = \lim_{x \to -\infty} u(x) = 0\]
</p>

<p>
pour tout \(u \in \mathcal{F}\).
</p>
</div>
</div>
<div id="outline-container-org05c9554" class="outline-3">
<h3 id="org05c9554"><span class="section-number-3">6.6.</span> Delta de Dirac</h3>
<div class="outline-text-3" id="text-6-6">
<p>
La distribution \(\dirac \in F^\dual\) de Dirac est définie par :
</p>

<p>
\[\forme{\dirac}{u} = u(0)\]
</p>

<p>
pour tout \(u \in F\). Elle correspond bien sûr à l'intégrale :
</p>

<p>
\[\int_\setR \dirac(x) \cdot u(x) \ dx = u(0)\]
</p>

<p>
On remarque que :
</p>

<p>
\[\int_{A^2} \dirac(\xi - x) \cdot K(\xi,\eta) \cdot \dirac(\eta - y) \ d\mu(\xi) \ d\nu(\eta) = K(x,y)\]
</p>
</div>
</div>
<div id="outline-container-orgabe1c91" class="outline-3">
<h3 id="orgabe1c91"><span class="section-number-3">6.7.</span> Dérivée</h3>
<div class="outline-text-3" id="text-6-7">
<p>
En intégrant par parties, on a :
</p>

<div class="org-center">
<p>
\(
&int;<sub>\setR</sub> \OD{u}{x}(x) &sdot; v(x) \ dx =
lim<sub>a &rarr; +&infin;</sub> \left[ u(a) \cdot v(a) - u(-a) \cdot v(-a) \right]
</p>
<ul class="org-ul">
<li>&int;<sub>\setR</sub> u(x) &sdot; \OD{v}{x}(x) \ dx</li>
</ul>
<p>
\)
</p>
</div>

<p>
mais comme les limites à l'infini s'annulent, cette expression se réduit à :
</p>

<p>
\[\int_{\setR} \OD{u}{x}(x) \cdot v(x) \ dx = - \int_{\setR} u(x) \cdot \OD{v}{x}(x) \ dx\]
</p>

<p>
Par extension, on définit la dérivée \(\OD{u}{x}\) d'une distribution \(u\) par :
</p>

<p>
\[\forme{\OD{u}{x}}{v} = - \forme{u}{\OD{v}{x}}\]
</p>

<p>
pour tout \(v\in F\).
</p>
</div>
<div id="outline-container-org5ab3a49" class="outline-4">
<h4 id="org5ab3a49"><span class="section-number-4">6.7.1.</span> Échelon</h4>
<div class="outline-text-4" id="text-6-7-1">
<p>
Comme application, considérons la fonction échelon \(e_+\) :
</p>

<div class="org-center">
<p>
\(
e_+(x) = \indicatrice<sub>[0,+&infin;)</sub> =
</p>
\begin{cases}
1 & \mbox{si  } t \ge 0 \)

\(
0 & \mbox{si  } t < 0
\end{cases}
<p>
\)
</p>
</div>

<p>
Pour tout \(v\in F\), on a :
</p>

\begin{align}
\forme{\OD{e_+}{x}}{v} &= - \forme{e_+}{\OD{v}{x}} \)

\(
&= - \int_0^{+\infty} \OD{v}{x}(x) dx
\end{align}

<p>
Appliquons à présent le théorème fondamental. Il vient :
</p>

<p>
\[\forme{\OD{e_+}{x}}{v} = - \left[\lim_{x \to +\infty} v(x) - v(0)\right] = v(0)\]
</p>

<p>
On en déduit que :
</p>

<p>
\[\OD{e_+}{x} = \dirac\]
</p>

<p>
au sens des distributions.
</p>
</div>
</div>
</div>
<div id="outline-container-org2e6589f" class="outline-3">
<h3 id="org2e6589f"><span class="section-number-3">6.8.</span> Dilatation</h3>
<div class="outline-text-3" id="text-6-8">
<p>
Soit \(d_a\) l'opérateur de dilatation :
</p>

<p>
\[d_a(u)(x) = u(a \cdot x)\]
</p>

<p>
où \(a > 0\) est un réel strictement positif.
</p>

<p>
Le changement de variable \(\xi = a \cdot x\) nous donne \(d\xi = a \ dx\) et donc :
</p>

<p>
\[\int_{\setR} \hat{u}(a \ x) \ v(x) \ dx = \unsur{a} \int_{\setR} \hat{u}(\xi) \ v\left( \xi/a \right) \ d\xi\]
</p>

<p>
On définit donc l'extension de cet opérateur aux distributions par :
</p>

<p>
\[\forme{d_a(u)}{v} = \unsur{a} \forme{u}{d_{1/a}(v)}\]
</p>
</div>
</div>
<div id="outline-container-org3d32b29" class="outline-3">
<h3 id="org3d32b29"><span class="section-number-3">6.9.</span> Réflexion</h3>
<div class="outline-text-3" id="text-6-9">
<p>
L'opérateur de réflexion \(r\) se définit par :
</p>

<p>
\[r(u)(x) = u(-x)\]
</p>

<p>
Le changement de variable \(\xi = -x\) nous donne \(d\xi = -dx\) et donc :
</p>

\begin{align}
\int_{\setR} \hat{u}(-x) \ v(x) \ dx &= \lim_{a \to +\infty}\int_{-a}^a \hat{u}(-x) \ v(x) \ dx \)

\(
&= \lim_{a \to +\infty} - \int_a^{-a} \hat{u}(\xi) \ v(-\xi) \ d\xi \)

\(
&= \lim_{a \to +\infty} \int^{-a}_a \hat{u}(\xi) \ v(-\xi) \ d\xi
\end{align}

<p>
On définit donc l'extension de cet opérateur aux distributions par :
</p>

<p>
\[\forme{r(u)}{v} = \forme{u}{r(v)}\]
</p>
</div>
</div>
<div id="outline-container-orga77afd8" class="outline-3">
<h3 id="orga77afd8"><span class="section-number-3">6.10.</span> Translation</h3>
<div class="outline-text-3" id="text-6-10">
<p>
L'opérateur de translation \(t_a\) est défini par :
</p>

<p>
\[t_a(u)(x) = u(x - a)\]
</p>

<p>
Le changement de variable \(\xi = x - a\) nous donne \(d\xi = dx\) et donc :
</p>

<p>
\[\int_{\setR} \hat{u}(x-a) v(x) dx = \int_{\setR} \hat{u}(\xi) v(\xi+a) d\xi\]
</p>

<p>
On définit donc les extensions de cet opérateur aux distributions par :
</p>

<p>
\[\forme{t_a(u)}{v} = \forme{u}{t_{-a}(v)}\]
</p>
</div>
</div>
<div id="outline-container-org3ffbeca" class="outline-3">
<h3 id="org3ffbeca"><span class="section-number-3">6.11.</span> Convolution</h3>
<div class="outline-text-3" id="text-6-11">
<p>
Les intégrales unidimensionnelles permettent de définir l'opérateur de convolution
\(\convolution\). Soit deux fonctions \(u, v : \setR \mapsto \setR\), leur convolution
est une fonction \(u \convolution v : \setR \mapsto \setR\) définie par :
</p>

<p>
\[(u \convolution v)(t) = \int_{-\infty}^{+\infty} u(t-s) \ v(s) \ ds\]
</p>

<p>
pour tout \(t \in \setR\).
</p>
</div>
<div id="outline-container-org5d800f4" class="outline-4">
<h4 id="org5d800f4"><span class="section-number-4">6.11.1.</span> Dirac</h4>
<div class="outline-text-4" id="text-6-11-1">
<p>
En utilisant les résultats ci-dessus, on arrive facilement à :
</p>

<p>
\[\int_{\setR} u(x) \ \dirac(x-a) \ dx = u(a)\]
</p>

<p>
Comme :
</p>

<p>
\[\int_{\setR} u(x) \ \dirac(-x) \ dx = \int_{\setR} u(-x) \ \dirac(x) \ dx = u(0)\]
</p>

<p>
on en déduit que \(\dirac(-x) = \dirac(x)\) et :
</p>

<p>
\[\int_{\setR} \dirac(x-y) \ u(y)  \ dy = u(x)\]
</p>

<p>
c'est-à-dire :
</p>

<p>
\[\dirac \convolution u = u\]
</p>

<p>
La distribution de Dirac est neutre pour le produit de convolution.
On peut montrer que ce neutre est unique.
</p>
</div>
</div>
</div>
<div id="outline-container-org63b6fa4" class="outline-3">
<h3 id="org63b6fa4"><span class="section-number-3">6.12.</span> Corrélation</h3>
<div class="outline-text-3" id="text-6-12">
<p>
Les intégrales unidimensionnelles permettent de définir l'opérateur de corrélation
\(\correlation\). Soit deux fonctions \(u, v : \setR \mapsto \setR\), leur corrélation
est une fonction \(u \correlation v : \setR \mapsto \setR\) définie par :
</p>

<p>
\[(u \correlation v)(t) = \int_{-\infty}^{+\infty} u(s+t) \ v(s) \ ds\]
</p>

<p>
pour tout \(t \in \setR\).
</p>
</div>
</div>
</div>
<div id="outline-container-org26746a3" class="outline-2">
<h2 id="org26746a3"><span class="section-number-2">7.</span> Formes différentielles</h2>
<div class="outline-text-2" id="text-7">
<div id="text-table-of-contents-7" role="doc-toc">
<ul>
<li><a href="#org2858c7e">7.1. Dépendances</a></li>
<li><a href="#orgf32c292">7.2. Intégrale d'un tenseur</a></li>
<li><a href="#org93416d8">7.3. Produit extérieur</a></li>
<li><a href="#orgf6628c7">7.4. Tenseur différentiel</a></li>
<li><a href="#orgba7e3e5">7.5. Paramétrisation</a></li>
<li><a href="#org1e6afe2">7.6. Changement de variable</a></li>
<li><a href="#orgc650c62">7.7. Intégrales de ligne vectorielles</a></li>
<li><a href="#org06bbc7b">7.8. Intégrales de ligne scalaires</a></li>
<li><a href="#orge339903">7.9. Contour fermé</a></li>
<li><a href="#orgd98fe3e">7.10. Intégrales de surface vectorielles</a></li>
<li><a href="#org7b4d491">7.11. Intégrales de surface scalaires</a></li>
<li><a href="#org74980ea">7.12. Intégrale de flux</a></li>
<li><a href="#orgf411bb8">7.13. Différentielle</a></li>
<li><a href="#org7735479">7.14. Théorème de Stokes</a></li>
</ul>
</div>

<p>
\label{chap:formedif}
</p>
</div>
<div id="outline-container-org2858c7e" class="outline-3">
<h3 id="org2858c7e"><span class="section-number-3">7.1.</span> Dépendances</h3>
<div class="outline-text-3" id="text-7-1">
<ul class="org-ul">
<li>Chapitre \ref{chap:differ} : Les différentielles</li>
<li>Chapitre \ref{chap:integral} : Les intégrales</li>
</ul>
</div>
</div>
<div id="outline-container-orgf32c292" class="outline-3">
<h3 id="orgf32c292"><span class="section-number-3">7.2.</span> Intégrale d'un tenseur</h3>
<div class="outline-text-3" id="text-7-2">
<p>
Soit \((\canonique_1,\canonique_2,...,\canonique_n)\) la bace canonique de \(\setR^n\) et la fonction tensorielle \(T : A \mapsto \tenseur_m(\setR^n)\) qui, à chaque \(x \in A\) associe un tenseur \(T(x)\) de la forme :
</p>

<p>
\[T(x) = \sum_{i,j,...,p} t_{ij...p}(x) \cdot \canonique_i \otimes \canonique_j \otimes ... \otimes \canonique_p\]
</p>

<p>
L'intégrale de cette fonction est définie par :
</p>

<p>
\[\int_A T(x) \ d\mu(x) = \sum_{i,j,...,p} I_{ij...p} \cdot \canonique_i \otimes \canonique_j \otimes ... \otimes \canonique_p\]
</p>

<p>
où chaque coordonnée \(I_{ij...p}\) est l'intégrale de la coordonnée correspondante de \(T\) :
</p>

<p>
\[I_{ij...p} = \int_A t_{ij...p}(x) \ d\mu(x)\]
</p>
</div>
</div>
<div id="outline-container-org93416d8" class="outline-3">
<h3 id="org93416d8"><span class="section-number-3">7.3.</span> Produit extérieur</h3>
<div class="outline-text-3" id="text-7-3">
<p>
Soit \(d\mu = dx = dx_1 \ ... \ dx_n\) la mesure de Lebesgue sur \(\setR^n\). On sait que \(dx\) représente la mesure de l'élément de volume \([x_1,x_1 + dx_1] \times ... \times [x_n,x_n + dx_N]\). Etant donné que nous avons construit le produit extérieur pour représenter (au signe près) des mesures de surfaces et de volumes, il est tout à fait naturel de le faire intervenir dans une mesure de Lebesgue. Soit la base canonique \((e_1,e_2,...,e_n)\) de \(\setR^n\) et les vecteurs :
</p>

<p>
\[\delta_i = dx_i \cdot e_i\]
</p>

<p>
où les \(dx_i\) sont bien évidemment des scalaires. Si nous évaluons le produit extérieur des ces vecteurs, nous obtenons :
</p>

<p>
\[\delta_1 \wedge \delta_2 \wedge ... \wedge \delta_n = \sum_{i,j,...,k} \permutation_{ij...k} \cdot dx_1 \cdot \delta_{1i} \cdot dx_2 \cdot \delta_{2j} \hdots \cdot dx_n \cdot \delta_{nk}\]
</p>

<p>
Le seul terme ne s'annulant pas étant \(\permutation_{1,2,...,n} = 1\), on a finalement :
</p>

<p>
\[\delta_1 \wedge \delta_2 \wedge ... \wedge \delta_n = dx_1 \cdot dx_2 \cdot ... \cdot dx_n = dx\]
</p>

<p>
Cette constatation nous amène à définir une mesure plus générale. Considérons à présent des vecteurs infinitésimaux \(\upsilon_1,\upsilon_2,...,\upsilon_n \in \setR^n\), c'est à dire des vecteurs dont la norme tendra vers zéro dans l'intégrale. Afin de garantir la positivité de la mesure, nous définissons :
</p>

<p>
\[du = \abs{\upsilon_1 \wedge \upsilon_2 \wedge ... \wedge \upsilon_n}\]
</p>
</div>
</div>
<div id="outline-container-orgf6628c7" class="outline-3">
<h3 id="orgf6628c7"><span class="section-number-3">7.4.</span> Tenseur différentiel</h3>
<div class="outline-text-3" id="text-7-4">
<p>
Il est même possible de définir des tenseurs différentiels \(dU\) en choisissant \(m \le n\) et en posant :
</p>

<p>
\[dU = \upsilon_1 \wedge \upsilon_2 \wedge ... \wedge \upsilon_m\]
</p>

<p>
Il est clair que \(dU \in \tenseur_{n - m}(\setR^n)\). On nomme ce type de tenseur une forme différentielle.
</p>
</div>
</div>
<div id="outline-container-orgba7e3e5" class="outline-3">
<h3 id="orgba7e3e5"><span class="section-number-3">7.5.</span> Paramétrisation</h3>
<div class="outline-text-3" id="text-7-5">
<p>
Soit \(m,n \in \setN\) avec \(m \le n\) et la fonction \(\phi : U \subseteq \setR^m \mapsto \setR^n\), dérivable et inversible. Le but est de paramétrer \(x\) sur \(\phi(U)\) par la relation \(x = \phi(u)\) pour tout \(u \in U\). Nous utilisons la base canonique \((e_1,e_2,...,e_m)\) de \(\setR^m\) et nous posons :
</p>

<p>
\[\delta_i = \phi(u + du_i \ e_i) - \phi(u) = \partial_i \phi(u) \ du_i\]
</p>

<p>
Sur \(\phi(U)\), on utilise le tenseur différentiel :
</p>

<p>
\[dX = \delta_1 \wedge \delta_2 \wedge ... \wedge \delta_m\]
</p>

<p>
La linéarité du produit extérieur nous permet d'ecrire :
</p>

\begin{align}
dX &= \partial_1 \phi(u) \wedge \partial_2 \phi(u) \wedge ... \wedge \partial_m \phi(u) \ du_1 \ du_2 \ ... \ du_m \)

\(
&= \partial_1 \phi(u) \wedge \partial_2 \phi(u) \wedge ... \wedge \partial_m \phi(u) \ du
\end{align}

<p>
On définit le tenseur \(W \in \tenseur_{n - m}(\setR^n)\) associé à \(dX\) par :
</p>

<p>
\[W(u) = \partial_1 \phi(u) \wedge \partial_2 \phi(u) \wedge ... \wedge \partial_m \phi(u)\]
</p>

<p>
Deux cas peuvent alors se présenter.
</p>
</div>
<div id="outline-container-org0089e1b" class="outline-4">
<h4 id="org0089e1b"><span class="section-number-4">7.5.1.</span> Fonction tensorielle</h4>
<div class="outline-text-4" id="text-7-5-1">
<p>
On peut évaluer l'intégrale d'une fonction tensorielle \(f : \setR^n \mapsto \tenseur_p(\setR^n)\) en utilisant la contraction maximale avec \(dX\). Comme on a l'équivalence \(x \in \phi(U) \leftrightarrow u \in U\), on a alors :
</p>

<p>
\[\int_{\phi(U)} f(x) : dX = \int_U (f\circ\phi)(u) : W(u) \ du\]
</p>

<p>
Dans le cas particulier où \(p = n - m\), on obtiendra un scalaire.
</p>
</div>
</div>
<div id="outline-container-orgb64533f" class="outline-4">
<h4 id="orgb64533f"><span class="section-number-4">7.5.2.</span> Fonction scalaire</h4>
<div class="outline-text-4" id="text-7-5-2">
<p>
On peut évaluer l'intégrale d'une fonction scalaire \(f : \setR^n \mapsto \setR\) en utilisant la norme de \(dX\). On a alors \(dx = \norme{dX}\) et :
</p>

<p>
\[\int_{\phi(U)} f(x) dx = \int_U (f\circ\phi)(u) \cdot \norme{W(u)} \ du\]
</p>
</div>
</div>
<div id="outline-container-orge8d11a8" class="outline-4">
<h4 id="orge8d11a8"><span class="section-number-4">7.5.3.</span> Pavé</h4>
<div class="outline-text-4" id="text-7-5-3">
<p>
Un cas particulier important est celui où \(U = [\alpha_1,\beta_1] \times .. \times [\alpha_m,\beta_m]\) pour certains \(\alpha_i,\beta_i \in \setR\). On a alors :
</p>

<p>
\[\int_{\phi(U)} \sim \int_{\alpha_1}^{\beta_1} du_1 \int_{\alpha_2}^{\beta_2} du_2 \ ... \int_{\alpha_m}^{\beta_m} du_m\]
</p>
</div>
</div>
</div>
<div id="outline-container-org1e6afe2" class="outline-3">
<h3 id="org1e6afe2"><span class="section-number-3">7.6.</span> Changement de variable</h3>
<div class="outline-text-3" id="text-7-6">
<p>
Nous considérons à présent le cas où \(m = n\). Nous utilisons la base canonique \((e_1,e_2,...,e_n)\) de \(\setR^n\) et nous posons de nouveau :
</p>

<p>
\[\delta_i = \phi(u + du_i \ e_i) - \phi(u) = \partial_i \phi(u) \ du_i\]
</p>

<p>
On utilise la mesure :
</p>

<p>
\[dx = \abs{\delta_1 \wedge \delta_2 \wedge ... \wedge \delta_n}\]
</p>

<p>
On a alors :
</p>

\begin{align}
dx &= \abs{\partial_1 \phi(u) \wedge \partial_2 \phi(u) \wedge ... \wedge \partial_n \phi(u) \ du} \)

\(
&= \abs{\sum_{i,j,...,k} \permutation_{ij...k} \cdot \partial_1 \phi_i(u) \cdot \partial_2 \phi_j(u) \cdot \ \hdots \ \cdot \partial_n \phi_k(u)} \ du \)

\(
&= \abs{\det \partial \phi(u)} \ du
\end{align}

<p>
On voit donc apparaître le déterminant de la Jacobienne de \(\phi\). Comme on a l'équivalence \(x \in A \leftrightarrow u \in \phi^{-1}(A)\), le changement de variable peut s'écrire :
</p>

<p>
\[\int_A f(x) \ dx = \int_{\phi^{-1}(A)} (f\circ\phi)(u) \cdot \abs{ \det \partial \phi(u) } \ du\]
</p>
</div>
<div id="outline-container-orgfc203f9" class="outline-4">
<h4 id="orgfc203f9"><span class="section-number-4">7.6.1.</span> Pavé</h4>
<div class="outline-text-4" id="text-7-6-1">
<p>
Un cas particulier important est celui où \(\phi^{-1}(A) = [\alpha_1,\beta_1] \times .. \times [\alpha_n,\beta_n]\) pour certains \(\alpha_i,\beta_i \in \setR\). On a alors :
</p>

<p>
\[\int_A \sim \int_{\alpha_1}^{\beta_1} du_1 \int_{\alpha_2}^{\beta_2} du_2 \ ... \int_{\alpha_n}^{\beta_n} du_n\]
</p>
</div>
</div>
</div>
<div id="outline-container-orgc650c62" class="outline-3">
<h3 id="orgc650c62"><span class="section-number-3">7.7.</span> Intégrales de ligne vectorielles</h3>
<div class="outline-text-3" id="text-7-7">
<p>
Soit une fonction continue \(\gamma : [a,b] \to \setR^n\) définissant la courbe \(\Lambda = \gamma([a,b])\).  L'intégrale de ligne d'une fonction \(f : \setR^n \mapsto \setR^n\) sur cette courbe est l'intégrale de la contraction d'ordre \(1\) de \(f\) avec \(d\gamma\), qui revient ici au produit scalaire du vecteur \(f(x) \in \Lambda\) par le vecteur \(\partial \gamma(t)\). On a donc :
</p>

<p>
\[\int_\Lambda f\cdot d\Lambda = \int_a^b \scalaire{(f \circ \gamma)(t)}{\partial \gamma(t) } dt\]
</p>
</div>
</div>
<div id="outline-container-org06bbc7b" class="outline-3">
<h3 id="org06bbc7b"><span class="section-number-3">7.8.</span> Intégrales de ligne scalaires</h3>
<div class="outline-text-3" id="text-7-8">
<p>
Dans le cas d'une fonction \(g : \setR^n \mapsto \setR\), on utilise comme mesure la longueur \(\norme{\partial \gamma(t)}\) de chaque petit segment \(d\Lambda\). On a alors :
</p>

<p>
\[\int_\Lambda g \ d\Lambda = \int_a^b (g \circ \gamma)(t) \cdot \norme{\partial \gamma(t)} \ dt\]
</p>
</div>
</div>
<div id="outline-container-orge339903" class="outline-3">
<h3 id="orge339903"><span class="section-number-3">7.9.</span> Contour fermé</h3>
<div class="outline-text-3" id="text-7-9">
<p>
Si \(\gamma(a) = \gamma(b)\), on dit que le contour fermé, et on note en général :
</p>

<p>
\[\oint_\Lambda = \int_\Lambda\]
</p>
</div>
</div>
<div id="outline-container-orgd98fe3e" class="outline-3">
<h3 id="orgd98fe3e"><span class="section-number-3">7.10.</span> Intégrales de surface vectorielles</h3>
<div class="outline-text-3" id="text-7-10">
<p>
Soit \(f : \setR^n \mapsto \setR^n\), \(\sigma : A \subseteq \setR^{n - 1} \mapsto \setR^n\) et la surface \(\Theta = \sigma(A)\). On définit les vecteurs :
</p>

<p>
\[\delta_i = \deriveepartielle{\sigma}{u_i} du_i\]
</p>

<p>
pour \(i = 1, ..., n - 1\). L'intégrale de surface est simplement la contraction d'ordre \(1\) :
</p>

<p>
\[\int_\Theta f \cdot d\Theta = \int_A \scalaire{(f \circ \sigma)(u)}{ \delta_1 \wedge ... \wedge \delta_{n-1} }\]
</p>

<p>
qui nous donne un scalaire. Dans le cas particulier où \(n = 3\) et où \(A = [U_1,U_2] \times [V_1,V_2]\), on a :
</p>

<p>
\[\int_\Theta f \cdot d\Theta = \int_{U_1}^{U_2} du \ \int_{V_1}^{V_2} (f \circ \sigma)(u,v) \cdot \left( \deriveepartielle{\sigma}{u}(u,v) \wedge \deriveepartielle{\sigma}{v}(u,v) \right) \ dv\]
</p>
</div>
</div>
<div id="outline-container-org7b4d491" class="outline-3">
<h3 id="org7b4d491"><span class="section-number-3">7.11.</span> Intégrales de surface scalaires</h3>
<div class="outline-text-3" id="text-7-11">
<p>
Soit \(f : \setR^n \mapsto \setR^n\), \(\sigma : A \subseteq \setR^{n - 1} \mapsto \setR^n\) et la surface \(\Theta = \sigma(A)\). On définit les vecteurs :
</p>

<p>
\[\delta_i = \deriveepartielle{\sigma}{u_i} du_i\]
</p>

<p>
pour \(i = 1, ..., n - 1\). Utilisant comme mesure la norme du produit extérieur des \(\delta_i\), on obtient :
</p>

<p>
\[\int_\Theta f \ d\Theta = \int_A (f \circ \sigma)(u) \cdot \norme{ \delta_1 \wedge ... \wedge \delta_{n-1} }\]
</p>

<p>
Dans le cas particulier où \(n = 3\) et où \(A = [U_1,U_2] \times [V_1,V_2]\), on a :
</p>

<p>
\[\int_\Theta f \ d\Theta = \int_{U_1}^{U_2} du \ \int_{V_1}^{V_2} (f \circ \sigma)(u,v) \cdot \norme{ \deriveepartielle{\sigma}{u}(u,v) \wedge \deriveepartielle{\sigma}{v}(u,v) } \ dv\]
</p>
</div>
</div>
<div id="outline-container-org74980ea" class="outline-3">
<h3 id="org74980ea"><span class="section-number-3">7.12.</span> Intégrale de flux</h3>
<div class="outline-text-3" id="text-7-12">
<p>
Soit \(A \subseteq \setR^n\) et la fonction \(a : \setR^n \to \setR\) telle que :
</p>

<p>
\[A = \{ x \in \setR^n : a(x) \le 0 \}\]
</p>

<p>
On s'arrange de plus pour avoir \(a\) constante sur la frontière :
</p>

<p>
\[\partial A = \{ x \in \setR^n : a(x) = 0 \}\]
</p>

<p>
On introduit le vecteur normal :
</p>

<p>
\[n = \unsur{\norme{\deriveepartielle{a}{x}}} \cdot \deriveepartielle{a}{x}\]
</p>

<p>
L'intégrale du flux sortant de la fonction \(f : \setR^n \to \setR^n\) est alors donnée par :
</p>

<p>
\[\int_{\partial A} \scalaire{f}{n} \ d\mu\]
</p>
</div>
</div>
<div id="outline-container-orgf411bb8" class="outline-3">
<h3 id="orgf411bb8"><span class="section-number-3">7.13.</span> Différentielle</h3>
<div class="outline-text-3" id="text-7-13">
<p>
Soit une fonction \(f : \setR^n \mapsto \setR\), les vecteurs infinitésimaux \(\delta_1,...,\delta_{n - 1} \in \setR^n\) et la forme différentielle :
</p>

<p>
\[\omega = f \cdot \delta_1 \wedge \delta_2 \wedge ... \wedge \delta_{n-1}\]
</p>

<p>
Si \(f\) est différentiable, on définit la différentielle de \(\omega\) par :
</p>

<p>
\[d\omega = \sum_i \deriveepartielle{f}{x_i} \cdot \kappa_i \wedge \delta_1 \wedge \delta_2 \wedge ... \wedge \delta_{n-1}\]
</p>

<p>
où :
</p>

<p>
\[\kappa_i = dx_i \cdot e_i\]
</p>

<p>
On note aussi symboliquement :
</p>

<p>
\[d\omega = df \wedge dx_1 \wedge ... \wedge dx_{n-1}\]
</p>

<p>
On peut montrer sous certaines conditions que l'intégrale
sur la frontière de \(A\) est alors donnée par :
</p>

<p>
\[\int_{\partial A} \omega = \int_A d\omega\]
</p>
</div>
</div>
<div id="outline-container-org7735479" class="outline-3">
<h3 id="org7735479"><span class="section-number-3">7.14.</span> Théorème de Stokes</h3>
<div class="outline-text-3" id="text-7-14">
<p>
Soit \(f,g : \setR^2 \mapsto \setR\) et les vecteurs infinitésimaux :
</p>

<div class="org-center">
<p>
\(
\delta x = e_1 \ dx \)
</p>

<p>
\(
\delta y = e_2 \ dy
\)
</p>
</div>

<p>
Considérons la forme différentielle :
</p>

<p>
\[\omega = f \delta x + g \delta y\]
</p>

<p>
Si les fonctions sont différentiables, on a alors :
</p>

<p>
\[d\omega = \deriveepartielle{f}{x} \delta x \wedge \delta x +  \deriveepartielle{f}{y} \delta y \wedge \delta x + \deriveepartielle{g}{x} \delta x \wedge \delta y +  \deriveepartielle{g}{y} \delta y \wedge \delta y\]
</p>

<p>
Mais comme :
</p>

<div class="org-center">
<p>
\(
\delta x \wedge \delta x = \delta y \wedge \delta y = 0 \)
</p>

<p>
\(
\delta y \wedge \delta x = - \delta x \wedge \delta y
\)
</p>
</div>

<p>
il vient :
</p>

<p>
\[d\omega =  \left( \deriveepartielle{g}{x} - \deriveepartielle{f}{y} \right) \delta x \wedge \delta y\]
</p>

<p>
En intégrant, on obtient alors :
</p>

<p>
\[\int_{\partial A} (f \ \delta x + g \ \delta y)  = \int_A \left( \deriveepartielle{g}{x} - \deriveepartielle{f}{y} \right) dx \wedge dy\]
</p>

<p>
Mais comme nous somme dans la base canonique, on a \(\delta x \wedge \delta y = dx \ dy\) et :
</p>

<p>
\[\int_{\partial A} (f \ \delta x + g \ \delta y)  = \int_A \left( \deriveepartielle{g}{x} - \deriveepartielle{f}{y} \right) \ dx \ dy\]
</p>
</div>
</div>
</div>
<div id="outline-container-orgca17a42" class="outline-2">
<h2 id="orgca17a42"><span class="section-number-2">8.</span> Géométrie différentielle</h2>
<div class="outline-text-2" id="text-8">
<div id="text-table-of-contents-8" role="doc-toc">
<ul>
<li><a href="#org7d69fac">8.1. Dépendances</a></li>
<li><a href="#orge957cc2">8.2. Indices covariants et contravariants</a></li>
<li><a href="#orgcdd8816">8.3. Coordonnées curvilignes</a></li>
<li><a href="#orgded7e95">8.4. Changement de variable</a></li>
<li><a href="#orgf827358">8.5. Produit scalaire</a></li>
<li><a href="#orgb17f448">8.6. Dérivées primales d'un vecteur</a></li>
<li><a href="#org24e5e7f">8.7. Dérivées duales d'un vecteur</a></li>
<li><a href="#orgeee66b4">8.8. Dérivées d'un tenseur</a></li>
<li><a href="#org8b5a2dd">8.9. Produit scalaire et symboles de Christoffel</a></li>
<li><a href="#orgf984995">8.10. Bases biorthonormées</a></li>
<li><a href="#orgbf042db">8.11. Dérivées des changements de variable</a></li>
</ul>
</div>

<p>
\label{chap:geometri}
</p>
</div>
<div id="outline-container-org7d69fac" class="outline-3">
<h3 id="org7d69fac"><span class="section-number-3">8.1.</span> Dépendances</h3>
<div class="outline-text-3" id="text-8-1">
<ul class="org-ul">
<li>Chapitre \ref{chap:vecteur} : Les vecteurs</li>
<li>Chapitre \ref{chap:ps} : Les produits scalaires</li>
<li>Chapitre \ref{chap:tenseur} : Les tenseurs</li>
</ul>
</div>
</div>
<div id="outline-container-orge957cc2" class="outline-3">
<h3 id="orge957cc2"><span class="section-number-3">8.2.</span> Indices covariants et contravariants</h3>
<div class="outline-text-3" id="text-8-2">
<p>
Les indices inférieurs (le \(i\) des vecteurs \(a_i^j\) par exemple) des
tenseurs sont appelés <i>indices covariants</i>.
</p>

<p>
Les indices supérieurs (le \(j\) des vecteurs \(a_i^j\) par exemple) des
tenseurs sont appelés <i>indices contravariants</i>.
</p>

<p>
Ne pas confondre ces <i>indices supérieurs</i> contravariants , très
utilisés en calcul tensoriel, avec les puissances ! Dans le contexte
des tenseurs, une éventuelle puissance d'un scalaire \(\theta_i^j\)
serait notée au besoin par :
</p>

<p>
\[\big( \theta_j^i \big)^m = \theta_j^i \cdot ... \cdot \theta_j^i\]
</p>
</div>
</div>
<div id="outline-container-orgcdd8816" class="outline-3">
<h3 id="orgcdd8816"><span class="section-number-3">8.3.</span> Coordonnées curvilignes</h3>
<div class="outline-text-3" id="text-8-3">
<p>
Soit l'espace vectoriel \(E = \setR^n\) sur \(\setR\). Les coordonnées curvilignes sont basées sur la notion de position \(r\), exprimée comme une fonction de certains paramètres \(x \in \setR^n\) que nous appelons « coordonnées » de \(r\) :
</p>

<p>
\[r = \rho(x)\]
</p>

<p>
où \(\rho : \setR^n \to \setR^n\). Nous envisageons également le cas du changement de variable. La position dépend alors d'un autre jeu de coordonnées \(y \in \setR^n\) :
</p>

<p>
\[r = \sigma(y)\]
</p>

<p>
où \(\sigma : \setR^n \to \setR^n\). Nous définissons les vecteurs fondamentaux \(e_i\) et \(e^i\) au moyen de ces fonctions :
</p>

<div class="org-center">
<p>
\(
e_i(x) = \deriveepartielle{\rho}{x^i}(x) \)
</p>

<p>
\(
e^i(y) = \deriveepartielle{\sigma}{y_i}(y)
\)
</p>
</div>

<p>
de telle sorte que :
</p>

<p>
\[dr = \sum_i e_i \ dx^i  = \sum_i e^i \ dy_i\]
</p>

<p>
Nous supposons que \((e_1, ..., e_n)\) et \((e^1, ..., e^n)\) sont des bases de \(E\).
</p>
</div>
<div id="outline-container-org74466e2" class="outline-4">
<h4 id="org74466e2"><span class="section-number-4">8.3.1.</span> Courbe</h4>
<div class="outline-text-4" id="text-8-3-1">
<p>
Dans le cas où \(x\) et \(y\) ne dépendent que d'un paramètre \(t \in \setR\), on a :
</p>

<p>
\[\OD{r}{t} = \sum_i e_i \ \OD{x^i}{t}  = \sum_i e^i \ \OD{y_i}{t}\]
</p>

<p>
On dit alors que la position \(r\) décrit une courbe.
</p>
</div>
</div>
</div>
<div id="outline-container-orgded7e95" class="outline-3">
<h3 id="orgded7e95"><span class="section-number-3">8.4.</span> Changement de variable</h3>
<div class="outline-text-3" id="text-8-4">
<p>
Si les fonctions \(\rho\) et \(\sigma\) sont inversibles, on a :
</p>

<div class="org-center">
<p>
\(
x = \rho^{-1}(r) = (\rho^{-1} \circ \sigma)(y) = \phi(y) \)
</p>

<p>
\(
y = \sigma^{-1}(r) = (\sigma^{-1} \circ \rho)(x) = \psi(x)
\)
</p>
</div>

<p>
où nous avons implicitement défini \(\phi = \rho^{-1} \circ \sigma\) et \(\psi = \sigma^{-1} \circ \rho\). Nous notons \(\deriveepartielle{x^i}{y_j}\) et \(\deriveepartielle{y_i}{x^j}\) les coordonnées des dérivées de \(\phi\) et \(\psi\) suivant les bases formées par les \(e_i\) et les \(e^i\) :
</p>

<div class="org-center">
<p>
\(
\deriveepartielle{\phi}{y_j} = \sum_i \deriveepartielle{x^i}{y_j} \ e_i \)
</p>

<p>
\(
\deriveepartielle{\psi}{x^j} = \sum_i \deriveepartielle{y_i}{x^j} \ e^i
\)
</p>
</div>

<p>
La composition des dérivées nous donne les relations :
</p>

<div class="org-center">
<p>
\(
e_i = \sum_j \deriveepartielle{r}{y_j} \ \deriveepartielle{y_j}{x^i} =  \sum_j \deriveepartielle{y_j}{x^i} \ e^j \)
</p>

<p>
\(
e^i = \sum_j \deriveepartielle{r}{x^j} \ \deriveepartielle{x^j}{y_i} =  \sum_j \deriveepartielle{x^j}{y_i} \ e_j
\)
</p>
</div>

<p>
qui nous permettent de relier les \(e_i\) aux \(e^j\) et inversément.
</p>
</div>
</div>
<div id="outline-container-orgf827358" class="outline-3">
<h3 id="orgf827358"><span class="section-number-3">8.5.</span> Produit scalaire</h3>
<div class="outline-text-3" id="text-8-5">
<p>
Les produits intérieurs entre vecteurs de base se notent habituellement :
</p>

<div class="org-center">
<p>
\(
g_{ij} = \scalaire{e_i}{e_j} \)
</p>

<p>
\(
g_i^j = \scalaire{e_i}{e^j} \)
</p>

<p>
\(
g^{ij} = \scalaire{e^i}{e^j}
\)
</p>
</div>

<p>
Il est clair d'après les propriétés de symétrie de ce produit que :
</p>

<div class="org-center">
<p>
\(
g_{ij} = g_{ji} \)
</p>

<p>
\(
g^{ij} = g^{ji} \)
</p>

<p>
\(
g_i^j = g_j^i
\)
</p>
</div>

<p>
Le produit scalaire de deux vecteurs \(a,b\in E\) définis par :
</p>

<div class="org-center">
<p>
\(
a = \sum_i a^i \ e_i = \sum_i a_i \ e^i \)
</p>

<p>
\(
b = \sum_i b^i \ e_i = \sum_i b_i \ e^i
\)
</p>
</div>

<p>
peut s'écrire indifféremment comme :
</p>

<div class="org-center">
<p>
\(
\scalaire{a}{b} = \sum_{i,j} g_{ij} \ a^i \ b^j \)
</p>

<p>
\(
\scalaire{a}{b} = \sum_{i,j} g^{ij} \ a_i \ b_j \)
</p>

<p>
\(
\scalaire{a}{b} = \sum_{i,j} g_j^i \ a_i \ b^j \)
</p>

<p>
\(
\scalaire{a}{b} = \sum_{i,j} g_i^j \ a^i \ b_j
\)
</p>
</div>

<p>
Et en particulier, la longeur \(ds\) d'un changement de position \(dr\) vérifie :
</p>

<p>
\[(ds)^2 = \scalaire{dr}{dr} = \sum_{i,j} g_{ij} \ dx^i \ dx^j = \sum_{i,j} g^{ij} \ dy_i \ dy_j\]
</p>

<p>
De plus, les relations entre les vecteurs \(e_i\) et les vecteurs \(e^i\) permettent
de déduire, en utilisant la linéarité du produit scalaire :
</p>

<div class="org-center">
<p>
\(
g_{ij} = \sum_k \deriveepartielle{y_k}{x^i} \ g_j^k = \sum_{k,l} \deriveepartielle{y_k}{x^i} \ \deriveepartielle{y_l}{x^j} \ g^{kl} \)
</p>

<p>
\(
g^{ij} = \sum_k \deriveepartielle{x_k}{y^i} \ g_k^j = \sum_{k,l} \deriveepartielle{x^k}{y_i} \ \deriveepartielle{x^l}{y_j} \ g_{kl}
\)
</p>
</div>
</div>
</div>
<div id="outline-container-orgb17f448" class="outline-3">
<h3 id="orgb17f448"><span class="section-number-3">8.6.</span> Dérivées primales d'un vecteur</h3>
<div class="outline-text-3" id="text-8-6">
<p>
Nous allons à présent voir comment évolue un vecteur \(a\in E\), que l'on note sous la forme :
</p>

<p>
\[a = \sum_i a^i \ e_i\]
</p>

<p>
où les coordonnées \(a^i\in\setR\) tout comme les vecteurs de base \(e_i\) dépendent des coordonnées
\(x^i\). La règle de dérivation d'un produit nous donne :
</p>

<p>
\[da = \sum_i da^i \ e_i + \sum_k a^k \ de_k\]
</p>

<p>
La différentielle \(da^i\) s'obtient directement :
</p>

<p>
\[da^i = \sum_j \deriveepartielle{a^i}{x^j} \ dx^j\]
</p>

<p>
On peut suivre la même règle avec \(de_i\) :
</p>

<p>
\[de_k = \sum_j \deriveepartielle{e_k}{x^j} \ dx^j\]
</p>

<p>
Les symboles de Christoffel \(\christoffel{i}{kj}\) sont définis comme
les coordonnées de \(\deriveepartielle{e_k}{x^j}\) suivant la base \((e_1, ..., e_n)\) :
</p>

<p>
\[\deriveepartielle{e_k}{x^j} = \sum_i \christoffel{i}{kj} \ e_i\]
</p>

<p>
Notons que comme :
</p>

<p>
\[\deriveepartielle{e_k}{x^j} = \dfdxdy{r}{x^j}{x^k} = \dfdxdy{r}{x^k}{x^j} = \deriveepartielle{e_j}{x^k}\]
</p>

<p>
on a la symétrie :
</p>

<p>
\[\christoffel{i}{kj} = \christoffel{i}{jk}\]
</p>

<p>
On peut évaluer ces symboles si on connait par exemple les valeurs des :
</p>

\begin{align}
\scalaire{e^i}{ \deriveepartielle{e_k}{x^j} } &= \sum_m \christoffel{m}{kj} \ \scalaire{e^i}{e_m} \)

\(
&= \sum_m \christoffel{m}{kj} \ g^i_m
\end{align}

<p>
On a alors, pour chaque choix de \(k,j\) un système linéaire à résoudre. Il suffit d'inverser
la matrice \(G = (g^i_m)_{i,m}\) pour obtenir les valeurs des symboles.
</p>

<p>
La dérivation d'un vecteur \(a\in E\) s'écrit alors :
</p>

<p>
\[da = \sum_{i,j} e_i \ dx^j \ \left[ \deriveepartielle{a^i}{x^j} + \sum_k \christoffel{i}{kj} \ a^k  \right]\]
</p>

<p>
On définit les coordonnées :
</p>

<p>
\[\gradient_j a^i = \deriveepartielle{a^i}{x^j} + \sum_k \christoffel{i}{kj} \ a^k\]
</p>

<p>
Dans le cas où les coordonnées dépendent d'un paramètre \(t\in\setR\), on a :
</p>

\begin{align}
\OD{a}{t} &= \sum_{i,j} e_i \ \OD{x^j}{t} \ \left[ \deriveepartielle{a^i}{x^j} + \sum_k \christoffel{i}{kj} \ a^k  \right] \)

\(
&= \sum_{i} e_i \ \left[ \OD{a^i}{t} + \sum_{j,k} \christoffel{i}{kj} \ a^k \ \OD{x^j}{t} \right]
\end{align}
</div>
<div id="outline-container-org725a3eb" class="outline-4">
<h4 id="org725a3eb"><span class="section-number-4">8.6.1.</span> Dérivée seconde et géodésique</h4>
<div class="outline-text-4" id="text-8-6-1">
<p>
Considérons le cas :
</p>

<p>
\[a = \OD{r}{t} = \sum_i e_i \ \OD{x^i}{t}\]
</p>

<p>
Les coordonnées de \(a\) sont clairement \(a^i = \OD{x^i}{t}\) et la dérivée seconde :
</p>

<p>
\[\OOD{r}{t} = \OD{}{t}\OD{r}{t} = \OD{a}{t}\]
</p>

<p>
s'écrit :
</p>

<p>
\[\OOD{r}{t} = \sum_{i} e_i \ \left[ \OOD{x^i}{t} + \sum_{j,k} \christoffel{i}{kj} \ \OD{x^k}{t}\ \OD{x^j}{t} \right]\]
</p>

<p>
Les courbes \(x^i = x^i(t)\) vérifiant \(\OOD{r}{t} = 0\) sont appelées des géodésiques.
</p>
</div>
</div>
</div>
<div id="outline-container-org24e5e7f" class="outline-3">
<h3 id="org24e5e7f"><span class="section-number-3">8.7.</span> Dérivées duales d'un vecteur</h3>
<div class="outline-text-3" id="text-8-7">
<p>
Nous allons recommencer le même processus, écrivant cette fois  \(a\in E\) sous la forme :
</p>

<p>
\[a = \sum_i a_i \ e^i\]
</p>

<p>
Les coordonnées \(a_i\in\setR\) tout comme les vecteurs de base \(e^i\) dépendent des coordonnées
\(y_i\). En suivant la même méthode que ci-dessus, on obtient :
</p>

<p>
\[da = \sum_{i,j} e^i \ dy_j \left[ \deriveepartielle{a_i}{y_j} + \sum_k \christoffel{kj}{i} \ a_k  \right]\]
</p>


<p>
où l'on a introduit de nouveaux symboles de Christoffel, définis par :
</p>

<p>
\[\deriveepartielle{e^k}{y_j} = \sum_i \christoffel{kj}{i} \ e^i\]
</p>

<p>
Ces nouveaux symboles présentent la symétrie :
</p>

<p>
\[\christoffel{kj}{i} = \christoffel{jk}{i}\]
</p>
</div>
</div>
<div id="outline-container-orgeee66b4" class="outline-3">
<h3 id="orgeee66b4"><span class="section-number-3">8.8.</span> Dérivées d'un tenseur</h3>
<div class="outline-text-3" id="text-8-8">
<p>
On étend simplement la notion de dérivée aux tenseurs en appliquant
la formule :
</p>

<p>
\[d(a \otimes b) = da \otimes b + a \otimes db\]
</p>

<p>
où \(a\) et \(b\) sont deux tenseurs d'ordre quelconque.
Par exemple, pour le tenseur :
</p>

<p>
\[T = \sum_{i,j} T^i_j \ e_i \otimes e^j\]
</p>

<p>
on a :
</p>

<p>
\[dT = \sum_{i,j} \left[ dT^i_j \ e_i \otimes e^j + T^i_j \ de_i \otimes e^j + T^i_j \ e_i \otimes de^j \right]\]
</p>

<p>
qui devient, en introduisant les symboles de Christoffel :
</p>

<p>
\[dT = \sum_{i,j} e_i \otimes e^j \left[ dT^i_j + \sum_{k,m} \christoffel{i}{mk} \ T^m_j \ dx^k + \sum_{k,m} \christoffel{mk}{j} \ T^i_m \ dy_k \right]\]
</p>
</div>
</div>
<div id="outline-container-org8b5a2dd" class="outline-3">
<h3 id="org8b5a2dd"><span class="section-number-3">8.9.</span> Produit scalaire et symboles de Christoffel</h3>
<div class="outline-text-3" id="text-8-9">
<p>
Lorsqu'on différentie les \(g_{ij}\), on obtient :
</p>

\begin{align}
dg_{ij} &= \scalaire{de_i}{e_j} + \scalaire{e_i}{de_j} \)

\(
&= \sum_{k,l} \christoffel{k}{il} \ g_{kj} \ dx^l + \sum_{k,l} \christoffel{k}{jl} \ g_{ik} \ dx^l
\end{align}

<p>
On en déduit que :
</p>

<p>
\[\deriveepartielle{g_{ij}}{x^l} = \sum_k \christoffel{k}{il} \ g_{kj} + \sum_k \christoffel{k}{jl} \ g_{ik}\]
</p>

<p>
Définissons :
</p>

<p>
\[\gamma_{ijl} = \sum_k \christoffel{k}{il} \ g_{kj}\]
</p>

<p>
Les propriétés de symétrie des symboles de Christoffel nous montrent que :
</p>

<p>
\[\gamma_{ijl} = \gamma_{ilj}\]
</p>

<p>
Et comme (changement de l'indice \(l\) en \(k\)) :
</p>

<p>
\[\deriveepartielle{g_{ij}}{x^k} = \gamma_{ijk} + \gamma_{jik}\]
</p>

<p>
On en déduit :
</p>

\begin{align}
\deriveepartielle{g_{ij}}{x^k} + \deriveepartielle{g_{jk}}{x^i} - \deriveepartielle{g_{ki}}{x^j}
&= 2 \ \gamma_{jik} \)

\(
&= 2 \sum_l \christoffel{l}{jk} \ g_{li}
\end{align}

<p>
AFAIRE : LA FIN DU CHAPITRE EST A DÉBROUILLONNER
</p>
</div>
</div>
<div id="outline-container-orgf984995" class="outline-3">
<h3 id="orgf984995"><span class="section-number-3">8.10.</span> Bases biorthonormées</h3>
<div class="outline-text-3" id="text-8-10">
</div>
<div id="outline-container-org402528e" class="outline-4">
<h4 id="org402528e"><span class="section-number-4">8.10.1.</span> produit scalaire</h4>
<div class="outline-text-4" id="text-8-10-1">
<p>
Nous considérons tout au long de cette section le cas particulier où les bases
sont biorthonormées, c'est-à-dire :
</p>

<p>
\[g_i^j = \indicatrice_i^j\]
</p>

<p>
On déduit des relations liant les \(g^{ij},g_{ij}\) aux \(g_i^j\) que :
</p>

<div class="org-center">
<p>
\(
g_{ik} g^{kj} = \sum_{k,l,m} \deriveepartielle{y_l}{x^i}\deriveepartielle{x^m}{y_k} g_k^l g_m^j \)
</p>

<p>
\(
g_{ik} g^{kj} = \sum_{k,l,m} \deriveepartielle{y_l}{x^i}\deriveepartielle{x^m}{y_k} \indicatrice_k^l \indicatrice_m^j \)
</p>

<p>
\(
g_{ik} g^{kj} = \sum_{k} \deriveepartielle{y_k}{x^i}\deriveepartielle{x^j}{y_k} \)
</p>

<p>
\(
g_{ik} g^{kj} = \deriveepartielle{x^i}{x^j} = \indicatrice_i^j
\)
</p>
</div>

<p>
On aurait de même :
</p>

<p>
\[g^{ik} g_{kj} = \indicatrice_j^i\]
</p>
</div>
</div>
<div id="outline-container-org0aa0227" class="outline-4">
<h4 id="org0aa0227"><span class="section-number-4">8.10.2.</span> Coordonnées</h4>
<div class="outline-text-4" id="text-8-10-2">
<p>
Les coordonnées d'un tenseur de la forme :
</p>

<p>
\[T = \sum_{i,j,k,l} T_{i...j}^{k...l} e^i \otimes ... \otimes e^j \otimes e_k \otimes ... \otimes e_l\]
</p>

<p>
où il y a \(m\) indices \(i...j\) et \(n\) indices \(k...l\) s'obtiennent facilement
en utilisant la contraction double :
</p>

<p>
\[T_{i...j}^{k...l} = \dblecont{e_j \otimes ... \otimes e_i}{m}{T}{n}{e^l \otimes ... \otimes e^k}\]
</p>
</div>
</div>
</div>
<div id="outline-container-orgbf042db" class="outline-3">
<h3 id="orgbf042db"><span class="section-number-3">8.11.</span> Dérivées des changements de variable</h3>
<div class="outline-text-3" id="text-8-11">
<div class="org-center">
<p>
\(
\deriveepartielle{x^i}{y_j} = \scalaire{e_i}{ \deriveepartielle{\phi}{y_j} } \)
</p>

<p>
\(
\deriveepartielle{y_i}{x^j} = \scalaire{e^i}{ \deriveepartielle{\psi}{x^j} }
\)
</p>
</div>
</div>
<div id="outline-container-org56d5610" class="outline-4">
<h4 id="org56d5610"><span class="section-number-4">8.11.1.</span> Christoffel</h4>
<div class="outline-text-4" id="text-8-11-1">
<p>
Tenant compte de cette identité, l'équation reliant les symboles de Christoffel
aux produits scalaires devient :
</p>

<p>
\[\christoffel{m}{jk} = \frac{1}{2}\sum_i g^{im}\left[\deriveepartielle{g_{ij}}{x^k} + \deriveepartielle{g_{jk}}{x^i} - \deriveepartielle{g_{ki}}{x^j}\right]\]
</p>
</div>
</div>
<div id="outline-container-orgc39d408" class="outline-4">
<h4 id="orgc39d408"><span class="section-number-4">8.11.2.</span> Dérivée d'un vecteur</h4>
<div class="outline-text-4" id="text-8-11-2">
<p>
La relation :
</p>

<p>
\[d\scalaire{e^i}{e_j} = d\indicatrice_i^j = 0\]
</p>

<p>
nous conduit à :
</p>

<div class="org-center">
<p>
\(
\scalaire{de^i}{e_j}+\scalaire{e^i}{de_j} = 0 \)
</p>

<p>
\(
\sum_{k,l} \christoffel{ik}{l} \scalaire{e^l}{e_j} dy_k +
\sum_{k,l} \christoffel{l}{jk} \scalaire{e^i}{e_l} dx^k = 0 \)
</p>

<p>
\(
\sum_k \christoffel{ik}{l} dy_k = - \sum_k \christoffel{l}{jk} dx^m
\)
</p>
</div>

<p>
Par ailleurs :
</p>

<p>
\[da_i = \deriveepartielle{a_i}{y_j} dy_j = \deriveepartielle{a_i}{x^j} dx^j\]
</p>

<p>
On peut donc réexprimer la dérivée duale comme :
</p>

<p>
\[da = \sum_{i,j} e^i dx^j \left[ \deriveepartielle{a_i}{x^j} - \sum_k \christoffel{k}{ij} a_k \right]\]
</p>
</div>
</div>
<div id="outline-container-orgbef4f9a" class="outline-4">
<h4 id="orgbef4f9a"><span class="section-number-4">8.11.3.</span> Gradient</h4>
<div class="outline-text-4" id="text-8-11-3">
<p>
On peut également définir le gradient d'un vecteur par :
</p>

<p>
\[\gradient a = \sum_{i,j} \gradient_j a^i e_i \otimes e^j\]
</p>

<p>
de telle sorte que l'on ait :
</p>

<p>
\[da = \scalaire{\gradient a}{dr} = \gradient a \cdot dr\]
</p>
</div>
</div>
<div id="outline-container-org6bb039c" class="outline-4">
<h4 id="org6bb039c"><span class="section-number-4">8.11.4.</span> Dérivée d'un tenseur</h4>
<div class="outline-text-4" id="text-8-11-4">
<p>
\[dT = \sum_{i,j,k} e_i \otimes e^j dx^k \left[ \deriveepartielle{T^i_j}{x^k} + \sum_m \christoffel{i}{mk} T^m_j - \sum_m \christoffel{m}{jk} T^i_m \right]\]
</p>

<p>
On définit alors les coordonnées :
</p>

<p>
\[\gradient_k T^i_j = \deriveepartielle{T^i_j}{x^k} + \sum_m \christoffel{i}{mk} T^m_j - \sum_m \christoffel{m}{jk} T^i_m\]
</p>
</div>
</div>
<div id="outline-container-orgb80402c" class="outline-4">
<h4 id="orgb80402c"><span class="section-number-4">8.11.5.</span> Tenseur de courbure</h4>
<div class="outline-text-4" id="text-8-11-5">
<p>
Appliquons la formule de dérivation des coordonnées d'un tenseur dans le cas particulier où :
</p>

<p>
\[T^i_j = \gradient_j a^i\]
</p>

<p>
On a :
</p>

<div class="org-center">
<p>
\(
\deriveepartielle{T^i_j}{x^k} = \dfdxdy{a^i}{x^j}{x^k}
</p>
<ul class="org-ul">
<li>&sum;<sub>m</sub> \christoffel{i}{jm} \deriveepartielle{a^m}{x^k}</li>
<li>&sum;<sub>m</sub> \deriveepartielle{}{x^k}\christoffel{i}{jm} a<sup>m</sup> \\ \)</li>
</ul>

<p>
\(
</p>

<p>
&sum;<sub>l</sub> \christoffel{i}{kl} T<sup>l</sup><sub>j</sub> =
&sum;<sub>l</sub> \christoffel{i}{kl} \deriveepartielle{a^i}{x^j} +
&sum;<sub>l,m</sub> \christoffel{i}{kl} \christoffel{l}{jm} a<sup>m</sup> \\ \)
</p>

<p>
\(
</p>

<p>
-&sum;<sub>l</sub> \christoffel{l}{jk} T<sup>i</sup><sub>l</sub> =
-&sum;<sub>l</sub> \christoffel{l}{jk} \deriveepartielle{a^i}{x^l} -
&sum;<sub>l,m</sub> \christoffel{l}{jk} \christoffel{i}{lm} a<sup>m</sup>
\)
</p>
</div>

<p>
La somme de tous ces termes vaut \(\gradient_k T^i_j = \gradient_k \gradient_j a^i\).
En interchangeant les indices \(j\) et \(k\), on obtient \(\gradient_j \gradient_k a^i\).
On en déduit, en utilisant les propriétés de symétrie que :
</p>

<p>
\[\gradient_k \gradient_j a^i - \gradient_j \gradient_k a^i = \sum_m R^i_{m,kj} a^m\]
</p>

<p>
où les \(R_{...}^{...}\) sont définis par :
</p>

<p>
\[R^i_{m,kj} = \deriveepartielle{}{x^k}\christoffel{i}{jm} - \deriveepartielle{}{x^j}\christoffel{i}{km} + \sum_l \christoffel{i}{kl}\christoffel{l}{jm} - \sum_l \christoffel{i}{jl}\christoffel{l}{km}\]
</p>

<p>
Ce sont les coordonnées du tenseur de courbure de Riemann-Christoffel.
</p>
</div>
</div>
</div>
</div>
<div id="outline-container-org33dc391" class="outline-2">
<h2 id="org33dc391"><span class="section-number-2">9.</span> L'espace vectoriel des polynômes</h2>
<div class="outline-text-2" id="text-9">
<div id="text-table-of-contents-9" role="doc-toc">
<ul>
<li><a href="#org537eecb">9.1. Introduction</a></li>
<li><a href="#org480a21e">9.2. Polynômes orthogonaux</a></li>
<li><a href="#orga2caa5b">9.3. Legendre</a></li>
<li><a href="#org5b11c46">9.4. Interpolation</a></li>
</ul>
</div>

<p>
\label{chap:vectpoly}
</p>
</div>
<div id="outline-container-org537eecb" class="outline-3">
<h3 id="org537eecb"><span class="section-number-3">9.1.</span> Introduction</h3>
<div class="outline-text-3" id="text-9-1">
<p>
AFAIRE : ARRANGER LE CHAPITRE
</p>

<p>
Il est clair d'après la définition des polynômes que les espaces
\(\mathcal{P}_n\) sont des espaces vectoriels pour l'ensemble des
scalaires \(S=\setR\) et que :
</p>

<p>
\[\mathcal{P}_n = \ev{\mu_0,\mu_1,...,\mu_n}\]
</p>

<p>
Nous allons montrer que \((\mu_0,\mu_1,...,\mu_n)\) forme
une base de \(\mathcal{P}_n\). Pour cela, il nous reste à prouver
l'indépendance linéaire des \(\mu_i\) :
</p>

<p>
\[\sum_{i=0}^n a_i \mu_i = 0 \quad\Rightarrow\quad a_0 = a_1 = ... = a_n = 0\]
</p>

<p>
c'est-à-dire :
</p>

<p>
\[\sum_{i=0}^n a_i x^i = 0 \quad\forall x \in\setR \quad\Rightarrow\quad a_0 = a_1 = ... = a_n = 0\]
</p>

<p>
Nous allons le montrer par récurrence.
</p>

<p>
Comme \(\mu_0=1\) on a évidemment :
</p>

<p>
\[a_0 1 = 0 \quad\Rightarrow\quad a_0 = 0\]
</p>

<p>
et la thèse est vraie pour \(n=0\). Supposons à présent qu'elle
soit vraie pour \(n-1\). Choisissons \(p\in\mathcal{P}_n\) :
</p>

<p>
\[p(x) = \sum_{i=0}^n a_i x^i\]
</p>

<p>
et supposons que \(p(x) = 0\) pour tout \(x\in\setR\). Comme \(p(0)=0\),
on a :
</p>

<p>
\[a_0 = 0\]
</p>

<p>
donc :
</p>

<p>
\[p(x) = \sum_{i=1}^n a_i x^i = x q(x) = 0\]
</p>

<p>
où l'on à définit \(q\in\mathcal{P}_{n-1}\) par :
</p>

<p>
\[q(x) = \sum_{i=1}^n a_i x^{i-1}\]
</p>

<p>
Il est clair que, pour tout \(x\ne 0\), \(q(x) = 0\). Mais comme les polynômes
sont des fonctions continues, on a :
</p>

<p>
\[q(0) = \lim_{ \substack{ x \rightarrow 0 \\ x \ne 0 } } q(x) = 0\]
</p>

<p>
Donc \(q\) s'annule également en \(0\). On en conclut que \(q(x)\) est
nul pour tout \(x\in\setR\). Par l'hypothèse de récurrence, les
coefficients de ce polynôme sont tous nuls :
</p>

<p>
\[a_1 = a_2 = ... = a_n = 0\]
</p>

<p>
Rassemblant les résultats, il vient :
</p>

<p>
\[a_0 = a_1 = ... = a_n = 0\]
</p>

<p>
et \((\mu_0,\mu_1,...,\mu_n)\) forme bien une base de \(\mathcal{P}_n\).
</p>
</div>
</div>
<div id="outline-container-org480a21e" class="outline-3">
<h3 id="org480a21e"><span class="section-number-3">9.2.</span> Polynômes orthogonaux</h3>
<div class="outline-text-3" id="text-9-2">
<p>
Nous allons à présent voir comment construire des suites de polynômes
orthogonaux pour le produit scalaire :
</p>

<p>
\[\scalaire{p}{q} = \int_a^b p(x) q(x) d\mu(x)\]
</p>

<p>
ou, lorsque c'est possible :
</p>

<p>
\[\scalaire{p}{q} = \int_a^b p(x) q(x) w(x) dx\]
</p>
</div>
<div id="outline-container-orgb0c6a99" class="outline-4">
<h4 id="orgb0c6a99"><span class="section-number-4">9.2.1.</span> Récurrence</h4>
<div class="outline-text-4" id="text-9-2-1">
<p>
On pourrait bien entendu partir de la suite de la base canonique
de monômes \((1,x,x^2,...,x^n)\) et l'orthogonaliser en utilisant le
procédé de Gram-Schmidt, mais on peut arriver à un algorithme plus
rapide en utilisant les propriétés des polynômes. Soit \((\phi_n)_n\)
une suite de polynômes orthonormés, où \(\phi_i\) est de degré \(i\).
On a donc :
</p>

<p>
\[\scalaire{\phi_m}{\phi_n} = \int_A \phi_m(x) \phi_n(x) d\mu(x) = \delta_{mn}\]
</p>

<p>
Supposons que \((\phi_0,...,\phi_n)\) forme une base de \(\mathcal{P}_n\).
On peut vérifier que \((\phi_0,...,\phi_n,x\phi_n)\) forme une base de
\(\mathcal{P}_{n+1}\). On peut donc représenter \(\phi_{n+1}\) comme :
</p>

<p>
\[\phi_{n+1}(x) = a_n x\phi_n(x) + b_n \phi_n(x) + c_n \phi_{n-1}(x) + \sum_{i=0}^{n-2} d_i \phi_i(x)\]
</p>

<p>
Soit \(i \in \{0, ..., n-2\}\). La condition d'orthogonalité de
\(\phi_{n+1}\) avec \(\phi_i\) s'écrit :
</p>

<p>
\[\scalaire{\phi_i}{\phi_{n+1}} = a_n \scalaire{\phi_i}{x\phi_n} + b_n \scalaire{\phi_i}{\phi_n} + c_n \scalaire{\phi_i}{\phi_{n-1}} + \sum_{j=0}^{n-2} d_j \scalaire{\phi_i}{\phi_j} = 0\]
</p>

<p>
L'orthogonalité implique que :
</p>

<div class="org-center">
<p>
\(
\scalaire{\phi_i}{\phi_n} = \scalaire{\phi_i}{\phi_{n-1}} = 0 \)
</p>

<p>
\(
\scalaire{\phi_i}{\phi_j} = \delta_{ij}
\)
</p>
</div>

<p>
On a aussi :
</p>

<p>
\[\scalaire{\phi_i}{x\phi_n} = \int_A x \phi_i(x) \phi_n(x) d\mu(x) = \scalaire{x\phi_i}{\phi_n}\]
</p>

<p>
Mais comme \(\phi_i\) est de degré \(i\), \(x\phi_i\) est de degré \(i+1\)
et on peut l'exprimer comme :
</p>

<p>
\[x \phi_i = \sum_{j=0}^{i+1} \alpha_i \phi_j\]
</p>

<p>
Le produit scalaire devient alors :
</p>

<p>
\[\scalaire{\phi_i}{x\phi_n} = \sum_{j=0}^{i+1} \alpha_i \scalaire{\phi_j}{\phi_n} = 0\]
</p>

<p>
puisque \(j \le i+1 < n\). On en conclut que :
</p>

<p>
\[\scalaire{\phi_i}{\phi_{n+1}} = \sum_{j=0}^{n-2} d_j \delta_{ij} = d_i = 0\]
</p>

<p>
Les conditions :
</p>

<div class="org-center">
<p>
\(
\scalaire{\phi_{n+1}}{\phi_n} = 0 \)
</p>

<p>
\(
\scalaire{\phi_{n+1}}{\phi_{n-1}} = 0
\)
</p>
</div>

<p>
impliquent respectivement que :
</p>

<div class="org-center">
<p>
\(
b_n = -a_n\scalaire{\phi_n}{x\phi_n} \)
</p>

<p>
\(
c_n = -a_n\scalaire{\phi_{n-1}}{x\phi_n}
\)
</p>
</div>

<p>
La condition de normalisation :
</p>

<p>
\[\scalaire{\phi_{n+1}}{\phi_{n+1}} = a_n \scalaire{x\phi_n}{\phi_{n+1}} = 1\]
</p>

<p>
nous donne alors la valeur de \(a_n\) :
</p>

<div class="org-center">
<p>
\(
a_n^2 \scalaire{x\phi_n}{x\phi_n}} -
a_n^2 \scalaire{x\phi_n}{\phi_n}^2 -
a_n^2 \scalaire{x\phi_n}{\phi_{n-1}}^2 = 1 \)
</p>

<p>
\(
a_n = \left[\scalaire{x\phi_n}{x\phi_n} -
\scalaire{x\phi_n}{\phi_n}^2 -
\scalaire{x\phi_n}{\phi_{n-1}}^2\right]^{-1/2}
\)
</p>
</div>

<p>
On voit donc que le choix du produit scalaire détermine :
</p>

<div class="org-center">
<p>
\(
\phi_0 = \unsur{\sqrt{\scalaire{1}{1}}} \)
</p>

<p>
\(
\phi_1 = a_1 (x - \scalaire{\phi_0}{x} \phi_0)
\)
</p>
</div>

<p>
ainsi que toute la suite de polynômes.
</p>
</div>
</div>
<div id="outline-container-orgedfb79a" class="outline-4">
<h4 id="orgedfb79a"><span class="section-number-4">9.2.2.</span> Approximation</h4>
<div class="outline-text-4" id="text-9-2-2">
<p>
Soit une suite de polynômes orthonormaux \((\phi_0,...\phi_n)\)
pour le produit scalaire :
</p>

<p>
\[\scalaire{u}{v} = \int_A u(x) v(x) d\mu(x)\]
</p>

<p>
Nous cherchons l'approximation de \(u\) :
</p>

<p>
\[w(x) = \sum_{i=0}^n w_i \phi_i(x)\]
</p>

<p>
qui minimise l'erreur au sens intégral :
</p>

<p>
\[\scalaire{u-w}{u-w} = \int_A [u(x)-w(x)]^2 d\mu(x)\]
</p>

<p>
sur \(\mathcal{P}_n\). Imposant que la dérivée par rapport aux \(w_i\) soit nulle, on
obtient :
</p>

<p>
\[2 \int_A \phi_i(x) [u(x)-w(x)] d\mu(x) = 0\]
</p>

<p>
Mais comme :
</p>

<p>
\[w_i = \int_A \phi_i(x) w(x) d\mu(x)\]
</p>

<p>
on obtient :
</p>

<p>
\[w_i = \int_A \phi_i(x) u(x) d\mu(x) = \scalaire{\phi_i}{u}\]
</p>

<p>
Ce qui n'a rien d'étonnant au vu  des résultats du chapitre \ref{chap:vector}.
On peut vérifier facilement que la hessienne de l'erreur par rapport aux \(w_i\)
est bien positive. L'approximation ainsi définie :
</p>

<div class="org-center">
<p>
\(
w(x) = \sum_{i=0}^n \phi_i(x) \int_A \phi_i(y) u(y) d\mu(y) \)
</p>

<p>
\(
w(x) = \sum_{i=0}^n \int_A \phi_i(x) \phi_i(y) u(y) d\mu(y)
\)
</p>
</div>

<p>
minimise donc bien l'erreur sur l'ensemble des polynômes de degré \(n\).
</p>
</div>
</div>
<div id="outline-container-org2716b32" class="outline-4">
<h4 id="org2716b32"><span class="section-number-4">9.2.3.</span> Intégration de Gauss</h4>
<div class="outline-text-4" id="text-9-2-3">
<p>
Soit une suite de polynômes orthonormaux \((\phi_0,...\phi_n)\)
pour le produit scalaire :
</p>

<p>
\[\scalaire{u}{v} = \int_A u(x) v(x) d\mu(x)\]
</p>

<p>
Considérons la formule d'intégration :
</p>

<p>
\[I(f) = \sum_{i=0}^n w_i f(x_i)\]
</p>

<p>
supposée approximer l'intégrale :
</p>

<p>
\[\langle f \rangle = \scalaire{f}{1} = \int_A f(x) d\mu(x)\]
</p>

<p>
Fixons les points \(x_0 < x_1 < ... < x_n\) et imposons que
la formule soit exacte pour \(\phi_0,...,\phi_n\). On a :
</p>

<p>
\[\langle \phi_k \rangle = \sum_{i=0}^n w_i \phi_k(x_i)\]
</p>

<p>
où \(k = 0,1,...,n\). Définissant les matrices et vecteurs :
</p>

<div class="org-center">
<p>
\(
\varphi = (\langle \phi_k \rangle)_k \)
</p>

<p>
\(
W  = (w_i)_i \)
</p>

<p>
\(
\Phi = \left(\phi_i(x_j)\right)_{i,j}
\)
</p>
</div>

<p>
ces conditions se ramènent à :
</p>

<p>
\[\Phi W = \varphi\]
</p>

<p>
Si la matrice \(\Phi(n+1,n+1)\) est inversible, on a alors :
</p>

<p>
\[W = \Phi^{-1} \varphi\]
</p>

<p>
La formule est alors valable pour tout polynôme de \(\mathcal{P}_n\).
Notons que
</p>

<p>
\[\langle \phi_k \rangle = \unsur{\phi_0} \scalaire{\phi_k}{\phi_0}\]
</p>

<p>
s'annule pour tout \(k\ne 0\). Si les racines de \(\phi_{n+1}\) sont
toutes distinctes, on peut choisir les \(x_i\) tels que :
</p>

<p>
\[\phi_{n+1}(x_i) = 0\]
</p>

<p>
On a alors :
</p>

<p>
\[\langle \phi_{n+1} \rangle = I(\phi_{n+1}) = 0\]
</p>

<p>
et la formule devient valable sur \(\mathcal{P}_{n+1}\). Mieux,
considérons un polynôme \(p\) de degré \(n+m+1\) où \(m \ge 0\) et sa
division euclidienne par \(\phi_{n+1}\). On a :
</p>

<p>
\[p(x) = q(x) \phi_{n+1}(x) + r(x)\]
</p>

<p>
Comme \(q\) est de degré \(m\), on a :
</p>

<p>
\[q = \sum_{i=0}^m q_i \phi_i\]
</p>

<p>
Si \(m \le n\), on a donc :
</p>

<p>
\[\langle q \phi_{n+1} \rangle = \sum_{i=0}^n q_i \scalaire{\phi_i}{\phi_{n+1}} = 0\]
</p>

<p>
et :
</p>

<p>
\[\langle p \rangle = \langle r \rangle = I(r)\]
</p>

<p>
puisque \(r\) est de degré \(n\) au plus. Comme \(\phi_{n+1}\) s'annule en
les \(x_i\), on a aussi ;
</p>

<p>
\[I(p) = I(r)\]
</p>

<p>
Rassemblant tout ces résultats, on obtient :
</p>

<p>
\[\int_A f(x) d\mu(x) = \sum_{i=0}^n w_i f(x_i)\]
</p>

<p>
pour tout polynôme \(f\in\mathcal{P}_{2n+1}\). En pratique, on utilise
ces formules d'intégration pour des fonctions qui ne sont pas forcément
des polynômes.
</p>
</div>
</div>
</div>
<div id="outline-container-orga2caa5b" class="outline-3">
<h3 id="orga2caa5b"><span class="section-number-3">9.3.</span> Legendre</h3>
<div class="outline-text-3" id="text-9-3">
<p>
Les polynômes de Legendre sont orthogonaux pour le produit scalaire :
</p>

<p>
\[\int_{-1}^1 P_n(x) P_m(x) dx = \frac{2}{2 n + 1} \delta_{mn}\]
</p>

<p>
Ils obéissent à la récurrence :
</p>

<div class="org-center">
<p>
\(
P_0(x) = 1 \)
</p>

<p>
\(
P_1(x) = x \)
</p>

<p>
\(
(n+1) P_{n+1}(x) = (2 n + 1) x P_n(x) - n P_{n-1}(x)
\)
</p>
</div>
</div>
</div>
<div id="outline-container-org5b11c46" class="outline-3">
<h3 id="org5b11c46"><span class="section-number-3">9.4.</span> Interpolation</h3>
<div class="outline-text-3" id="text-9-4">
<p>
Un problème d'interpolation consiste à trouver les coefficients :
\(a_i\in\setR\) tels que la fonction :
</p>

<p>
\[u = \sum_{i=1}^n a_i u_i\]
</p>

<p>
où les \(u_i\) sont des polynômes de degré \(n\), vérifie :
</p>

<p>
\[\form{\phi_i}{u} = y_i\]
</p>

<p>
pour tout \(i=1,2,...,n\), où les \(\phi_i\) sont des formes linéaires de \(\mathcal{P}_N^D\)
et les \(y_i\) des réels donnés.
</p>

<p>
On utilise couramment des bases biorthogonales :
</p>

<p>
\[\form{\phi_i}{u_j} = \delta_{ij}\]
</p>

<p>
et on a alors simplement :
</p>

<p>
\[a_i = \form{\phi_i}{u}\]
</p>

<p>
L'exemple le plus courant est :
</p>

<div class="org-center">
<p>
\(
\form{\phi_i}{u} = u(x_i) \)
</p>

<p>
\(
y_i = f(x_i)
\)
</p>
</div>

<p>
pour une certaine fonction \(f\) à interpoler. Les conditions ci-dessus
se résument alors à l'égalité de \(f\) et de \(u\) en un nombre fini de points :
</p>

<p>
\[u(x_i) = f(x_i)\]
</p>

<p>
On rencontre parfois aussi le cas :
</p>

<div class="org-center">
<p>
\(
\form{\phi_i}{u} = \OD{u}{x}(x_i) \)
</p>

<p>
\(
y_i = \OD{f}{x}(x_i)
\)
</p>
</div>
</div>
<div id="outline-container-orga15ae33" class="outline-4">
<h4 id="orga15ae33"><span class="section-number-4">9.4.1.</span> Lagrange</h4>
<div class="outline-text-4" id="text-9-4-1">
<p>
Les polynômes de Lagrange \(\Lambda_i\) sont biorthogonaux aux formes :
</p>

<p>
\[\form{\phi_i}{u} = u(x_i)\]
</p>

<p>
On a donc :
</p>

<p>
\[\form{\phi_j}{\Lambda_i} = \Lambda_i(x_j) = \delta_{ij}\]
</p>

<p>
Le polynôme \(\Lambda_i\) doit donc s'annuler en tout les points \(x_j\),
où \(j \ne i\). On peut donc le factoriser comme :
</p>

<p>
\[\Lambda_i(x) = A_i \prod_{j \in E_i} (x-x_j) = A_i P_i(x)\]
</p>

<p>
où \(E_i = \{ 1,2,...,n \} \setminus \{i\}\). Mais comme \(\Lambda_i(x_i) = 1\), on a :
</p>

<p>
\[A_i = \unsur{P_i(x_i)}\]
</p>

<p>
et :
</p>

<p>
\[\Lambda_i(x) = \prod_{j \in E_i} \frac{(x-x_j)}{(x_i - x_j)}\]
</p>

<p>
Donc si on souhaite construire un polynôme :
</p>

<p>
\[w(x) = \sum_{i=1}^{n} u_i \Lambda_i(x)\]
</p>

<p>
qui interpole \(u\) en les \(x_i\) :
</p>

<p>
\[u(x_i) = w(x_i)\]
</p>

<p>
pour tout \(i = 1,2,...,n\), il faut et il suffit de prendre :
</p>

<p>
\[u_i = \form{\phi_i}{u} = u(x_i)\]
</p>
</div>
</div>
<div id="outline-container-orgb28457e" class="outline-4">
<h4 id="orgb28457e"><span class="section-number-4">9.4.2.</span> Newton</h4>
<div class="outline-text-4" id="text-9-4-2">
<p>
L'interpolation de Newton utilise des polynômes construit récursivement à partir
des polynômes de degré inférieur. Soit \(f\) la fonction à interpoler, \(p_{i,j}\) le polynôme
de degré \(j-i\) :
</p>

<p>
\[p_{ij}(x) = \sum_{j=0}^{j-i} a_k x^k\]
</p>

<p>
tels que :
</p>

<p>
\[p_{ij}(x_k) = f(x_k)\]
</p>

<p>
pour tous \(k\in\{i,i+1,...,j\}\). On voit que si \(i=j\), on a :
</p>

<p>
\[p_{ii} = f(x_i)\]
</p>

<p>
Pour \(i < j\), on peut construire les \(p_{i,j}\) par récurrence. On vérifie
que :
</p>

<p>
\[p_{ij}(x) = \frac{(x-x_i)p_{i+1,j}(x)-(x-x_j)p_{i,j-1}(x)}{x_j-x_i}\]
</p>

<p>
satisfait bien aux conditions d'interpolation ci-dessus.
</p>
</div>
</div>
</div>
</div>
</div>
<div id="postamble" class="status">
<p class="author">Auteur: chimay</p>
<p class="date">Created: 2025-10-20 lun 17:55</p>
<p class="validation"><a href="https://validator.w3.org/check?uri=referer">Validate</a></p>
</div>
</body>
</html>
