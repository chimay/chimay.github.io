<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN"
"http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" lang="fr" xml:lang="fr">
<head>
<!-- 2023-05-10 mer 16:46 -->
<meta http-equiv="Content-Type" content="text/html;charset=utf-8" />
<meta name="viewport" content="width=device-width, initial-scale=1" />
<title>Eclats de vers : Matemat 13 : Probabilité - 1</title>
<meta name="author" content="chimay" />
<meta name="generator" content="Org Mode" />
<style>
  .title  { text-align: center;
             margin-bottom: .2em; }
  .subtitle { text-align: center;
              font-size: medium;
              font-weight: bold;
              margin-top:0; }
  .todo   { font-family: monospace; color: red; }
  .done   { font-family: monospace; color: green; }
  .priority { font-family: monospace; color: orange; }
  .tag    { background-color: #eee; font-family: monospace;
            padding: 2px; font-size: 80%; font-weight: normal; }
  .timestamp { color: #bebebe; }
  .timestamp-kwd { color: #5f9ea0; }
  .org-right  { margin-left: auto; margin-right: 0px;  text-align: right; }
  .org-left   { margin-left: 0px;  margin-right: auto; text-align: left; }
  .org-center { margin-left: auto; margin-right: auto; text-align: center; }
  .underline { text-decoration: underline; }
  #postamble p, #preamble p { font-size: 90%; margin: .2em; }
  p.verse { margin-left: 3%; }
  pre {
    border: 1px solid #e6e6e6;
    border-radius: 3px;
    background-color: #f2f2f2;
    padding: 8pt;
    font-family: monospace;
    overflow: auto;
    margin: 1.2em;
  }
  pre.src {
    position: relative;
    overflow: auto;
  }
  pre.src:before {
    display: none;
    position: absolute;
    top: -8px;
    right: 12px;
    padding: 3px;
    color: #555;
    background-color: #f2f2f299;
  }
  pre.src:hover:before { display: inline; margin-top: 14px;}
  /* Languages per Org manual */
  pre.src-asymptote:before { content: 'Asymptote'; }
  pre.src-awk:before { content: 'Awk'; }
  pre.src-authinfo::before { content: 'Authinfo'; }
  pre.src-C:before { content: 'C'; }
  /* pre.src-C++ doesn't work in CSS */
  pre.src-clojure:before { content: 'Clojure'; }
  pre.src-css:before { content: 'CSS'; }
  pre.src-D:before { content: 'D'; }
  pre.src-ditaa:before { content: 'ditaa'; }
  pre.src-dot:before { content: 'Graphviz'; }
  pre.src-calc:before { content: 'Emacs Calc'; }
  pre.src-emacs-lisp:before { content: 'Emacs Lisp'; }
  pre.src-fortran:before { content: 'Fortran'; }
  pre.src-gnuplot:before { content: 'gnuplot'; }
  pre.src-haskell:before { content: 'Haskell'; }
  pre.src-hledger:before { content: 'hledger'; }
  pre.src-java:before { content: 'Java'; }
  pre.src-js:before { content: 'Javascript'; }
  pre.src-latex:before { content: 'LaTeX'; }
  pre.src-ledger:before { content: 'Ledger'; }
  pre.src-lisp:before { content: 'Lisp'; }
  pre.src-lilypond:before { content: 'Lilypond'; }
  pre.src-lua:before { content: 'Lua'; }
  pre.src-matlab:before { content: 'MATLAB'; }
  pre.src-mscgen:before { content: 'Mscgen'; }
  pre.src-ocaml:before { content: 'Objective Caml'; }
  pre.src-octave:before { content: 'Octave'; }
  pre.src-org:before { content: 'Org mode'; }
  pre.src-oz:before { content: 'OZ'; }
  pre.src-plantuml:before { content: 'Plantuml'; }
  pre.src-processing:before { content: 'Processing.js'; }
  pre.src-python:before { content: 'Python'; }
  pre.src-R:before { content: 'R'; }
  pre.src-ruby:before { content: 'Ruby'; }
  pre.src-sass:before { content: 'Sass'; }
  pre.src-scheme:before { content: 'Scheme'; }
  pre.src-screen:before { content: 'Gnu Screen'; }
  pre.src-sed:before { content: 'Sed'; }
  pre.src-sh:before { content: 'shell'; }
  pre.src-sql:before { content: 'SQL'; }
  pre.src-sqlite:before { content: 'SQLite'; }
  /* additional languages in org.el's org-babel-load-languages alist */
  pre.src-forth:before { content: 'Forth'; }
  pre.src-io:before { content: 'IO'; }
  pre.src-J:before { content: 'J'; }
  pre.src-makefile:before { content: 'Makefile'; }
  pre.src-maxima:before { content: 'Maxima'; }
  pre.src-perl:before { content: 'Perl'; }
  pre.src-picolisp:before { content: 'Pico Lisp'; }
  pre.src-scala:before { content: 'Scala'; }
  pre.src-shell:before { content: 'Shell Script'; }
  pre.src-ebnf2ps:before { content: 'ebfn2ps'; }
  /* additional language identifiers per "defun org-babel-execute"
       in ob-*.el */
  pre.src-cpp:before  { content: 'C++'; }
  pre.src-abc:before  { content: 'ABC'; }
  pre.src-coq:before  { content: 'Coq'; }
  pre.src-groovy:before  { content: 'Groovy'; }
  /* additional language identifiers from org-babel-shell-names in
     ob-shell.el: ob-shell is the only babel language using a lambda to put
     the execution function name together. */
  pre.src-bash:before  { content: 'bash'; }
  pre.src-csh:before  { content: 'csh'; }
  pre.src-ash:before  { content: 'ash'; }
  pre.src-dash:before  { content: 'dash'; }
  pre.src-ksh:before  { content: 'ksh'; }
  pre.src-mksh:before  { content: 'mksh'; }
  pre.src-posh:before  { content: 'posh'; }
  /* Additional Emacs modes also supported by the LaTeX listings package */
  pre.src-ada:before { content: 'Ada'; }
  pre.src-asm:before { content: 'Assembler'; }
  pre.src-caml:before { content: 'Caml'; }
  pre.src-delphi:before { content: 'Delphi'; }
  pre.src-html:before { content: 'HTML'; }
  pre.src-idl:before { content: 'IDL'; }
  pre.src-mercury:before { content: 'Mercury'; }
  pre.src-metapost:before { content: 'MetaPost'; }
  pre.src-modula-2:before { content: 'Modula-2'; }
  pre.src-pascal:before { content: 'Pascal'; }
  pre.src-ps:before { content: 'PostScript'; }
  pre.src-prolog:before { content: 'Prolog'; }
  pre.src-simula:before { content: 'Simula'; }
  pre.src-tcl:before { content: 'tcl'; }
  pre.src-tex:before { content: 'TeX'; }
  pre.src-plain-tex:before { content: 'Plain TeX'; }
  pre.src-verilog:before { content: 'Verilog'; }
  pre.src-vhdl:before { content: 'VHDL'; }
  pre.src-xml:before { content: 'XML'; }
  pre.src-nxml:before { content: 'XML'; }
  /* add a generic configuration mode; LaTeX export needs an additional
     (add-to-list 'org-latex-listings-langs '(conf " ")) in .emacs */
  pre.src-conf:before { content: 'Configuration File'; }

  table { border-collapse:collapse; }
  caption.t-above { caption-side: top; }
  caption.t-bottom { caption-side: bottom; }
  td, th { vertical-align:top;  }
  th.org-right  { text-align: center;  }
  th.org-left   { text-align: center;   }
  th.org-center { text-align: center; }
  td.org-right  { text-align: right;  }
  td.org-left   { text-align: left;   }
  td.org-center { text-align: center; }
  dt { font-weight: bold; }
  .footpara { display: inline; }
  .footdef  { margin-bottom: 1em; }
  .figure { padding: 1em; }
  .figure p { text-align: center; }
  .equation-container {
    display: table;
    text-align: center;
    width: 100%;
  }
  .equation {
    vertical-align: middle;
  }
  .equation-label {
    display: table-cell;
    text-align: right;
    vertical-align: middle;
  }
  .inlinetask {
    padding: 10px;
    border: 2px solid gray;
    margin: 10px;
    background: #ffffcc;
  }
  #org-div-home-and-up
   { text-align: right; font-size: 70%; white-space: nowrap; }
  textarea { overflow-x: auto; }
  .linenr { font-size: smaller }
  .code-highlighted { background-color: #ffff00; }
  .org-info-js_info-navigation { border-style: none; }
  #org-info-js_console-label
    { font-size: 10px; font-weight: bold; white-space: nowrap; }
  .org-info-js_search-highlight
    { background-color: #ffff00; color: #000000; font-weight: bold; }
  .org-svg { }
</style>
<link rel="stylesheet" type="text/css" href="../style/defaut.css" />
<script type="text/x-mathjax-config">
    MathJax.Hub.Config({
        displayAlign: "center",
        displayIndent: "0em",

        "HTML-CSS": { scale: 100,
                        linebreaks: { automatic: "false" },
                        webFont: "TeX"
                       },
        SVG: {scale: 100,
              linebreaks: { automatic: "false" },
              font: "TeX"},
        NativeMML: {scale: 100},
        TeX: { equationNumbers: {autoNumber: "AMS"},
               MultLineWidth: "85%",
               TagSide: "right",
               TagIndent: ".8em"
             }
});
</script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.0/MathJax.js?config=TeX-AMS_HTML"></script>
</head>
<body>
<div id="content" class="content">
<h1 class="title">Eclats de vers : Matemat 13 : Probabilité - 1</h1>
<p>
<a href="index.html">Index des Grimoires</a>
</p>

<p>
<a href="../index.html">Retour à l’accueil</a>
</p>

<div id="table-of-contents" role="doc-toc">
<h2>Table des matières</h2>
<div id="text-table-of-contents" role="doc-toc">
<ul>
<li><a href="#org5dc25ec">1. Probabilité</a></li>
</ul>
</div>
</div>

<p>
\( \newcommand{\parentheses}[1]{\left(#1\right)}
\newcommand{\crochets}[1]{\left[#1\right]}
\newcommand{\accolades}[1]{\left\{#1\right\}}
\newcommand{\ensemble}[1]{\left\{#1\right\}}
\newcommand{\identite}{\mathrm{Id}}
\newcommand{\indicatrice}{\boldsymbol{\delta}}
\newcommand{\dirac}{\delta}
\newcommand{\moinsun}{{-1}}
\newcommand{\inverse}{\ddagger}
\newcommand{\pinverse}{\dagger}
\newcommand{\topologie}{\mathfrak{T}}
\newcommand{\ferme}{\mathfrak{F}}
\newcommand{\img}{\mathbf{i}}
\newcommand{\binome}[2]{
\left\{ \begin{array}{c}
#1 \\
#2 \\
\end{array} \right\}
}
\newcommand{\canonique}{\mathfrak{c}}
\newcommand{\tenseuridentite}{\boldsymbol{\mathcal{I}}}
\newcommand{\permutation}{\boldsymbol{\epsilon}}
\newcommand{\matriceZero}{\mathfrak{0}}
\newcommand{\matriceUn}{\mathfrak{1}}
\newcommand{\christoffel}[2]{
\left\{ \begin{array}{c}
#1 \\
#2 \\
\end{array} \right\}
}
\newcommand{\lagrangien}{\mathfrak{L}}
\newcommand{\sousens}{\mathfrak{P}}
\newcommand{\partition}{\mathrm{Partition}}
\newcommand{\tribu}{\mathrm{Tribu}}
\newcommand{\topologies}{\mathrm{Topo}}
\newcommand{\setB}{\mathbb{B}}
\newcommand{\setN}{\mathbb{N}}
\newcommand{\setZ}{\mathbb{Z}}
\newcommand{\setQ}{\mathbb{Q}}
\newcommand{\setR}{\mathbb{R}}
\newcommand{\setC}{\mathbb{C}}
\newcommand{\corps}{\mathbb{K}}
\newcommand{\boule}{\mathfrak{B}}
\newcommand{\intervalleouvert}[2]{\relax \ ] #1 , #2 [ \ \relax}
\newcommand{\intervallesemiouvertgauche}[2]{\relax \ ] #1 , #2 ]}
\newcommand{\intervallesemiouvertdroite}[2]{[ #1 , #2 [ \ \relax}
\newcommand{\fonction}{\mathbb{F}}
\newcommand{\bijection}{\mathrm{Bij}}
\newcommand{\polynome}{\mathrm{Poly}}
\newcommand{\lineaire}{\mathrm{Lin}}
\newcommand{\continue}{\mathrm{Cont}}
\newcommand{\homeomorphisme}{\mathrm{Hom}}
\newcommand{\etagee}{\mathrm{Etagee}}
\newcommand{\lebesgue}{\mathrm{Leb}}
\newcommand{\lipschitz}{\mathrm{Lip}}
\newcommand{\suitek}{\mathrm{Suite}}
\newcommand{\matrice}{\mathbb{M}}
\newcommand{\krylov}{\mathrm{Krylov}}
\newcommand{\tenseur}{\mathbb{T}}
\newcommand{\essentiel}{\mathfrak{E}}
\newcommand{\relation}{\mathrm{Rel}}
\newcommand{\strictinferieur}{\ < \ }
\newcommand{\strictsuperieur}{\ > \ }
\newcommand{\ensinferieur}{\eqslantless}
\newcommand{\enssuperieur}{\eqslantgtr}
\newcommand{\esssuperieur}{\gtrsim}
\newcommand{\essinferieur}{\lesssim}
\newcommand{\essegal}{\eqsim}
\newcommand{\union}{\ \cup \ }
\newcommand{\intersection}{\ \cap \ }
\newcommand{\opera}{\divideontimes}
\newcommand{\autreaddition}{\boxplus}
\newcommand{\autremultiplication}{\circledast}
\newcommand{\commutateur}[2]{\left[ #1 , #2 \right]}
\newcommand{\convolution}{\circledcirc}
\newcommand{\correlation}{\ \natural \ }
\newcommand{\diventiere}{\div}
\newcommand{\modulo}{\bmod}
\newcommand{\pgcd}{pgcd}
\newcommand{\ppcm}{ppcm}
\newcommand{\produitscalaire}[2]{\left\langle #1 \left|\right\relax #2 \right\rangle}
\newcommand{\scalaire}[2]{\left\langle #1 \| #2 \right\rangle}
\newcommand{\braket}[3]{\left\langle #1 \right| #2 \left| #3 \right\rangle}
\newcommand{\orthogonal}{\bot}
\newcommand{\forme}[2]{\left\langle #1 , #2 \right\rangle}
\newcommand{\biforme}[3]{\left\langle #1 , #2 , #3 \right\rangle}
\newcommand{\contraction}[3]{\left\langle #1 \odot #3 \right\rangle_{#2}}
\newcommand{\dblecont}[5]{\left\langle #1 \right| #3 \left| #5 \right\rangle_{#2,#4}}
\newcommand{\major}{major}
\newcommand{\minor}{minor}
\newcommand{\maxim}{maxim}
\newcommand{\minim}{minim}
\newcommand{\argument}{arg}
\newcommand{\argmin}{arg\ min}
\newcommand{\argmax}{arg\ max}
\newcommand{\supessentiel}{ess\ sup}
\newcommand{\infessentiel}{ess\ inf}
\newcommand{\dual}{\star}
\newcommand{\distance}{\mathfrak{dist}}
\newcommand{\norme}[1]{\left\| #1 \right\|}
\newcommand{\normetrois}[1]{\left|\left\| #1 \right\|\right|}
\newcommand{\adh}{adh}
\newcommand{\interieur}{int}
\newcommand{\frontiere}{\partial}
\newcommand{\image}{im}
\newcommand{\domaine}{dom}
\newcommand{\noyau}{ker}
\newcommand{\support}{supp}
\newcommand{\signe}{sign}
\newcommand{\abs}[1]{\left| #1 \right|}
\newcommand{\unsur}[1]{\frac{1}{#1}}
\newcommand{\arrondisup}[1]{\lceil #1 \rceil}
\newcommand{\arrondiinf}[1]{\lfloor #1 \rfloor}
\newcommand{\conjugue}{conj}
\newcommand{\conjaccent}[1]{\overline{#1}}
\newcommand{\division}{division}
\newcommand{\difference}{\boldsymbol{\Delta}}
\newcommand{\differentielle}[2]{\mathfrak{D}^{#1}_{#2}}
\newcommand{\OD}[2]{\frac{d #1}{d #2}}
\newcommand{\OOD}[2]{\frac{d^2 #1}{d #2^2}}
\newcommand{\NOD}[3]{\frac{d^{#3} #1}{d #2^{#3}}}
\newcommand{\deriveepartielle}[2]{\frac{\partial #1}{\partial #2}}
\newcommand{\dblederiveepartielle}[2]{\frac{\partial^2 #1}{\partial #2 \partial #2}}
\newcommand{\dfdxdy}[3]{\frac{\partial^2 #1}{\partial #2 \partial #3}}
\newcommand{\dfdxdx}[2]{\frac{\partial^2 #1}{\partial #2^2}}
\newcommand{\gradient}{\mathbf{\nabla}}
\newcommand{\combilin}[1]{\mathrm{span}\{ #1 \}}
\newcommand{\trace}{tr}
\newcommand{\proba}{\mathbb{P}}
\newcommand{\probaof}[1]{\mathbb{P}\left[#1\right]}
\newcommand{\esperof}[1]{\mathbb{E}\left[#1\right]}
\newcommand{\cov}[2]{\mathrm{cov} \left( #1 , #2 \right) }
\newcommand{\var}[1]{\mathrm{var} \left( #1 \right) }
\newcommand{\rand}{\mathrm{rand}}
\newcommand{\variation}[1]{\left\langle #1 \right\rangle}
\newcommand{\composante}{comp}
\newcommand{\bloc}{bloc}
\newcommand{\ligne}{ligne}
\newcommand{\colonne}{colonne}
\newcommand{\diagonale}{diag}
\newcommand{\matelementaire}{\mathrm{Elem}}
\newcommand{\matpermutation}{permut}
\newcommand{\matunitaire}{\mathrm{Unitaire}}
\newcommand{\gaussjordan}{\mathrm{GaussJordan}}
\newcommand{\householder}{\mathrm{Householder}}
\newcommand{\rang}{rang}
\newcommand{\schur}{\mathrm{Schur}}
\newcommand{\singuliere}{\mathrm{DVS}}
\newcommand{\convexe}{\mathrm{Convexe}}
\newcommand{\petito}[1]{o\left(#1\right)}
\newcommand{\grando}[1]{O\left(#1\right)} \)
</p>

<div id="outline-container-org5dc25ec" class="outline-2">
<h2 id="org5dc25ec"><span class="section-number-2">1.</span> Probabilité</h2>
<div class="outline-text-2" id="text-1">
<div id="text-table-of-contents" role="doc-toc">
<ul>
<li><a href="#org5f6c94c">1.1. Probabilité</a></li>
<li><a href="#org5aaa4e9">1.2. Variable aléatoire</a></li>
<li><a href="#orgde5a6fb">1.3. Mesure induite</a></li>
<li><a href="#orgb3699aa">1.4. Collection induite</a></li>
<li><a href="#org433b760">1.5. Espérance</a></li>
<li><a href="#orga9f1161">1.6. Espérance et mesure induite</a></li>
<li><a href="#orgd14adf0">1.7. Fonction génératrice des moments</a></li>
<li><a href="#orgae7350d">1.8. Variance</a></li>
<li><a href="#org14761a8">1.9. Covariance</a></li>
<li><a href="#org3bb4640">1.10. Variance d'une combinaison linéaire</a></li>
<li><a href="#orgf6546d6">1.11. Produit scalaire</a></li>
<li><a href="#org0e6e307">1.12. Probabilité conditionnelle</a></li>
<li><a href="#org88473dd">1.13. Espérance conditionnelle à un ensemble</a></li>
<li><a href="#org2b5ad9e">1.14. Espérance conditionnelle à une tribu</a></li>
<li><a href="#orgd63fc20">1.15. Ensemble discret</a></li>
</ul>
</div>

<p>
\label{chap:proba}
</p>
</div>


<div id="outline-container-org5f6c94c" class="outline-3">
<h3 id="org5f6c94c"><span class="section-number-3">1.1.</span> Probabilité</h3>
<div class="outline-text-3" id="text-1-1">
<p>
Une probabilité \(\proba\) sur un ensemble d'événements \(\Omega\) est une mesure définie sur \(\mathcal{S}=\{ A : A \subseteq \Omega \}\) et à valeurs dans \([0,1]\) :
</p>

<p>
\[\proba : \mathcal{S} \mapsto [0,1] \quad\]
</p>

<p>
Cette probabilité doit vérifier la normalisation :
</p>

<p>
\[\probaof{\Omega} = 1\]
</p>

<p>
ainsi que l'additivité :
</p>

<p>
\[\probaof{\bigcup_i \Phi_i} = \sum_i \probaof{\Phi_i}\]
</p>

<p>
lorsque les ensembles \(\Phi_i\) sont disjoints deux à deux :
</p>

<div class="org-center">
<p>
\(
&Phi;<sub>i</sub> &cap; &Phi;<sub>j</sub> =
</p>
\begin{cases}
\Phi_i & i = j \\
\emptyset & i \ne j
\end{cases}
<p>
\)
</p>
</div>

<p>
On en déduit directement que :
</p>

<p>
\[\probaof{\Phi} = \probaof{\Phi \cup \emptyset} = \probaof{\Phi} + \probaof{\emptyset}\]
</p>

<p>
d'où \(\probaof{\emptyset} = 0\).
</p>

<p>
La grandeur \(\probaof{\Phi}\) peut s'interpréter comme la probabilité que l'un des événements de \(\Phi\) se réalise.
</p>
</div>
</div>


<div id="outline-container-org5aaa4e9" class="outline-3">
<h3 id="org5aaa4e9"><span class="section-number-3">1.2.</span> Variable aléatoire</h3>
<div class="outline-text-3" id="text-1-2">
<p>
Une variable aléatoire \(X\) associe une valeur réelle a chaque élément de \(\Omega\). On a donc \(X : \Omega \mapsto \setR\).
</p>
</div>
</div>


<div id="outline-container-orgde5a6fb" class="outline-3">
<h3 id="orgde5a6fb"><span class="section-number-3">1.3.</span> Mesure induite</h3>
<div class="outline-text-3" id="text-1-3">
<p>
Etant donné une variable aléatoire \(X\), on peut définir une mesure induite \(\mathcal{L}_X : \sousens(\setR) \mapsto [0,1]\), qui exprime la probabilité qu'un événement \(\omega \in \Omega\) donne une valeur appartenant à un sous-ensemble \(U \subseteq \setR\) :
</p>

<p>
\[\mathcal{L}_X(U) = \probaof{X^{-1}(U)} = \probaof{ \{ \omega\in\Omega : X(\omega) \in U \} }\]
</p>
</div>


<div id="outline-container-orga3eabdf" class="outline-4">
<h4 id="orga3eabdf"><span class="section-number-4">1.3.1.</span> Variables conjointes</h4>
<div class="outline-text-4" id="text-1-3-1">
<p>
La mesure induite par deux variables aléatoires \(X\) et \(Y\) se définit par :
</p>

<p>
\[\mathcal{L}_{X,Y}(D) = \probaof{ \{ \omega\in\Omega : (X(\omega),Y(\omega)) \in D \} }\]
</p>

<p>
pour tout \(D \subseteq\setR^2\).
</p>

<p>
On voit clairement que :
</p>

<div class="org-center">
<p>
\(
\mathcal{L}_X(U) = \mathcal{L}_{X,Y}(U \times \setR) \\
\mathcal{L}_Y(U) = \mathcal{L}_{X,Y}(\setR \times U)
\)
</p>
</div>
</div>
</div>
</div>


<div id="outline-container-orgb3699aa" class="outline-3">
<h3 id="orgb3699aa"><span class="section-number-3">1.4.</span> Collection induite</h3>
<div class="outline-text-3" id="text-1-4">
<p>
Soit \(X\) une variable aléatoire et \(U \subseteq \setR\). On définit le sous-ensemble de \(\Omega\) :
</p>

<p>
\[\Theta(X,U) = \{ \omega \in \Omega : X(\omega) \in U \}\]
</p>

<p>
ou de manière équivalente en utilisant la relation inverse \(X^{-1}\) :
</p>

<p>
\[\Theta(X,U) = X^{-1}(U)\]
</p>

<p>
La collection \(\Lambda(X)\) induite par \(X\) est un ensemble regroupant les \(\Theta(X,U)\) pour tous les sous-ensembles de \(\setR\) :
</p>

<p>
\[\Lambda(X) = \{ \Theta(X,U) : U \subseteq \setR \}\]
</p>

<p>
Comme :
</p>

<div class="org-center">
<p>
\(
\Theta(X,\emptyset) = \emptyset \\
\Theta(X,\setR) = \Omega
\)
</p>
</div>

<p>
il est clair que l'on a \(\emptyset, \Omega \in \Lambda(X)\) quelle que soit la variable aléatoire \(X\).
</p>
</div>


<div id="outline-container-org94e080a" class="outline-4">
<h4 id="org94e080a"><span class="section-number-4">1.4.1.</span> Fonctions indicatrices</h4>
<div class="outline-text-4" id="text-1-4-1">
<p>
Si \(\Phi \subseteq \Omega\) et \(X = \indicatrice_\Phi\), on a :
</p>

<div class="org-center">
<p>
\(
\Theta(\indicatrice_\Phi, \{1\}) = \{ \omega : \indicatrice_\Phi(\omega) = 1 \} = \Phi \\
\Theta(\indicatrice_\Phi, \{0\}) = \{ \omega : \indicatrice_\Phi(\omega) = 0 \} = \Omega \setminus \Phi
\)
</p>
</div>

<p>
De même, si un ensemble \(U \subseteq \setR\) :
</p>

<ul class="org-ul">
<li>ne contient ni \(1\) ni \(0\), on a \(\Theta(\indicatrice_\Phi,U) = \emptyset\)</li>
<li>contient \(1\) et \(0\), on a \(\Theta(\indicatrice_\Phi,U) = \Omega\)</li>
<li>contient \(1\) et pas \(0\), on a \(\Theta(\indicatrice_\Phi,U) = \Phi\)</li>
<li>contient \(0\) et pas \(1\), on a \(\Theta(\indicatrice_\Phi,U) = \Omega \setminus \Phi\)</li>
</ul>

<p>
On a donc :
</p>

<p>
\[\Lambda(\indicatrice_\Phi) = \{ \emptyset, \Omega, \Phi, \Omega \setminus \Phi \}\]
</p>
</div>
</div>
</div>


<div id="outline-container-org433b760" class="outline-3">
<h3 id="org433b760"><span class="section-number-3">1.5.</span> Espérance</h3>
<div class="outline-text-3" id="text-1-5">
<p>
L'espérance d'une variable aléatoire \(X\) est simplement une moyenne pondérée par les probablités que \(X\)
prennent telle ou telle valeur :
</p>

<p>
\[\esperof{X} = \int_{\Omega} X(\omega) \ d\proba(\omega)\]
</p>
</div>


<div id="outline-container-org821646e" class="outline-4">
<h4 id="org821646e"><span class="section-number-4">1.5.1.</span> Indicatrice</h4>
<div class="outline-text-4" id="text-1-5-1">
<p>
Notons que pour tout \(\Phi \subseteq \Omega\), on a :
</p>

\begin{align}
\esperof{\indicatrice_\Phi} &= \int_\Omega \indicatrice_\Phi \ d\proba \\
&= \int_\Phi \ d\proba
\end{align}

<p>
et donc :
</p>

<p>
\[\esperof{\indicatrice_\Phi} = \probaof{\Phi}\]
</p>
</div>
</div>


<div id="outline-container-org54cbf5f" class="outline-4">
<h4 id="org54cbf5f"><span class="section-number-4">1.5.2.</span> Fonction d'une variable aléatoire</h4>
<div class="outline-text-4" id="text-1-5-2">
<p>
Pour toute fonction \(G : \setR \mapsto \setR\), on a bien évidemment \(G \circ X : \Omega \mapsto \setR\) et on peut définir :
</p>

<p>
\[\esperof{G(X)} = \int_\Omega (G \circ X)(\omega) \ d\proba(\omega)\]
</p>
</div>
</div>


<div id="outline-container-org91f8745" class="outline-4">
<h4 id="org91f8745"><span class="section-number-4">1.5.3.</span> Fonction de plusieurs variables aléatoires</h4>
<div class="outline-text-4" id="text-1-5-3">
<p>
De même, si \(X\) et \(Y\) sont deux variables aléatoires, pour toute fonction \(G : \setR^2 \mapsto \setR\), on a évidemment \(G(X,Y) \in \setR\) et on peut définir :
</p>

<p>
\[\esperof{G(X,Y)} = \int_\Omega G\left(X(\omega),Y(\omega)\right) \ d\proba(\omega)\]
</p>

<p>
Le cas particulier \(G(X,Y) = a \ X + b \ Y\), où \(a,b \in \setR\), nous montre la linéarité de l'espérance, qui découle directement de celle de l'intégrale :
</p>

<p>
\[\esperof{a \ X + b \ Y} = a \ \esperof{X} + b \ \esperof{Y}\]
</p>
</div>
</div>
</div>


<div id="outline-container-orga9f1161" class="outline-3">
<h3 id="orga9f1161"><span class="section-number-3">1.6.</span> Espérance et mesure induite</h3>
<div class="outline-text-3" id="text-1-6">
<p>
Soit une variable aléatoire \(X\) et la fonction étagée \(G : \setR \mapsto \setR\) définie
pour tout \(x \in \setR\) par :
</p>

<p>
\[G(x) = \sum_i g_i \ \indicatrice_{A_i}(x)\]
</p>

<p>
où les \(A_i\) forment une partition de \(\setR\) et où les \(g_i\) sont supposés sans
perte de généralité être des réels distincts. Soit la partition de \(\Omega\) constituée
des ensembles :
</p>

<p>
\[\Omega_i = X^{-1}(A_i) = \{ \omega \in \Omega : X(\omega) \in A_i \}\]
</p>

<p>
On voit que \((G \circ X)(\omega) = g_i\) pour tout \(\omega \in \Omega_i\).
Calculons l'espérance de \(G(X)\) :
</p>

\begin{align}
\esperof{G(X)} &= \int_\Omega (G \circ X)(\omega) \ d\proba(\omega) \\
&= \sum_i \int_{\Omega_i} (G \circ X)(\omega) \ d\proba(\omega) \\
&= \sum_i \int_{\Omega_i} g_i \ d\proba(\omega) \\
&= \sum_i g_i \int_{\Omega_i} \ d\proba(\omega) \\
&= \sum_i g_i \ \probaof{\Omega_i}
\end{align}

<p>
Par définition de la mesure induite, on a :
</p>

<p>
\[\mathcal{L}_X(A_i) = \probaof{X^{-1}(A_i)} = \probaof{\Omega_i}\]
</p>

<p>
L'espérance de \(G(X)\) peut donc s'exprimer comme :
</p>

<p>
\[\esperof{G(X)} = \sum_i g_i \ \mathcal{L}_X(A_i)\]
</p>

<p>
Mais le membre de droite n'est autre que l'intégrale de \(G\) sur \(\setR\)
utilisant la mesure \(\mathcal{L}_X\) :
</p>

<p>
\[\esperof{G(X)} = \int_\setR G(x) \ d\mathcal{L}_X(x)\]
</p>

<p>
Comme cette expression doit être valable pour toute fonction en escalier, on en conclut que :
</p>

<p>
\[\esperof{G(X)} = \int_\setR G(x) \ d\mathcal{L}_X(x)\]
</p>

<p>
pour toute fonction intégrable \(G\).
</p>
</div>


<div id="outline-container-orgb0a0f61" class="outline-4">
<h4 id="orgb0a0f61"><span class="section-number-4">1.6.1.</span> Identité</h4>
<div class="outline-text-4" id="text-1-6-1">
<p>
Le cas particulier \(G = \identite\) nous donne :
</p>

<p>
\[\esperof{X} = \int_\setR x \ d\mathcal{L}_X(x)\]
</p>
</div>
</div>


<div id="outline-container-orgcaf44c8" class="outline-4">
<h4 id="orgcaf44c8"><span class="section-number-4">1.6.2.</span> Densité</h4>
<div class="outline-text-4" id="text-1-6-2">
<p>
Si il existe une fonction \(f_X : \setR \mapsto \setR\) telle que \(d\mathcal{L}_X = f_X \ dx\),
où \(dx\) correspond à la mesure de Lebesgue sur \(\setR\), on a :
</p>

<p>
\[\esperof{G(X)} = \int_\setR G(x) \ f_X(x) \ dx\]
</p>

<p>
ainsi que :
</p>

<p>
\[\esperof{X} = \int_\setR x \ f_X(x) \ dx\]
</p>

<p>
On nomme cette fonction \(f_X\) la densité de la variable aléatoire \(X\).
</p>

<p>
Remarquons que \(f_X\) est positive par positivité de la mesure. Comme :
</p>

<p>
\[\esperof{1} = 1\]
</p>

<p>
on obtient la propriété de normalité :
</p>

<p>
\[\int_\setR f_X(x) \ dx = 1\]
</p>
</div>


<div id="outline-container-org9464f33" class="outline-5">
<h5 id="org9464f33"><span class="section-number-5">1.6.2.1.</span> Variable aléatoire gaussienne</h5>
<div class="outline-text-5" id="text-1-6-2-1">
<p>
Une variable aléatoire est dite normale de paramètres \(\mu\), \(\sigma\) si sa fonction
densité vérifie :
</p>

<p>
\[f_{X}(x) = \frac{1}{ \sigma\sqrt{2 \pi} } \exp\left(-\frac{(x-\mu)^2}{2 \sigma^2}\right)\]
</p>
</div>
</div>
</div>


<div id="outline-container-org6ca93a8" class="outline-4">
<h4 id="org6ca93a8"><span class="section-number-4">1.6.3.</span> Variables conjointes</h4>
<div class="outline-text-4" id="text-1-6-3">
<p>
Soit les variables aléatoires \(X, Y\) et la fonction étagée \(G : \setR^2 \mapsto \setR\) définie
pour tout \(x, y \in \setR\) par :
</p>

<p>
\[G(x,y) = \sum_i g_i \ \indicatrice_{A_i}(x,y)\]
</p>

<p>
où les \(A_i\) forment une partition de \(\setR^2\) et où les \(g_i\) sont supposés sans
perte de généralité être des réels distincts. Soit la partition de \(\Omega\) constituée
des ensembles :
</p>

<p>
\[\Omega_i = \{ \omega \in \Omega : (X(\omega), Y(\omega)) \in A_i \}\]
</p>

<p>
On voit que \(G(X(\omega), Y(\omega)) = g_i\) pour tout \(\omega \in \Omega_i\).
Calculons l'espérance de \(G(X,Y)\) :
</p>

\begin{align}
\esperof{G(X,Y)} &= \int_\Omega G(X(\omega), Y(\omega)) \ d\proba(\omega) \\
&= \sum_i \int_{\Omega_i} G(X(\omega), Y(\omega)) \ d\proba(\omega) \\
&= \sum_i \int_{\Omega_i} g_i \ d\proba(\omega) \\
&= \sum_i g_i \int_{\Omega_i} \ d\proba(\omega) \\
&= \sum_i g_i \ \probaof{\Omega_i}
\end{align}

<p>
Par définition de la mesure induite, on a :
</p>

<p>
\[\mathcal{L}_{X,Y}(A_i) = \probaof{\Omega_i}\]
</p>

<p>
L'espérance de \(G(X)\) peut donc s'exprimer comme :
</p>

<p>
\[\esperof{G(X,Y)} = \sum_i g_i \ \mathcal{L}_{X,Y}(A_i)\]
</p>

<p>
Mais le membre de droite n'est autre que l'intégrale de \(G\) sur \(\setR^2\)
utilisant la mesure \(\mathcal{L}_{X,Y}\) :
</p>

<p>
\[\esperof{G(X,Y)} = \int_{\setR^2} G(x,y) \ d\mathcal{L}_{X,Y}(x,y)\]
</p>

<p>
Comme cette expression doit être valable pour toute fonction en escalier, on en conclut que :
</p>

<p>
\[\esperof{G(X,Y)} = \int_{\setR^2} G(x,y) \ d\mathcal{L}_{X,Y}(x,y)\]
</p>

<p>
pour toute fonction intégrable \(G\).
</p>
</div>
</div>


<div id="outline-container-orgb33e9f3" class="outline-4">
<h4 id="orgb33e9f3"><span class="section-number-4">1.6.4.</span> Densité conjointe</h4>
<div class="outline-text-4" id="text-1-6-4">
<p>
Si il existe une fonction \(f_{X,Y} : \setR^2 \mapsto \setR\) telle que
\(d\mathcal{L}_{X,Y} = f_{X,Y} \ dx \ dy\), où \(dx \ dy\) correspond à la mesure de Lebesgue
sur \(\setR^2\), on a :
</p>

<p>
\[\esperof{G(X,Y)} = \int_{\setR^2} G(x,y) \ f_{X,Y}(x,y) \ dx \ dy\]
</p>

<p>
En considérant le cas particulier \(G(X,Y) = X\), on obtient :
</p>

\begin{align}
\esperof{X} &= \int_{\setR^2} x \ f_{X,Y}(x,y) \ dx \ dy \\
&= \int_\setR x \ \left[\int_\setR f_{X,Y}(x,y) \ dy\right] \ dx
\end{align}

<p>
En définissant la fonction associée \(f_X\) par :
</p>

<p>
\[f_X(x) = \int_\setR f_{X,Y}(x,y) \ dy\]
</p>

<p>
on peut dès lors écrire l'espérance de \(X\) comme :
</p>

<p>
\[\esperof{X} = \int_\setR x \ f_X(x) \ dx\]
</p>

<p>
En suivant le même déroulement pour \(\esperof{Y}\), et en définissant :
</p>

<p>
\[f_Y(y) = \int_\setR f_{X,Y}(x,y) \ dx\]
</p>

<p>
on peut écrire l'espérance de \(Y\) comme :
</p>

<p>
\[\esperof{Y} = \int_\setR y \ f_Y(y) \ dy\]
</p>
</div>


<div id="outline-container-org83e4266" class="outline-5">
<h5 id="org83e4266"><span class="section-number-5">1.6.4.1.</span> Distribution normale</h5>
<div class="outline-text-5" id="text-1-6-4-1">
<p>
On dit que les variables aléatoires \(X_1, ..., X_N\) présentent une distribution normale multivariée si il existe :
</p>

<div class="org-center">
<p>
\(
\mu = \left( \mu_i \right)_i \\
\Theta = \left( \sigma_{ij} \right)_{i,j}
\)
</p>
</div>

<p>
tels que la fonction densité associée à \(X = (X_1, ..., X_N)^T\) s'écrive :
</p>

<p>
\[\f_X(x) = \unsur{2 \pi^{n/2} \det{A}} \exp\left(-\unsur{2} (x-\mu)^T \cdot \Theta^{-1} \cdot (x-\mu) \right)\]
</p>

<p>
pour tout \(x \in \setR^N\). On a alors :
</p>

<div class="org-center">
<p>
\(
\esperof{X_i} = \mu_i \\
\cov{X_i}{X_j} = \sigma_{ij}
\)
</p>
</div>

<p>
On a aussi la fonction génératrice :
</p>

<p>
\[\Psi_X(u) = \exp\left(u^T \cdot \mu + \unsur{2} u^T \cdot \Theta^{-1} \cdot u\right)\]
</p>

<p>
pour tout \(u \in \setR^N\).
</p>
</div>
</div>
</div>
</div>

<div id="outline-container-orgd14adf0" class="outline-3">
<h3 id="orgd14adf0"><span class="section-number-3">1.7.</span> Fonction génératrice des moments</h3>
<div class="outline-text-3" id="text-1-7">
<p>
On définit le moment générateur d'une densité par :
</p>

<p>
\[\Psi_X(u) = \esperof{\exp(X \cdot u)}\]
</p>

<p>
L'intérêt de cette fonction est qu'elle permet de calculer facilement
les espérances des puissances naturelles de \(X\). En effet :
</p>

<p>
\[\frac{d^k \Psi_X}{du^k}(u) = \esperof{X^k \ \exp(X \cdot u)}\]
</p>

<p>
et donc :
</p>

<p>
\[\OD{\Psi}{u}(0) = \esperof{X^k \ \exp(0)} = \esperof{X^k}\]
</p>
</div>


<div id="outline-container-orgab226c1" class="outline-4">
<h4 id="orgab226c1"><span class="section-number-4">1.7.1.</span> Variable gaussienne</h4>
<div class="outline-text-4" id="text-1-7-1">
<p>
A titre d'exemple, nous calculons le  moment générateur associé à une densité gaussienne :
</p>

<p>
\[\Psi(u) = \unsur{\sqrt{2 \pi} \sigma} \int_\setR \exp(x u) \exp\left(-\frac{(x-\mu)^2}{2 \sigma^2}\right) dx\]
</p>

<p>
On obtient en développant :
</p>

\begin{align}
\Psi(u) &= \unsur{\sqrt{2 \pi} \sigma} \int_\setR \exp\left(x u - \frac{(x-\mu)^2}{2 \sigma^2}\right) dx \\
&= \unsur{\sqrt{2 \pi} \sigma} \exp(\mu u + \unsur{2} u^2 \sigma^2) \int_\setR \exp\left(- \frac{(x-(\mu + u \sigma^2) )^2}{2 \sigma^2}\right) dx
\end{align}

<p>
Comme l'intégrale vaut \(\sqrt{2 \pi} \sigma\), on obtient finalement :
</p>

<p>
\[\Psi(u) = \exp(u \mu + \unsur{2} u^2 \sigma^2)\]
</p>
</div>
</div>
</div>


<div id="outline-container-orgae7350d" class="outline-3">
<h3 id="orgae7350d"><span class="section-number-3">1.8.</span> Variance</h3>
<div class="outline-text-3" id="text-1-8">
<p>
La variance de \(X\) est la variation carrée moyenne de \(X\) autour de son espérance \(\esperof{X}\) :
</p>

<p>
\[\var{X} = \esperof{\left(X-\esperof{X}\right)^2}\]
</p>

<p>
Comme la variable \(Z = \left(X-\esperof{X}\right)^2\) est positive, son espérance doit également etre positive et \(\var{X} \ge 0\).
</p>

<p>
En développant la définition et en utilisant la linéarité de l'espérance, on obtient :
</p>

\begin{align}
\var{X} &= \esperof{X^2 - 2 \ X \cdot \esperof{X} + \esperof{X}^2} \\
&= \esperof{X^2} - 2 \ \esperof{X} \cdot \esperof{X} + \esperof{X}^2 \cdot \esperof{1} \\
&= \esperof{X^2} - 2 \ \esperof{X}^2 + \esperof{X}^2
\end{align}

<p>
soit :
</p>

<p>
\[\var{X} = \esperof{X^2} - \esperof{X}^2\]
</p>
</div>


<div id="outline-container-orgfc7157a" class="outline-4">
<h4 id="orgfc7157a"><span class="section-number-4">1.8.1.</span> Invariance sous translation</h4>
<div class="outline-text-4" id="text-1-8-1">
<p>
Notons que si \(X,Y\) sont deux variables aléatoires reliées par :
</p>

<p>
\[Y = X + a\]
</p>

<p>
où \(a \in \setR\), on a :
</p>

\begin{align}
\var{Y} &= \esperof{\left(Y-\esperof{Y}\right)^2} \\
&= \esperof{\left(X + a -\esperof{X+a}\right)^2} \\
&= \esperof{\left(X + a -\esperof{X} - a\right)^2} \\
&= \esperof{\left(X -\esperof{X}\right)^2} \\
&= \var{X}
\end{align}

<p>
La variance est donc invariante sous translation :
</p>

<p>
\[\var{X+a} = \var{X}\]
</p>
</div>
</div>
</div>


<div id="outline-container-org14761a8" class="outline-3">
<h3 id="org14761a8"><span class="section-number-3">1.9.</span> Covariance</h3>
<div class="outline-text-3" id="text-1-9">
<p>
La covariance de deux variables aléatoire \(X,Y\) se définit par :
</p>

<p>
\[\cov{X}{Y} = \esperof{(X-\esperof{X}) \cdot (Y-\esperof{Y})}\]
</p>

<p>
En développant et en utilisant la linéarité de l'espérance, on obtient :
</p>

\begin{align}
\cov{X}{Y} &= \esperof{X \cdot Y} - \esperof{X} \cdot \esperof{Y} - \esperof{Y} \cdot \esperof{X} + \esperof{X} \cdot \esperof{Y} \\
&= \esperof{X \cdot Y} - \esperof{X} \cdot \esperof{Y}
\end{align}

<p>
On voit également que la variance d'une variable aléatoire \(X\) n'est rien d'autre que sa covariance avec elle-même :
</p>

<p>
\[\var{X} = \cov{X}{X}\]
</p>
</div>


<div id="outline-container-org08734a2" class="outline-4">
<h4 id="org08734a2"><span class="section-number-4">1.9.1.</span> Invariance sous translation</h4>
<div class="outline-text-4" id="text-1-9-1">
<p>
Suivant le même raisonnement que pour la variance, on considère les variables aléatoires \(W,X,Y,Z\) reliées par :
</p>

<div class="org-center">
<p>
\(
W = X + a \\
Z = Y + b
\)
</p>
</div>

<p>
où \(a,b \in \setR\). La covariance entre \(W\) et \(Z\) s'exprime alors :
</p>

\begin{align}
\cov{W}{Z} &= \esperof{(W - \esperof{W})(Z - \esperof{Z})} \\
&= \esperof{(X + a - \esperof{X} - a)(Y + b - \esperof{Y} - b)} \\
&= \esperof{(X - \esperof{X})(Y - \esperof{Y})} \\
&= \cov{X}{Y}
\end{align}

<p>
La covariance est donc invariante sous translation :
</p>

<p>
\[\cov{X+a}{Y+b} = \cov{X}{Y}\]
</p>
</div>
</div>
</div>


<div id="outline-container-org3bb4640" class="outline-3">
<h3 id="org3bb4640"><span class="section-number-3">1.10.</span> Variance d'une combinaison linéaire</h3>
<div class="outline-text-3" id="text-1-10">
<p>
Nous utilisons la notation :
</p>

<p>
\[X_0 = X - \esperof{X}\]
</p>

<p>
pour toute variable aléatoire \(X\). Cette variables aléatoire \(X_0\) a la propriété
d'avoir une espérance nulle car :
</p>

<p>
\[\esperof{X_0} = \esperof{X - \esperof{X} } = \esperof{X} - \esperof{X} = 0\]
</p>

<p>
La variance d'une telle variable peut s'écrire :
</p>

<p>
\[\var{X_0} = \esperof{X_0^2} - \esperof{X_0}^2 = \esperof{X_0^2}\]
</p>

<p>
Quant à la covariance, elle s'écrit :
</p>

<p>
\[\cov{X_0}{Y_0} = \esperof{X_0 \ Y_0} - \esperof{X_0} \ \esperof{Y_0} = \esperof{X_0 \ Y_0}\]
</p>

<p>
Soit les réels \(a,b\). Par linéarité de l'espérance, on a :
</p>

<p>
\[\esperof{a \ X + b \ Y} = a \ \esperof{X} + b \ \esperof{Y}\]
</p>

<p>
La variance de la combinaison linéaire \(a \ X + b \ Y\) s'écrit :
</p>

\begin{align}
\var{a \ X + b \ Y} &= \esperof{(a \ X + b \ Y - \esperof{a \ X + b \ Y})^2} \\
&= \esperof{(a \ X + b \ Y - a \ \esperof{X} - b \ \esperof{Y})^2} \\
&= \esperof{(a \ X_0 + b \ Y_0)^2}
\end{align}

<p>
En développant, on arrive à :
</p>

\begin{align}
\var{a \ X + b \ Y} &= \esperof{a^2 \ X_0^2 + 2 \ a \ b \ X_0 \ Y_0 + b^2 \ Y_0^2} \\
&= a^2 \ \esperof{X_0^2} + 2 \ a \ b \ \esperof{X_0 \ Y_0} + b^2 \ \esperof{Y_0^2}
\end{align}

<p>
et donc :
</p>

<p>
\[\var{a \ X + b \ Y} = a^2 \ \var{X_0} + 2 \ a \ b \ \cov{X_0}{Y_0} + b^2 \ \var{Y_0}\]
</p>

<p>
L'invariance sous translation nous permet alors d'écrire :
</p>

<p>
\[\var{a \ X + b \ Y} = a^2 \ \var{X} + 2 \ a \ b \ \cov{X}{Y} + b^2 \ \var{Y}\]
</p>
</div>
</div>


<div id="outline-container-orgf6546d6" class="outline-3">
<h3 id="orgf6546d6"><span class="section-number-3">1.11.</span> Produit scalaire</h3>
<div class="outline-text-3" id="text-1-11">
<p>
Nous allons voir que la covariance est un produit scalaire. Nous utilisons la notation :
</p>

<p>
\[X_0 = X - \esperof{X}\]
</p>

<p>
pour toute variable aléatoire \(X\). Cette variables aléatoire \(X_0\) a la propriété
d'avoir une espérance nulle car :
</p>

<p>
\[\esperof{X_0} = \esperof{X - \esperof{X} } = \esperof{X} - \esperof{X} = 0\]
</p>

<p>
On en déduit que :
</p>

<p>
\[\cov{X_0}{Y_0} = \esperof{X_0 \ Y_0} - \esperof{X_0} \ \esperof{Y_0} = \esperof{X_0 \ Y_0}\]
</p>

<p>
La symétrie est vérifiée :
</p>

<p>
\[\cov{Y_0}{X_0} = \esperof{Y_0 \cdot X_0} = \esperof{X_0 \cdot Y_0} = \cov{X_0}{Y_0}\]
</p>

<p>
En ce qui concerne le caractère défini positif, on a :
</p>

<p>
\[\cov{X_0}{X_0} = \esperof{X_0^2} \ge 0\]
</p>

<p>
De plus, si \(X_0\) est tel que \(\cov{X_0}{X_0} = 0\), on a :
</p>

<p>
\[\int_\Omega X_0^2 \ d\proba(\omega) = 0\]
</p>

<p>
ce qui entraîne la nullité essentielle \(X_0 \essegal 0\) sur \(\Omega\).
</p>

<p>
Soit les réels \(a,b\). On voit que la linéarité est bien respectée :
</p>

\begin{align}
\cov{X_0}{a \ Y_0 + b \ Z_0} &= \esperof{X_0 \ (a \ Y_0 + b \ Z_0)} \\
&= a \ \esperof{X_0 \ Y_0} + b \ \esperof{X_0 \ Z_0} \\
&= a \ \cov{X_0}{Y_0} + b \ \cov{X_0}{Z_0}
\end{align}

<p>
Nous venons de montrer que la covariance est essentiellement un produit scalaire
pour toute variable aléatoires à espérance nulles \(X_0, Y_0\). Comme la covariance
est invariante sous translation, on voit que :
</p>

<p>
\[\cov{X}{Y} = \cov{X_0}{Y_0}\]
</p>

<p>
est également un produit scalaire pour toutes variables aléatoires \(X,Y\).
</p>
</div>


<div id="outline-container-orgdf6160b" class="outline-4">
<h4 id="orgdf6160b"><span class="section-number-4">1.11.1.</span> Cauchy-Schwartz</h4>
<div class="outline-text-4" id="text-1-11-1">
<p>
En appliquant l'inégalité de Cauchy-Schwartz à ce produit scalaire, on obtient :
</p>

<p>
\[\cov{X}{Y}^2 \le \cov{X}{X} \ \cov{Y}{Y} = \var{X} \ \var{Y}\]
</p>

<p>
où, en prenant la racine :
</p>

<p>
\[\cov{X}{Y} \le \sqrt{\var{X} \ \var{Y}}\]
</p>
</div>
</div>
</div>


<div id="outline-container-org0e6e307" class="outline-3">
<h3 id="org0e6e307"><span class="section-number-3">1.12.</span> Probabilité conditionnelle</h3>
<div class="outline-text-3" id="text-1-12">
<p>
\label{sec:proba_cond}
</p>

<p>
On définit une nouvelle famille de probabilités :
</p>

<p>
\[\probaof{A | B} = \frac{ \probaof{A \cap B} }{ \probaof{B} }\]
</p>

<p>
où \(A,B\) sont des sous-ensembles quelconque de \(\Omega\), et où \(B\) est tel que :
</p>

<p>
\[\probaof{B} > 0\]
</p>

<p>
Comme \(B \cap B = B\), on a :
</p>

<p>
\[\probaof{ B | B } = 1\]
</p>

<p>
On est donc certain qu'un événement de \(B\) va se produire. En fait, pour tout ensemble \(C\) tel que \(B \subseteq C\), on a \(C \cap B = B\) et :
</p>

<p>
\[\probaof{ C | B } = 1\]
</p>

<p>
On déduit de l'inégalité :
</p>

<p>
\[\probaof{A \cap B} \le \probaof{B}\]
</p>

<p>
que :
</p>

<p>
\[\probaof{A | B} \le 1\]
</p>

<p>
D'un autre coté, comme \(\probaof{B} \le 1\), on a :
</p>

<p>
\[\probaof{A | B} \ge \probaof{A \cap B} \ge 0\]
</p>

<p>
L'additivité est également satisfaite :
</p>

\begin{align}
\probaof{ \cup_i A_i | B} &= \frac{ \probaof{(\cup_i A_i) \cap B} }{ \probaof{B} } \\
&= \frac{ \probaof{\cup_i (A_i \cap B)} }{ \probaof{B} } \\
&= \sum_i \frac{ \probaof{A_i \cap B} }{ \probaof{B} } = \sum_i \probaof{ A_i | B}
\end{align}

<p>
pour toute famille de \(A_i\) disjoints deux à deux. Les fonctions :
</p>

<p>
\[\proba_B\left[ A \right] = \probaof{A | B}\]
</p>

<p>
forment donc bien une famille de probabilités. On dit que \(\probaof{A | B}\) est la probabilité conditionnelle de \(A\) sachant \(B\).
</p>

<p>
Lorsque \(B = \Omega\), on retrouve d'ailleurs :
</p>

<p>
\[\probaof{A | \Omega} = \probaof{A}\]
</p>
</div>


<div id="outline-container-orge36a0b0" class="outline-4">
<h4 id="orge36a0b0"><span class="section-number-4">1.12.1.</span> Indépendance</h4>
<div class="outline-text-4" id="text-1-12-1">
<p>
On dit que deux ensembles d'événements \(A\) et \(B\) sont indépendants si :
</p>

<p>
\[\probaof{A | B} = \probaof{A}\]
</p>

<p>
c'est-à-dire si :
</p>

<p>
\[\probaof{A \cap B} = \probaof{A} \cdot \probaof{B}\]
</p>
</div>
</div>


<div id="outline-container-org2575c2c" class="outline-4">
<h4 id="org2575c2c"><span class="section-number-4">1.12.2.</span> Application</h4>
<div class="outline-text-4" id="text-1-12-2">
<p>
Une technique fréquemment employée pour évaluer \(\probaof{A}\) est d'utiliser
une partition \(B_1,...,B_n\) de \(\Omega\). Utilisant \(A = A \cup \Omega\), on a alors :
</p>

<p>
\[\probaof{A} = \sum_i \probaof{A \cap B_i} = \sum_i \probaof{A | B_i} \cdot \probaof{B_i}\]
</p>
</div>
</div>
</div>


<div id="outline-container-org88473dd" class="outline-3">
<h3 id="org88473dd"><span class="section-number-3">1.13.</span> Espérance conditionnelle à un ensemble</h3>
<div class="outline-text-3" id="text-1-13">
<p>
Soit \(A \subseteq \Omega\). On a vu que :
</p>

<p>
\[\esperof{\indicatrice_A} = \probaof{A}\]
</p>

<p>
pour toute fonction indicatrice d'un sous-ensemble \(A\) de \(\Omega\). Par analogie, on aimerait bien obtenir une expression d'une espérance conditionnelle vérifiant :
</p>

<p>
\[\esperof{\indicatrice_A | B} = \probaof{A | B}\]
</p>

<p>
pour un ensemble \(B \subseteq \Omega\) donné vérifiant \(\probaof{B} > 0\).
</p>

<p>
Soit \(\Omega_1, ..., \Omega_N\) une partition de \(\Omega\) et \(Z\) une variable aléatoire en escalier :
</p>

<p>
\[Z(\omega) = \sum_i Z_i \ \indicatrice_{\Omega_i}(\omega)\]
</p>

<p>
On voit que :
</p>

\begin{align}
\esperof{Z | B} &= \sum_i Z_i\ \esperof{\indicatrice_{\Omega_i} | B} \\
&= \sum_i Z_i\ \probaof{\Omega_i | B}
\end{align}

<p>
Or :
</p>

<p>
\[\probaof{\Omega_i | B} = \frac{ \probaof{\Omega_i \cap B} }{ \probaof{B} }\]
</p>

<p>
On a donc :
</p>

<p>
\[\esperof{Z | B} = \unsur{ \probaof{B} } \sum_i Z_i\ \probaof{\Omega_i \cap B}\]
</p>

<p>
Considérons la nouvelle partition :
</p>

<div class="org-center">
<p>
\(
\Phi_i^+ = \Omega_i \cap B \\
\Phi_i^- = \Omega_i \cap (\Omega \setminus B)
\)
</p>
</div>

<p>
Comme \(\Phi_i^+ \cup \Phi_i^- = \Omega_i\), on a clairement \(\indicatrice_{\Phi_i^+} + \indicatrice_{\Phi_i^-} = \indicatrice_{\Omega_i}\) et on peut réexprimer \(Z\) comme :
</p>

<p>
\[Z(\omega) = \sum_i Z_i\ \indicatrice_{\Phi_i^+}(\omega) + \sum_i Z_i\ \indicatrice_{\Phi_i^-}(\omega)\]
</p>

<p>
L'expression de l'espérance conditionelle devient :
</p>

<p>
\[\esperof{Z | B} = \unsur{ \probaof{B} } \left[ \sum_i Z_i\ \probaof{\Phi_i^+ \cap B} + \sum_i Z_i\ \probaof{\Phi_i^- \cap B} \right]\]
</p>

<p>
Remarquons que par construction :
</p>

<div class="org-center">
<p>
\(
\Phi_i^+ \cap B = \Phi_i^+ \\
\Phi_i^- \cap B = \emptyset
\)
</p>
</div>

<p>
Par conséquent, les termes en \(\probaof{\Phi_i^- \cap B}\) s'annulent et on a :
</p>

<p>
\[\esperof{Z | B} = \unsur{ \probaof{B} } \sum_i Z_i\ \probaof{\Phi_i^+}\]
</p>

<p>
Mais comme \(\bigcup_i \Phi_i^+ = B\), les \(\Phi_i^+\) forment une partition de \(B\) et on peut écrire cette expression sous la forme intégrale :
</p>

<p>
\[\esperof{Z | B} = \frac{ \int_B Z\ \ d\proba }{ \int_B \ d\proba }\]
</p>

<p>
Comme cette relation doit être valable pour toute variable aléatoire en escalier \(Z\), elle l'est également pour une variable aléatoire quelconque \(X\) :
</p>

<p>
\[\esperof{X | B} = \frac{ \int_B X\ \ d\proba }{ \int_B \ d\proba }\]
</p>
</div>


<div id="outline-container-org52365cb" class="outline-4">
<h4 id="org52365cb"><span class="section-number-4">1.13.1.</span> Densité conditionnelle</h4>
<div class="outline-text-4" id="text-1-13-1">
<p>
Soient \(X,Y\) deux variables aléatoires. Un cas particulier important d'espérance conditionnelle est celui où :
</p>

<p>
\[B_y = \{ \omega : Y(\omega) = y \}\]
</p>

<p>
On note alors :
</p>

<p>
\[\esperof{X | Y = y} = \esperof{X | B_y}\]
</p>

<p>
On remarque que :
</p>

<p>
\[(X,Y)(B_y) = \{ (x,y) \in \setR^2 : x \in \setR \}\]
</p>

<p>
Par conséquent, si il existe une fonction densité \(f_{X,Y}\) associée à \(X,Y\), on peut écrire :
</p>

\begin{align}
\int_{B_y} X \ d\proba &= \int_{(X,Y)(B_y)} x \ f_{X,Y}(x,y) \ dx \ dy \\
&= \int_\setR x \ f_{X,Y}(x,y) \ dx
\end{align}

<p>
ainsi que :
</p>

<p>
\[\int_{B_y} \ d\proba = \int_\setR f_{X,Y}(x,y) \ dx\]
</p>

<p>
L'espérance conditionnelle s'écrit alors :
</p>

<p>
\[\esperof{X | Y = y} = \frac{\int_\setR x \ f_{X,Y}(x,y) \ dx}{\int_\setR f_{X,Y}(x,y) \ dx}\]
</p>

<p>
Donc, si on définit :
</p>

<p>
\[f_{X | Y}(x,y) = \frac{f_{X,Y}(x,y)}{ \int_\setR f_{X,Y}(x,y) \ dx}\]
</p>

<p>
on a tout simplement :
</p>

<p>
\[\esperof{X | Y = y} = \int_\setR x \ f_{X | Y}(x,y) \ dx\]
</p>
</div>
</div>
</div>


<div id="outline-container-org2b5ad9e" class="outline-3">
<h3 id="org2b5ad9e"><span class="section-number-3">1.14.</span> Espérance conditionnelle à une tribu</h3>
<div class="outline-text-3" id="text-1-14">
</div>
<div id="outline-container-orgc2dd239" class="outline-4">
<h4 id="orgc2dd239"><span class="section-number-4">1.14.1.</span> Tribu et espace fonctionnel</h4>
<div class="outline-text-4" id="text-1-14-1">
<p>
Soit \(\Gamma \subseteq \sousens(\Omega)\) une collection de sous-ensembles de \(\Omega\) formant une tribu sur \(\Omega\) (voir section \ref{sec:tribu}),
et \(\mathcal{F}(\Gamma)\) l'ensemble des variables aléatoires \(W\) telles que :
</p>

<p>
\[\Lambda(W) \subseteq \Gamma\]
</p>

<p>
où \(\Lambda(W)\) est la collection induite par \(W\).
</p>
</div>
</div>


<div id="outline-container-orga82263b" class="outline-4">
<h4 id="orga82263b"><span class="section-number-4">1.14.2.</span> Minimisation</h4>
<div class="outline-text-4" id="text-1-14-2">
<p>
L'espérance conditionnelle est construite comme le meilleur estimateur au sens des moindres carrés d'une variable aléatoire \(X\) sur \(\mathcal{F}(\Gamma)\). Soit la fonctionnelle \(I : \mathcal{F}(\Gamma) \mapsto \setR\) représentant l'erreur :
</p>

<p>
\[I(Z) = \int_\Omega \left[ Z(\omega) - X(\omega) \right]^2 \ d\proba(\omega)\]
</p>

<p>
Nous allons minimiser \(I\) sur \(\mathcal{F}(\Gamma)\). Pour ce faire, on utilise la technique du calcul variationnel (voir chapitre \ref{chap:varia}). On commence par définir :
</p>

<p>
\[J_W(\epsilon) = I(Z^* + \epsilon W) = \int_\Omega (Z^* + \epsilon W - X)^2 \ d\proba\]
</p>

<p>
où la variable aléatoire \(Z^*\) est l'optimum recherché, et où \(W \in \mathcal{F}(\Gamma)\), \(\epsilon \in \setR\). La dérivée s'écrit :
</p>

<p>
\[\OD{J_W}{\epsilon}(\epsilon) = \int_\Omega 2 (Z^* +\epsilon W - X) W \ d\proba = 0\]
</p>

<p>
Comme celle-ci doit s'annuler en \(\epsilon = 0\), on a :
</p>

<p>
\[\OD{J_W}{\epsilon}(0) = \int_\Omega 2 (Z^* - X) W \ d\proba = 0\]
</p>

<p>
Autrement dit :
</p>

<p>
\[\int_\Omega W Z^* \ d\proba = \int_\Omega W X \ d\proba\]
</p>

<p>
équation qui doit être vérifiée pour tout \(W \in \mathcal{F}(\Gamma)\).
</p>
</div>
</div>


<div id="outline-container-org6af3d55" class="outline-4">
<h4 id="org6af3d55"><span class="section-number-4">1.14.3.</span> Unicité</h4>
<div class="outline-text-4" id="text-1-14-3">
<p>
Nous supposons dorénavant que \(\mathcal{F}(\Gamma)\) est un espace vectoriel. Soient \(Z_1, Z_2 \in \mathcal{F}(\Gamma)\) des variables aléatoires qui minimisent tous deux la fonctionnelle \(I\). On a :
</p>

<p>
\[\int_\Omega W Z_1 \ d\proba = \int_\Omega W Z_2 \ d\proba = \int_\Omega W X \ d\proba\]
</p>

<p>
pour tout \(W \in \mathcal{F}(\Gamma)\). Donc :
</p>

<p>
\[\int_\Omega W (Z_1 - Z_2) \ d\proba = 0\]
</p>

<p>
Mais comme \(Z_1 - Z_2 \in \mathcal{F}(\Gamma)\), il suffit de considérer le cas \(W = Z_1 - Z_2\) pour avoir :
</p>

<p>
\[\int_\Omega (Z_1 - Z_2)^2 \ d\proba = 0\]
</p>

<p>
On en conclut que \(Z_1 = Z_2\) presque partout sur \(\Omega\). L'espérance conditionnelle est donc unique pour \(X\) et \(\Gamma\) donnés.
</p>
</div>
</div>


<div id="outline-container-org48dc047" class="outline-4">
<h4 id="org48dc047"><span class="section-number-4">1.14.4.</span> Définition</h4>
<div class="outline-text-4" id="text-1-14-4">
<p>
Forts de ces résultats, on définit l'espérance de \(X\) conditionnellement à la tribu \(\Gamma\) comme étant :
</p>

<p>
\[\esperof{X | \Gamma} = \arg\min_{Z \in \mathcal{F}(\Gamma) } \int_{\Omega} \left[ Z - X \right]^2 \ d\proba\]
</p>

<p>
On a donc :
</p>

<p>
\[\int_\Omega W\ \esperof{X | \Gamma} \ d\proba = \int_\Omega W\ X \ d\proba\]
</p>

<p>
pour tout \(W \in \mathcal{F}(\Gamma)\).
</p>
</div>
</div>


<div id="outline-container-org5abd78b" class="outline-4">
<h4 id="org5abd78b"><span class="section-number-4">1.14.5.</span> Fonctions indicatrices</h4>
<div class="outline-text-4" id="text-1-14-5">
<p>
Soit un ensemble \(\Phi \in \Gamma\). Les propriétés de \(\Gamma\) nous disent que \(\Omega \setminus \Phi \in \Gamma\). Donc :
</p>

<p>
\[\Lambda(\indicatrice_\Phi) = \{ \emptyset, \Omega, \Phi, \Omega \setminus \Phi \} \subseteq \Gamma\]
</p>

<p>
et \(\indicatrice_\Phi \in \mathcal{F}(\Gamma)\). On en déduit que :
</p>

<p>
\[\int_\Omega \indicatrice_\Phi \ \esperof{X | \Gamma} \ d\proba = \int_\Omega \indicatrice_\Phi \ X \ d\proba\]
</p>

<p>
c'est-à-dire :
</p>

<p>
\[\int_\Phi \esperof{X | \Gamma} \ d\proba = \int_\Phi X \ d\proba\]
</p>

<p>
pour tout \(\Phi \in \Gamma\).
</p>

<p>
Comme \(\Omega \in \Gamma\), on a en particulier :
</p>

<p>
\[\int_\Omega \esperof{X | \Gamma} \ d\proba = \int_\Omega X \ d\proba\]
</p>

<p>
c'est-à-dire :
</p>

<p>
\[\esperof{ \esperof{X | \Gamma} } = \esperof{X}\]
</p>
</div>
</div>


<div id="outline-container-org084ab82" class="outline-4">
<h4 id="org084ab82"><span class="section-number-4">1.14.6.</span> Variable aléatoire dans l'espace fonctionnel</h4>
<div class="outline-text-4" id="text-1-14-6">
<p>
Une conséquence directe de la définition de l'espérance conditionnelle est que si \(Z \in \mathcal{F}(\Gamma)\), on a :
</p>

<p>
\[\int_\Omega (Z - Z)^2 \ d\proba = 0\]
</p>

<p>
Par conséquent, \(Z\) minimise la fonctionnelle :
</p>

<p>
\[I(Y) = \int_\Omega (Y - Z)^2 \ d\proba \ge 0\]
</p>

<p>
sur \(\mathcal{F}(\Gamma)\) et :
</p>

<p>
\[\esperof{Z | \Gamma} = Z\]
</p>
</div>
</div>


<div id="outline-container-orga036b4e" class="outline-4">
<h4 id="orga036b4e"><span class="section-number-4">1.14.7.</span> Tour</h4>
<div class="outline-text-4" id="text-1-14-7">
<p>
Soit la tribu \(\Delta \subseteq \Gamma\) et \(X\) une variable aléatoire et \(W \in \mathcal{F}(\Delta)\). On a :
</p>

<p>
\[\Lambda(W) \subseteq \Delta \subseteq \Gamma\]
</p>

<p>
Par conséquent \(W \in \mathcal{F}(\Gamma)\) et les équations suivantes sont vérifiées :
</p>

<div class="org-center">
<p>
\(
\int_\Omega W\ \esperof{X | \Delta} \ d\proba = \int_\Omega W\ X \ d\proba \\
\int_\Omega W\ \esperof{X | \Gamma} \ d\proba = \int_\Omega W\ X \ d\proba
\)
</p>
</div>

<p>
On en déduit que :
</p>

<p>
\[\int_\Omega W\ \esperof{X | \Delta} \ d\proba = \int_\Omega W\ \esperof{X | \Gamma} \ d\proba\]
</p>

<p>
Comme cette dernière équation est valable pour tout \(W \in \mathcal{F}(\Delta)\), on en déduit que \(\esperof{X | \Delta}\) est le meilleur estimateur de \(\esperof{X | \Gamma}\) sur \(\mathcal{F}(\Delta)\). Ce qui revient à dire que :
</p>

<p>
\[\esperof{ \esperof{X | \Gamma} | \Delta } = \esperof{X | \Delta}\]
</p>
</div>
</div>


<div id="outline-container-orgc82f11c" class="outline-4">
<h4 id="orgc82f11c"><span class="section-number-4">1.14.8.</span> Couple de variables aléatoires</h4>
<div class="outline-text-4" id="text-1-14-8">
<p>
Etant donné deux variables aléatoires \(X,Y\), on définit :
</p>

<p>
\[\esperof{X | Y} = \esperof{X | \Lambda(Y)}\]
</p>

<p>
Comme \(\Gamma = \Lambda(Y)\), l'espace \(\mathcal{F}(\Gamma)\) est l'ensemble des variables aléatoires \(W\) telles que :
</p>

<p>
\[\Lambda(W) \subseteq \Lambda(Y)\]
</p>
</div>
</div>
</div>


<div id="outline-container-orgd63fc20" class="outline-3">
<h3 id="orgd63fc20"><span class="section-number-3">1.15.</span> Ensemble discret</h3>
<div class="outline-text-3" id="text-1-15">
<p>
Nous allons à présent considérer le cas particulier où l'ensemble des événements peut s'écrire
comme :
</p>

<p>
\[\Omega = \{ \omega_i : i \in \setN \}\]
</p>

<p>
Nous notons \(p_i\) les probabilités associées aux singletons :
</p>

<p>
\[p_i = \probaof{ \{\omega_i\} }\]
</p>

<p>
Étant donnée une variable aléatoire \(X\), on note :
</p>

<p>
\[x_i = X(\omega_i)\]
</p>

<p>
L'espérance d'une telle variable s'écrit simplement :
</p>

<p>
\[\esperof{X} = \sum_i x_i \ p_i\]
</p>
</div>
</div>
</div>
</div>
<div id="postamble" class="status">
<p class="author">Auteur: chimay</p>
<p class="date">Created: 2023-05-10 mer 16:46</p>
<p class="validation"><a href="https://validator.w3.org/check?uri=referer">Validate</a></p>
</div>
</body>
</html>
