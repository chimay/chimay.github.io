<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN"
"http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" lang="fr" xml:lang="fr">
<head>
<!-- 2019-10-01 mar 12:19 -->
<meta http-equiv="Content-Type" content="text/html;charset=utf-8" />
<meta name="viewport" content="width=device-width, initial-scale=1" />
<title>Eclats de vers : Matemat 13 : Probabilit√©</title>
<meta name="generator" content="Org mode" />
<meta name="author" content="chimay" />
<style type="text/css">
 <!--/*--><![CDATA[/*><!--*/
  .title  { text-align: center;
             margin-bottom: .2em; }
  .subtitle { text-align: center;
              font-size: medium;
              font-weight: bold;
              margin-top:0; }
  .todo   { font-family: monospace; color: red; }
  .done   { font-family: monospace; color: green; }
  .priority { font-family: monospace; color: orange; }
  .tag    { background-color: #eee; font-family: monospace;
            padding: 2px; font-size: 80%; font-weight: normal; }
  .timestamp { color: #bebebe; }
  .timestamp-kwd { color: #5f9ea0; }
  .org-right  { margin-left: auto; margin-right: 0px;  text-align: right; }
  .org-left   { margin-left: 0px;  margin-right: auto; text-align: left; }
  .org-center { margin-left: auto; margin-right: auto; text-align: center; }
  .underline { text-decoration: underline; }
  #postamble p, #preamble p { font-size: 90%; margin: .2em; }
  p.verse { margin-left: 3%; }
  pre {
    border: 1px solid #ccc;
    box-shadow: 3px 3px 3px #eee;
    padding: 8pt;
    font-family: monospace;
    overflow: auto;
    margin: 1.2em;
  }
  pre.src {
    position: relative;
    overflow: visible;
    padding-top: 1.2em;
  }
  pre.src:before {
    display: none;
    position: absolute;
    background-color: white;
    top: -10px;
    right: 10px;
    padding: 3px;
    border: 1px solid black;
  }
  pre.src:hover:before { display: inline;}
  /* Languages per Org manual */
  pre.src-asymptote:before { content: 'Asymptote'; }
  pre.src-awk:before { content: 'Awk'; }
  pre.src-C:before { content: 'C'; }
  /* pre.src-C++ doesn't work in CSS */
  pre.src-clojure:before { content: 'Clojure'; }
  pre.src-css:before { content: 'CSS'; }
  pre.src-D:before { content: 'D'; }
  pre.src-ditaa:before { content: 'ditaa'; }
  pre.src-dot:before { content: 'Graphviz'; }
  pre.src-calc:before { content: 'Emacs Calc'; }
  pre.src-emacs-lisp:before { content: 'Emacs Lisp'; }
  pre.src-fortran:before { content: 'Fortran'; }
  pre.src-gnuplot:before { content: 'gnuplot'; }
  pre.src-haskell:before { content: 'Haskell'; }
  pre.src-hledger:before { content: 'hledger'; }
  pre.src-java:before { content: 'Java'; }
  pre.src-js:before { content: 'Javascript'; }
  pre.src-latex:before { content: 'LaTeX'; }
  pre.src-ledger:before { content: 'Ledger'; }
  pre.src-lisp:before { content: 'Lisp'; }
  pre.src-lilypond:before { content: 'Lilypond'; }
  pre.src-lua:before { content: 'Lua'; }
  pre.src-matlab:before { content: 'MATLAB'; }
  pre.src-mscgen:before { content: 'Mscgen'; }
  pre.src-ocaml:before { content: 'Objective Caml'; }
  pre.src-octave:before { content: 'Octave'; }
  pre.src-org:before { content: 'Org mode'; }
  pre.src-oz:before { content: 'OZ'; }
  pre.src-plantuml:before { content: 'Plantuml'; }
  pre.src-processing:before { content: 'Processing.js'; }
  pre.src-python:before { content: 'Python'; }
  pre.src-R:before { content: 'R'; }
  pre.src-ruby:before { content: 'Ruby'; }
  pre.src-sass:before { content: 'Sass'; }
  pre.src-scheme:before { content: 'Scheme'; }
  pre.src-screen:before { content: 'Gnu Screen'; }
  pre.src-sed:before { content: 'Sed'; }
  pre.src-sh:before { content: 'shell'; }
  pre.src-sql:before { content: 'SQL'; }
  pre.src-sqlite:before { content: 'SQLite'; }
  /* additional languages in org.el's org-babel-load-languages alist */
  pre.src-forth:before { content: 'Forth'; }
  pre.src-io:before { content: 'IO'; }
  pre.src-J:before { content: 'J'; }
  pre.src-makefile:before { content: 'Makefile'; }
  pre.src-maxima:before { content: 'Maxima'; }
  pre.src-perl:before { content: 'Perl'; }
  pre.src-picolisp:before { content: 'Pico Lisp'; }
  pre.src-scala:before { content: 'Scala'; }
  pre.src-shell:before { content: 'Shell Script'; }
  pre.src-ebnf2ps:before { content: 'ebfn2ps'; }
  /* additional language identifiers per "defun org-babel-execute"
       in ob-*.el */
  pre.src-cpp:before  { content: 'C++'; }
  pre.src-abc:before  { content: 'ABC'; }
  pre.src-coq:before  { content: 'Coq'; }
  pre.src-groovy:before  { content: 'Groovy'; }
  /* additional language identifiers from org-babel-shell-names in
     ob-shell.el: ob-shell is the only babel language using a lambda to put
     the execution function name together. */
  pre.src-bash:before  { content: 'bash'; }
  pre.src-csh:before  { content: 'csh'; }
  pre.src-ash:before  { content: 'ash'; }
  pre.src-dash:before  { content: 'dash'; }
  pre.src-ksh:before  { content: 'ksh'; }
  pre.src-mksh:before  { content: 'mksh'; }
  pre.src-posh:before  { content: 'posh'; }
  /* Additional Emacs modes also supported by the LaTeX listings package */
  pre.src-ada:before { content: 'Ada'; }
  pre.src-asm:before { content: 'Assembler'; }
  pre.src-caml:before { content: 'Caml'; }
  pre.src-delphi:before { content: 'Delphi'; }
  pre.src-html:before { content: 'HTML'; }
  pre.src-idl:before { content: 'IDL'; }
  pre.src-mercury:before { content: 'Mercury'; }
  pre.src-metapost:before { content: 'MetaPost'; }
  pre.src-modula-2:before { content: 'Modula-2'; }
  pre.src-pascal:before { content: 'Pascal'; }
  pre.src-ps:before { content: 'PostScript'; }
  pre.src-prolog:before { content: 'Prolog'; }
  pre.src-simula:before { content: 'Simula'; }
  pre.src-tcl:before { content: 'tcl'; }
  pre.src-tex:before { content: 'TeX'; }
  pre.src-plain-tex:before { content: 'Plain TeX'; }
  pre.src-verilog:before { content: 'Verilog'; }
  pre.src-vhdl:before { content: 'VHDL'; }
  pre.src-xml:before { content: 'XML'; }
  pre.src-nxml:before { content: 'XML'; }
  /* add a generic configuration mode; LaTeX export needs an additional
     (add-to-list 'org-latex-listings-langs '(conf " ")) in .emacs */
  pre.src-conf:before { content: 'Configuration File'; }

  table { border-collapse:collapse; }
  caption.t-above { caption-side: top; }
  caption.t-bottom { caption-side: bottom; }
  td, th { vertical-align:top;  }
  th.org-right  { text-align: center;  }
  th.org-left   { text-align: center;   }
  th.org-center { text-align: center; }
  td.org-right  { text-align: right;  }
  td.org-left   { text-align: left;   }
  td.org-center { text-align: center; }
  dt { font-weight: bold; }
  .footpara { display: inline; }
  .footdef  { margin-bottom: 1em; }
  .figure { padding: 1em; }
  .figure p { text-align: center; }
  .inlinetask {
    padding: 10px;
    border: 2px solid gray;
    margin: 10px;
    background: #ffffcc;
  }
  #org-div-home-and-up
   { text-align: right; font-size: 70%; white-space: nowrap; }
  textarea { overflow-x: auto; }
  .linenr { font-size: smaller }
  .code-highlighted { background-color: #ffff00; }
  .org-info-js_info-navigation { border-style: none; }
  #org-info-js_console-label
    { font-size: 10px; font-weight: bold; white-space: nowrap; }
  .org-info-js_search-highlight
    { background-color: #ffff00; color: #000000; font-weight: bold; }
  .org-svg { width: 90%; }
  /*]]>*/-->
</style>
<link rel="stylesheet" type="text/css" href="../style/defaut.css" />
<script type="text/javascript">
/*
@licstart  The following is the entire license notice for the
JavaScript code in this tag.

Copyright (C) 2012-2019 Free Software Foundation, Inc.

The JavaScript code in this tag is free software: you can
redistribute it and/or modify it under the terms of the GNU
General Public License (GNU GPL) as published by the Free Software
Foundation, either version 3 of the License, or (at your option)
any later version.  The code is distributed WITHOUT ANY WARRANTY;
without even the implied warranty of MERCHANTABILITY or FITNESS
FOR A PARTICULAR PURPOSE.  See the GNU GPL for more details.

As additional permission under GNU GPL version 3 section 7, you
may distribute non-source (e.g., minimized or compacted) forms of
that code without the copy of the GNU GPL normally required by
section 4, provided you include this license notice and a URL
through which recipients can access the Corresponding Source.


@licend  The above is the entire license notice
for the JavaScript code in this tag.
*/
<!--/*--><![CDATA[/*><!--*/
 function CodeHighlightOn(elem, id)
 {
   var target = document.getElementById(id);
   if(null != target) {
     elem.cacheClassElem = elem.className;
     elem.cacheClassTarget = target.className;
     target.className = "code-highlighted";
     elem.className   = "code-highlighted";
   }
 }
 function CodeHighlightOff(elem, id)
 {
   var target = document.getElementById(id);
   if(elem.cacheClassElem)
     elem.className = elem.cacheClassElem;
   if(elem.cacheClassTarget)
     target.className = elem.cacheClassTarget;
 }
/*]]>*///-->
</script>
<script type="text/x-mathjax-config">
    MathJax.Hub.Config({
        displayAlign: "center",
        displayIndent: "0em",

        "HTML-CSS": { scale: 100,
                        linebreaks: { automatic: "false" },
                        webFont: "TeX"
                       },
        SVG: {scale: 100,
              linebreaks: { automatic: "false" },
              font: "TeX"},
        NativeMML: {scale: 100},
        TeX: { equationNumbers: {autoNumber: "AMS"},
               MultLineWidth: "85%",
               TagSide: "right",
               TagIndent: ".8em"
             }
});
</script>
<script type="text/javascript"
        src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.0/MathJax.js?config=TeX-AMS_HTML"></script>
</head>
<body>
<div id="content">
<h1 class="title">Eclats de vers : Matemat 13 : Probabilit√©</h1>
<p>
<a href="index.html">Index des Grimoires</a>
</p>

<p>
<a href="file:///home/david/racine/site/orgmode/index.html">Retour √† l‚Äôaccueil</a>
</p>

<div id="table-of-contents">
<h2>Table des mati√®res</h2>
<div id="text-table-of-contents">
<ul>
<li><a href="#org70e3600">1. Probabilit√©</a></li>
<li><a href="#org62ed2a9">2. Statistiques</a></li>
<li><a href="#org251e012">3. Calcul stochastique</a></li>
</ul>
</div>
</div>

<p>
\( \newcommand{\parentheses}[1]{\left(#1\right)}
\newcommand{\crochets}[1]{\left[#1\right]}
\newcommand{\accolades}[1]{\left\{#1\right\}}
\newcommand{\ensemble}[1]{\left\{#1\right\}}
\newcommand{\identite}{\mathrm{Id}}
\newcommand{\indicatrice}{\boldsymbol{\delta}}
\newcommand{\dirac}{\delta}
\newcommand{\moinsun}{{-1}}
\newcommand{\inverse}{\ddagger}
\newcommand{\pinverse}{\dagger}
\newcommand{\topologie}{\mathfrak{T}}
\newcommand{\ferme}{\mathfrak{F}}
\newcommand{\img}{\mathbf{i}}
\newcommand{\binome}[2]{
\left\{ \begin{array}{c}
#1 \\
#2 \\
\end{array} \right\}
}
\newcommand{\canonique}{\mathfrak{c}}
\newcommand{\tenseuridentite}{\boldsymbol{\mathcal{I}}}
\newcommand{\permutation}{\boldsymbol{\epsilon}}
\newcommand{\matriceZero}{\mathfrak{0}}
\newcommand{\matriceUn}{\mathfrak{1}}
\newcommand{\christoffel}[2]{
\left\{ \begin{array}{c}
#1 \\
#2 \\
\end{array} \right\}
}
\newcommand{\lagrangien}{\mathfrak{L}}
\newcommand{\sousens}{\mathfrak{P}}
\newcommand{\partition}{\mathrm{Partition}}
\newcommand{\tribu}{\mathrm{Tribu}}
\newcommand{\topologies}{\mathrm{Topo}}
\newcommand{\setB}{\mathbb{B}}
\newcommand{\setN}{\mathbb{N}}
\newcommand{\setZ}{\mathbb{Z}}
\newcommand{\setQ}{\mathbb{Q}}
\newcommand{\setR}{\mathbb{R}}
\newcommand{\setC}{\mathbb{C}}
\newcommand{\corps}{\mathbb{K}}
\newcommand{\boule}{\mathfrak{B}}
\newcommand{\intervalleouvert}[2]{\relax \ ] #1 , #2 [ \ \relax}
\newcommand{\intervallesemiouvertgauche}[2]{\relax \ ] #1 , #2 ]}
\newcommand{\intervallesemiouvertdroite}[2]{[ #1 , #2 [ \ \relax}
\newcommand{\fonction}{\mathbb{F}}
\newcommand{\bijection}{\mathrm{Bij}}
\newcommand{\polynome}{\mathrm{Poly}}
\newcommand{\lineaire}{\mathrm{Lin}}
\newcommand{\continue}{\mathrm{Cont}}
\newcommand{\homeomorphisme}{\mathrm{Hom}}
\newcommand{\etagee}{\mathrm{Etagee}}
\newcommand{\lebesgue}{\mathrm{Leb}}
\newcommand{\lipschitz}{\mathrm{Lip}}
\newcommand{\suitek}{\mathrm{Suite}}
\newcommand{\matrice}{\mathbb{M}}
\newcommand{\krylov}{\mathrm{Krylov}}
\newcommand{\tenseur}{\mathbb{T}}
\newcommand{\essentiel}{\mathfrak{E}}
\newcommand{\relation}{\mathrm{Rel}}
\newcommand{\strictinferieur}{\ < \ }
\newcommand{\strictsuperieur}{\ > \ }
\newcommand{\ensinferieur}{\eqslantless}
\newcommand{\enssuperieur}{\eqslantgtr}
\newcommand{\esssuperieur}{\gtrsim}
\newcommand{\essinferieur}{\lesssim}
\newcommand{\essegal}{\eqsim}
\newcommand{\union}{\ \cup \ }
\newcommand{\intersection}{\ \cap \ }
\newcommand{\opera}{\divideontimes}
\newcommand{\autreaddition}{\boxplus}
\newcommand{\autremultiplication}{\circledast}
\newcommand{\commutateur}[2]{\left[ #1 , #2 \right]}
\newcommand{\convolution}{\circledcirc}
\newcommand{\correlation}{\ \natural \ }
\newcommand{\diventiere}{\div}
\newcommand{\modulo}{\bmod}
\newcommand{\pgcd}{pgcd}
\newcommand{\ppcm}{ppcm}
\newcommand{\produitscalaire}[2]{\left\langle #1 \left|\right\relax #2 \right\rangle}
\newcommand{\scalaire}[2]{\left\langle #1 \| #2 \right\rangle}
\newcommand{\braket}[3]{\left\langle #1 \right| #2 \left| #3 \right\rangle}
\newcommand{\orthogonal}{\bot}
\newcommand{\forme}[2]{\left\langle #1 , #2 \right\rangle}
\newcommand{\biforme}[3]{\left\langle #1 , #2 , #3 \right\rangle}
\newcommand{\contraction}[3]{\left\langle #1 \odot #3 \right\rangle_{#2}}
\newcommand{\dblecont}[5]{\left\langle #1 \right| #3 \left| #5 \right\rangle_{#2,#4}}
\newcommand{\major}{major}
\newcommand{\minor}{minor}
\newcommand{\maxim}{maxim}
\newcommand{\minim}{minim}
\newcommand{\argument}{arg}
\newcommand{\argmin}{arg\ min}
\newcommand{\argmax}{arg\ max}
\newcommand{\supessentiel}{ess\ sup}
\newcommand{\infessentiel}{ess\ inf}
\newcommand{\dual}{\star}
\newcommand{\distance}{\mathfrak{dist}}
\newcommand{\norme}[1]{\left\| #1 \right\|}
\newcommand{\normetrois}[1]{\left|\left\| #1 \right\|\right|}
\newcommand{\adh}{adh}
\newcommand{\interieur}{int}
\newcommand{\frontiere}{\partial}
\newcommand{\image}{im}
\newcommand{\domaine}{dom}
\newcommand{\noyau}{ker}
\newcommand{\support}{supp}
\newcommand{\signe}{sign}
\newcommand{\abs}[1]{\left| #1 \right|}
\newcommand{\unsur}[1]{\frac{1}{#1}}
\newcommand{\arrondisup}[1]{\lceil #1 \rceil}
\newcommand{\arrondiinf}[1]{\lfloor #1 \rfloor}
\newcommand{\conjugue}{conj}
\newcommand{\conjaccent}[1]{\overline{#1}}
\newcommand{\division}{division}
\newcommand{\difference}{\boldsymbol{\Delta}}
\newcommand{\differentielle}[2]{\mathfrak{D}^{#1}_{#2}}
\newcommand{\OD}[2]{\frac{d #1}{d #2}}
\newcommand{\OOD}[2]{\frac{d^2 #1}{d #2^2}}
\newcommand{\NOD}[3]{\frac{d^{#3} #1}{d #2^{#3}}}
\newcommand{\deriveepartielle}[2]{\frac{\partial #1}{\partial #2}}
\newcommand{\dblederiveepartielle}[2]{\frac{\partial^2 #1}{\partial #2 \partial #2}}
\newcommand{\dfdxdy}[3]{\frac{\partial^2 #1}{\partial #2 \partial #3}}
\newcommand{\dfdxdx}[2]{\frac{\partial^2 #1}{\partial #2^2}}
\newcommand{\gradient}{\mathbf{\nabla}}
\newcommand{\combilin}[1]{\mathrm{span}\{ #1 \}}
\newcommand{\trace}{tr}
\newcommand{\proba}{\mathbb{P}}
\newcommand{\probaof}[1]{\mathbb{P}\left[#1\right]}
\newcommand{\esperof}[1]{\mathbb{E}\left[#1\right]}
\newcommand{\cov}[2]{\mathrm{cov} \left( #1 , #2 \right) }
\newcommand{\var}[1]{\mathrm{var} \left( #1 \right) }
\newcommand{\rand}{\mathrm{rand}}
\newcommand{\variation}[1]{\left\langle #1 \right\rangle}
\newcommand{\composante}{comp}
\newcommand{\bloc}{bloc}
\newcommand{\ligne}{ligne}
\newcommand{\colonne}{colonne}
\newcommand{\diagonale}{diag}
\newcommand{\matelementaire}{\mathrm{Elem}}
\newcommand{\matpermutation}{permut}
\newcommand{\matunitaire}{\mathrm{Unitaire}}
\newcommand{\gaussjordan}{\mathrm{GaussJordan}}
\newcommand{\householder}{\mathrm{Householder}}
\newcommand{\rang}{rang}
\newcommand{\schur}{\mathrm{Schur}}
\newcommand{\singuliere}{\mathrm{DVS}}
\newcommand{\convexe}{\mathrm{Convexe}}
\newcommand{\petito}[1]{o\left(#1\right)}
\newcommand{\grando}[1]{O\left(#1\right)} \)
</p>

<div id="outline-container-org70e3600" class="outline-2">
<h2 id="org70e3600"><span class="section-number-2">1</span> Probabilit√©</h2>
<div class="outline-text-2" id="text-1">
<div id="text-table-of-contents">
<ul>
<li><a href="#org1e4d391">1.1. Probabilit√©</a></li>
<li><a href="#org057e1e5">1.2. Variable al√©atoire</a></li>
<li><a href="#org9f8b588">1.3. Mesure induite</a></li>
<li><a href="#orgda9b747">1.4. Collection induite</a></li>
<li><a href="#orgea4287d">1.5. Esp√©rance</a></li>
<li><a href="#orgb498534">1.6. Esp√©rance et mesure induite</a></li>
<li><a href="#org9ad7281">1.7. Fonction g√©n√©ratrice des moments</a></li>
<li><a href="#orga363c16">1.8. Variance</a></li>
<li><a href="#orgcfb0762">1.9. Covariance</a></li>
<li><a href="#orgdf9ac70">1.10. Variance d'une combinaison lin√©aire</a></li>
<li><a href="#org37e46df">1.11. Produit scalaire</a></li>
<li><a href="#org0055425">1.12. Probabilit√© conditionnelle</a></li>
<li><a href="#orgb7f9dfd">1.13. Esp√©rance conditionnelle √† un ensemble</a></li>
<li><a href="#org6709ab9">1.14. Esp√©rance conditionnelle √† une tribu</a></li>
<li><a href="#org933cacc">1.15. Ensemble discret</a></li>
</ul>
</div>

<p>
\label{chap:proba}
</p>
</div>


<div id="outline-container-org1e4d391" class="outline-3">
<h3 id="org1e4d391"><span class="section-number-3">1.1</span> Probabilit√©</h3>
<div class="outline-text-3" id="text-1-1">
<p>
Une probabilit√© \(\proba\) sur un ensemble d'√©v√©nements \(\Omega\) est une mesure d√©finie sur \(\mathcal{S}=\{ A : A \subseteq \Omega \}\) et √† valeurs dans \([0,1]\)¬†:
</p>

<p>
\[\proba : \mathcal{S} \mapsto [0,1] \quad\]
</p>

<p>
Cette probabilit√© doit v√©rifier la normalisation¬†:
</p>

<p>
\[\probaof{\Omega} = 1\]
</p>

<p>
ainsi que l'additivit√©¬†:
</p>

<p>
\[\probaof{\bigcup_i \Phi_i} = \sum_i \probaof{\Phi_i}\]
</p>

<p>
lorsque les ensembles \(\Phi_i\) sont disjoints deux √† deux¬†:
</p>

<div class="org-center">
<p>
\(
&Phi;<sub>i</sub> &cap; &Phi;<sub>j</sub> =
</p>
\begin{cases}
\Phi_i & i = j \\
\emptyset & i \ne j
\end{cases}
<p>
\)
</p>
</div>

<p>
On en d√©duit directement que¬†:
</p>

<p>
\[\probaof{\Phi} = \probaof{\Phi \cup \emptyset} = \probaof{\Phi} + \probaof{\emptyset}\]
</p>

<p>
d'o√π \(\probaof{\emptyset} = 0\).
</p>

<p>
La grandeur \(\probaof{\Phi}\) peut s'interpr√©ter comme la probabilit√© que l'un des √©v√©nements de \(\Phi\) se r√©alise.
</p>
</div>
</div>


<div id="outline-container-org057e1e5" class="outline-3">
<h3 id="org057e1e5"><span class="section-number-3">1.2</span> Variable al√©atoire</h3>
<div class="outline-text-3" id="text-1-2">
<p>
Une variable al√©atoire \(X\) associe une valeur r√©elle a chaque √©l√©ment de \(\Omega\). On a donc \(X : \Omega \mapsto \setR\).
</p>
</div>
</div>


<div id="outline-container-org9f8b588" class="outline-3">
<h3 id="org9f8b588"><span class="section-number-3">1.3</span> Mesure induite</h3>
<div class="outline-text-3" id="text-1-3">
<p>
Etant donn√© une variable al√©atoire \(X\), on peut d√©finir une mesure induite \(\mathcal{L}_X : \sousens(\setR) \mapsto [0,1]\), qui exprime la probabilit√© qu'un √©v√©nement \(\omega \in \Omega\) donne une valeur appartenant √† un sous-ensemble \(U \subseteq \setR\)¬†:
</p>

<p>
\[\mathcal{L}_X(U) = \probaof{X^{-1}(U)} = \probaof{ \{ \omega\in\Omega : X(\omega) \in U \} }\]
</p>
</div>


<div id="outline-container-org779441c" class="outline-4">
<h4 id="org779441c"><span class="section-number-4">1.3.1</span> Variables conjointes</h4>
<div class="outline-text-4" id="text-1-3-1">
<p>
La mesure induite par deux variables al√©atoires \(X\) et \(Y\) se d√©finit par¬†:
</p>

<p>
\[\mathcal{L}_{X,Y}(D) = \probaof{ \{ \omega\in\Omega : (X(\omega),Y(\omega)) \in D \} }\]
</p>

<p>
pour tout \(D \subseteq\setR^2\).
</p>

<p>
On voit clairement que¬†:
</p>

<div class="org-center">
<p>
\(
\mathcal{L}_X(U) = \mathcal{L}_{X,Y}(U \times \setR) \\
\mathcal{L}_Y(U) = \mathcal{L}_{X,Y}(\setR \times U)
\)
</p>
</div>
</div>
</div>
</div>


<div id="outline-container-orgda9b747" class="outline-3">
<h3 id="orgda9b747"><span class="section-number-3">1.4</span> Collection induite</h3>
<div class="outline-text-3" id="text-1-4">
<p>
Soit \(X\) une variable al√©atoire et \(U \subseteq \setR\). On d√©finit le sous-ensemble de \(\Omega\)¬†:
</p>

<p>
\[\Theta(X,U) = \{ \omega \in \Omega : X(\omega) \in U \}\]
</p>

<p>
ou de mani√®re √©quivalente en utilisant la relation inverse \(X^{-1}\)¬†:
</p>

<p>
\[\Theta(X,U) = X^{-1}(U)\]
</p>

<p>
La collection \(\Lambda(X)\) induite par \(X\) est un ensemble regroupant les \(\Theta(X,U)\) pour tous les sous-ensembles de \(\setR\)¬†:
</p>

<p>
\[\Lambda(X) = \{ \Theta(X,U) : U \subseteq \setR \}\]
</p>

<p>
Comme¬†:
</p>

<div class="org-center">
<p>
\(
\Theta(X,\emptyset) = \emptyset \\
\Theta(X,\setR) = \Omega
\)
</p>
</div>

<p>
il est clair que l'on a \(\emptyset, \Omega \in \Lambda(X)\) quelle que soit la variable al√©atoire \(X\).
</p>
</div>


<div id="outline-container-orgc40ceb1" class="outline-4">
<h4 id="orgc40ceb1"><span class="section-number-4">1.4.1</span> Fonctions indicatrices</h4>
<div class="outline-text-4" id="text-1-4-1">
<p>
Si \(\Phi \subseteq \Omega\) et \(X = \indicatrice_\Phi\), on a¬†:
</p>

<div class="org-center">
<p>
\(
\Theta(\indicatrice_\Phi, \{1\}) = \{ \omega : \indicatrice_\Phi(\omega) = 1 \} = \Phi \\
\Theta(\indicatrice_\Phi, \{0\}) = \{ \omega : \indicatrice_\Phi(\omega) = 0 \} = \Omega \setminus \Phi
\)
</p>
</div>

<p>
De m√™me, si un ensemble \(U \subseteq \setR\)¬†:
</p>

<ul class="org-ul">
<li>ne contient ni \(1\) ni \(0\), on a \(\Theta(\indicatrice_\Phi,U) = \emptyset\)</li>
<li>contient \(1\) et \(0\), on a \(\Theta(\indicatrice_\Phi,U) = \Omega\)</li>
<li>contient \(1\) et pas \(0\), on a \(\Theta(\indicatrice_\Phi,U) = \Phi\)</li>
<li>contient \(0\) et pas \(1\), on a \(\Theta(\indicatrice_\Phi,U) = \Omega \setminus \Phi\)</li>
</ul>

<p>
On a donc¬†:
</p>

<p>
\[\Lambda(\indicatrice_\Phi) = \{ \emptyset, \Omega, \Phi, \Omega \setminus \Phi \}\]
</p>
</div>
</div>
</div>


<div id="outline-container-orgea4287d" class="outline-3">
<h3 id="orgea4287d"><span class="section-number-3">1.5</span> Esp√©rance</h3>
<div class="outline-text-3" id="text-1-5">
<p>
L'esp√©rance d'une variable al√©atoire \(X\) est simplement une moyenne pond√©r√©e par les probablit√©s que \(X\)
prennent telle ou telle valeur¬†:
</p>

<p>
\[\esperof{X} = \int_{\Omega} X(\omega) \ d\proba(\omega)\]
</p>
</div>


<div id="outline-container-org2d23e25" class="outline-4">
<h4 id="org2d23e25"><span class="section-number-4">1.5.1</span> Indicatrice</h4>
<div class="outline-text-4" id="text-1-5-1">
<p>
Notons que pour tout \(\Phi \subseteq \Omega\), on a¬†:
</p>

\begin{align}
\esperof{\indicatrice_\Phi} &= \int_\Omega \indicatrice_\Phi \ d\proba \\
&= \int_\Phi \ d\proba
\end{align}

<p>
et donc¬†:
</p>

<p>
\[\esperof{\indicatrice_\Phi} = \probaof{\Phi}\]
</p>
</div>
</div>


<div id="outline-container-org35fe6d9" class="outline-4">
<h4 id="org35fe6d9"><span class="section-number-4">1.5.2</span> Fonction d'une variable al√©atoire</h4>
<div class="outline-text-4" id="text-1-5-2">
<p>
Pour toute fonction \(G : \setR \mapsto \setR\), on a bien √©videmment \(G \circ X : \Omega \mapsto \setR\) et on peut d√©finir¬†:
</p>

<p>
\[\esperof{G(X)} = \int_\Omega (G \circ X)(\omega) \ d\proba(\omega)\]
</p>
</div>
</div>


<div id="outline-container-org66f4ed1" class="outline-4">
<h4 id="org66f4ed1"><span class="section-number-4">1.5.3</span> Fonction de plusieurs variables al√©atoires</h4>
<div class="outline-text-4" id="text-1-5-3">
<p>
De m√™me, si \(X\) et \(Y\) sont deux variables al√©atoires, pour toute fonction \(G : \setR^2 \mapsto \setR\), on a √©videmment \(G(X,Y) \in \setR\) et on peut d√©finir¬†:
</p>

<p>
\[\esperof{G(X,Y)} = \int_\Omega G\left(X(\omega),Y(\omega)\right) \ d\proba(\omega)\]
</p>

<p>
Le cas particulier \(G(X,Y) = a \ X + b \ Y\), o√π \(a,b \in \setR\), nous montre la lin√©arit√© de l'esp√©rance, qui d√©coule directement de celle de l'int√©grale¬†:
</p>

<p>
\[\esperof{a \ X + b \ Y} = a \ \esperof{X} + b \ \esperof{Y}\]
</p>
</div>
</div>
</div>


<div id="outline-container-orgb498534" class="outline-3">
<h3 id="orgb498534"><span class="section-number-3">1.6</span> Esp√©rance et mesure induite</h3>
<div class="outline-text-3" id="text-1-6">
<p>
Soit une variable al√©atoire \(X\) et la fonction √©tag√©e \(G : \setR \mapsto \setR\) d√©finie
pour tout \(x \in \setR\) par¬†:
</p>

<p>
\[G(x) = \sum_i g_i \ \indicatrice_{A_i}(x)\]
</p>

<p>
o√π les \(A_i\) forment une partition de \(\setR\) et o√π les \(g_i\) sont suppos√©s sans
perte de g√©n√©ralit√© √™tre des r√©els distincts. Soit la partition de \(\Omega\) constitu√©e
des ensembles¬†:
</p>

<p>
\[\Omega_i = X^{-1}(A_i) = \{ \omega \in \Omega : X(\omega) \in A_i \}\]
</p>

<p>
On voit que \((G \circ X)(\omega) = g_i\) pour tout \(\omega \in \Omega_i\).
Calculons l'esp√©rance de \(G(X)\)¬†:
</p>

\begin{align}
\esperof{G(X)} &= \int_\Omega (G \circ X)(\omega) \ d\proba(\omega) \\
&= \sum_i \int_{\Omega_i} (G \circ X)(\omega) \ d\proba(\omega) \\
&= \sum_i \int_{\Omega_i} g_i \ d\proba(\omega) \\
&= \sum_i g_i \int_{\Omega_i} \ d\proba(\omega) \\
&= \sum_i g_i \ \probaof{\Omega_i}
\end{align}

<p>
Par d√©finition de la mesure induite, on a¬†:
</p>

<p>
\[\mathcal{L}_X(A_i) = \probaof{X^{-1}(A_i)} = \probaof{\Omega_i}\]
</p>

<p>
L'esp√©rance de \(G(X)\) peut donc s'exprimer comme¬†:
</p>

<p>
\[\esperof{G(X)} = \sum_i g_i \ \mathcal{L}_X(A_i)\]
</p>

<p>
Mais le membre de droite n'est autre que l'int√©grale de \(G\) sur \(\setR\)
utilisant la mesure \(\mathcal{L}_X\)¬†:
</p>

<p>
\[\esperof{G(X)} = \int_\setR G(x) \ d\mathcal{L}_X(x)\]
</p>

<p>
Comme cette expression doit √™tre valable pour toute fonction en escalier, on en conclut que¬†:
</p>

<p>
\[\esperof{G(X)} = \int_\setR G(x) \ d\mathcal{L}_X(x)\]
</p>

<p>
pour toute fonction int√©grable \(G\).
</p>
</div>


<div id="outline-container-orgb3347fe" class="outline-4">
<h4 id="orgb3347fe"><span class="section-number-4">1.6.1</span> Identit√©</h4>
<div class="outline-text-4" id="text-1-6-1">
<p>
Le cas particulier \(G = \identite\) nous donne¬†:
</p>

<p>
\[\esperof{X} = \int_\setR x \ d\mathcal{L}_X(x)\]
</p>
</div>
</div>


<div id="outline-container-org9b9adcd" class="outline-4">
<h4 id="org9b9adcd"><span class="section-number-4">1.6.2</span> Densit√©</h4>
<div class="outline-text-4" id="text-1-6-2">
<p>
Si il existe une fonction \(f_X : \setR \mapsto \setR\) telle que \(d\mathcal{L}_X = f_X \ dx\),
o√π \(dx\) correspond √† la mesure de Lebesgue sur \(\setR\), on a¬†:
</p>

<p>
\[\esperof{G(X)} = \int_\setR G(x) \ f_X(x) \ dx\]
</p>

<p>
ainsi que¬†:
</p>

<p>
\[\esperof{X} = \int_\setR x \ f_X(x) \ dx\]
</p>

<p>
On nomme cette fonction \(f_X\) la densit√© de la variable al√©atoire \(X\).
</p>

<p>
Remarquons que \(f_X\) est positive par positivit√© de la mesure. Comme¬†:
</p>

<p>
\[\esperof{1} = 1\]
</p>

<p>
on obtient la propri√©t√© de normalit√©¬†:
</p>

<p>
\[\int_\setR f_X(x) \ dx = 1\]
</p>
</div>


<div id="outline-container-org081543b" class="outline-5">
<h5 id="org081543b"><span class="section-number-5">1.6.2.1</span> Variable al√©atoire gaussienne</h5>
<div class="outline-text-5" id="text-1-6-2-1">
<p>
Une variable al√©atoire est dite normale de param√®tres \(\mu\), \(\sigma\) si sa fonction
densit√© v√©rifie¬†:
</p>

<p>
\[f_{X}(x) = \frac{1}{ \sigma\sqrt{2 \pi} } \exp\left(-\frac{(x-\mu)^2}{2 \sigma^2}\right)\]
</p>
</div>
</div>
</div>


<div id="outline-container-org59ec1fa" class="outline-4">
<h4 id="org59ec1fa"><span class="section-number-4">1.6.3</span> Variables conjointes</h4>
<div class="outline-text-4" id="text-1-6-3">
<p>
Soit les variables al√©atoires \(X, Y\) et la fonction √©tag√©e \(G : \setR^2 \mapsto \setR\) d√©finie
pour tout \(x, y \in \setR\) par¬†:
</p>

<p>
\[G(x,y) = \sum_i g_i \ \indicatrice_{A_i}(x,y)\]
</p>

<p>
o√π les \(A_i\) forment une partition de \(\setR^2\) et o√π les \(g_i\) sont suppos√©s sans
perte de g√©n√©ralit√© √™tre des r√©els distincts. Soit la partition de \(\Omega\) constitu√©e
des ensembles¬†:
</p>

<p>
\[\Omega_i = \{ \omega \in \Omega : (X(\omega), Y(\omega)) \in A_i \}\]
</p>

<p>
On voit que \(G(X(\omega), Y(\omega)) = g_i\) pour tout \(\omega \in \Omega_i\).
Calculons l'esp√©rance de \(G(X,Y)\)¬†:
</p>

\begin{align}
\esperof{G(X,Y)} &= \int_\Omega G(X(\omega), Y(\omega)) \ d\proba(\omega) \\
&= \sum_i \int_{\Omega_i} G(X(\omega), Y(\omega)) \ d\proba(\omega) \\
&= \sum_i \int_{\Omega_i} g_i \ d\proba(\omega) \\
&= \sum_i g_i \int_{\Omega_i} \ d\proba(\omega) \\
&= \sum_i g_i \ \probaof{\Omega_i}
\end{align}

<p>
Par d√©finition de la mesure induite, on a¬†:
</p>

<p>
\[\mathcal{L}_{X,Y}(A_i) = \probaof{\Omega_i}\]
</p>

<p>
L'esp√©rance de \(G(X)\) peut donc s'exprimer comme¬†:
</p>

<p>
\[\esperof{G(X,Y)} = \sum_i g_i \ \mathcal{L}_{X,Y}(A_i)\]
</p>

<p>
Mais le membre de droite n'est autre que l'int√©grale de \(G\) sur \(\setR^2\)
utilisant la mesure \(\mathcal{L}_{X,Y}\)¬†:
</p>

<p>
\[\esperof{G(X,Y)} = \int_{\setR^2} G(x,y) \ d\mathcal{L}_{X,Y}(x,y)\]
</p>

<p>
Comme cette expression doit √™tre valable pour toute fonction en escalier, on en conclut que¬†:
</p>

<p>
\[\esperof{G(X,Y)} = \int_{\setR^2} G(x,y) \ d\mathcal{L}_{X,Y}(x,y)\]
</p>

<p>
pour toute fonction int√©grable \(G\).
</p>
</div>
</div>


<div id="outline-container-org538bca7" class="outline-4">
<h4 id="org538bca7"><span class="section-number-4">1.6.4</span> Densit√© conjointe</h4>
<div class="outline-text-4" id="text-1-6-4">
<p>
Si il existe une fonction \(f_{X,Y} : \setR^2 \mapsto \setR\) telle que
\(d\mathcal{L}_{X,Y} = f_{X,Y} \ dx \ dy\), o√π \(dx \ dy\) correspond √† la mesure de Lebesgue
sur \(\setR^2\), on a¬†:
</p>

<p>
\[\esperof{G(X,Y)} = \int_{\setR^2} G(x,y) \ f_{X,Y}(x,y) \ dx \ dy\]
</p>

<p>
En consid√©rant le cas particulier \(G(X,Y) = X\), on obtient¬†:
</p>

\begin{align}
\esperof{X} &= \int_{\setR^2} x \ f_{X,Y}(x,y) \ dx \ dy \\
&= \int_\setR x \ \left[\int_\setR f_{X,Y}(x,y) \ dy\right] \ dx
\end{align}

<p>
En d√©finissant la fonction associ√©e \(f_X\) par¬†:
</p>

<p>
\[f_X(x) = \int_\setR f_{X,Y}(x,y) \ dy\]
</p>

<p>
on peut d√®s lors √©crire l'esp√©rance de \(X\) comme¬†:
</p>

<p>
\[\esperof{X} = \int_\setR x \ f_X(x) \ dx\]
</p>

<p>
En suivant le m√™me d√©roulement pour \(\esperof{Y}\), et en d√©finissant¬†:
</p>

<p>
\[f_Y(y) = \int_\setR f_{X,Y}(x,y) \ dx\]
</p>

<p>
on peut √©crire l'esp√©rance de \(Y\) comme¬†:
</p>

<p>
\[\esperof{Y} = \int_\setR y \ f_Y(y) \ dy\]
</p>
</div>


<div id="outline-container-org10d4db8" class="outline-5">
<h5 id="org10d4db8"><span class="section-number-5">1.6.4.1</span> Distribution normale</h5>
<div class="outline-text-5" id="text-1-6-4-1">
<p>
On dit que les variables al√©atoires \(X_1, ..., X_N\) pr√©sentent une distribution normale multivari√©e si il existe¬†:
</p>

<div class="org-center">
<p>
\(
\mu = \left( \mu_i \right)_i \\
\Theta = \left( \sigma_{ij} \right)_{i,j}
\)
</p>
</div>

<p>
tels que la fonction densit√© associ√©e √† \(X = (X_1, ..., X_N)^T\) s'√©crive¬†:
</p>

<p>
\[\f_X(x) = \unsur{2 \pi^{n/2} \det{A}} \exp\left(-\unsur{2} (x-\mu)^T \cdot \Theta^{-1} \cdot (x-\mu) \right)\]
</p>

<p>
pour tout \(x \in \setR^N\). On a alors¬†:
</p>

<div class="org-center">
<p>
\(
\esperof{X_i} = \mu_i \\
\cov{X_i}{X_j} = \sigma_{ij}
\)
</p>
</div>

<p>
On a aussi la fonction g√©n√©ratrice¬†:
</p>

<p>
\[\Psi_X(u) = \exp\left(u^T \cdot \mu + \unsur{2} u^T \cdot \Theta^{-1} \cdot u\right)\]
</p>

<p>
pour tout \(u \in \setR^N\).
</p>
</div>
</div>
</div>
</div>

<div id="outline-container-org9ad7281" class="outline-3">
<h3 id="org9ad7281"><span class="section-number-3">1.7</span> Fonction g√©n√©ratrice des moments</h3>
<div class="outline-text-3" id="text-1-7">
<p>
On d√©finit le moment g√©n√©rateur d'une densit√© par¬†:
</p>

<p>
\[\Psi_X(u) = \esperof{\exp(X \cdot u)}\]
</p>

<p>
L'int√©r√™t de cette fonction est qu'elle permet de calculer facilement
les esp√©rances des puissances naturelles de \(X\). En effet¬†:
</p>

<p>
\[\frac{d^k \Psi_X}{du^k}(u) = \esperof{X^k \ \exp(X \cdot u)}\]
</p>

<p>
et donc¬†:
</p>

<p>
\[\OD{\Psi}{u}(0) = \esperof{X^k \ \exp(0)} = \esperof{X^k}\]
</p>
</div>


<div id="outline-container-orge8c1c4f" class="outline-4">
<h4 id="orge8c1c4f"><span class="section-number-4">1.7.1</span> Variable gaussienne</h4>
<div class="outline-text-4" id="text-1-7-1">
<p>
A titre d'exemple, nous calculons le  moment g√©n√©rateur associ√© √† une densit√© gaussienne¬†:
</p>

<p>
\[\Psi(u) = \unsur{\sqrt{2 \pi} \sigma} \int_\setR \exp(x u) \exp\left(-\frac{(x-\mu)^2}{2 \sigma^2}\right) dx\]
</p>

<p>
On obtient en d√©veloppant¬†:
</p>

\begin{align}
\Psi(u) &= \unsur{\sqrt{2 \pi} \sigma} \int_\setR \exp\left(x u - \frac{(x-\mu)^2}{2 \sigma^2}\right) dx \\
&= \unsur{\sqrt{2 \pi} \sigma} \exp(\mu u + \unsur{2} u^2 \sigma^2) \int_\setR \exp\left(- \frac{(x-(\mu + u \sigma^2) )^2}{2 \sigma^2}\right) dx
\end{align}

<p>
Comme l'int√©grale vaut \(\sqrt{2 \pi} \sigma\), on obtient finalement¬†:
</p>

<p>
\[\Psi(u) = \exp(u \mu + \unsur{2} u^2 \sigma^2)\]
</p>
</div>
</div>
</div>


<div id="outline-container-orga363c16" class="outline-3">
<h3 id="orga363c16"><span class="section-number-3">1.8</span> Variance</h3>
<div class="outline-text-3" id="text-1-8">
<p>
La variance de \(X\) est la variation carr√©e moyenne de \(X\) autour de son esp√©rance \(\esperof{X}\)¬†:
</p>

<p>
\[\var{X} = \esperof{\left(X-\esperof{X}\right)^2}\]
</p>

<p>
Comme la variable \(Z = \left(X-\esperof{X}\right)^2\) est positive, son esp√©rance doit √©galement etre positive et \(\var{X} \ge 0\).
</p>

<p>
En d√©veloppant la d√©finition et en utilisant la lin√©arit√© de l'esp√©rance, on obtient¬†:
</p>

\begin{align}
\var{X} &= \esperof{X^2 - 2 \ X \cdot \esperof{X} + \esperof{X}^2} \\
&= \esperof{X^2} - 2 \ \esperof{X} \cdot \esperof{X} + \esperof{X}^2 \cdot \esperof{1} \\
&= \esperof{X^2} - 2 \ \esperof{X}^2 + \esperof{X}^2
\end{align}

<p>
soit¬†:
</p>

<p>
\[\var{X} = \esperof{X^2} - \esperof{X}^2\]
</p>
</div>


<div id="outline-container-org7f427e5" class="outline-4">
<h4 id="org7f427e5"><span class="section-number-4">1.8.1</span> Invariance sous translation</h4>
<div class="outline-text-4" id="text-1-8-1">
<p>
Notons que si \(X,Y\) sont deux variables al√©atoires reli√©es par¬†:
</p>

<p>
\[Y = X + a\]
</p>

<p>
o√π \(a \in \setR\), on a¬†:
</p>

\begin{align}
\var{Y} &= \esperof{\left(Y-\esperof{Y}\right)^2} \\
&= \esperof{\left(X + a -\esperof{X+a}\right)^2} \\
&= \esperof{\left(X + a -\esperof{X} - a\right)^2} \\
&= \esperof{\left(X -\esperof{X}\right)^2} \\
&= \var{X}
\end{align}

<p>
La variance est donc invariante sous translation¬†:
</p>

<p>
\[\var{X+a} = \var{X}\]
</p>
</div>
</div>
</div>


<div id="outline-container-orgcfb0762" class="outline-3">
<h3 id="orgcfb0762"><span class="section-number-3">1.9</span> Covariance</h3>
<div class="outline-text-3" id="text-1-9">
<p>
La covariance de deux variables al√©atoire \(X,Y\) se d√©finit par¬†:
</p>

<p>
\[\cov{X}{Y} = \esperof{(X-\esperof{X}) \cdot (Y-\esperof{Y})}\]
</p>

<p>
En d√©veloppant et en utilisant la lin√©arit√© de l'esp√©rance, on obtient¬†:
</p>

\begin{align}
\cov{X}{Y} &= \esperof{X \cdot Y} - \esperof{X} \cdot \esperof{Y} - \esperof{Y} \cdot \esperof{X} + \esperof{X} \cdot \esperof{Y} \\
&= \esperof{X \cdot Y} - \esperof{X} \cdot \esperof{Y}
\end{align}

<p>
On voit √©galement que la variance d'une variable al√©atoire \(X\) n'est rien d'autre que sa covariance avec elle-m√™me¬†:
</p>

<p>
\[\var{X} = \cov{X}{X}\]
</p>
</div>


<div id="outline-container-orgc5c0a67" class="outline-4">
<h4 id="orgc5c0a67"><span class="section-number-4">1.9.1</span> Invariance sous translation</h4>
<div class="outline-text-4" id="text-1-9-1">
<p>
Suivant le m√™me raisonnement que pour la variance, on consid√®re les variables al√©atoires \(W,X,Y,Z\) reli√©es par¬†:
</p>

<div class="org-center">
<p>
\(
W = X + a \\
Z = Y + b
\)
</p>
</div>

<p>
o√π \(a,b \in \setR\). La covariance entre \(W\) et \(Z\) s'exprime alors¬†:
</p>

\begin{align}
\cov{W}{Z} &= \esperof{(W - \esperof{W})(Z - \esperof{Z})} \\
&= \esperof{(X + a - \esperof{X} - a)(Y + b - \esperof{Y} - b)} \\
&= \esperof{(X - \esperof{X})(Y - \esperof{Y})} \\
&= \cov{X}{Y}
\end{align}

<p>
La covariance est donc invariante sous translation¬†:
</p>

<p>
\[\cov{X+a}{Y+b} = \cov{X}{Y}\]
</p>
</div>
</div>
</div>


<div id="outline-container-orgdf9ac70" class="outline-3">
<h3 id="orgdf9ac70"><span class="section-number-3">1.10</span> Variance d'une combinaison lin√©aire</h3>
<div class="outline-text-3" id="text-1-10">
<p>
Nous utilisons la notation¬†:
</p>

<p>
\[X_0 = X - \esperof{X}\]
</p>

<p>
pour toute variable al√©atoire \(X\). Cette variables al√©atoire \(X_0\) a la propri√©t√©
d'avoir une esp√©rance nulle car¬†:
</p>

<p>
\[\esperof{X_0} = \esperof{X - \esperof{X} } = \esperof{X} - \esperof{X} = 0\]
</p>

<p>
La variance d'une telle variable peut s'√©crire¬†:
</p>

<p>
\[\var{X_0} = \esperof{X_0^2} - \esperof{X_0}^2 = \esperof{X_0^2}\]
</p>

<p>
Quant √† la covariance, elle s'√©crit¬†:
</p>

<p>
\[\cov{X_0}{Y_0} = \esperof{X_0 \ Y_0} - \esperof{X_0} \ \esperof{Y_0} = \esperof{X_0 \ Y_0}\]
</p>

<p>
Soit les r√©els \(a,b\). Par lin√©arit√© de l'esp√©rance, on a¬†:
</p>

<p>
\[\esperof{a \ X + b \ Y} = a \ \esperof{X} + b \ \esperof{Y}\]
</p>

<p>
La variance de la combinaison lin√©aire \(a \ X + b \ Y\) s'√©crit¬†:
</p>

\begin{align}
\var{a \ X + b \ Y} &= \esperof{(a \ X + b \ Y - \esperof{a \ X + b \ Y})^2} \\
&= \esperof{(a \ X + b \ Y - a \ \esperof{X} - b \ \esperof{Y})^2} \\
&= \esperof{(a \ X_0 + b \ Y_0)^2}
\end{align}

<p>
En d√©veloppant, on arrive √†¬†:
</p>

\begin{align}
\var{a \ X + b \ Y} &= \esperof{a^2 \ X_0^2 + 2 \ a \ b \ X_0 \ Y_0 + b^2 \ Y_0^2} \\
&= a^2 \ \esperof{X_0^2} + 2 \ a \ b \ \esperof{X_0 \ Y_0} + b^2 \ \esperof{Y_0^2}
\end{align}

<p>
et donc¬†:
</p>

<p>
\[\var{a \ X + b \ Y} = a^2 \ \var{X_0} + 2 \ a \ b \ \cov{X_0}{Y_0} + b^2 \ \var{Y_0}\]
</p>

<p>
L'invariance sous translation nous permet alors d'√©crire¬†:
</p>

<p>
\[\var{a \ X + b \ Y} = a^2 \ \var{X} + 2 \ a \ b \ \cov{X}{Y} + b^2 \ \var{Y}\]
</p>
</div>
</div>


<div id="outline-container-org37e46df" class="outline-3">
<h3 id="org37e46df"><span class="section-number-3">1.11</span> Produit scalaire</h3>
<div class="outline-text-3" id="text-1-11">
<p>
Nous allons voir que la covariance est un produit scalaire. Nous utilisons la notation¬†:
</p>

<p>
\[X_0 = X - \esperof{X}\]
</p>

<p>
pour toute variable al√©atoire \(X\). Cette variables al√©atoire \(X_0\) a la propri√©t√©
d'avoir une esp√©rance nulle car¬†:
</p>

<p>
\[\esperof{X_0} = \esperof{X - \esperof{X} } = \esperof{X} - \esperof{X} = 0\]
</p>

<p>
On en d√©duit que¬†:
</p>

<p>
\[\cov{X_0}{Y_0} = \esperof{X_0 \ Y_0} - \esperof{X_0} \ \esperof{Y_0} = \esperof{X_0 \ Y_0}\]
</p>

<p>
La sym√©trie est v√©rifi√©e¬†:
</p>

<p>
\[\cov{Y_0}{X_0} = \esperof{Y_0 \cdot X_0} = \esperof{X_0 \cdot Y_0} = \cov{X_0}{Y_0}\]
</p>

<p>
En ce qui concerne le caract√®re d√©fini positif, on a¬†:
</p>

<p>
\[\cov{X_0}{X_0} = \esperof{X_0^2} \ge 0\]
</p>

<p>
De plus, si \(X_0\) est tel que \(\cov{X_0}{X_0} = 0\), on a¬†:
</p>

<p>
\[\int_\Omega X_0^2 \ d\proba(\omega) = 0\]
</p>

<p>
ce qui entra√Æne la nullit√© essentielle \(X_0 \essegal 0\) sur \(\Omega\).
</p>

<p>
Soit les r√©els \(a,b\). On voit que la lin√©arit√© est bien respect√©e¬†:
</p>

\begin{align}
\cov{X_0}{a \ Y_0 + b \ Z_0} &= \esperof{X_0 \ (a \ Y_0 + b \ Z_0)} \\
&= a \ \esperof{X_0 \ Y_0} + b \ \esperof{X_0 \ Z_0} \\
&= a \ \cov{X_0}{Y_0} + b \ \cov{X_0}{Z_0}
\end{align}

<p>
Nous venons de montrer que la covariance est essentiellement un produit scalaire
pour toute variable al√©atoires √† esp√©rance nulles \(X_0, Y_0\). Comme la covariance
est invariante sous translation, on voit que¬†:
</p>

<p>
\[\cov{X}{Y} = \cov{X_0}{Y_0}\]
</p>

<p>
est √©galement un produit scalaire pour toutes variables al√©atoires \(X,Y\).
</p>
</div>


<div id="outline-container-orge61a535" class="outline-4">
<h4 id="orge61a535"><span class="section-number-4">1.11.1</span> Cauchy-Schwartz</h4>
<div class="outline-text-4" id="text-1-11-1">
<p>
En appliquant l'in√©galit√© de Cauchy-Schwartz √† ce produit scalaire, on obtient¬†:
</p>

<p>
\[\cov{X}{Y}^2 \le \cov{X}{X} \ \cov{Y}{Y} = \var{X} \ \var{Y}\]
</p>

<p>
o√π, en prenant la racine¬†:
</p>

<p>
\[\cov{X}{Y} \le \sqrt{\var{X} \ \var{Y}}\]
</p>
</div>
</div>
</div>


<div id="outline-container-org0055425" class="outline-3">
<h3 id="org0055425"><span class="section-number-3">1.12</span> Probabilit√© conditionnelle</h3>
<div class="outline-text-3" id="text-1-12">
<p>
\label{sec:proba_cond}
</p>

<p>
On d√©finit une nouvelle famille de probabilit√©s¬†:
</p>

<p>
\[\probaof{A | B} = \frac{ \probaof{A \cap B} }{ \probaof{B} }\]
</p>

<p>
o√π \(A,B\) sont des sous-ensembles quelconque de \(\Omega\), et o√π \(B\) est tel que¬†:
</p>

<p>
\[\probaof{B} > 0\]
</p>

<p>
Comme \(B \cap B = B\), on a¬†:
</p>

<p>
\[\probaof{ B | B } = 1\]
</p>

<p>
On est donc certain qu'un √©v√©nement de \(B\) va se produire. En fait, pour tout ensemble \(C\) tel que \(B \subseteq C\), on a \(C \cap B = B\) et¬†:
</p>

<p>
\[\probaof{ C | B } = 1\]
</p>

<p>
On d√©duit de l'in√©galit√©¬†:
</p>

<p>
\[\probaof{A \cap B} \le \probaof{B}\]
</p>

<p>
que¬†:
</p>

<p>
\[\probaof{A | B} \le 1\]
</p>

<p>
D'un autre cot√©, comme \(\probaof{B} \le 1\), on a¬†:
</p>

<p>
\[\probaof{A | B} \ge \probaof{A \cap B} \ge 0\]
</p>

<p>
L'additivit√© est √©galement satisfaite¬†:
</p>

\begin{align}
\probaof{ \cup_i A_i | B} &= \frac{ \probaof{(\cup_i A_i) \cap B} }{ \probaof{B} } \\
&= \frac{ \probaof{\cup_i (A_i \cap B)} }{ \probaof{B} } \\
&= \sum_i \frac{ \probaof{A_i \cap B} }{ \probaof{B} } = \sum_i \probaof{ A_i | B}
\end{align}

<p>
pour toute famille de \(A_i\) disjoints deux √† deux. Les fonctions¬†:
</p>

<p>
\[\proba_B\left[ A \right] = \probaof{A | B}\]
</p>

<p>
forment donc bien une famille de probabilit√©s. On dit que \(\probaof{A | B}\) est la probabilit√© conditionnelle de \(A\) sachant \(B\).
</p>

<p>
Lorsque \(B = \Omega\), on retrouve d'ailleurs¬†:
</p>

<p>
\[\probaof{A | \Omega} = \probaof{A}\]
</p>
</div>


<div id="outline-container-org34792ff" class="outline-4">
<h4 id="org34792ff"><span class="section-number-4">1.12.1</span> Ind√©pendance</h4>
<div class="outline-text-4" id="text-1-12-1">
<p>
On dit que deux ensembles d'√©v√©nements \(A\) et \(B\) sont ind√©pendants si¬†:
</p>

<p>
\[\probaof{A | B} = \probaof{A}\]
</p>

<p>
c'est-√†-dire si¬†:
</p>

<p>
\[\probaof{A \cap B} = \probaof{A} \cdot \probaof{B}\]
</p>
</div>
</div>


<div id="outline-container-orge3c387b" class="outline-4">
<h4 id="orge3c387b"><span class="section-number-4">1.12.2</span> Application</h4>
<div class="outline-text-4" id="text-1-12-2">
<p>
Une technique fr√©quemment employ√©e pour √©valuer \(\probaof{A}\) est d'utiliser
une partition \(B_1,...,B_n\) de \(\Omega\). Utilisant \(A = A \cup \Omega\), on a alors¬†:
</p>

<p>
\[\probaof{A} = \sum_i \probaof{A \cap B_i} = \sum_i \probaof{A | B_i} \cdot \probaof{B_i}\]
</p>
</div>
</div>
</div>


<div id="outline-container-orgb7f9dfd" class="outline-3">
<h3 id="orgb7f9dfd"><span class="section-number-3">1.13</span> Esp√©rance conditionnelle √† un ensemble</h3>
<div class="outline-text-3" id="text-1-13">
<p>
Soit \(A \subseteq \Omega\). On a vu que¬†:
</p>

<p>
\[\esperof{\indicatrice_A} = \probaof{A}\]
</p>

<p>
pour toute fonction indicatrice d'un sous-ensemble \(A\) de \(\Omega\). Par analogie, on aimerait bien obtenir une expression d'une esp√©rance conditionnelle v√©rifiant¬†:
</p>

<p>
\[\esperof{\indicatrice_A | B} = \probaof{A | B}\]
</p>

<p>
pour un ensemble \(B \subseteq \Omega\) donn√© v√©rifiant \(\probaof{B} > 0\).
</p>

<p>
Soit \(\Omega_1, ..., \Omega_N\) une partition de \(\Omega\) et \(Z\) une variable al√©atoire en escalier¬†:
</p>

<p>
\[Z(\omega) = \sum_i Z_i \ \indicatrice_{\Omega_i}(\omega)\]
</p>

<p>
On voit que¬†:
</p>

\begin{align}
\esperof{Z | B} &= \sum_i Z_i\ \esperof{\indicatrice_{\Omega_i} | B} \\
&= \sum_i Z_i\ \probaof{\Omega_i | B}
\end{align}

<p>
Or¬†:
</p>

<p>
\[\probaof{\Omega_i | B} = \frac{ \probaof{\Omega_i \cap B} }{ \probaof{B} }\]
</p>

<p>
On a donc¬†:
</p>

<p>
\[\esperof{Z | B} = \unsur{ \probaof{B} } \sum_i Z_i\ \probaof{\Omega_i \cap B}\]
</p>

<p>
Consid√©rons la nouvelle partition¬†:
</p>

<div class="org-center">
<p>
\(
\Phi_i^+ = \Omega_i \cap B \\
\Phi_i^- = \Omega_i \cap (\Omega \setminus B)
\)
</p>
</div>

<p>
Comme \(\Phi_i^+ \cup \Phi_i^- = \Omega_i\), on a clairement \(\indicatrice_{\Phi_i^+} + \indicatrice_{\Phi_i^-} = \indicatrice_{\Omega_i}\) et on peut r√©exprimer \(Z\) comme¬†:
</p>

<p>
\[Z(\omega) = \sum_i Z_i\ \indicatrice_{\Phi_i^+}(\omega) + \sum_i Z_i\ \indicatrice_{\Phi_i^-}(\omega)\]
</p>

<p>
L'expression de l'esp√©rance conditionelle devient¬†:
</p>

<p>
\[\esperof{Z | B} = \unsur{ \probaof{B} } \left[ \sum_i Z_i\ \probaof{\Phi_i^+ \cap B} + \sum_i Z_i\ \probaof{\Phi_i^- \cap B} \right]\]
</p>

<p>
Remarquons que par construction¬†:
</p>

<div class="org-center">
<p>
\(
\Phi_i^+ \cap B = \Phi_i^+ \\
\Phi_i^- \cap B = \emptyset
\)
</p>
</div>

<p>
Par cons√©quent, les termes en \(\probaof{\Phi_i^- \cap B}\) s'annulent et on a¬†:
</p>

<p>
\[\esperof{Z | B} = \unsur{ \probaof{B} } \sum_i Z_i\ \probaof{\Phi_i^+}\]
</p>

<p>
Mais comme \(\bigcup_i \Phi_i^+ = B\), les \(\Phi_i^+\) forment une partition de \(B\) et on peut √©crire cette expression sous la forme int√©grale¬†:
</p>

<p>
\[\esperof{Z | B} = \frac{ \int_B Z\ \ d\proba }{ \int_B \ d\proba }\]
</p>

<p>
Comme cette relation doit √™tre valable pour toute variable al√©atoire en escalier \(Z\), elle l'est √©galement pour une variable al√©atoire quelconque \(X\)¬†:
</p>

<p>
\[\esperof{X | B} = \frac{ \int_B X\ \ d\proba }{ \int_B \ d\proba }\]
</p>
</div>


<div id="outline-container-org8a74b7e" class="outline-4">
<h4 id="org8a74b7e"><span class="section-number-4">1.13.1</span> Densit√© conditionnelle</h4>
<div class="outline-text-4" id="text-1-13-1">
<p>
Soient \(X,Y\) deux variables al√©atoires. Un cas particulier important d'esp√©rance conditionnelle est celui o√π¬†:
</p>

<p>
\[B_y = \{ \omega : Y(\omega) = y \}\]
</p>

<p>
On note alors¬†:
</p>

<p>
\[\esperof{X | Y = y} = \esperof{X | B_y}\]
</p>

<p>
On remarque que¬†:
</p>

<p>
\[(X,Y)(B_y) = \{ (x,y) \in \setR^2 : x \in \setR \}\]
</p>

<p>
Par cons√©quent, si il existe une fonction densit√© \(f_{X,Y}\) associ√©e √† \(X,Y\), on peut √©crire¬†:
</p>

\begin{align}
\int_{B_y} X \ d\proba &= \int_{(X,Y)(B_y)} x \ f_{X,Y}(x,y) \ dx \ dy \\
&= \int_\setR x \ f_{X,Y}(x,y) \ dx
\end{align}

<p>
ainsi que¬†:
</p>

<p>
\[\int_{B_y} \ d\proba = \int_\setR f_{X,Y}(x,y) \ dx\]
</p>

<p>
L'esp√©rance conditionnelle s'√©crit alors¬†:
</p>

<p>
\[\esperof{X | Y = y} = \frac{\int_\setR x \ f_{X,Y}(x,y) \ dx}{\int_\setR f_{X,Y}(x,y) \ dx}\]
</p>

<p>
Donc, si on d√©finit¬†:
</p>

<p>
\[f_{X | Y}(x,y) = \frac{f_{X,Y}(x,y)}{ \int_\setR f_{X,Y}(x,y) \ dx}\]
</p>

<p>
on a tout simplement¬†:
</p>

<p>
\[\esperof{X | Y = y} = \int_\setR x \ f_{X | Y}(x,y) \ dx\]
</p>
</div>
</div>
</div>


<div id="outline-container-org6709ab9" class="outline-3">
<h3 id="org6709ab9"><span class="section-number-3">1.14</span> Esp√©rance conditionnelle √† une tribu</h3>
<div class="outline-text-3" id="text-1-14">
</div>
<div id="outline-container-org1dee003" class="outline-4">
<h4 id="org1dee003"><span class="section-number-4">1.14.1</span> Tribu et espace fonctionnel</h4>
<div class="outline-text-4" id="text-1-14-1">
<p>
Soit \(\Gamma \subseteq \sousens(\Omega)\) une collection de sous-ensembles de \(\Omega\) formant une tribu sur \(\Omega\) (voir section¬†\ref{sec:tribu}),
et \(\mathcal{F}(\Gamma)\) l'ensemble des variables al√©atoires \(W\) telles que¬†:
</p>

<p>
\[\Lambda(W) \subseteq \Gamma\]
</p>

<p>
o√π \(\Lambda(W)\) est la collection induite par \(W\).
</p>
</div>
</div>


<div id="outline-container-org62af977" class="outline-4">
<h4 id="org62af977"><span class="section-number-4">1.14.2</span> Minimisation</h4>
<div class="outline-text-4" id="text-1-14-2">
<p>
L'esp√©rance conditionnelle est construite comme le meilleur estimateur au sens des moindres carr√©s d'une variable al√©atoire \(X\) sur \(\mathcal{F}(\Gamma)\). Soit la fonctionnelle \(I : \mathcal{F}(\Gamma) \mapsto \setR\) repr√©sentant l'erreur¬†:
</p>

<p>
\[I(Z) = \int_\Omega \left[ Z(\omega) - X(\omega) \right]^2 \ d\proba(\omega)\]
</p>

<p>
Nous allons minimiser \(I\) sur \(\mathcal{F}(\Gamma)\). Pour ce faire, on utilise la technique du calcul variationnel (voir chapitre¬†\ref{chap:varia}). On commence par d√©finir¬†:
</p>

<p>
\[J_W(\epsilon) = I(Z^* + \epsilon W) = \int_\Omega (Z^* + \epsilon W - X)^2 \ d\proba\]
</p>

<p>
o√π la variable al√©atoire \(Z^*\) est l'optimum recherch√©, et o√π \(W \in \mathcal{F}(\Gamma)\), \(\epsilon \in \setR\). La d√©riv√©e s'√©crit¬†:
</p>

<p>
\[\OD{J_W}{\epsilon}(\epsilon) = \int_\Omega 2 (Z^* +\epsilon W - X) W \ d\proba = 0\]
</p>

<p>
Comme celle-ci doit s'annuler en \(\epsilon = 0\), on a¬†:
</p>

<p>
\[\OD{J_W}{\epsilon}(0) = \int_\Omega 2 (Z^* - X) W \ d\proba = 0\]
</p>

<p>
Autrement dit¬†:
</p>

<p>
\[\int_\Omega W Z^* \ d\proba = \int_\Omega W X \ d\proba\]
</p>

<p>
√©quation qui doit √™tre v√©rifi√©e pour tout \(W \in \mathcal{F}(\Gamma)\).
</p>
</div>
</div>


<div id="outline-container-org8ba6e3f" class="outline-4">
<h4 id="org8ba6e3f"><span class="section-number-4">1.14.3</span> Unicit√©</h4>
<div class="outline-text-4" id="text-1-14-3">
<p>
Nous supposons dor√©navant que \(\mathcal{F}(\Gamma)\) est un espace vectoriel. Soient \(Z_1, Z_2 \in \mathcal{F}(\Gamma)\) des variables al√©atoires qui minimisent tous deux la fonctionnelle \(I\). On a¬†:
</p>

<p>
\[\int_\Omega W Z_1 \ d\proba = \int_\Omega W Z_2 \ d\proba = \int_\Omega W X \ d\proba\]
</p>

<p>
pour tout \(W \in \mathcal{F}(\Gamma)\). Donc¬†:
</p>

<p>
\[\int_\Omega W (Z_1 - Z_2) \ d\proba = 0\]
</p>

<p>
Mais comme \(Z_1 - Z_2 \in \mathcal{F}(\Gamma)\), il suffit de consid√©rer le cas \(W = Z_1 - Z_2\) pour avoir¬†:
</p>

<p>
\[\int_\Omega (Z_1 - Z_2)^2 \ d\proba = 0\]
</p>

<p>
On en conclut que \(Z_1 = Z_2\) presque partout sur \(\Omega\). L'esp√©rance conditionnelle est donc unique pour \(X\) et \(\Gamma\) donn√©s.
</p>
</div>
</div>


<div id="outline-container-org04a9897" class="outline-4">
<h4 id="org04a9897"><span class="section-number-4">1.14.4</span> D√©finition</h4>
<div class="outline-text-4" id="text-1-14-4">
<p>
Forts de ces r√©sultats, on d√©finit l'esp√©rance de \(X\) conditionnellement √† la tribu \(\Gamma\) comme √©tant¬†:
</p>

<p>
\[\esperof{X | \Gamma} = \arg\min_{Z \in \mathcal{F}(\Gamma) } \int_{\Omega} \left[ Z - X \right]^2 \ d\proba\]
</p>

<p>
On a donc¬†:
</p>

<p>
\[\int_\Omega W\ \esperof{X | \Gamma} \ d\proba = \int_\Omega W\ X \ d\proba\]
</p>

<p>
pour tout \(W \in \mathcal{F}(\Gamma)\).
</p>
</div>
</div>


<div id="outline-container-org93663e3" class="outline-4">
<h4 id="org93663e3"><span class="section-number-4">1.14.5</span> Fonctions indicatrices</h4>
<div class="outline-text-4" id="text-1-14-5">
<p>
Soit un ensemble \(\Phi \in \Gamma\). Les propri√©t√©s de \(\Gamma\) nous disent que \(\Omega \setminus \Phi \in \Gamma\). Donc¬†:
</p>

<p>
\[\Lambda(\indicatrice_\Phi) = \{ \emptyset, \Omega, \Phi, \Omega \setminus \Phi \} \subseteq \Gamma\]
</p>

<p>
et \(\indicatrice_\Phi \in \mathcal{F}(\Gamma)\). On en d√©duit que¬†:
</p>

<p>
\[\int_\Omega \indicatrice_\Phi \ \esperof{X | \Gamma} \ d\proba = \int_\Omega \indicatrice_\Phi \ X \ d\proba\]
</p>

<p>
c'est-√†-dire¬†:
</p>

<p>
\[\int_\Phi \esperof{X | \Gamma} \ d\proba = \int_\Phi X \ d\proba\]
</p>

<p>
pour tout \(\Phi \in \Gamma\).
</p>

<p>
Comme \(\Omega \in \Gamma\), on a en particulier¬†:
</p>

<p>
\[\int_\Omega \esperof{X | \Gamma} \ d\proba = \int_\Omega X \ d\proba\]
</p>

<p>
c'est-√†-dire¬†:
</p>

<p>
\[\esperof{ \esperof{X | \Gamma} } = \esperof{X}\]
</p>
</div>
</div>


<div id="outline-container-orgea29a0f" class="outline-4">
<h4 id="orgea29a0f"><span class="section-number-4">1.14.6</span> Variable al√©atoire dans l'espace fonctionnel</h4>
<div class="outline-text-4" id="text-1-14-6">
<p>
Une cons√©quence directe de la d√©finition de l'esp√©rance conditionnelle est que si \(Z \in \mathcal{F}(\Gamma)\), on a¬†:
</p>

<p>
\[\int_\Omega (Z - Z)^2 \ d\proba = 0\]
</p>

<p>
Par cons√©quent, \(Z\) minimise la fonctionnelle¬†:
</p>

<p>
\[I(Y) = \int_\Omega (Y - Z)^2 \ d\proba \ge 0\]
</p>

<p>
sur \(\mathcal{F}(\Gamma)\) et¬†:
</p>

<p>
\[\esperof{Z | \Gamma} = Z\]
</p>
</div>
</div>


<div id="outline-container-orgde4ec02" class="outline-4">
<h4 id="orgde4ec02"><span class="section-number-4">1.14.7</span> Tour</h4>
<div class="outline-text-4" id="text-1-14-7">
<p>
Soit la tribu \(\Delta \subseteq \Gamma\) et \(X\) une variable al√©atoire et \(W \in \mathcal{F}(\Delta)\). On a¬†:
</p>

<p>
\[\Lambda(W) \subseteq \Delta \subseteq \Gamma\]
</p>

<p>
Par cons√©quent \(W \in \mathcal{F}(\Gamma)\) et les √©quations suivantes sont v√©rifi√©es¬†:
</p>

<div class="org-center">
<p>
\(
\int_\Omega W\ \esperof{X | \Delta} \ d\proba = \int_\Omega W\ X \ d\proba \\
\int_\Omega W\ \esperof{X | \Gamma} \ d\proba = \int_\Omega W\ X \ d\proba
\)
</p>
</div>

<p>
On en d√©duit que¬†:
</p>

<p>
\[\int_\Omega W\ \esperof{X | \Delta} \ d\proba = \int_\Omega W\ \esperof{X | \Gamma} \ d\proba\]
</p>

<p>
Comme cette derni√®re √©quation est valable pour tout \(W \in \mathcal{F}(\Delta)\), on en d√©duit que \(\esperof{X | \Delta}\) est le meilleur estimateur de \(\esperof{X | \Gamma}\) sur \(\mathcal{F}(\Delta)\). Ce qui revient √† dire que¬†:
</p>

<p>
\[\esperof{ \esperof{X | \Gamma} | \Delta } = \esperof{X | \Delta}\]
</p>
</div>
</div>


<div id="outline-container-org900c162" class="outline-4">
<h4 id="org900c162"><span class="section-number-4">1.14.8</span> Couple de variables al√©atoires</h4>
<div class="outline-text-4" id="text-1-14-8">
<p>
Etant donn√© deux variables al√©atoires \(X,Y\), on d√©finit¬†:
</p>

<p>
\[\esperof{X | Y} = \esperof{X | \Lambda(Y)}\]
</p>

<p>
Comme \(\Gamma = \Lambda(Y)\), l'espace \(\mathcal{F}(\Gamma)\) est l'ensemble des variables al√©atoires \(W\) telles que¬†:
</p>

<p>
\[\Lambda(W) \subseteq \Lambda(Y)\]
</p>
</div>
</div>
</div>


<div id="outline-container-org933cacc" class="outline-3">
<h3 id="org933cacc"><span class="section-number-3">1.15</span> Ensemble discret</h3>
<div class="outline-text-3" id="text-1-15">
<p>
Nous allons √† pr√©sent consid√©rer le cas particulier o√π l'ensemble des √©v√©nements peut s'√©crire
comme¬†:
</p>

<p>
\[\Omega = \{ \omega_i : i \in \setN \}\]
</p>

<p>
Nous notons \(p_i\) les probabilit√©s associ√©es aux singletons¬†:
</p>

<p>
\[p_i = \probaof{ \{\omega_i\} }\]
</p>

<p>
√âtant donn√©e une variable al√©atoire \(X\), on note¬†:
</p>

<p>
\[x_i = X(\omega_i)\]
</p>

<p>
L'esp√©rance d'une telle variable s'√©crit simplement¬†:
</p>

<p>
\[\esperof{X} = \sum_i x_i \ p_i\]
</p>
</div>
</div>
</div>

<div id="outline-container-org62ed2a9" class="outline-2">
<h2 id="org62ed2a9"><span class="section-number-2">2</span> Statistiques</h2>
<div class="outline-text-2" id="text-2">
<div id="text-table-of-contents">
<ul>
<li><a href="#org3a647a7">2.1. Echantillons</a></li>
<li><a href="#org73ad8ed">2.2. L'in√©galit√© de Markov</a></li>
<li><a href="#org7e3d56c">2.3. La loi des grands nombres</a></li>
<li><a href="#orgb2f8276">2.4. Fr√©quence et probabilit√©</a></li>
<li><a href="#org3b164ae">2.5. Estimateurs non biais√©s</a></li>
<li><a href="#orgc86fb1e">2.6. Estimation des esp√©rance et des variances</a></li>
<li><a href="#org920663d">2.7. Maximum de vraisemblance</a></li>
<li><a href="#org466b620">2.8. Echantillon de densit√© donn√©e</a></li>
</ul>
</div>

<p>
\label{chap:stat}
</p>
</div>


<div id="outline-container-orge89d8f3" class="outline-4">
<h4 id="orge89d8f3"><span class="section-number-4">2.0.1</span> Ind√©pendance</h4>
<div class="outline-text-4" id="text-2-0-1">
<p>
On dit que les variables al√©atoires \(X_1\), \(X_2\), &#x2026;, \(X_N\) sont ind√©pendantes si¬†:
</p>

<p>
\[\esperof{\prod_i X_i} = \prod_i \esperof{X_i}\]
</p>

<p>
On en d√©duit que¬†:
</p>

<p>
\[\cov{X_i}{X_j} = \var{X_i} \ \indicatrice_{ij}\]
</p>

<p>
et donc¬†:
</p>

<p>
\[\var{\sum_i X_i} = \sum_i \var{X_i}\]
</p>
</div>
</div>


<div id="outline-container-org3a647a7" class="outline-3">
<h3 id="org3a647a7"><span class="section-number-3">2.1</span> Echantillons</h3>
<div class="outline-text-3" id="text-2-1">
<p>
Nous nous int√©ressons dans la suite de ce chapitre √† des √©chantillons de \(N\) variables al√©atoires ind√©pendantes \(X_1,...,X_N\) telles que¬†:
</p>

<div class="org-center">
<p>
\(
\esperof{X_i} = \mu \\
\cov{X_i}{X_j} = \sigma \ \indicatrice_{ij}
\)
</p>
</div>
</div>
</div>


<div id="outline-container-org73ad8ed" class="outline-3">
<h3 id="org73ad8ed"><span class="section-number-3">2.2</span> L'in√©galit√© de Markov</h3>
<div class="outline-text-3" id="text-2-2">
<p>
Soit une variable al√©atoire \(X\). On d√©finit la variable associ√©e¬†:
</p>

<div class="org-center">
<p>
\(
Y =
\begin{cases}
a^2 & \mbox{ si } \abs{X-b} \ge a \\
0 & \mbox{ si } \abs{X-b} < a
\end{cases} \\
\)
</p>
</div>

<p>
Comme¬†:
</p>

<p>
\[Y \le (X-b)^2\]
</p>

<p>
on a \(\esperof{Y} \le \esperof{(X-b)^2}\). D'un autre cot√©¬†:
</p>

<p>
\[\esperof{Y} = a^2 \ \probaof{\abs{X-b} \ge a}\]
</p>

<p>
Rassemblant ces deux r√©sultats, on obtient la propri√©t√©¬†:
</p>

<p>
\[\probaof{\abs{X-b} \ge a} \le \unsur{a^2} \ \esperof{(X-b)^2}\]
</p>

<p>
connue sous le nom d'in√©galit√© de Markov.
</p>

<p>
Le cas particulier \(b = \esperof{X}\) nous donne¬†:
</p>

<p>
\[\probaof{\abs{X-\esperof{X}} \ge a} \le \unsur{a^2} \ \var{X}\]
</p>
</div>
</div>


<div id="outline-container-org7e3d56c" class="outline-3">
<h3 id="org7e3d56c"><span class="section-number-3">2.3</span> La loi des grands nombres</h3>
<div class="outline-text-3" id="text-2-3">
<p>
Soit la moyenne¬†:
</p>

<p>
\[M_N = \unsur{N} \sum_{i=1}^N X_i\]
</p>

<p>
On a¬†:
</p>

<p>
\[\esperof{M_N} = \unsur{N} \ N \ \mu = \mu\]
</p>

<p>
L'ind√©pendance entre les variables nous am√®ne √†¬†:
</p>

\begin{align}
\var{M_N} &= \unsur{N^2} \var{\sum_i X_i} \\
&= \unsur{N^2} \sum_i \var{X_i} \\
&= \unsur{N^2} \ N \ \sigma^2
\end{align}

<p>
et donc¬†:
</p>

<p>
\[\var{M_N} = \frac{\sigma^2}{N}\]
</p>

<p>
Soit \(a > 0\). L'in√©galit√© de Markov nous dit que¬†:
</p>

<p>
\[\probaof{\abs{M_N - \mu} \ge a} \le \frac{\sigma^2}{a^2 N}\]
</p>

<p>
Soit √† pr√©sent \(\epsilon > 0\). Si on veut¬†:
</p>

<p>
\[\probaof{\abs{M_N - \mu} \ge a} \le \frac{\sigma^2}{a^2 N} \strictinferieur \epsilon\]
</p>

<p>
il suffit de choisir¬†:
</p>

<p>
\[N > \frac{\sigma^2}{a^2 \epsilon}\]
</p>

<p>
On en conclut que¬†:
</p>

<p>
\[\lim_{N \to +\infty} \probaof{M_N = \mu} = 1\]
</p>
</div>
</div>


<div id="outline-container-orgb2f8276" class="outline-3">
<h3 id="orgb2f8276"><span class="section-number-3">2.4</span> Fr√©quence et probabilit√©</h3>
<div class="outline-text-3" id="text-2-4">
<p>
Appliquons la loi des grands nombres √† la fonction indicatrice \(\indicatrice_A\).
On a alors \(X_i = 1\) lorsque \(\omega \in A\) et \(X_i = 0\) lorsque \(\omega \notin A\).
La moyenne s'√©crit donc¬†:
</p>

<p>
\[M_N = \frac{n(A)}{N}\]
</p>

<p>
o√π \(n(A)\) est le nombre de \(X_i\) valant 1, autrement dit le nombre d'√©v√©nements
\(\omega\) appartenant √† \(A\). Comme¬†:
</p>

<p>
\[\mu = \esperof{\indicatrice_A} = \probaof{A}\]
</p>

<p>
on en d√©duit que la fr√©quence \(n(A) / N\) converge vers la probabilit√© de \(A\)¬†:
</p>

<p>
\[\lim_{N \to +\infty} \probaof{\frac{n(A)}{N} = \probaof{A}} = 1\]
</p>
</div>
</div>


<div id="outline-container-org3b164ae" class="outline-3">
<h3 id="org3b164ae"><span class="section-number-3">2.5</span> Estimateurs non biais√©s</h3>
<div class="outline-text-3" id="text-2-5">
<p>
Soit une fonction \(G : \setR^n \mapsto \setR\)¬†:
</p>

<p>
\[G : (X_1,...,X_N) \mapsto G(X_1,...,X_N)\]
</p>

<p>
On dit que \(\hat{G} : \setR^n \mapsto \setR\) est un estimateur non biais√© de \(G\) si¬†:
</p>

<p>
\[\esperof{\hat{G}} = \esperof{G}\]
</p>
</div>
</div>


<div id="outline-container-orgc86fb1e" class="outline-3">
<h3 id="orgc86fb1e"><span class="section-number-3">2.6</span> Estimation des esp√©rance et des variances</h3>
<div class="outline-text-3" id="text-2-6">
<p>
Soit¬†:
</p>

<p>
\[M_N(X_1,...,X_N) = \unsur{N} \sum_{i=1}^N X_i\]
</p>

<p>
La loi des grands nombres nous dit que¬†:
</p>

<p>
\[\esperof{M_N} = \mu\]
</p>

<p>
La moyenne \(M_N\) est donc un estimateur non biais√© de l'esp√©rance \(\mu\).
</p>

<p>
Soit les variables √† esp√©rances nulles¬†:
</p>

<div class="org-center">
<p>
\(
X_i^* = X_i - \mu \\
M_N^* = M_N - \mu
\)
</p>
</div>

<p>
On obtient directement¬†:
</p>

<p>
\[M_N^* = \unsur{N} \sum_i X_i^*\]
</p>

<p>
On voit √©galement que¬†:
</p>

<p>
\[X_i - M_N = X_i - \mu + \mu - M_N = X_i^* - M_N^*\]
</p>

<p>
Donc¬†:
</p>

<p>
\[\esperof{\sum_i (X_i - M_N)^2} = \esperof{\sum_i \left( X_i^* - M_N^* \right)^2}\]
</p>

<p>
En d√©veloppant, on obtient successivement¬†:
</p>

\begin{align}
\esperof{\sum_i (X_i - M_N)^2} &= \esperof{ \sum_i \left( X_i^* \right)^2 } - 2 \ \esperof{M_N^* \sum_i X_i^*} + \esperof{ \left( M_N^* \right)^2 } \\
&= \sum_i \esperof{ \left( X_i^* \right)^2 } - 2 \ N \ \esperof{ \left( M_N^* \right)^2 } + \esperof{ \left( M_N^* \right)^2 }
\end{align}

<p>
Mais comme¬†:
</p>

<div class="org-center">
<p>
\(
\var{M_N^*} = \esperof{ \left( M_N^* \right)^2 } = \frac{\sigma^2}{N} \\
\esperof{ \left( X_i^* \right)^2 } = \var{X_i} = \sigma^2
\)
</p>
</div>

<p>
l'expression devient¬†:
</p>

<p>
\[\esperof{\sum_i (X_i - M_N)^2} = (N - 2 + 1) \ \sigma^2 = (N-1) \ \sigma^2\]
</p>

<p>
On en conclut que¬†:
</p>

<p>
\[S^2 = \unsur{N-1} \sum_{i=1}^{N} (X_i - M_N)^2\]
</p>

<p>
est un estimateur non biais√© de la variance¬†:
</p>

<p>
\[\esperof{S^2} = \sigma^2\]
</p>
</div>
</div>


<div id="outline-container-org920663d" class="outline-3">
<h3 id="org920663d"><span class="section-number-3">2.7</span> Maximum de vraisemblance</h3>
<div class="outline-text-3" id="text-2-7">
<p>
Il s'agit de trouver les param√®tres \(\hat{\theta}\) (esp√©rance, variance, &#x2026;) qui maximisent la vraisemblance¬†:
</p>

<p>
\[V(\hat{\theta}) = \prod_i  \probaof{ \{\omega : X_i(\omega) = x_i \} | \theta = \hat{\theta} }\]
</p>

<p>
Notons que cela revient √† maximiser¬†:
</p>

<p>
\[\ln\prod_{i=1}^N  \probaof{ \{\omega : X_i(\omega) = x_i \} | \theta = \hat{\theta} } = \sum_{i=1}^N  \ln\probaof{ \{\omega : X_i(\omega) = x_i \} | \theta = \hat{\theta} }\]
</p>

<p>
ce qui est souvent plus facile.
</p>

<p>
En pratique, lorsque la fonction de densit√© \(f_\theta\) est connue, on maximise¬†:
</p>

<p>
\[\phi(\theta) = \sum_i \ln f_\theta(x_i)\]
</p>

<p>
en imposant¬†:
</p>

<p>
\[\deriveepartielle{\phi}{\theta}(\hat{\theta}) = 0\]
</p>
</div>
</div>


<div id="outline-container-org466b620" class="outline-3">
<h3 id="org466b620"><span class="section-number-3">2.8</span> Echantillon de densit√© donn√©e</h3>
<div class="outline-text-3" id="text-2-8">
<p>
Il s'agit d'un algorithme permettant de g√©n√©rer \(N\) nombres al√©atoires¬†:
</p>

<p>
\[\{ x_1, ..., x_N \}\]
</p>

<p>
suivant la densit√© \(f\). Soit \(\epsilon \ge 0\) une erreur maximale et \([a,b]\) tel que :
</p>

<p>
\[\int_a^b f(x) \ dx = 1 - \epsilon\]
</p>

<p>
Soit :
</p>

<p>
\[M = \sup_{x \in [a,b]} f(x)\]
</p>

<p>
et la g√©n√©ratrice¬†:
</p>

<p>
\[\rand(a,b)\]
</p>

<p>
qui renvoie des variables al√©atoires de densit√© uniforme sur \([a,b]\).
</p>

<p>
On part de \(A_0 = \emptyset\). A chaque it√©ration, on g√©n√©re deux nombres de densit√©s uniformes¬†:
</p>

<div class="org-center">
<p>
\(
x = \rand(a,b) \\
y = \rand(0,M)
\)
</p>
</div>

<p>
Afin de modifier cette densit√©, on n'ajoute \(x\) √† la liste d√©j√† obtenue¬†:
</p>

<p>
\[A_i = A_{i-1} \cup \{ x \}\]
</p>

<p>
que si \(y < f(x)\). Autrement, on ne fait rien et on passe √† l'it√©ration suivante.
</p>

<p>
La comparaison de \(y\) et de \(f(x)\) sert donc de filtre √† l'algorithme.
</p>
</div>
</div>
</div>


<div id="outline-container-org251e012" class="outline-2">
<h2 id="org251e012"><span class="section-number-2">3</span> Calcul stochastique</h2>
<div class="outline-text-2" id="text-3">
<div id="text-table-of-contents">
<ul>
<li><a href="#org2642d4a">3.1. Processus stochastique</a></li>
<li><a href="#org36c2bf0">3.2. Int√©grale d'Ito</a></li>
<li><a href="#orgf7c5831">3.3. Variation quadratique</a></li>
<li><a href="#orgb1e262b">3.4. Variation conjointe</a></li>
<li><a href="#orgc9fe1f8">3.5. Relations variations quadratiques - conjointes</a></li>
<li><a href="#org11a5992">3.6. Variation d'ordre quelconque</a></li>
<li><a href="#org22a7a8d">3.7. Calcul d'Ito</a></li>
<li><a href="#org903e266">3.8. Mouvement Brownien</a></li>
</ul>
</div>

<p>
\label{chap:stocha}
</p>
</div>


<div id="outline-container-org2642d4a" class="outline-3">
<h3 id="org2642d4a"><span class="section-number-3">3.1</span> Processus stochastique</h3>
<div class="outline-text-3" id="text-3-1">
<p>
Un processus stochastique est une fonction¬†:
</p>

<p>
\[X : [0,+\infty) \times \Omega \mapsto \setR, \quad (t,\omega) \mapsto X(t,\omega)\]
</p>

<p>
On sous-entend souvent l'√©v√©nement \(\omega\), et on note \(X(t)=X(t,\omega)\).
</p>
</div>
</div>


<div id="outline-container-org36c2bf0" class="outline-3">
<h3 id="org36c2bf0"><span class="section-number-3">3.2</span> Int√©grale d'Ito</h3>
<div class="outline-text-3" id="text-3-2">
<p>
Il s'agit d'une int√©grale utilisant un processus stochastique \(X\) comme mesure¬†:
</p>

<p>
\[I(t) = \int_0^t f(s) \ dX(s) = \lim_{\delta \to 0} \sum_k f(t_k) (X(t_{k+1}) - X(t_k))\]
</p>
</div>
</div>


<div id="outline-container-orgf7c5831" class="outline-3">
<h3 id="orgf7c5831"><span class="section-number-3">3.3</span> Variation quadratique</h3>
<div class="outline-text-3" id="text-3-3">
<p>
Soit \(\delta \strictsuperieur 0\) et les \(N\) temps \(t_k = k \cdot \delta\) o√π \(k = 0,...,\arrondisup{\frac{T}{\delta}}\). On d√©finit la variation quadratique d'une fonction \(f\)¬†:
</p>

<p>
\[\variation{f}(T) = \lim_{\delta \to 0} \sum_k (f(t_{k+1}) - f(t_k))^2\]
</p>

<p>
Si la d√©riv√©e de \(f\) existe, la variation quadratique s'annule car¬†:
</p>

<p>
\[(f(t_{k+1}) - f(t_k))^2 \to \delta^2 \ \OD{f}{t}(t_k)^2\]
</p>

<p>
Comme \(\delta^2 \to \delta \ ds\), on a¬†:
</p>

<p>
\[\variation{f}(T) = \lim_{\delta \to 0} \delta \int_0^T \left(\OD{f}{t}(s)\right)^2 ds = 0\]
</p>
</div>
</div>


<div id="outline-container-orgb1e262b" class="outline-3">
<h3 id="orgb1e262b"><span class="section-number-3">3.4</span> Variation conjointe</h3>
<div class="outline-text-3" id="text-3-4">
<p>
Consid√©rons maintenant deux processus stochastiques \(X,Y\). Nous d√©finissons la variation conjointe \(\variation{X,Y}\)¬†:
</p>

<p>
\[\variation{X,Y}(T) = \lim_{\delta \to 0} \sum_k (X(t_{k+1}) - X(t_k)) \ (Y(t_{k+1}) - Y(t_k))\]
</p>

<p>
Dans le cas o√π les d√©riv√©es de \(X\) et de \(Y\) existent, on a √©videmment : \(\variation{X,Y} = 0\).
</p>

<p>
Pour une fonction \(f : \setR^2 \mapsto \setR\) quelconque, nous avons¬†:
</p>

<p>
\[df(X,Y) = f(X + \ dX,Y + dY) - f(X,Y)\]
</p>

<p>
Dans le cas particulier o√π \(f(X,Y)=X \cdot Y\), cette expression se r√©duit √†¬†:
</p>

\begin{align}
d(X \cdot Y) &= (X + \ dX) \cdot (Y + dY) - X \cdot Y \\
&= \ dX \cdot Y + X \cdot dY + \ dX \cdot dY
\end{align}

<p>
Mais comme¬†:
</p>

<p>
\[\variation{X,Y}(t) = \int_0^t \ dX \cdot dY\]
</p>

<p>
on a en d√©finitive¬†:
</p>

\begin{align}
X(t) Y(t) - X(0) Y(0) &= \int_0^t d(X Y)(s) \\
&= \int_0^t X(s) \ dY(s) + \int_0^t Y(s) \ dX(s) + \variation{X,Y}(t)
\end{align}
</div>
</div>


<div id="outline-container-orgc9fe1f8" class="outline-3">
<h3 id="orgc9fe1f8"><span class="section-number-3">3.5</span> Relations variations quadratiques - conjointes</h3>
<div class="outline-text-3" id="text-3-5">
<p>
La d√©finition nous donne directement¬†:
</p>

<p>
\[\variation{X} = \variation{X,X}\]
</p>

<p>
On peut aussi v√©rifier que¬†:
</p>

<p>
\[(X + Y)^2 - (X - Y)^2 = 4 \ X \ Y\]
</p>

<p>
d'o√π l'on d√©duit¬†:
</p>

<p>
\[\variation{X,Y} = \unsur{4} ( \variation{X + Y} - \variation{X - Y} )\]
</p>
</div>
</div>


<div id="outline-container-org11a5992" class="outline-3">
<h3 id="org11a5992"><span class="section-number-3">3.6</span> Variation d'ordre quelconque</h3>
<div class="outline-text-3" id="text-3-6">
<p>
Soit \(\delta \strictsuperieur 0\) et les temps \(t_k = k \delta\) o√π \(k = 0,...,\arrondisup{\frac{T}{\delta}}\). On d√©finit la variation d'ordre \(n\) d'une fonction \(f\)¬†:
</p>

<p>
\[\variation{f}^n(T) = \lim_{\delta \to 0} \sum_k (f(t_{k+1}) - f(t_k))^n\]
</p>
</div>
</div>


<div id="outline-container-org22a7a8d" class="outline-3">
<h3 id="org22a7a8d"><span class="section-number-3">3.7</span> Calcul d'Ito</h3>
<div class="outline-text-3" id="text-3-7">
<p>
Soit une fonction \(F : \setR^n \mapsto \setR\) et \(N\) processus stochastiques \(X_i\) dont les variations d'ordre \(n \ge 3\) s'annulent. Soit \(X=(X_1,...,X_N)\). On peut √©crire le d√©veloppement en s√©rie de Taylor d'ordre 2¬†:
</p>

<p>
\[F(X + \Delta) - F(X) \approx \deriveepartielle{F}{X}(X) \Delta + \unsur{2} \Delta^T \dblederiveepartielle{F}{X}(X) \Delta\]
</p>

<p>
En faisant tendre \(\Delta \to 0\), on obtient¬†:
</p>

<p>
\[dF = \deriveepartielle{F}{X} \ dX + \unsur{2} \ dX^T \dblederiveepartielle{F}{X} \ dX\]
</p>

<p>
On a donc la formule de Ito pour une fonction $f : \setR<sup>n</sup> \mapsto \setR $¬†:
</p>

<p>
\[dF = \sum_i \deriveepartielle{F}{X_i} \ dX_i + \unsur{2} \sum_{i,j} \dfdxdy{F}{X_i}{X_j} \ dX_i \ dX_j\]
</p>

<p>
Ce qui nous permet d'√©valuer une variation de \(F\)¬†:
</p>

<div class="org-center">
<p>
\(
F(X(t)) - F(X(0)) =  \sum_i \int_0^t \deriveepartielle{F}{X_i}(X(s)) \ dX_i(s) + \\
\unsur{2} \sum_{i,j} \int_0^t \dfdxdy{F}{X_i}{X_j}(X(s)) \ d\variation{X_i,X_j}(s)
\)
</p>
</div>
</div>


<div id="outline-container-org4868291" class="outline-4">
<h4 id="org4868291"><span class="section-number-4">3.7.1</span> D√©riv√©es ordinaires</h4>
<div class="outline-text-4" id="text-3-7-1">
<p>
Dans le cas d'une seule variable, on a¬†:
</p>

<p>
\[dF = \sum_i \OD{F}{X} \ dX + \unsur{2} \OOD{F}{X} \ dX \ dX\]
</p>

<p>
Ce qui nous permet d'√©valuer une variation de \(F\)¬†:
</p>

<div class="org-center">
<p>
\(
F(X(t)) - F(X(0)) = \sum_i \int_0^t \OD{F}{X}(X(s)) \ dX(s) + \\
\unsur{2} \int_0^t \OOD{F}{X}(X(s)) d\variation{X}(s)
\)
</p>
</div>
</div>
</div>
</div>


<div id="outline-container-org903e266" class="outline-3">
<h3 id="org903e266"><span class="section-number-3">3.8</span> Mouvement Brownien</h3>
<div class="outline-text-3" id="text-3-8">
<p>
Un mouvement brownien est un processus stochastique¬†:
</p>

<p>
\[B : [0,+\infty) \times \Omega \mapsto \setR, \quad (t,\omega) \mapsto B(t,\omega)\]
</p>

<p>
continu par rapport √† \(t\)¬†:
</p>

<p>
\[B_\omega : t \mapsto B(t,\omega) \in \Cont([0,+\infty))\]
</p>

<p>
De plus, si on d√©finit¬†:
</p>

<p>
\[\mathcal{B}_t : \omega \mapsto B(t,\omega)\]
</p>

<p>
on a la propri√©t√© d'ind√©pendance des variations temporelles¬†:
</p>

<p>
\[\cov{\mathcal{B}_u - \mathcal{B}_t}{\mathcal{B}_t - \mathcal{B}_s} = 0\]
</p>

<p>
pour tout \(s \strictinferieur t \strictinferieur u\) positifs. On demande aussi qu'une variation \(\mathcal{B}_t - \mathcal{B}_s\) suive une loi normale d'esp√©rance nulle et de variance \(t-s\)¬†:
</p>

<div class="org-center">
<p>
\(
\esperof{\mathcal{B}_t - \mathcal{B}_s}=0 \\
\var{\mathcal{B}_t - \mathcal{B}_s} = t - s
\)
</p>
</div>
</div>


<div id="outline-container-org9fce666" class="outline-4">
<h4 id="org9fce666"><span class="section-number-4">3.8.1</span> Variation quadratique</h4>
<div class="outline-text-4" id="text-3-8-1">
<p>
Si les mouvements browniens sont continus, ils ne sont pas d√©rivables. Comme les variations sont normalement distribu√©es avec une moyenne nulle et une variance \(t - s\), on en d√©duit (en utilisant par exemple le moment g√©n√©rateur des densit√©s normales)¬†:
</p>

<div class="org-center">
<p>
\(
\esperof{(\mathcal{B}_t - \mathcal{B}_s)^2} = t - s \\
\var{(\mathcal{B}_t - \mathcal{B}_s)^2} = 2 \ (t - s)^2
\)
</p>
</div>

<p>
La variation quadratique des mouvement browniens peut s'√©crire¬†:
</p>

<p>
\[\variation{B_\omega}(T) = \lim_{\delta \to 0} \sum_k (B_\omega(t_{k+1}) - B_\omega(t_k))^2\]
</p>

<p>
Lorsque \(\delta \to 0\), on a \(N \to +\infty\) et la loi des grands nombres nous dit que
chaque terme de la somme de droite converge vers la variance \(\delta\). Comme on a \(N\)
termes, on obtient¬†:
</p>

<p>
\[\sum_k B_\omega(t_{k+1}) - B_\omega(t_k) \to N \delta = T\]
</p>

<p>
On a donc¬†:
</p>

<p>
\[\variation{\mathcal{B}_\omega}(T) = T\]
</p>

<p>
Ce que l'on note symboliquement sous forme diff√©rentielle par¬†:
</p>

<p>
\[dB(t) \cdot dB(t) = dt\]
</p>
</div>
</div>


<div id="outline-container-org77d2b29" class="outline-4">
<h4 id="org77d2b29"><span class="section-number-4">3.8.2</span> Variations d'ordre quelconque</h4>
<div class="outline-text-4" id="text-3-8-2">
<p>
Les variations \(\variation{B}^n\) d'un mouvement brownien s'annulent pour \(n \ge 3\).
</p>
</div>
</div>


<div id="outline-container-org19c34d0" class="outline-4">
<h4 id="org19c34d0"><span class="section-number-4">3.8.3</span> Multidimensionnel</h4>
<div class="outline-text-4" id="text-3-8-3">
<p>
Nous d√©finissons un mouvement Brownien de dimension \(n\) comme une collection de \(n\) mouvements Browniens \(B_i\) ind√©pendants et v√©rifiant¬†:
</p>

<p>
\[\variation{B_i,B_j}(t) = \indicatrice_{ij} \cdot t\]
</p>
</div>
</div>


<div id="outline-container-orgc29036d" class="outline-4">
<h4 id="orgc29036d"><span class="section-number-4">3.8.4</span> Calul d'Ito</h4>
<div class="outline-text-4" id="text-3-8-4">
<p>
Dans le cas de \(N\) mouvement browniens \(B_i\), les √©quations d'Ito deviennent¬†:
</p>

<p>
\[dF = \sum_i \deriveepartielle{F}{X_i} dB_i + \unsur{2} \sum_{i,j} \dfdxdy{F}{X_i}{X_j} \ dB_i \ dB_j\]
</p>

<p>
Mais comme \(dB_i dB_j = d\variation{B_i,B_j} = \indicatrice_{ij} \ dt\), on a¬†:
</p>

<p>
\[dF = \sum_i \deriveepartielle{F}{X_i} \ dB_i + \unsur{2} \sum_i \dfdxdy{F}{X_i}{X_i} \ dt\]
</p>

<p>
Ce qui nous permet d'√©valuer une variation de \(F\)¬†:
</p>

<div class="org-center">
<p>
\(
F(X(t)) - F(X(0)) = \sum_i \int_0^t \deriveepartielle{F}{X_i}(X(s)) \ dX_i(s) + \\
\unsur{2} \sum_i \int_0^t \dfdxdy{F}{X_i}{X_i}(X(s)) \ ds
\)
</p>
</div>
</div>
</div>


<div id="outline-container-orga823c70" class="outline-4">
<h4 id="orga823c70"><span class="section-number-4">3.8.5</span> D√©riv√©e ordinaire</h4>
<div class="outline-text-4" id="text-3-8-5">
<p>
Le cas particulier unidimensionnel nous donne¬†:
</p>

<p>
\[dF(B) = \OD{F}{X}(B) \ dB + \unsur{2}\OOD{F}{X}(B) \ dB \cdot dB\]
</p>

<p>
Mais comme¬†:
</p>

<p>
\[d\variation{B} = dB \cdot dB = dt\]
</p>

<p>
on a¬†:
</p>

<p>
\[dF(B) = \OD{F}{X}(B) \ dB + \unsur{2}\OOD{F}{X}(B) \ dt\]
</p>

<p>
et¬†:
</p>

<p>
\[F(B(t))-F(B(0)) = \int_0^t \OD{F}{X}(B(s)) \ dB(s) + \unsur{2} \int_0^t \OOD{F}{X}(B(s)) \ ds\]
</p>

<p>
AFAIRE : PROCESSUS DE POISSON
</p>
</div>
</div>
</div>
</div>
</div>
<div id="postamble" class="status">
<p class="author">Auteur: chimay</p>
<p class="date">Created: 2019-10-01 mar 12:19</p>
<p class="validation"><a href="http://validator.w3.org/check?uri=referer">Validate</a></p>
</div>
</body>
</html>
