
#+STARTUP: showall

#+TITLE: Eclats de vers : Matemat : Applications linéaires
#+AUTHOR: chimay
#+EMAIL: or du val chez gé courriel commercial
#+LANGUAGE: fr
#+LINK_HOME: file:../index.html
#+LINK_UP: file:index.html
#+HTML_HEAD: <link rel="stylesheet" type="text/css" href="../style/defaut.css" />

#+OPTIONS: H:6
#+OPTIONS: toc:nil

#+TAGS: noexport(n)

[[file:index.org][Index des Grimoires]]

#+INCLUDE: "../include/navigan-1.org"

#+TOC: headlines 1

#+INCLUDE: "../include/latex/latex.org"

\label{chap:lineaire}

* Dépendances

  - Chapitre \ref{chap:fonction} : Les fonctions

* Définition

Soit les espaces vectoriels $E$ et $F$ sur $\corps$ et la fonction $f : E \mapsto F$. On dit que $f$ est linéaire si, pour tout $x,y \in E$ et $\alpha, \beta \in \corps$, on a :

$$f(\alpha \cdot x + \beta \cdot y) = \alpha \cdot f(x) + \beta \cdot f(y)$$

On note $\lineaire(E,F)$ l'ensemble des fonctions linéaires de $E$ vers $F$.


* Identité

L'application identité est clairement linéaire.


* Inverse

Soit $u = f(x)$ et $v = f(y)$. Si l'application inverse existe, on a $x = f^{-1}(u)$ et $y = f^{-1}(v)$. En composant à gauche par $f^{-1}$ la définition de la linéarité, on obtient :

$$\alpha \cdot f^{-1}(u) + \beta \cdot f^{-1}(v) = f^{-1}(\alpha \cdot u + \beta \cdot v)$$

ce qui montre que l'inverse est également linéaire.


* Valeur au vecteur nul

Choisissons un $x \in E$. on voit que :

$$f(0) = f(0 \cdot x) = 0 \cdot f(x) = 0$$

La valeur d'une application linéaire s'annule au vecteur nul.


* Norme des applications linéaires

La norme d'une application linéaire est définie comme étant l'extension maximale qu'elle produit :

$$\norme{f} = \sup \left\{ \frac{ \norme{f(x)} }{ \norme{x} } : x \in E, \ x \ne 0 \right\}$$

On a donc :

$$\norme{f(x)} \le \norme{f} \cdot \norme{x}$$

pour tout $x \in E \setminus \{ 0 \}$.


** Vérification

Nous allons vérifier qu'il s'agit bien d'une norme. On a $\norme{f} \ge 0$ par positivité de la norme sur $E$ et $F$. La condition $\norme{f} = 0$ implique $\norme{f(x)} = 0$ et donc $f(x) = 0$ pour tout $x \ne 0$. Comme $f$ est linéaire, on a aussi $f(0) = 0$ et $f = 0$.

Si $f,g$ sont linéaires, on a :

$$\norme{f(x) + g(x)} \le \norme{f(x)} + \norme{g(x)} \le \norme{f} \cdot \norme{x} + \norme{g} \cdot \norme{x} = (\norme{f} + \norme{g}) \cdot \norme{x}$$

pour tout $x \ne 0$. En divisant par $\norme{x}$ et en passant au supremum, on obtient :

$$\norme{f + g} \le \norme{f} + \norme{g}$$

Enfin, si $\alpha \in \corps$, on a :

$$\frac{ \norme{\alpha \cdot f(x)} }{ \norme{x} } = \abs{\alpha} \cdot \frac{ \norme{f(x)} }{ \norme{x} }$$

En passant au supremum, on obtient :

$$\norme{\alpha \cdot f} = \abs{\alpha} \cdot \norme{f}$$


** Notation

Lorsqu'il est nécessaire de différentier la norme au sens des applications linéaires d'autres types de normes utilisées, on note :

$$\norme{f}_\lineaire = \norme{f}$$


** Définition alternative

Soit $N \in \corps$, avec $N \strictsuperieur 0$ et :

$$B = \{ u \in E : \norme{u} = N \}$$

Soit $x \in E$ avec $x \ne 0$ et :

$$\lambda = \frac{ \norme{x} }{N}$$

Définissons :

$$u = \frac{x}{\lambda}$$

On voit que :

$$\norme{u} = \norme{\unsur{\lambda} \cdot x} = \unsur{\lambda} \cdot \norme{x} = \frac{N}{ \norme{x} } \cdot \norme{x} = N$$

On a donc $u \in B$. Le rapport des normes s'écrit :

$$\frac{ \norme{f(x)} }{ \norme{x} } = \frac{ \norme{f(x)} }{ N \cdot \lambda } = \unsur{N} \norme{ \frac{f(x)}{ \lambda } } = \unsur{ \norme{u} } \cdot \norme{ f\left( \frac{x}{ \lambda } \right) } = \frac{ \norme{f(u)} }{ \norme{u} }$$

On en conclut que :

$$\frac{ \norme{f(x)} }{ \norme{x} } = \frac{ \norme{f(u)} }{ \norme{u} } \le \sup \Big\{ \frac{ \norme{f(v)} }{ \norme{v} } : v \in B \Big\}$$

Comme ce doit être valable quelque soit $x \ne 0$, on obtient :

$$\norme{f} \le \sup \Big\{ \frac{ \norme{f(v)} }{ \norme{v} } : v \in B \Big\}$$

en passant au supremum sur $x$.

Choisissons à présent $u \in B$. On a alors :

$$\frac{ \norme{f(u)} }{ \norme{u} } \le \norme{f}$$

En passant au supremum sur $u$, on obtient :

$$\sup \Big\{ \frac{ \norme{f(v)} }{ \norme{v} } : v \in B \Big\} \le \norme{f}$$

On en conclut que les deux supremums sont égaux :

$$\sup \left\{ \frac{ \norme{f(v)} }{ \norme{v} } : v \in B \right\} = \norme{f}$$


** Norme unitaire

Une conséquence importante du résultat ci-dessus est le cas particulier $N = 1$. On a alors :

$$\norme{f} = \sup \left\{ \norme{f(v)} : v \in E, \ \norme{v} = 1 \right\}$$


* Norme d'une composée

Soit $f : E \mapsto F$ et $g : F \mapsto G$ deux applications linéaires de normes finies. Si $x \in E$ avec $x \ne 0$ on a $f(x) \in F$ et :

$$\norme{g \circ f(x)} \le \norme{g} \cdot \norme{f(x)} \le \norme{g} \cdot \norme{f} \cdot \norme{x}$$

En divisant par $\norme{x} \ne 0$ :

$$\frac{ \norme{g \circ f(x)} }{ \norme{x} } \le \norme{g} \cdot \norme{f}$$

et en passant au supremum sur $x \ne 0$, on en conclut que :

$$\norme{g \circ f} \le \norme{g} \cdot \norme{f}$$


* Norme d'une puissance

On a clairement :

$$\norme{f^n} = \norme{f \circ ... \circ f} \le \norme{f}^n$$


* Continuité

Nous allons montrer que, pour tout $f \in \lineaire(A,B)$, on a l'équivalence entre l'hypothèse d'une norme de $f$ finie et l'hypothèse de $f$ continue.

Si la norme est finie, on a :

$$\norme{f(x) - f(a)} = \norme{f(x - a)} \le \norme{f} \cdot \norme{x-a}$$

qui tend bien vers $0$ lorsque $x$ tend vers $a$. Inversément, si $f$ est continue, on peut trouver un $\delta \strictsuperieur 0$ tel que :

$$\norme{f(x) - f(0)} \le 1$$

pour tout $x$ vérifiant $\distance(x,0) = \norme{x} \le \delta$. Posons $B = \{ x \in A : \norme{x} = \delta \}$. On a alors :

$$\sup_{x \in B} \frac{\norme{f(x)}}{\norme{x}} = \unsur{\delta} \sup_{x \in B} \norme{f(x) - f(0)} \le \unsur{\delta}$$

La norme est donc finie :

$$\norme{f} = \sup_{x \in B} \frac{\norme{f(x)}}{\norme{x}} \le \unsur{\delta} \strictinferieur +\infty$$


* $n$-linéarité

On dit que la fonction $f : E_1 \times ... \times E_n \mapsto F$ est $n$-linéaire si elle est linéaire par rapport à chacune des composantes de son argument, les autres composantes restant inchangées :

$$f(...,\alpha x + \beta y,...) = \alpha \cdot f(...,x,...) + \beta \cdot f(...,y,...)$$

pour tout $\alpha,\beta \in \corps$ et $x,y \in E$. On note $\lineaire_n(E_1,...,E_n,F)$ l'ensemble des fonctions $n$-linéaires de $E_1 \times ... \times E_n$ vers $F$.


** Norme

La norme est définie dans ce cas par :

$$\norme{f} = \sup \left\{ \frac{ \norme{f(x_1,...,x_n)} }{ \prod_{i = 1}^n \norme{x_i} } : (x_1,...,x_n) \in E_1 \times ... \times E_n \right\}$$

Si cette norme est finie, on a :

$$\norme{f(x_1,...,x_n)} \le \norme{f} \cdot \prod_{i = 1}^n \norme{x_i}$$

pour tout $(x_1,...,x_n) \in E_1 \times ... \times E_n$.


** Bilinéarité

On dit aussi des fonctions $2$-linéaires qu'elles sont bilinéaires. La norme d'une fonction $f : E_1 \times E_2 \mapsto F$ bilinéaire est définie par :

$$\norme{f} = \sup \left\{ \frac{ \norme{f(u,v)} }{ \norme{u} \cdot \norme{v} } : (u,v) \in E_1 \times E_2 \right\}$$

Si cette norme est finie, on a :

$$\norme{f(u,v)} \le \norme{f} \cdot \norme{u} \cdot \norme{v}$$

pour tout $(u,v) \in E_1 \times E_2$.


* Représentation matricielle

Soit une application linéaire $\mathcal{A} : E \to F$. Choisissons $x \in E$ et posons :

$$y = \mathcal{A}(x)$$

Si on dispose d'une base $(e_1,...,e_n)$ de $E$ et d'une base $(f_1,...,f_m)$ de $F$, on a :

#+BEGIN_CENTER
\(
x = \sum_{i = 1}^n x_i \cdot e_i \)

\(
y = \sum_{i = 1}^m y_i \cdot f_i
\)
#+END_CENTER

pour certains $x_i,y_i \in \corps$. La linéarité de $\mathcal{A}$ implique que :

$$y = \sum_{j = 1}^n \mathcal{A}(e_j) \cdot x_j$$

Si les $a_{ij} \in \corps$ sont les coordonnées de $\mathcal{A}(e_j)$ dans la base des $f_i$, on a :

$$\mathcal{A}(e_j) = \sum_{i = 1}^m a_{ij} \cdot f_i$$

En substituant cette expression, on obtient :

$$y = \sum_{i = 1}^m f_i \sum_{j = 1}^n a_{ij} \cdot x_j$$

La $i^{ème}$ coordonnée de $y$ est donc donnée par :

$$y_i = \sum_{j = 1}^n a_{ij} \cdot x_j$$

On définit la matrice $A \in \matrice(\corps,m,n)$ associée à $\mathcal{A}$ en posant :

$$A = ( a_{ij} )_{i,j}$$

Nous définissons ensuite le produit d'une matrice avec le « vecteur » $x$ équivalent de $\corps^n$ :

$$A \cdot x = \left( \sum_j a_{ij} \cdot x_j  \right)_i$$

de telle sorte que :

$$A \cdot x = \mathcal{A}(x)$$


** Norme

La norme d'une matrice est la norme de l'application linéaire associée, c'est-à-dire :

$$\norme{A}_2 = \sup \left\{ \frac{ \norme{A \cdot x} }{ \norme{x} } : x \in \corps^n, \ x \ne 0 \right\}$$

Soit :

$$M = \max_{i,j} \abs{\composante_{ij} A}$$

On a alors :

$$\norme{A \cdot x} \le M \cdot m \cdot n \cdot \max_i x_i \le M \cdot m \cdot n \cdot \norme{x}$$

ce qui montre que :

$$\norme{A} \le M \cdot m \cdot n \strictinferieur \infty$$

La norme d'une matrice finie ($m,n \strictinferieur \infty$) existe toujours.


** Image

L'image d'une matrice est l'image de l'application linéaire associée, c'est-à-dire :

$$\image A = \{ A \cdot x : x \in \corps^n \}$$

Si $c_i = \colonne_i A$, on a :

$$A = [ c_1 \ c_2 \ ... \ c_n ]$$

On voit que :

$$A \cdot x = \sum_i c_i \cdot x_i$$

autrement dit l'image de $A$ est l'espace vectoriel engendré par ses colonnes :

$$\image A = \combilin{c_1,c_2,...,c_n}$$


** Noyau

Le noyau d'une matrice est le noyau de l'application linéaire associée, c'est-à-dire :

$$\noyau A = \{ x \in \corps^n : A \cdot x = 0 \}$$


* Produit matriciel

Soit à présent les matrices $A \in \matrice(\corps,m,n)$ et $B \in \matrice(\corps,n,p)$ données par :

#+BEGIN_CENTER
\(
A = ( a_{ij} )_{i,j} \)

\(
B = ( b_{ij} )_{i,j}
\)
#+END_CENTER

où les $a_{ij},b_{ij} \in K$. Soit les applications linéaires $\mathcal{B} : \corps^p \to \corps^n$ et $\mathcal{A} : \corps^n \to \corps^m$ définies par :

#+BEGIN_CENTER
\(
\mathcal{B}(x) = B \cdot x \)

\(
\mathcal{A}(y) = A \cdot y
\)
#+END_CENTER

pour tout $x \in \corps^p$, $y \in \corps^n$. Choisissons $z \in \corps^m$ et relions $x,y,z$ par :

#+BEGIN_CENTER
\(
y = \mathcal{B}(x) \)

\(
z = \mathcal{A}(y) = \big( \mathcal{A} \circ \mathcal{B} \big)(x)
\)
#+END_CENTER

Examinons les composantes de $z$ en fonction de celles de $x$ :

$$z_i = \sum_k a_{ik} \cdot y_k = \sum_k a_{ik} \sum_j b_{kj} \cdot x_j = \sum_{k,j} a_{ik} \cdot b_{kj} \cdot x_j$$

On en déduit que la composée $\mathcal{C} = \mathcal{A} \circ \mathcal{B}$ est représentée par la matrice $C \in \matrice(\corps,m,p)$ de composantes :

$$\composante_{ij} C = c_{ij} = \sum_k a_{ik} \cdot b_{kj}$$

Il suffit donc de définir le produit matriciel $A \cdot B$ par :

$$A \cdot B = \left(\sum_{k=1}^n a_{ik} \cdot b_{kj}\right)_{i,j}$$

pour avoir :

$$(A \cdot B) \cdot x = \big( \mathcal{A} \circ \mathcal{B} \big)(x)$$

Le produit matriciel représente donc une composée d'applications linéaires. Pour que ce produit soit bien défini, il est nécessaire que le nombre
de colonnes $n$ de $A$ et le nombre de lignes de $B$ soient identiques.

On voit également que le produit matrice - vecteur défini précédemment en est un cas particulier lorsque $p = 1$.


** Notation

En pratique, on laisse souvent tomber le ``$\cdot$'' et on note $A B$
au lieu de $A \cdot B$ lorsqu'il est évident que $A$ et $B$ sont deux
matrices différentes.


** Taille

Le produit d'une matrice de taille $(m,n)$ par une matrice de taille $(n,p)$ est une matrice de taille $(m,p)$.


** Lignes et colonnes

Si $x_i^T = \ligne_i(A)$ et $y_j = \colonne_j(B)$, on voit que :

$$\composante_{ij} (A \cdot B) = x_i^T \cdot y_j$$


** Associativité

Soit les matrices $A \in \matrice(\corps,m,n)$, $B \in \matrice(\corps,n,p)$ et $C \in \matrice(\corps,p,q)$ données par :

#+BEGIN_CENTER
\(
A = ( a_{ij} )_{i,j} \)

\(
B = ( b_{ij} )_{i,j} \)

\(
C = ( c_{ij} )_{i,j}
\)
#+END_CENTER

où les $a_{ij},b_{ij},c_{ij} \in K$. La relation :

$$A \cdot (B \cdot C) = \left( \sum_{k,l} a_{ik} \cdot b_{kl} \cdot c_{lj} \right)_{i,j} = (A \cdot B) \cdot C$$

nous montre que la multiplication entre matrices est associative. On définit :

$$A \cdot B \cdot C = A \cdot (B \cdot C) = (A \cdot B) \cdot C$$


** Distributivité

On a aussi les propriétés de distribution :

#+BEGIN_CENTER
\(
A \cdot (B + C) = A \cdot B + A \cdot C \)

\(
(B + C) \cdot D = B \cdot D + C \cdot D
\)
#+END_CENTER

où $A \in \matrice(\corps,m,n)$, $B,C \in \matrice(\corps,n,p)$ et $D \in \matrice(\corps,p,q)$.


** Non commutativité

Par contre, on peut trouver des matrices $A$ et $B$ telles que :

$$A \cdot B \ne B \cdot A$$

La multiplication matricielle n'est donc en général pas commutative. D'ailleurs, pour que ces deux produits existent simultanément, il faut que $A$ et $B$ soient toutes deux carrées, ce qui n'est pas forcément le cas.


** Commutateur

La matrice associée au commutateur :

$$[\mathcal{A},\mathcal{B}] = \mathcal{A} \circ \mathcal{B} - \mathcal{B} \circ \mathcal{A}$$

est donnée par le commutateur équivalent :

$$[A,B] = A \cdot B - B \cdot A$$


** Transposée

On vérifie que :

$$(A \cdot B)^T = B^T \cdot A^T$$


* Blocs

En utilisant l'associativité de l'addition, on peut facilement vérifier que la formule de multiplication reste valable lorsqu'on considère des blocs de matrices au lieu des éléments, à condition de respecter l'ordre de multiplication. Un exemple fréquemment utilisé :

#+BEGIN_CENTER
\(
\begin{Matrix}{cc}
A_{11} & A_{12} \\ A_{21} & A_{22}
\end{Matrix}
\cdot
\begin{Matrix}{cc}
B_{11} & B_{12} \\ B_{21} & B_{22}
\end{Matrix}
=
\begin{Matrix}{cc}
A_{11} \cdot B_{11} +  A_{12} \cdot B_{21} & A_{11} \cdot B_{12} +  A_{12} \cdot B_{22} \)

\(
A_{21} \cdot B_{11} +  A_{22} \cdot B_{21} & A_{21} \cdot B_{12} +  A_{22} \cdot B_{22}
\end{Matrix}
\)
#+END_CENTER


** Bloc-diagonale

Un cas particulier important :

#+BEGIN_CENTER
\(
\begin{Matrix}{cc}
A_1 & 0 \\ 0 & A_2
\end{Matrix}
\cdot
\begin{Matrix}{cc}
B_1 & 0 \\ 0 & B_2
\end{Matrix}
=
\begin{Matrix}{cc}
A_1 \cdot B_1 & 0 \)

\(
0 & A_2 \cdot B_2
\end{Matrix}
\)
#+END_CENTER


* Matrice identité

La matrice identité $I \in \matrice(\corps,n,n)$ correspond à la fonction $\identité$. On a donc :

$$I \cdot x = x$$

pour tout $x \in \corps^n$. Si $(\canonique_1,...\canonique_n)$ est la base canonique de $\corps^n$, on a donc :

$$I \cdot \canonique_i = \canonique_i$$

ce qui entraîne directement :

$$I = ( \indicatrice_{ij} )_{i,j}$$

On remarque que :

$$I = [\canonique_1 \ \hdots \ \canonique_n]$$


** Neutre

Comme la fonction identité est neutre pour la composition, la matrice unité correspondante $I \in \matrice(\corps,m,n)$ doit être neutre pour la multiplication avec toutes les matrices de dimensions compatibles. Soit $A \in \matrice(\corps,m,n)$ et $B \in \matrice(\corps,n,p)$. On vérifie que l'on a bien :

#+BEGIN_CENTER
\(
A \cdot I = A \)

\(
I \cdot B = B
\)
#+END_CENTER


** Notation

On note aussi $I_n$ pour préciser que $I$ est de taille $(n,n)$.


* Inverse

Lorsqu'elle existe, la matrice inverse de $A$, notée $A^{-1}$, reflète l'application linéaire inverse sous-jacente. Elle est donc l'unique matrice telle que :

$$A^{-1} \cdot A = A \cdot A^{-1} = I$$


* Inverse d'un produit

Soit $A$ et $B$ deux matrices inversibles. Les relations $C \cdot (A \cdot B) = I$ et $(A \cdot B) \cdot D = I$ nous donnent :

$$C = D = B^{-1} \cdot A^{-1}$$

et donc :

$$(A \cdot B)^{-1} = B^{-1} \cdot A^{-1}$$


** Inverse à gauche et à droite

On dit que $L$ est un inverse à gauche de $A$ si $L \cdot A = I$. On dit que $R$ est un inverse à droite de $A$ si $A \cdot R = I$.


* Puissance

Il est possible de multiplier une matrice carrée $A$ avec elle-même.
On peut donc définir l'exposant par :

#+BEGIN_CENTER
\(
A^0 = I \)

\(
A^k = A \cdot A^{k-1}
\)
#+END_CENTER


** Négative

Si l'inverse $A^{-1}$ existe, on définit également :

$$A^{-k} = (A^{-1})^k$$


* Polynômes matriciels

Ici, $\corps$ n'est plus un corps mais l'anneau des matrices $X$ de taille $(N,N)$. On dit que $p : \matrice(\corps,N,N) \mapsto \matrice(\corps,N,N)$ est un polynôme matriciel si il existe $a_0,...,a_n \in \corps$ tels que :

$$p(X) = \sum_{i = 0}^n a_i \cdot X^i$$

pour tout $X \in \matrice(\corps,N,N)$.


