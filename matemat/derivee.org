
#+STARTUP: showall

#+TITLE: Eclats de vers : Matemat : Dérivées
#+AUTHOR: chimay
#+EMAIL: or du val chez gé courriel commercial
#+LANGUAGE: fr
#+LINK_HOME: file:../index.html
#+LINK_UP: file:index.html
#+HTML_HEAD: <link rel="stylesheet" type="text/css" href="../style/defaut.css" />

#+OPTIONS: H:6
#+OPTIONS: toc:nil

#+TAGS: noexport(n)

[[file:index.org][Index des Grimoires]]

#+INCLUDE: "../include/navigan-1.org"

#+TOC: headlines 1

#+INCLUDE: "../include/latex/latex.org"

\label{chap:derivee}

* Dépendances

  - Chapitre \ref{chap:differ} : Les différentielles


* Fonction constante

Si $f$ est constante, on peut trouver $c \in F$ tel que :

$$f(x) = c$$

pour tout $x \in \Omega$. On a alors :

$$\partial_j f(a) = \lim_{\lambda \to 0} \frac{c - c}{\lambda} = 0$$

Par conséquent :

$$\partial f(a) = 0$$


* Identité

Considérons le cas particulier où $m = n$ et où $f = \identite$. Lorsque $i = j$, nous avons :

$$\partial_i f_i(a) = \lim_{\lambda \to 0} \frac{x_i + \lambda - x_i}{\lambda} = \lim_{\lambda \to 0} \frac{\lambda}{\lambda} = 1$$

Lorsque $i \ne j$, on a par contre :

$$\partial_j f_i(a) = \lim_{\lambda \to 0} \frac{x_i - x_i}{\lambda} = \lim_{\lambda \to 0} \frac{0}{\lambda} = 0$$

On en conclut que :

$$\partial f(a) = \partial \identite(a) = ( \indicatrice_{ij} )_{i,j} = I$$

La Jacobienne de la fonction identité est la matrice identité.


* Composition de fonctions

Nous allons nous intéresser à présent au moyen d'obtenir la dérivée d'une composée de fonctions $f : E \mapsto F$ et $g : F \mapsto G$. Supposons que $f$ soit différentiable en $a$ et que $g$ soit différentiable en $b = f(a)$. On a alors :

#+BEGIN_CENTER
\(
df(a) = f(a + da) - f(a) = \partial f(a) \cdot da + E_f(da) \)

\(
dg(b) = g(b + db) - g(b) = \partial g(b) \cdot db + E_g(db)
\)
#+END_CENTER

Choisissons en particulier $db = f(a + da) - f(a)$. On a alors :

\begin{align}
dg(b) &= g(f(a + da)) - g(f(a)) \)

\(
&= (g \circ f)(a + da) - (g \circ f)(a) \)

\(
&= d(g \circ f)(a)
\end{align}

Mais d'un autre coté :

\begin{align}
dg(b) &= \partial g(f(a)) \cdot df(a) + E_g(df(a)) \)

\(
&= \partial g(f(a)) \cdot \left( \partial f(a) \cdot da + E_f(da) \right) + E_g(df(a)) \)

\(
&= \partial g(f(a)) \cdot \partial f(a) \cdot da + E_f(da) \cdot da + E_g(df(a)) \)

\(
\end{align}

Il est aisé de vérifier que :

$$\lim_{h \to 0} \frac{\partial g(f(a)) \cdot E_f(da)  + E_g(df(a))}{\norme{h}} = 0$$

puisque $\partial g(f(a))$ ne dépend pas de $da$. On a donc montré que $g \circ f$ est différentiable en $a$ et que :

$$\partial (g \circ f)(a) =  \partial g(f(a)) \cdot \partial f(a) = (\partial g \circ f)(a) \cdot \partial f(a)$$

La dérivée d'une composée de fonctions est donc tout simplement le produit des Jacobiennes.


** Notation

Soit le schéma fonctionnel :

$$(x_1,...,x_n) \mapsto (y_1,...,y_m) \mapsto (z_1,...,z_p)$$

On note aussi :

#+BEGIN_CENTER
\(
\deriveepartielle{z_i}{x_j} = \sum_k \deriveepartielle{z_i}{y_k} \cdot \deriveepartielle{y_k}{x_j} \\ \)

\(
\deriveepartielle{z}{x^T} = \deriveepartielle{z}{y^T} \cdot \deriveepartielle{y}{x^T}
\)
#+END_CENTER

Pour $z : t \mapsto (x_1(t),x_2(t),...,x_n(t))$, on a également :

$$\OD{z}{t} = \sum_k \deriveepartielle{z}{x_k} \cdot \OD{x_k}{t}$$

Si $x,y,z \in \corps$, on a encore :

$$\OD{z}{x} = \OD{z}{y} \cdot \OD{y}{x}$$


* Inverse fonctionnel

En considérant le cas particulier $g = f^{-1}$, on a $g \circ f = \identite$. Choisissons un vecteur $a$ où $f$ est différentiable et posons $b = f(a)$. On a :

$$\partial f^{-1}(b) \cdot \partial f(a) = \partial \identite(a) = I$$

La jacobienne de l'inverse d'une fonction est donc l'inverse matriciel de sa jacobienne :

$$\partial f^{-1}(b) = \left[ \partial f(a) \right]^{-1}$$


** Notation

On note aussi :

$$\deriveepartielle{y}{x}  = \left(\deriveepartielle{x}{y}\right)^{-1}$$

ou, pour des fonctions $y : \corps \mapsto \corps$ :

$$\OD{y}{x}  = \left(\OD{x}{y}\right)^{-1}$$


* Addition

Si $f,g$ sont différentiables en $a$, on a :

#+BEGIN_CENTER
\(
f(a + h) - f(a) = \partial f(a) \cdot h + E_f(h) \)

\(
g(a + h) - g(a) = \partial g(a) \cdot h + E_g(h)
\)
#+END_CENTER

En additionnant les équations ci-dessus, on obtient :

#+BEGIN_CENTER
\(
\left[ f(a + h) + g(a + h) \right] - \left[ f(a) + g(a) \right]
= [ \partial f(a) + \partial g(a) ] \cdot h \)

\(
\qquad \qquad + E_f(h) + E_g(h)
\)
#+END_CENTER

Il est clair que :

$$\lim_{h \to 0} \frac{\norme{E_f(h) + E_g(h)}}{\norme{h}} = 0$$

La fonction $f + g$ est donc différentiable en $a$ et :

$$\partial (f + g)(a) = \partial f(a) + \partial g(a)$$

On peut montrer, cette fois en soustrayant les deux équations que :

$$\partial (f - g)(a) = \partial f(a) - \partial g(a)$$


* Produit scalaire

Considérons deux fonctions $f,g$ différentiables en $a$ :

#+BEGIN_CENTER
\(
f(a + h) = f(a) + \partial f(a) \cdot h + E_f(h) \)

\(
g(a + h) = g(a) + \partial g(a) \cdot h + E_g(h) \)

\(
\)
#+END_CENTER

Leur produit scalaire s'écrit :

#+BEGIN_CENTER
\(
\scalaire{f(a + h)}{g(a + h)} = \scalaire{f(a)}{g(a)} + \scalaire{f(a)}{\partial g(a) \cdot h} + \scalaire{\partial f(a) \cdot h}{g(a)} \)

\(
\qquad \qquad \qquad + E_{f \cdot g}(h)
\)
#+END_CENTER

où :

#+BEGIN_CENTER
\(
E_{f \cdot g}(h) = \scalaire{E_f(h)}{E_g(h)} + \scalaire{E_f(h)}{g(a)} + \scalaire{E_f(h)}{\partial g(a)} + \)

\(
\qquad \qquad \scalaire{f(a)}{E_g(h)} + \scalaire{\partial f(a)}{E_g(h)}
\)
#+END_CENTER

On a donc :

$$\lim_{h \to 0} \frac{\norme{E_{f \cdot g}(h)}}{\norme{h}} = 0$$

ce qui nous montre que la différentielle du produit scalaire s'écrit :

$$\differentielle{ \scalaire{f}{g} }{a}(h) = \scalaire{f(a)}{\partial g(a) \cdot h} + \scalaire{\partial f(a) \cdot h}{g(a)}$$

En terme de composantes, on a :

$$\differentielle{ \scalaire{f}{g} }{a}(h) = \sum_{j = 1}^n \varpi_j \cdot h_j \cdot \sum_{i = 1}^m \left[ f_i(a) \cdot \partial_j g_i(a) + \partial_j f_i(a) \cdot g_i(a) \right]$$

La représentation matricielle s'écrit donc :

$$\partial \scalaire{f}{g}(a) = \left[ \partial g(a) \right]^T \cdot f(a) + \left[ \partial f(a) \right]^T \cdot g(a)$$


** Dérivée ordinaire

Dans le cas où $m = n = 1$, cette expression se simplifie en :

$$\OD{}{x}(f \cdot g)(a) = f(a) \cdot \OD{g}{x}(a) +  \OD{f}{x}(a) \cdot g(a)$$


** Constante

Si une des deux fonctions est constante, soit $g(x) = c$ pour tout $x$, on a :

$$\partial g(x) = 0$$

et :

$$\OD{}{x}(f \cdot c)(a) = f(a) \cdot 0 +  \OD{f}{x}(a) \cdot c = c \cdot \OD{f}{x}(a)$$


** Notation

On note aussi :

$$d(f \cdot g) = df \cdot g + f \cdot dg$$


* Inverse multiplicatif

Soit les fonctions $f,g : \corps \mapsto \corps$ reliées par l'équation :

$$f \cdot g = 1$$

En dérivant, on obtient :

$$\OD{f}{x} \cdot g + f \cdot \OD{g}{x} = \OD{1}{x} = 0$$

Donc, si $g \ne 0$, on a :

$$\OD{f}{x} = - \frac{f}{g} \cdot \OD{g}{x}$$

Mais comme $f(x) = 1/g(x)$, cela nous donne :

$$\OD{}{x}\left(\unsur{g}\right)(x) = - \unsur{g(x)^2} \cdot \OD{g}{x}(x)$$


* Fraction

En appliquant les résultats précédents, on obtient :

$$\OD{}{x}\left( \frac{f}{g} \right)(x) = \OD{}{x}\left( f \cdot \unsur{g} \right)(x) = \OD{f}{x}(x) \cdot g(x) - \frac{f(x)}{g(x)^2} \cdot \OD{g}{x}(x)$$

et finalement :

$$\OD{}{x}\left( \frac{f}{g} \right)(x) = \frac{\OD{f}{x}(x) \cdot g(x) - f(x) \cdot \OD{g}{x}}{g(x)^2}$$


* Dérivée d'une limite

Soit la suite de fonctions :

$$F = \{ f_n \in \setR^\setR : n \in \setN \}$$

convergeant en tout point $x \in \setR$ vers une fonction $f : \setR \mapsto \setR$ :

$$\lim_{n \to \infty} f_n(x) = f(x)$$

Si la fonction $f$ est différentiable en $a \in \setR$, on a :

\begin{align}
\partial f(a) &= \lim_{h \to 0} \unsur{h} \big[f(a + h) - f(a)\big] \)

\(
&= \lim_{h \to 0} \unsur{h} \crochets{\lim_{n \to \infty} f_n(a + h) - \lim_{n \to \infty} f_n(a)} \)

\(
&= \lim_{h \to 0} \lim_{n \to \infty} \frac{f_n(a + h) - f_n(a)}{h}
\end{align}


