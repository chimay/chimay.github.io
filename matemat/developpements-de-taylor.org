
#+STARTUP: showall

#+TITLE: Eclats de vers : Matemat : Développements de Taylor
#+AUTHOR: chimay
#+EMAIL: or du val chez gé courriel commercial
#+LANGUAGE: fr
#+LINK_HOME: file:../index.html
#+LINK_UP: file:index.html
#+HTML_HEAD: <link rel="stylesheet" type="text/css" href="../style/defaut.css" />

#+OPTIONS: H:6
#+OPTIONS: toc:nil

#+TAGS: noexport(n)

[[file:index.org][Index des Grimoires]]

#+INCLUDE: "../include/navigan-1.org"

#+TOC: headlines 1

#+INCLUDE: "../include/latex/latex.org"

* Polynômes de Taylor

Considérons un polynôme $p : \setR \mapsto \setR$ de degré $n$ défini par :

$$p(x) = \sum_{i = 0}^n \gamma_i \cdot x^i$$

pour tout $x \in \setR$. Calculons ses dérivées :

\begin{align*}
\partial p(x) &= \sum_{i = 1}^n \gamma_i \cdot i \cdot x^{i - 1} \\
\partial^2 p(x) &= \sum_{i = 2}^n \gamma_i \cdot i \cdot (i - 1) \cdot x^{i - 2} \\
\vdots \\
\partial^k p(x) &= \sum_{i = k}^n \gamma_i \cdot \frac{i !}{(i - k) !} \cdot x^{i - k} \\
\vdots \\
\partial^n p(x) &= n! \cdot \gamma_n
\end{align*}

Lorsqu'on évalue ces dérivées en $0$, seuls les termes en $x^{k - k} = 1$ ne s'annulent pas. On obtient donc :

$$\partial^k p(0) = \frac{k !}{0 !} \cdot \gamma_k = k ! \cdot \gamma_k$$

ce qui nous donne l'expression des coefficients de $p$ en fonction de ses dérivées en $0$ :

$$\gamma_k = \unsur{k !} \cdot \partial^k p(0)$$

Le polynôme peut donc se réécrire :

$$p(x) = \sum_{i = 0}^n \unsur{i !} \cdot \partial^i p(0) \cdot x^i$$

Cette expression est appelée développement de Taylor de $p$ autour de $0$.


** Généralisation

Soit $a \in \setR$. La fonction $r$ définie par :

$$r(t) = p(t + a) = \sum_{i = 0}^n \gamma_i \cdot (t + a)^i$$

pour tout $t \in \setR$ est clairement un polynôme de degré $n$. On a $r(0) = p(a)$ et plus généralement :

$$\partial^i r(0) = \partial^i p(a)$$

pout tout $i \ge 0$. Le développement de Taylor de $r$ autour de $0$ s'écrit :

$$r(t) = \sum_{i = 0}^n \unsur{i !} \cdot \partial^i r(0) \cdot t^i$$

ou encore :

$$r(t) = \sum_{i = 0}^n \unsur{i !} \cdot \partial^i p(a) \cdot t^i$$

En posant $x = t + a$, on a $t = x - a$ et :

$$p(x) = p(t + a) = r(t) = r(x - a)$$

Le développement devient :

$$p(x) = \sum_{i = 0}^n \unsur{i !} \cdot \partial^i p(a) \cdot (x - a)^i$$

Cette expression est nommée développement de Taylor de $p$ autour de $a$.


* Opérateur de Taylor

Soit $\alpha, \beta \in \setR$ avec $\alpha \strictinferieur \beta$, une fonction $f \in \continue^N([\alpha,\beta],\setR)$ et $a \in [\alpha,\beta]$. Par analogie avec le développement de Taylor des polynômes, on définit l'opérateur de Taylor $T_a^N$ par :

$$T_a^N(f)(x) = \sum_{k = 0}^N \unsur{k !} \cdot \partial^k f(a) \cdot (x - a)^k$$

pour tout $x \in [\alpha,\beta]$.


** Erreur

L'erreur $E_a^N$ de l'opérateur $T_a^N$ est donnée par :

$$E_a^N(f)(x) = f(x) - T_a^N(f)(x)$$

pour tout $x \in [\alpha,\beta]$.


** Polynômes

Si $p$ est un polynôme de degré $N$, on a bien entendu $T_a^N(p) = p$ pour tout $a \in \setR$ et $E_a^N(p) = 0$.


* Forme intégrale


** Premier ordre

Soit $\alpha, \beta \in \setR$ avec $\alpha \strictinferieur \beta$ et la fonction $f \in \continue^2([\alpha,\beta],\setR)$. Le théorème fondamental nous dit que :

$$\int_a^x \partial f(t) \ dt = f(x) - f(a)$$

pour tout $a,x \in [\alpha,\beta]$. Appliquant le même théorème à la dérivée $\partial f$, on a aussi :

$$\int_a^x \partial^2 f(t) \ dt = \partial f(x) - \partial f(a)$$


*** Intégration par parties

Soit $u = \partial f$ et $v = \identite$. on a :

$$\int_a^x u(x) \ \partial v(x) \ dx = \int_a^x \partial f(t) \cdot 1 \ dt = \int_a^x \partial f(t) \ dt$$

L'intégration par parties nous donne :

$$\int_a^x u(x) \ \partial v(x) \ dx = v(x) \ u(x) - v(a) \ u(a) - \int_a^x v(t) \ \partial u(t) \ dt$$

En tenant compte des définitions de $u$ et $v$, on obtient :

$$\int_a^x \partial f(t) \ dt = x \ \partial f(x) - a \ \partial f(a) - \int_a^x t \ \partial^2 f(t) \ dt$$


Appliquons le théorème fondamental au membre de gauche :

$$f(x) - f(a) = x \ \partial f(x) - a \ \partial f(a) - \int_a^x t \ \partial^2 f(t) \ dt$$

ou encore :

$$f(x) = f(a) + x \ \partial f(x) - a \ \partial f(a) - \int_a^x t \ \partial^2 f(t) \ dt$$

En multipliant la relation :

$$\partial f(x) - \partial f(a) = \int_a^x \partial^2 f(t) \ dt$$

par $x$, on arrive au résultat :

$$x \ \partial f(x) = x \ \partial f(a) + \int_a^x x \ \partial^2 f(t) \ dt$$

En remplaçant $x\ \partial f(x)$ par le membre de droite dans
l’expression de $f(x)$, on obtient :

$$f(x) = f(a) + x \ \partial f(a) + \int_a^x x \ \partial^2 f(t) \ dt - a \ \partial f(a) - \int_a^x t \ \partial^2 f(t) \ dt$$

et finalement :

$$f(x) = f(a) + (x - a) \cdot \partial f(a) + \int_a^x (x - t) \cdot \partial^2 f(t) \ dt$$

Le membre de droite est appelé développement de Taylor du premier ordre de $f$ sous forme intégrale.


** Second ordre

Soit $f \in \continue^3([\alpha,\beta],\setR)$. Comme $\continue^3 \subseteq \continue^2$, $f$ admet un développement de Taylor du premier ordre sous forme intégrale. Nous allons intégrer par parties le terme :

$$\int_a^x (x - t) \cdot \partial^2 f(t) \ dt$$

On sait que :

$$\OD{}{t} \left[ \unsur{2} (x - t)^2 \right] = (x - t) \cdot (-1) = - (x - t)$$

Posons $u = \partial^2 f$ et :

$$v : t \mapsto \unsur{2} (x - t)^2$$

On a :

$$\int_a^x \partial v(t) \ u(t) \ dt = - \int_a^x (x - t) \ \partial^2 f(t) \ dt$$

et :

$$\int_a^x v(t) \ \partial u(t) \ dt = \unsur{2} \int_a^x (x - t)^2 \ \partial^3 f(t) \ dt$$

Enfin :

\begin{align*}
\int_a^x \partial (v \cdot u)(t) \ dt &= \unsur{2} \ (x - x)^2 \ \partial^2 f(x) - \unsur{2} \ (x - a)^2 \ \partial^2 f(a) \\
&= 0 - \unsur{2} \ (x - a)^2 \ \partial^2 f(a) \\
&= - \unsur{2} \ (x - a)^2 \ \partial^2 f(a)
\end{align*}

On en conclut que :

$$- \int_a^x (x - t) \ \partial^2 f(t) \ dt = - \unsur{2} \ (x - a)^2 \ \partial^2 f(a) - \unsur{2} \int_a^x (x - t)^2 \ \partial^3 f(t) \ dt$$

ou encore :

$$\int_a^x (x - t) \ \partial^2 f(t) \ dt = \unsur{2} \ (x - a)^2 \ \partial^2 f(a) + \unsur{2} \int_a^x (x - t)^2 \ \partial^3 f(t) \ dt$$

Le développement du premier ordre peut dont se réécrire :

$$f(x) = f(a) + (x - a) \ \partial f(a) + \unsur{2} \ (x - a)^2 \ \partial^2 f(a) + \unsur{2} \int_a^x (x - t)^2 \ \partial^3 f(t) \ dt$$

Le membre de droite est appelé développement du second ordre de $f$ sous forme intégrale.


** Ordre $N$

Soit $f \in \continue^{N + 1}([\alpha,\beta],\setR)$. On montre en intégrant par parties que :

$$\int_a^x (x - t)^{k - 1} \ \partial^k f(t) \ dt = \unsur{k} \ (x - a)^k \ \partial^k f(a) + \unsur{k} \int_a^x (x - t)^k \ \partial^{k + 1} f(t) \ dt$$

pour tout $k \in \setZ[2,N]$. On en déduit par récurrence le développement de Taylor d'ordre $N$ de $f$ sous forme intégrale :

$$f(x) = \sum_{k = 0}^N \unsur{k !} \cdot \partial^k f(a) \cdot (x - a)^k + \unsur{N !} \int_a^x (x - t)^N \ \partial^{N + 1} f(t) \ dt$$


* Erreur

On a :

$$E_a^N(f)(x) = f(x) - T_a^N(f)(x) = \unsur{N !} \int_a^x (x - t)^N \ \partial^{N + 1} f(t) \ dt$$

En appliquant le théorème de Cauchy entre $a$ et $x$ aux fonctions $F,G$ définies par :

$$ F(z) = \int_a^z (x - t)^N \ \partial^{N + 1} f(t) \ dt $$

$$ G(z) = \int_a^z (x - t)^N \ dt $$

pour tout $z \in [\alpha,\beta]$, on voit que l'on peut trouver un $c
\in \intervalleouvert{a}{x}$ si $a \strictinferieur x$ ou un $c \in
\intervalleouvert{x}{a}$ si $x \strictinferieur a$ tel que :

$$(x - c)^N \ F(x) = (x - c)^N \ \partial^{N + 1} f(c) \ G(x)$$

ou encore :

$$\partial^{N + 1} f(c) \ G(x) = F(x)$$

Comme :

\begin{align*}
G(x) = \int_a^x (x - t)^N \ dt &= - \big[ (x - x)^{N + 1} - (x - a)^{N + 1} \big] / (N + 1) \\
&= - \big[ 0 - (x - a)^{N + 1} \big] / (N + 1) \\
&= (x - a)^{N + 1} / (N + 1)
\end{align*}

on a :

$$\partial^{N + 1} f(c) \ \frac{ (x - a)^{N + 1} }{N + 1} = F(x) = \int_a^x (x - t)^N \ \partial^{N + 1} f(t) \ dt$$

On en déduit que :

$$E_a^N(f)(x) = \partial^{N + 1} f(c) \ \frac{ (x - a)^{N + 1} }{(N + 1) !}$$


* Forme différentielle

Soit une fonction $f \in \continue^{N+1}([\alpha,\beta],\setR)$ et $a,x \in [\alpha,\beta]$. On définit la fonction $F : [\alpha,\beta] \mapsto \setR$ par :

$$ F(t) = \sum_{k = 0}^N \unsur{k !} \cdot \partial^k f(t) \cdot (x - t)^k
= f(t) + \partial f(t) \ (x - t) + \partial^2 f(t) \ \frac{(x - t)^2}{2} + ... $$

pour tout $t \in [\alpha,\beta]$. On a :

$$F(x) = f(x) + \partial f(x) \ (x - x) + \partial^2 f(x) \frac{(x - x)^2}{2} + ... = f(x) + 0 = f(x)$$

et :

$$F(a) = f(a) + \partial f(a) \ (x - a) + \partial^2 f(a) \frac{(x - a)^2}{2} + ... = T_a^N(f)(x)$$

La dérivée de $F$ s'écrit :

$$ \partial F(t) = \partial f(t) + \big[ \partial f(t) \ (-1) + \partial^2 f(t) \ (x - t) \big] +
\left[ - \partial^2 f(t) \ (x - t) + \partial^3 f(t) \ \frac{(x-t)^2}{2} \right] + \ldots +
\left[ - \partial^N f(t) \ \frac{(x-t)^{N - 1}}{(N - 1) !} + \partial^{N + 1} f(t) \ \frac{(x-t)^N}{N !} \right] $$

On voit que tous les termes s'annulent sauf le dernier, et :

$$\partial F(t) = \partial^{N + 1} f(t) \ \frac{(x-t)^N}{N !}$$

Soit $G \in \continue^1([\alpha,\beta],\setR)$. On peut appliquer le
théorème de Cauchy à $F$ et $G$ entre $a$ et $x$. On dispose alors
d'un $c \in \intervalleouvert{a}{x}$ si $a \strictinferieur x$ ou d'un
$c \in \intervalleouvert{x}{a}$ si $x \strictinferieur a$ tel que :

$$\partial F(c) \ \big[G(x) - G(a)\big] = \big[F(x) - F(a)\big] \ \partial G(c)$$

On a :

$$F(x) - F(a) = f(x) - T_a^N(f)(x) = E_a^N(f)(x)$$

On en conclut que :

$$E_a^N(f)(x) \ \partial G(c) = \partial^{N + 1} f(c) \ \frac{(x-c)^N}{N !} \ \big[G(x) - G(a)\big]$$


** Forme de Lagrange

Soit le choix :

$$G : t \mapsto (x - t)^{N + 1}$$

on a :

$$G(x) = (x - x)^{N + 1} = 0$$

et :

$$G(a) = (x - a)^{N + 1}$$

La dérivée s'écrit :

$$\partial G(t) = - (N  + 1) \ (x - t)^N$$

La relation de Cauchy devient :

$$- E_a^N(f)(x) \ (N  + 1) \ (x - c)^N = - \partial^{N + 1} f(c) \ \frac{(x-c)^N}{N !} \ (x - a)^{N + 1}$$

On a donc l'expression de l'erreur :

$$E_a^N(f)(x) = \partial^{N + 1} f(c) \ \frac{(x - a)^{N + 1}}{(N + 1) !}$$


** Forme de Cauchy

Soit le choix :

$$G : t \mapsto t - a$$

on a :

$$G(x) = x - a$$

et :

$$G(a) = a - a = 0$$

La dérivée s'écrit :

$$\partial G(t) = 1$$

La relation de Cauchy devient :

$$E_a^N(f)(x) = \partial^{N + 1} f(c) \ \frac{(x-c)^N}{N !} \ (x - a)$$


* Borne

Soit $f \in \continue^{N + 1}([\alpha,\beta],\setR)$. Comme $\partial^{N+1} f$ est continue, sa norme $\norme{.}_\infty$ sur $[\alpha,\beta]$ est finie et on a :

$$\abs{E_a^N(f)(x)} \le \norme{\partial^{N + 1} f}_\infty \ \frac{ \abs{x - a}^{N + 1} }{(N + 1) !}$$

On peut majorer cette expression en constatant que :

$$\abs{x - a} \le \abs{\beta - \alpha}$$

La borne de l'erreur devient alors :

$$\abs{E_a^N(f)(x)} \le \norme{\partial^{N + 1} f}_\infty \ \frac{ \abs{\beta - \alpha}^{N + 1} }{(N + 1) !}$$

Le membre de droite ne dépendant pas de $x$, on a :

$$\norme{E_a^N(f)}_\infty \le \norme{\partial^{N + 1} f}_\infty \ \frac{ \abs{\beta - \alpha}^{N + 1} }{(N + 1) !}$$


* Convergence

Soit $f \in \continue^\infty([\alpha,\beta],\setR)$. Si on peut trouver un $\sigma \in \setR$ tel que :

$$\norme{\partial^n f}_\infty \le \sigma$$

pour tout $n \in \setN$, on a :

$$\norme{E_a^N(f)}_\infty \le \sigma \ \frac{ \abs{\beta - \alpha}^{N + 1} }{(N + 1) !}$$

On en conclut que :

$$0 \le \lim_{N \to \infty} \norme{E_a^N(f)}_\infty \le \sigma \ \lim_{N \to \infty} \frac{ \abs{\beta - \alpha}^{N + 1} }{(N + 1) !} = 0$$

L'erreur converge vers zéro quand $N$ tend vers l'infini :

$$\lim_{N \to \infty} \norme{E_a^N(f)}_\infty = 0$$


* Dimension $n$


** Premier ordre

Soit $\Omega \subseteq \setR^n$, la fonction $f \in \continue^1(\Omega,\setR)$ et les vecteurs $u,v \in \setR^n$ tels que le segment $[u,v]$ est inclus dans $\Omega$. On définit la fonction $\lambda : [0,1] \mapsto \setR^n$ associée au segment $[u,v]$ par :

$$\lambda(s) = u + s \cdot (v - u)$$

pour tout $s \in [0,1]$, ainsi que la fonction $\varphi = f \circ \lambda$ qui vérifie :

$$\varphi(s) = (f \circ \lambda)(s) = f(u + s \cdot (v - u))$$

pour tout $s \in [0,1]$. On pose :

$$h = v - u$$

On a :

$$\varphi(0) = f(u)$$

La dérivée s'écrit :

$$\partial \varphi(s) = \sum_i \partial_i f(u + s \cdot h) \cdot h_i$$

ou, en utilisant la notation matricielle :

$$\partial \varphi(s) = \partial f(u + s \cdot h) \cdot h$$

On a la valeur particulière :

$$\partial \varphi(0) = \partial f(u) \cdot h$$

La dérivée seconde s'écrit :

$$\partial^2 \varphi(s) = \sum_{i,j} h_j \cdot \partial^2_{ji} f(u + s \cdot h) \cdot h_i$$

ou, en utilisant la notation matricielle :

$$\partial^2 \varphi(s) = h^\dual \cdot \partial^2 f(u + s \cdot h) \cdot h$$

Le développement du premier ordre de $\varphi$ autour de $0$ s'écrit donc :

$$\varphi(s) = f(u) + s \cdot \partial f(u) \cdot h + E_u^1(s,h)$$

avec :

$$E_u^1(s,h) = h^\dual \cdot \partial^2 f(u + c \cdot h) \cdot h \cdot \frac{(c - 0)^2}{2} =  h^\dual \cdot \partial^2 f(u + c \cdot h) \cdot h \cdot \frac{c^2}{2}$$

pour un certain $c \in \intervalleouvert{0}{s}$. Mais comme :

$$\varphi(1) = f(u + h) = f(v)$$

on en déduit le développement de $f$ :

$$f(v) = f(u) + \partial f(u) \cdot (v - u) + \mathcal{E}_u^1(h)$$

avec :

$$\mathcal{E}_u^1(h) = h^\dual \cdot \partial^2 f(u + c \cdot h) \cdot h \cdot \frac{c^2}{2}$$

pour un certain $c \in \intervalleouvert{0}{1}$.


*** Borne

Soit :

$$M^2 = \max_{i,j} \norme{\partial^2_{ij} f}_\infty$$

On a :

$$\abs{\mathcal{E}_u^1(h)} \le \unsur{2} \cdot n^2 \cdot M^2 \cdot \norme{h}^2$$


** Second ordre

Soit $f \in \continue^3(\Omega,\setR)$. Avec les mêmes notations que précédemment, on a :

$$\partial^2 \varphi(0) = h^\dual \cdot \partial^2 f(u) \cdot h$$

La dérivée tierce de $\varphi$ s'écrit :

$$\partial^3 \varphi(s) = \sum_{i,j,k} \partial^3_{kji} f(u + s \cdot h) \cdot h_i \cdot h_j \cdot h_k$$

ou, en utilisant la notation tensorielle :

$$\partial^3 \varphi(s) = \partial^3 f(u + s \cdot h) : h \otimes h \otimes h$$

Le développement  du second ordre de $\varphi$ autour de $0$ s'écrit :

$$\varphi(s) = f(u) + s \ \partial f(u) \cdot h + \frac{s^2}{2} \ h^\dual \cdot \partial^2 f(u) \cdot h + E_u^2(s,h)$$

avec :

$$E_u^2(s,h) = \partial^3 f(u + c \cdot h) : h \otimes h \otimes h \cdot \frac{c^3}{6}$$

pour un certain $c \in \intervalleouvert{0}{s}$. Mais comme :

$$\varphi(1) = f(u + h) = f(v)$$

on en déduit le développement de $f$ :

$$f(v) = f(u) + \partial f(u) \cdot h + h^\dual \cdot \partial^2 f(u) \cdot h + \mathcal{E}_u^2(h)$$

avec :

$$\mathcal{E}_u^2(h) = \partial^3 f(u + c \cdot h) : h \otimes h \otimes h \cdot \frac{c^3}{6}$$

pour un certain $c \in \intervalleouvert{0}{1}$.


*** Borne

Soit :

$$M^3 = \max_{i,j,k} \norme{\partial^3_{ijk} f}_\infty$$

On a :

$$\abs{\mathcal{E}_u^2(h)} \le \unsur{6} \cdot n^3 \cdot M^3 \cdot \norme{h}^3$$


* Notation

Soit la fonction $E : \Omega \subseteq \setR^m \mapsto \setR^n$, la
fonction $b : \setR \mapsto \setR$ et le vecteur $h \in \Omega$. On
note $E \sim \petito{b(h)}$, ou on dit que $E$ est en $\petito{b(h)}$,
pour signifier que :

$$\lim_{h \to 0} \frac{ \norme{E(h)} }{b(\norme{h})} = 0$$

On note $E \sim \grando{b(h)}$, ou on dit que $E$ est en $\grando{b(h)}$,
pour signifier qu'il existe $M \in \setR$ tel que :

$$\norme{E(h)} \le M \cdot b(\norme{h})$$

pour tout $h \in \Omega$.

** Puissance

Une famille de fonction souvent employée est la puissance :

$$b_k : x \mapsto x^k$$

pour un certain $k \in \setN$. On a alors $\petito{h^k}$ si :

$$\lim_{h \to 0} \frac{ \norme{E(h)} }{\norme{h}^k} = 0$$

et $\grando{h^k}$ si :

$$\norme{E(h)} \le M \cdot \norme{h}^k$$


** Relation

Si $E \sim \grando{h^k}$, on a :

$$0 \le \lim_{h \to 0} \frac{\norme{E(h)}}{\norme{h}^{k - 1}} \le \lim_{h \to 0} \frac{M \ \norme{h}^k}{\norme{h}^{k - 1}} = 0$$

d'où :

$$\lim_{h \to 0} \frac{\norme{E(h)}}{\norme{h}^{k - 1}} = 0$$

et $E \sim \petito{h^{k - 1}}$.


** Cas particulier

Le $\grando{1}$ implique une erreur bornée en valeur absolue, le $\petito{1}$ implique la continuité et le $\petito{h}$ la différentiabilité.


** Développement de Taylor

Pour toute fonction $f \in \continue^{N + 1}(\Omega, \setR^n)$, l'erreur $E_a^N(f)$ du développement de Taylor d'ordre $N$ est en $\grando{h^{N+1}}$.


* Extrapolation de Richardson

Supposons qu'une fonction $v$ nous donne une approximation de $V$ respectant :

$$v(h) \approx V + C \cdot h^m + O(h^{m+1})$$

pour un certain $C \in \setR$ et pour tout $h \in [0,R] \subseteq \setR$. L'entier $m$ est appelé l'ordre de l'approximation. Supposons que l'on dispose de deux estimations de $V_1 = v(h)$ et $V_2 = v(h/k)$. On a alors :

#+BEGIN_CENTER
\(
V_1 = v(h) = V + C \cdot h^m + O(h^{m+1}) \)

\(
V_2 = v\left(h/k\right) = V + C \cdot \left(\frac{h}{k}\right)^m
+ O(h^{m+1})
\)
#+END_CENTER

On se sert de la première équation pour obtenir une expression de $C \cdot h^m$ :

$$C \cdot h^m = V_1 - V + O(h^{m+1})$$

Posons :

$$r = \unsur{k^m}$$

On a alors :

$$V_2 = V + r \cdot C \cdot h^m + O(h^{m+1}) = V + r \cdot (V_1 - V) + O(h^{m+1})$$

On en conclut que :

$$(1 - r) \cdot V = V_2 - r \cdot V_1 + O(h^{m+1})$$

Ce qui nous donne l'approximation :

$$V = \frac{V_2 - r \cdot V_1}{1 - r} + O(h^{m+1})$$

Cette approximation est plus précise, car l'erreur n'est plus en $O(h^m)$ mais en $O(h^{m + 1})$. On appelle cette technique l'extrapolation de Richardson.

** Cas particulier

Un cas particulier intéressant est celui où l'approximation est d'ordre $1$ et où $k = 2$. On a alors :

$$V = 2 V_2 - V_1 + O(h^2) = V_2 + (V_2 - V_1) + O(h^2)$$

ce qui revient à faire l'approximation $V - V_2 \approx V_2 - V_1$.

* Nombres binômiaux

