
#+STARTUP: showall

#+TITLE: Eclats de vers : Matemat : Espaces vectoriels
#+AUTHOR: chimay
#+EMAIL: or du val chez gé courriel commercial
#+LANGUAGE: fr
#+LINK_HOME: file:../index.html
#+LINK_UP: file:index.html
#+HTML_HEAD: <link rel="stylesheet" type="text/css" href="../style/defaut.css" />

#+OPTIONS: H:6
#+OPTIONS: toc:nil

#+TAGS: noexport(n)

[[file:index.org][Index mathématique]]

#+INCLUDE: "../include/navigan-1.org"

#+TOC: headlines 1

#+INCLUDE: "../include/latex/latex.org"

\label{chap:vecteur}

* Dépendances

  - Chapitre \ref{chap:algebre} : Les structures algébriques
  - Chapitre \ref{chap:somme} : Les sommes

* Introduction

Soit un corps $\corps$, un ensemble quelconque $A$ et $n \in \setN$. Le
but des espaces vectoriels est de fournir un cadre général aux
n-tuples de $\corps^n$ et aux fonctions de $\corps^A$. Nous avons vu la
correspondance $\corps^n \leftrightarrow \corps^A$ dans le cas particulier
où $A$ possède un nombre fini d'éléments. Mais le lien entre les deux
types d'objets ne s'arrête pas là : la comparaison de deux fonctions
se base sur le même principe (étendu) que la comparaison de deux
n-tuples :

$$ f \le g \Leftrightarrow f(x) \le g(x) \qquad \forall x \in \domaine f $$

$$ u \le v \Leftrightarrow u_i \le v_i \qquad \forall i \in \{1,2,...,n\} $$

Nous avons également défini des produits mixtes $\cdot :
\corps \times \corps^n \to \corps^n$ :

$$ \alpha \cdot (u_i)_i = (\alpha \cdot u_i)_i $$

et $\cdot : \corps \times \corps^A \to \corps^A$ :

$$ (\alpha \cdot f)(x) = \alpha \cdot f(x) $$

semblables. Plus généralement, ces opérations respectent les mêmes
[[file:vecteurs-de-dimension-finie.org::#heading:proprietes_operations_sur_vecteurs_de_dimension_finie][propriétés]]
que les opérations sur les vecteurs de $\corps^n$

* Définition

Soit un groupe commutatif pour l'addition $E$, ainsi qu'un corps $\corps$
et une opération de multiplication mixte $\cdot : \corps \times E \mapsto
E$. Si, pour tout $u,v,w \in E$ et $\alpha, \beta \in \corps$, on a :

\begin{align*}
0 + u &= u \\
u + (-u) &= 0 \\
1 \cdot u &= 1 \\
u + v &= v + u \\
u + (v + w) &= (u + v) + w \\
(\alpha \cdot \beta) \cdot u &= \alpha \cdot (\beta \cdot u) \\
(\alpha + \beta) \cdot u &= \alpha \cdot u + \beta \cdot u \\
\alpha \cdot (u + v) &= \alpha \cdot u + \alpha \cdot v
\end{align*}

pour un certain vecteur nul $0 \in E$ neutre pour l’addition, on dit que
$E$ est un espace vectoriel sur $\corps$. On nomme alors « vecteurs »
les éléments de $E$ et « scalaires » les éléments de $\corps$.

On voit que l'addition induite sur $E$ par l'addition de $\corps$
transforme $E$ en groupe commutatif. On ne peut toutefois pas parler de
corps pour $E$. Comme contre-exemple, citons la multiplication matricielle
qui n'est pas une multiplication induite, et est non commutative.

Exemples d’espaces vectoriels :

- $\corps^n$
- $\corps^A$
- les matrices représentant des fonctions linéaires,
  nous pouvons également les ajouter dans la liste
- l’ensemble des polynômes de degré $n-1$ est défini par $n$
  coefficients et est donc similaire à $\corps^n$

** Notations

On note aussi :

$$ u - v = u + (-1) \cdot v $$

$$ u \cdot \alpha = \alpha \cdot u $$

$$ \alpha \cdot \beta \cdot u = (\alpha \cdot \beta) \cdot u $$

$$ \alpha u = \alpha \cdot u $$

Lorsque $\alpha$ a un inverse dans $\corps$, on a même les « fractions » :

$$\frac{u}{\alpha} = \alpha^{-1} \cdot u = \unsur{\alpha} \cdot u$$

** Corollaires

Les propriétés de la multiplication mixte nous montrent directement
que :

#+BEGIN_CENTER
\(
0 \cdot u = (1 - 1) \cdot u = u - u = 0 \)

\(
\alpha \cdot 0 = \alpha \cdot (u - u) = \alpha - \alpha = 0
\)
#+END_CENTER


** Attention

Ne pas confondre les additions définies sur $E$ et $\corps$, ni la
multiplication de $\corps$ avec la multiplication mixte, ni le neutre
de $E$ avec celui de $\corps$. Lorsqu'il y a un risque d'ambiguité,
on parle du vecteur nul $0 \in E$ et du scalaire nul $0 \in \corps$.

** Remarque

Le corps $\corps$ est souvent $\setR$ ou $\setC$.

* Sous-espace

On dit que $F \subseteq E$ est un sous-espace vectoriel de $E$ si $0
\in F$ et si :

$$z = \alpha \cdot u + \beta \cdot v$$

appartient à $F$ quels que soient les vecteurs $u,v \in F$ et les
scalaires $\alpha,\beta \in \corps$.

On vérifie par exemple que $E$ est un sous-espace vectoriel de lui-même.

** Espace engendré

L'espace engendré par les vecteurs $e_1,e_2,...,e_n \in E$ est l'ensemble
des combinaisons linéaires formées à partir des $e_i$ :

$$\combilin{e_1,...,e_n} = \left\{ \sum_{i=1}^{n} \alpha_i \cdot e_i : \alpha_i \in \corps \right\}$$

On vérifie que $\combilin{e_1,...,e_n}$ est un sous-espace vectoriel
de $E$.

* Indépendance linéaire

On dit qu'une série de vecteurs $e_1,...,e_n$ est linéairement
indépendante si pour toute suite de scalaires $\alpha_i$, la condition :

$$\sum_{i=1}^{n} \alpha_i \cdot e_i = 0$$

implique que tous les scalaires soient nuls :

$$\alpha_i = 0$$

pour tout $i \in \{1,2,...,n\}$.

** Coordonnées

Soit les vecteurs linéairement indépendants $(e_1,...,e_n)$ et $u \in
\combilin{e_1,...,e_n}$. On peut alors trouver une suite de scalaires
$\alpha_i$ tels que :

$$u = \sum_{i = 1}^n \alpha_i \cdot e_i$$

Les scalaires $\alpha_1,\alpha_2,\ldots,\alpha_n$ sont appelés
coordonnées de $u$ par rapport aux vecteurs $e_1,e_2,\ldots,e_n$.

*** Unicité

Soit les vecteurs linéairement indépendants $(e_1,...,e_n)$ et $u \in
\combilin{e_1,...,e_n}$. On peut alors trouver une suite de scalaires
$\alpha_i$ tels que :

$$u = \sum_{i = 1}^n \alpha_i \cdot e_i$$

Supposons que l'on ait également :

$$u = \sum_{i=1}^n \beta_i \cdot e_i$$

pour une autre suite de scalaires $\beta_i$. En soustrayant les deux
équations, on obtient :

$$\sum_{i=1}^n (\alpha_i - \beta_i) \cdot e_i = 0$$

L'indépendance linéaire des $e_i$ implique alors que $\alpha_i - \beta_i = 0$, c'est-à-dire :

$$\alpha_i = \beta_i$$

pour tout $i \in \{1,2,...,n\}$.

On a donc unicité des coordonnées.

** Absence de redondance

Soit $e_1,...,e_n \in E$ une suite de vecteurs linéairement indépendants.
Soit $i \in \{ 1,2,...,n \}$ et :

$$J(i) = \{1,2,...,n\} \setminus \{ i \}$$

Supposons que le vecteur $e_i$ soit une combinaison linéaire des autres
vecteurs :

$$e_i = \sum_{ j \in J(i) } \alpha_j \cdot e_j$$

On a donc :

$$e_i - \sum_{ j \in J(i) } \alpha_j \cdot e_j = 0$$

L'hypothèse d'indépendance linéaire voudrait que tous les coefficients
scalaires soient nuls, ce qui n'est manifestement pas le cas puisque le
coefficient de $e_i$ vaut 1 !

Notre hypothèse est donc fausse : aucun des vecteurs de la suite n'est
combinaison linéaire des autres. On dit qu'aucun vecteur n'est redondant
dans la suite.

* Base

Soit une série de vecteurs linéairement indépendants
$e_1,e_2,\ldots,e_n \in E$ qui engendrent complètement $E$ :

$$E \subseteq \combilin{e_1,...,e_n}$$

Soit $u \in E$. On peut alors trouver un unique jeu de coordonnées
$\alpha_1, \alpha_2, \ldots, \alpha_n \in \corps$ tels que :

$$u = \sum_{i = 1}^n \alpha_i \cdot e_i$$

On dit alors que $(e_1,...,e_n)$ forme une base de $E$.

** Dimension finie

On dit qu'un espace vectoriel $E$ est de dimension finie s'il posséde
au moins une base de la forme $(e_1,...,e_n)$, où $n \in \setN$ est
fini. Dans le cas où $E$ /ne possède pas/ une telle base, il est dit
de dimension infinie.

** Equivalence

On voit qu'étant donné une base de $E$, il y a équivalence entre
un vecteur $u \in E$ et un élément $(\alpha_1,\alpha_2,...,\alpha_n)
\in \corps^n$ formé par ses coordonnées.

Nous noterons donc également (et abusivement) $u = (x_1,x_2,...,x_n)$,
mais attention : il ne faut jamais perdre de vue que les $\alpha_i$
dépendent de la base utilisée. Le vecteur $u$ est lui invariant sous
changement de base.

