
#+STARTUP: showall

#+TITLE: Eclats de vers : Matemat : Matrices
#+AUTHOR: chimay
#+EMAIL: or du val chez gé courriel commercial
#+LANGUAGE: fr
#+LINK_HOME: file:../index.html
#+LINK_UP: file:index.html
#+HTML_HEAD: <link rel="stylesheet" type="text/css" href="../style/defaut.css" />

#+OPTIONS: H:6
#+OPTIONS: toc:nil

#+TAGS: noexport(n)

[[file:index.org][Index mathématique]]

#+INCLUDE: "../include/navigan-1.org"

#+TOC: headlines 1

#+INCLUDE: "../include/latex/latex.org"

\label{chap:matrices}

* Dépendances

  - Chapitre \ref{chap:reel} : Les réels
  - Chapitre \ref{chap:complexe} : Les complexes

* Introduction

#+TOC: headlines 1 local

** Définition

Soit un corps $\corps$, une fonction linéaire $\mathcal{A} : \corps^n
\mapsto \corps^m$ de composantes :

$$ \mathcal{A}_i = \composante_i \mathcal{A} $$

et la suite des vecteurs de la base canonique de $\corps^n$ :

$$ (\canonique_1,\canonique_2,...,\canonique_n) $$

Soit un vecteur $u \in \corps^n$ de composantes :

$$ u = (u_j)_j $$

Nous avons aussi :

$$ \mathcal{A}(u) = (\mathcal{A}_i(u))_i $$

En utilisant la linéarité de $\mathcal{A}$, on obtient :

$$ \mathcal{A}(u) = \sum_{j=1}^n \mathcal{A}(\canonique_j) \cdot u_j $$

Les composantes s’écrivent donc :

$$ \mathcal{A}_i(u) = \sum_{j=1}^n \mathcal{A}_i(\canonique_j) \cdot u_j $$

On voit que la fonction $\mathcal{A}$ est entièrement déterminée par les
coefficients scalaires :

$$ a_{ij} = \mathcal{A}_i(\canonique_j) $$

pour tout $i\in\{1,2,...,m\}$ et $j\in\{1,2,...,n\}$. On a donc :

$$ \mathcal{A}_i(u) = \sum_{j=1}^n a_{ij} \cdot u_j $$

La structure en double indice des coefficients $a_{ij}$ nous pousse à
représenter $\mathcal{A}$ par le tableau de $m$ lignes et $n$ colonnes :

$$ A = \begin{Matrix}{cccc}
a_{11} & a_{12} & \ldots & a_{1n} \\
a_{21} & a_{22} & \ldots & a_{2n} \\
\vdots &        & \ddots &  \vdots \\
a_{m1} & a_{m2} & \ldots & a_{mn}
\end{Matrix} $$

Un tableau $A$ de cette forme est appelé matrice de taille $(m,n)$. Les
coefficients $a_{ij}\in\corps$ sont appelés composantes de $A$.

On note $\matrice(\corps,m,n)$ l'ensemble des matrices à $m$ lignes et
$n$ colonnes.

** Composantes

Soit une matrice $A\in\matrice(\corps,m,n)$ :

$$ A = \begin{Matrix}{cccc}
a_{11} & a_{12} & \ldots & a_{1n} \\
a_{21} & a_{22} & \ldots & a_{2n} \\
\vdots &        & \ddots &  \vdots \\
a_{m1} & a_{m2} & \ldots & a_{mn}
\end{Matrix} $$

On note les composantes sous la forme plus compacte :

$$ A = ( a_{ij} )_{i \in \{1,2,...,m\},j \in \{1,2,...,n\}} $$

ou :

$$ A = [ a_{ij} ]_{i \in \{1,2,...,m\},j \in \{1,2,...,n\}} $$

Par convention, le numéro de ligne passe avant le numéro de colonne
à l'extérieur de la parenthèse (ou du crochet). Lorsque les nombres
de lignes et de colonnes sont évidentes d’après le contexte, on note
plus simplement :

$$ A = ( a_{ij} )_{i,j} $$

avec ou sans la virgule :

$$ A = (a_{ij})_{ij} $$

ou encore :

$$ A = [ a_{ij} ]_{i,j} = [ a_{ij} ]_{ij} $$

On dit aussi que $a_{ij}$ est la composante $i,j$ de $A$, et on le note :

$$a_{ij} = \composante_{i,j} A$$

avec ou sans virgule :

$$a_{ij} = \composante_{ij} A$$

** Egalité

Il est clair que deux matrices $A,B \in \matrice(\corps,m,n)$ :

$$ A = ( a_{ij} )_{i,j} $$

$$ B = ( b_{ij} )_{i,j} $$

sont égales ($A = B$) si et seulement si toutes leurs composantes
le sont :

$$a_{ij} = b_{ij}$$

pour tout $(i,j) \in \{ 1,2,...,m \} \times \{ 1,2,...,n \}$.

** Ordre partiel

Soit les matrices $A,B \in \matrice(\corps,m,n)$ :

$$ A = ( a_{ij} )_{i,j} $$

$$ B = ( b_{ij} )_{i,j} $$

On dit que $A$ est inférieure à $B$, et on le note $A \le B$, si :

$$a_{ij} \le b_{ij}$$

pour tout $(i,j) \in \{ 1,2,...,m \} \times \{ 1,2,...,n \}$.

** Scalaires

On peut associer à tout scalaire $a \in \corps$ une matrice $A$
de taille $(1,1)$ :

$$ A = [ a ] $$

et vice-versa. On a donc l’équivalence :

$$ \matrice(\corps,1,1) \equiv \corps $$

On assimile donc toute matrice de taille $(1,1)$ à un scalaire.

** Vecteurs lignes et colonnes

On peut considérer un vecteur ligne comme une matrice ne possédant
qu'une seule ligne. Un vecteur ligne est donc de taille générique
$(1,n)$.

De même, on peut considérer un vecteur colonne omme une matrice ne
possédant qu'une seule colonne. Un vecteur colonne est donc de taille
générique $(m,1)$.

*** Équivalence avec $\corps^n$

A partir d'un n-tuple $(x_1,x_2,...,x_n) \in \corps^n$, on peut former
un vecteur ligne $l \in \matrice(\corps,1,n)$ et un vecteur colonne $c
\in \matrice(\corps,n,1)$. Les réciproques étant également vraies,
on a les équivalences :

#+begin_center
$\matrice(\corps,1,n) \equiv \matrice(\corps,n,1) \equiv \corps^n$
#+end_center

* Opérations

#+TOC: headlines 1 local

** Multiplication d’une matrice et d’un vecteur

Soit une matrice $A \in \matrice(\corps,m,n)$ représentant une fonction
linéaire $\mathcal{A} : \corps^n \mapsto \corps^m$. On définit le
produit matrice - vecteur pour que :

$$ A \cdot u = \mathcal{A}(u) $$

pour tout vecteur colonne $u \in \corps^n$. Si :

$$ A = (a_{ij})_{i,j} $$

on a par définition :

$$ \mathcal{A}_i(u) = \sum_{j=1}^n a_{ij} \cdot u_j $$

où $\mathcal{A}_i$ est la $i^{ème}$ composante de $\mathcal{A}$. Comme
$u$ est un vecteur colonne, on impose que le résultat de $A \cdot u$ soit
également un vecteur colonne. La définition du produit matrice-vecteur
est dès lors immédiate :

$$ A \cdot u = \left[ \sum_{j=1}^n a_{ij} \cdot u_j \right]_i $$

** Produit matriciel

Soit les fonctions linéaires $\mathcal{A} : \corps^n \mapsto
\corps^m$ et $\mathcal{B} : \corps^p \mapsto \corps^n$ respectivement
représentées par les matrices $A \in \matrice(\corps,m,n)$ et $B \in
\matrice(\corps,n,p)$. On définit le produit matriciel $A \cdot B$
pour qu’il représente la fonction $\mathcal{A} \circ \mathcal{B}$. On
a donc :

$$ (A \cdot B) \cdot u
= (\mathcal{A} \circ \mathcal{B})(u)
= \mathcal{A}\left(\mathcal{B}(u)\right) $$

pour tout vecteur colonne $u \in \corps^p$. Par définition du produit
matrice-vecteur :

$$ (A \cdot B) \cdot u
= \mathcal{A}\left(\mathcal{B}(u)\right)
= A \cdot (B \cdot u) $$

En terme de composantes, si :

$$ A = (a_{ij})_{i,j} $$

$$ B = (b_{ij})_{i,j} $$

$$ u = (u_i)_i $$

on a :

\begin{align*}
(A \cdot B) \cdot u
&= \mathcal{A}\left(\mathcal{B}(u)\right) \\
&= \sum_{k=1}^n a_{ik} \cdot (B \cdot u)_k = \sum_{k=1}^n a_{ik} \sum_{j=1}^p b_{kj} \cdot u_j \\
&= \sum_{k=1}^n \sum_{j=1}^p a_{ik} \cdot b_{kj} \cdot u_j \\
&= \sum_{j=1}^p \left[ \sum_{k=1}^n a_{ik} \cdot b_{kj} \right] \cdot u_j
\end{align*}

La dernière ligne représente le produit d’une matrice
$C\in\matrice(\corps,m,p)$ de composantes :

$$ \composante_{ij} C = \sum_{k=1}^n a_{ik} \cdot b_{kj} $$

par $u$ :

$$ (A \cdot B) \cdot u = C \cdot u $$

Ce résultat étant valable pour tout vecteur colonne $u \in \corps^p$, on
en déduit que $C$ est identique à $A \cdot B$, ce qui nous donne :

$$A \cdot B = \left[\sum_{k=1}^n a_{ik} \cdot b_{kj}\right]_{i,j}$$

Le produit d'une matrice de taille $(m,n)$ par une matrice de taille
$(n,p)$ est une matrice de taille $(m,p)$. Pour que ce produit soit
bien défini, il est nécessaire que le nombre de colonnes $n$ de $A$
et le nombre de lignes de $B$ soient identiques.

*** Notation

En pratique, on laisse souvent tomber le ``$\cdot$'' et on note $A B$
au lieu de $A \cdot B$ lorsqu'il est évident que $A$ et $B$ sont deux
matrices différentes.

*** Propriétés

**** Associativité

Soit les matrices $A \in \matrice(\corps,m,n)$, $B \in
\matrice(\corps,n,p)$ et $C \in \matrice(\corps,p,q)$ définies par :

$$ A = ( a_{ij} )_{i,j} $$

$$ B = ( b_{ij} )_{i,j} $$

$$ C = ( c_{ij} )_{i,j} $$

La relation :

$$A \cdot (B \cdot C)
= \left[ \sum_{k,l} a_{ik} \cdot b_{kl} \cdot c_{lj} \right]_{i,j} = (A \cdot B) \cdot C$$

nous montre que la multiplication entre matrices est associative. On définit :

$$A \cdot B \cdot C = A \cdot (B \cdot C) = (A \cdot B) \cdot C$$

**** Non commutativité

Par contre, on peut trouver des matrices $A$ et $B$ telles que :

$$A \cdot B \ne B \cdot A$$

La multiplication matricielle n'est donc en général pas commutative. D'ailleurs, pour que ces deux produits existent simultanément, il faut que $A$ et $B$ soient toutes deux carrées, ce qui n'est pas forcément le cas.

*** Commutateur

La matrice associée au commutateur :

$$[\mathcal{A},\mathcal{B}] = \mathcal{A} \circ \mathcal{B} - \mathcal{B} \circ \mathcal{A}$$

est donnée par le commutateur équivalent :

$$[A,B] = A \cdot B - B \cdot A$$

** Addition

Soit les fonctions linéaires $\mathcal{A},\mathcal{B} : \corps^n
\mapsto \corps^m$ respectivement représentées par les matrices $A,B
\in \matrice(\corps,m,n)$. On définit l’addition matricielle $A + B$
pour qu’elle représente la fonction $\mathcal{A} + \mathcal{B}$. On
a donc :

$$ (A + B) \cdot u
= (\mathcal{A} + \mathcal{B})(u)
= \mathcal{A}(u) + \mathcal{B}(u) $$

pour tout vecteur colonne $u \in \corps^p$. Par définition du produit
matrice-vecteur :

$$ (A + B) \cdot u
= \mathcal{A}(u) + \mathcal{B}(u)
= A \cdot u + B \cdot u $$

En terme de composantes, si :

$$ A = (a_{ij})_{i,j} $$

$$ B = (b_{ij})_{i,j} $$

$$ u = (u_i)_i $$

on a :

\begin{align*}
(A + B) \cdot u
&= \mathcal{A}(u) + \mathcal{B}(u) \\
&= \sum_{j=1}^n a_{ij} \cdot u_j + \sum_{j=1}^n b_{ij} \cdot u_j \\
&= \sum_{j=1}^n \left[ a_{ij} + b_{ij} \right] \cdot u_j
\end{align*}

La dernière ligne représente le produit d’une matrice
$C\in\matrice(\corps,m,p)$ de composantes :

$$ \composante_{ij} C = a_{ij} + b_{ij} $$

par $u$ :

$$ (A + B) \cdot u = C \cdot u $$

Ce résultat étant valable pour tout vecteur colonne $u \in \corps^p$, on
en déduit que $C$ est identique à $A + B$, ce qui nous donne :

$$A + B = \left[ a_{ij} + b_{ij} \right]_{i,j}$$

*** Neutre

La matrice nulle $0$ est le neutre pour cette opération :

$$A + 0 = A$$

On en déduit que tous ses éléments doivent être nuls :

$$0 = ( 0 )_{i,j}$$

On note $0_{m,n}$ au lieu de $0$ lorsqu'on veut préciser que $0$ est
de taille $(m,n)$.

*** Opposé

L'opposé de $A$, noté $-A$, est tel que :

$$A + (-A) = 0$$

On a donc clairement :

$$-A = (-a_{ij})_{i,j}$$

** Soustraction

Soit les fonctions linéaires $\mathcal{A},\mathcal{B} : \corps^n
\mapsto \corps^m$ respectivement représentées par les matrices $A,B
\in \matrice(\corps,m,n)$. On définit la soustraction matricielle $A -
B$ par :

$$ A - B = A + (-B) $$

On a clairement :

$$A - B = \left[ a_{ij} - b_{ij} \right]_{i,j}$$

** Multiplication mixte

On définit la multiplication mixte $\cdot : \corps \times \matrice(\corps,m,n) \to \matrice(\corps,m,n)$ par :

$$\beta \cdot A = (\beta \cdot a_{ij})_{i,j}$$

où $\beta \in \corps$ et $A \in \matrice(\corps,m,n)$.

Choissons également $\gamma \in \corps$. On note comme d'habitude :

#+BEGIN_CENTER
\(
A \cdot \beta = \beta \cdot A \)

\(
\gamma \cdot \beta \cdot A = (\gamma \cdot \beta) \cdot A \)

\(
\beta A = \beta \cdot A
\)
#+END_CENTER

** Distributivité

Soit les matrices $A \in \matrice(\corps,m,n)$, $B \in
\matrice(\corps,n,p)$ et $C \in \matrice(\corps,n,p)$ et
$D \in \matrice(\corps,p,q)$ définies par :

$$ A = ( a_{ij} )_{i,j} $$

$$ B = ( b_{ij} )_{i,j} $$

$$ C = ( c_{ij} )_{i,j} $$

$$ D = ( d_{ij} )_{i,j} $$

La relation :

\begin{align*}
A \cdot (B + C)
&= \left[ \sum_k a_{ik} \cdot (b_{kj} + c_{kj}) \right]_{i,j} \\
&= \left[ \sum_k a_{ik} \cdot b_{kj} \right]_{i,j} + \left[ \sum_k a_{ik} \cdot c_{kj} \right]_{i,j} \\
&= A \cdot B + A \cdot C
\end{align*}

nous montre que :

$$ A \cdot (B + C) = A \cdot B + A \cdot C $$

La relation :

\begin{align*}
(B + C) \cdot D
&= \left[ \sum_k (b_{ik} + c_{ik}) \cdot d_{kj} \right]_{i,j} \\
&= \left[ \sum_k b_{ik} \cdot d_{kj} \right]_{i,j} + \left[ \sum_k c_{ik} \cdot d_{kj} \right]_{i,j} \\
&= B \cdot D + C \cdot D
\end{align*}

nous montre que :

$$ (B + C) \cdot D = B \cdot D + C \cdot D $$

On en déduit que la multiplication matricielle se distribue sur
l’addition matricielle.

* Matrice identité

La matrice identité $I \in \matrice(\corps,n,n)$ correspond à la
fonction $\identite$. On a donc :

$$I \cdot x = x$$

pour tout $x \in \corps^n$. Si $(\canonique_1,...\canonique_n)$ est
la base canonique de $\corps^n$ sous la forme de vecteurs colonnes,
on a donc :

$$I \cdot \canonique_i = \canonique_i$$

ce qui entraîne directement :

$$I = ( \indicatrice_{ij} )_{i,j}$$

On remarque que :

$$I = [\canonique_1 \ \ \ldots \ \ \canonique_n]$$

** Neutre

Comme la fonction identité est neutre pour la composition, la matrice
unité correspondante $I \in \matrice(\corps,m,n)$ doit être neutre pour
la multiplication avec toutes les matrices de dimensions compatibles. Soit
$A \in \matrice(\corps,m,n)$ et $B \in \matrice(\corps,n,p)$. On vérifie
que l'on a bien :

#+BEGIN_CENTER
\(
A \cdot I = A \)

\(
I \cdot B = B
\)
#+END_CENTER


** Notation

On note aussi $I_n$ pour préciser que $I$ est de taille $(n,n)$.

* Inverse

Lorsqu'elle existe, la matrice inverse de $A$, notée $A^{-1}$, reflète l'application linéaire inverse sous-jacente. Elle est donc l'unique matrice telle que :

$$A^{-1} \cdot A = A \cdot A^{-1} = I$$


* Inverse d'un produit

Soit $A$ et $B$ deux matrices inversibles. Les relations $C \cdot (A \cdot B) = I$ et $(A \cdot B) \cdot D = I$ nous donnent :

$$C = D = B^{-1} \cdot A^{-1}$$

et donc :

$$(A \cdot B)^{-1} = B^{-1} \cdot A^{-1}$$


** Inverse à gauche et à droite

On dit que $L$ est un inverse à gauche de $A$ si $L \cdot A = I$. On dit que $R$ est un inverse à droite de $A$ si $A \cdot R = I$.


* Puissance

Il est possible de multiplier une matrice carrée $A$ avec elle-même.
On peut donc définir l'exposant par :

#+BEGIN_CENTER
\(
A^0 = I \)

\(
A^k = A \cdot A^{k-1}
\)
#+END_CENTER


** Négative

Si l'inverse $A^{-1}$ existe, on définit également :

$$A^{-k} = (A^{-1})^k$$


* Polynômes matriciels

Ici, $\corps$ n'est plus un corps mais l'anneau des matrices $X$ de taille $(N,N)$. On dit que $p : \matrice(\corps,N,N) \mapsto \matrice(\corps,N,N)$ est un polynôme matriciel si il existe $a_0,...,a_n \in \corps$ tels que :

$$p(X) = \sum_{i = 0}^n a_i \cdot X^i$$

pour tout $X \in \matrice(\corps,N,N)$.


* Conjuguée

La conjuguée d'une matrice $A = (a_{ij})_{i,j} \in \matrice(\setC,m,n)$ est définie par la conjuguée des composantes :

$$\conjugue A = \bar{A} = ( \bar{a}_{ij} )_{i,j} = ( \conjugue a_{ij} )_{i,j}$$

* Formes lignes et colonnes

On peut exprimer une matrice $A \in \matrice(\corps,m,n)$ sous la forme
de lignes superposées. Soit :

$$ A =
\begin{Matrix}{c}
l_1 \\
l_2 \\
\vdots \\
l_m
\end{Matrix} $$

où les $l_i \in \matrice(\corps,1,n)$ sont les lignes de $A$. On note :

$$\ligne_i A = l_i$$

On peut aussi exprimer $A$ sous la forme de colonnes juxtaposées. Soit :

$$A = [c_1 \ \ c_2 \ldots \ \ c_n]$$

où les $c_i \in \matrice(\corps,m,1)$ sont les colonnes de $A$. On note :

$$\colonne_i A = c_i$$

** Lignes et colonnes

Si $x_i^T = \ligne_i(A)$ et $y_j = \colonne_j(B)$, on voit que :

$$\composante_{ij} (A \cdot B) = x_i^T \cdot y_j$$

* Blocs

#+TOC: headlines 1 local

** Partitionnement en blocs

Il est parfois très utile de partitionner une matrice en blocs, ou de
former une matrice plus grande à partir de matrices de tailles plus
petites. On note alors :

$$ A =
\begin{Matrix}{cc}
B & C \\
D & E
\end{Matrix} = \begin{Matrix}{cccccc}
b_{11} & \ldots & b_{1n} & c_{11} & \ldots & c_{1p} \\
\vdots &        & \vdots & \vdots &        & \vdots \\
b_{m1} & \ldots & b_{mn} & c_{m1} & \ldots & c_{mp} \\
d_{11} & \ldots & d_{1n} & e_{11} & \ldots & e_{1p} \\
\vdots &        & \vdots & \vdots &       & \vdots \\
d_{r1} & \ldots & d_{rn} & e_{r1} & \ldots & e_{rp}
\end{Matrix} $$

où les $b_{ij},c_{ij},d_{ij},e_{ij}$ sont respectivement les composantes
des matrices $B,C,D,E$. Ces matrices doivent évidemment être de
tailles compatibles (nombres de lignes identiques pour $B$ et $C$,
ainsi que pour $D$ et $E$ ; nombres de colonnes identiques pour $B$
et $D$, ainsi que pour $C$ et $E$).

** Extraction d’un bloc

Soit $A = (a_{ij})_{i,j}$. La fonction $\bloc_{ijkl}$ donne la
sous-matrice située entre la $i^{ème}$ et la $j^{ème}$ ligne, ainsi
qu'entre la $k^{ième}$ et la $l^{ième}$ colonne :

$$ \bloc_{ijkl} A = \begin{Matrix}{cccc}
a_{ik} & a_{i,k+1} & \ldots & a_{il} \\
a_{i+1,k} & a_{i+1,k+1} & \ldots & a_{i+1,l} \\
\vdots &        & \ddots &  \vdots \\
a_{jk} & a_{j,k+1} & \ldots & a_{jl}
\end{Matrix} $$

On appelle bloc une telle sous-matrice.

** Produit matriciel et blocs

En utilisant l'associativité de l'addition, on peut facilement vérifier
que la formule de multiplication reste valable lorsqu'on considère
des blocs de matrices au lieu des éléments, à condition de respecter
l'ordre de multiplication. Un exemple fréquemment utilisé :

#+BEGIN_CENTER
\(
\begin{Matrix}{cc}
A_{11} & A_{12} \\ A_{21} & A_{22}
\end{Matrix}
\cdot
\begin{Matrix}{cc}
B_{11} & B_{12} \\ B_{21} & B_{22}
\end{Matrix}
=
\begin{Matrix}{cc}
A_{11} \cdot B_{11} +  A_{12} \cdot B_{21} & A_{11} \cdot B_{12} +  A_{12} \cdot B_{22} \)

\(
A_{21} \cdot B_{11} +  A_{22} \cdot B_{21} & A_{21} \cdot B_{12} +  A_{22} \cdot B_{22}
\end{Matrix}
\)
#+END_CENTER

*** Bloc-diagonale

Un cas particulier important :

#+BEGIN_CENTER
\(
\begin{Matrix}{cc}
A_1 & 0 \\ 0 & A_2
\end{Matrix}
\cdot
\begin{Matrix}{cc}
B_1 & 0 \\ 0 & B_2
\end{Matrix}
=
\begin{Matrix}{cc}
A_1 \cdot B_1 & 0 \)

\(
0 & A_2 \cdot B_2
\end{Matrix}
\)
#+END_CENTER


* Transposée

Transposer une matrice $A = (a_{ij})_{i,j} \in \matrice(\corps,m,n)$ consiste à agencer ses lignes en colonnes et vice-versa :

$$ A^T = \begin{Matrix}{cccc}
a_{11} & a_{21} & \ldots & a_{m1} \\
a_{12} & a_{22} & \ldots & a_{m2} \\
\vdots &        & \ddots &  \vdots \\
a_{1n} & a_{2n} & \ldots & a_{mn}
\end{Matrix} = (a_{ji})_{i,j} $$

On voit que la transposée $A^T \in \matrice(\corps,n,m)$.

On a clairement :

$$\big( A^T \big)^T = A$$

** Produit matriciel

On vérifie que :

$$(A \cdot B)^T = B^T \cdot A^T$$

* Catégories

#+TOC: headlines 1 local

** Matrices carrées et rectangulaires

Les matrices $A \in \matrice(\corps,n,n)$ ayant même nombre
de colonnes et de lignes sont dites « carrées ». Les
matrices $A \in \matrice(\corps,m,n)$ avec $m \ne n$ sont dites
« rectangulaires ». Lorsque $m \strictinferieur n$, on dit que la
matrice est « longue ». Si $m \strictsuperieur n$, on dit que la
matrice est « haute ».

** Matrices diagonales

Une matrice diagonale contient des composantes nulles partout, à
l'exception de la diagonale où aucune contrainte n'est fixée. On peut
représenter une matrice diagonale $D \in \matrice(\corps,m,n)$ par :

$$ D = ( d_i \cdot \indicatrice_{ij} )_{i,j} $$

On peut aussi former une matrice diagonale de taille $(m,n)$ à partir
d'un vecteur $d = (d_i)_i$ de taille $(p,1)$, où $p \le \min \{ m ,
n \}$. On le note :

$$D = \diagonale_{m,n}(d) = \diagonale_{m,n}(d_1,...,d_p)$$

où :

#+BEGIN_CENTER
$$ \composante_{ij} \diagonale_{m,n}(d_1,...,d_p) =
\begin{cases}
d_i & \text{ si } i = j \in \{1,2,...,p\} \\
0 & \text{ sinon}
\end{cases} $$
#+END_CENTER

*** Carrée

Dans le cas particulier où $m = n$, on note aussi :

$$\diagonale_n(d) = \diagonale_{n,n}(d)$$

** Matrices triangulaires

Les composantes des matrices triangulaires supérieures ne peuvent être non nulles qu'au-dessus de la diagonale, c'est-à-dire lorsque $i \le j$. Elles peuvent se représenter par :

$$T = (t_{ij} \cdot \tau^+_{ij})_{i,j}$$

où :

#+BEGIN_CENTER
$$ \tau^+_{ij} =
\begin{cases}
1 & \mbox{ si } i \le j \\
0 & \mbox{ si } i \strictsuperieur j
\end{cases} $$
#+END_CENTER

Les composantes des matrices triangulaires inférieures ne peuvent être non nulles qu'au-dessous de la diagonale, c'est-à-dire lorsque $i \ge j$. Elles peuvent se représenter par :

$$T = (t_{ij} \cdot \tau^-_{ij})_{i,j}$$

où :

#+BEGIN_CENTER
$$ \tau^-_{ij} =
\begin{cases}
1 & \mbox{ si } i \ge j \\
0 & \mbox{ si } i \strictinferieur j
\end{cases} $$
#+END_CENTER


