
#+STARTUP: showall

#+TITLE: Eclats de vers : Matemat : Opérations sur les matrices
#+AUTHOR: chimay
#+EMAIL: or du val chez gé courriel commercial
#+LANGUAGE: fr
#+LINK_HOME: file:../index.html
#+LINK_UP: file:index.html
#+HTML_HEAD: <link rel="stylesheet" type="text/css" href="../style/defaut.css" />

#+OPTIONS: H:6
#+OPTIONS: toc:nil

#+TAGS: noexport(n)

[[file:index.org][Index mathématique]]

#+INCLUDE: "../include/navigan-1.org"

#+TOC: headlines 1

#+INCLUDE: "../include/latex/latex.org"

\label{chap:matrices}

* Dépendances

  - Chapitre \ref{chap:reel} : Les réels
  - Chapitre \ref{chap:complexe} : Les complexes

* Multiplication d’une matrice et d’un vecteur

Soit une matrice $A \in \matrice(\corps,m,n)$ représentant une fonction
linéaire $\mathcal{A} : \corps^n \mapsto \corps^m$. On définit le
produit matrice - vecteur pour que :

$$ A \cdot u = \mathcal{A}(u) $$

pour tout vecteur colonne $u \in \corps^n$. Si :

$$ A = (a_{ij})_{i,j} $$

on a par définition :

$$ \mathcal{A}_i(u) = \sum_{j=1}^n a_{ij} \cdot u_j $$

où $\mathcal{A}_i$ est la $i^{ème}$ composante de $\mathcal{A}$. Comme
$u$ est un vecteur colonne, on impose que le résultat de $A \cdot u$ soit
également un vecteur colonne. La définition du produit matrice-vecteur
est dès lors immédiate :

$$ A \cdot u = \left[ \sum_{j=1}^n a_{ij} \cdot u_j \right]_i $$

* Addition

Soit les fonctions linéaires $\mathcal{A},\mathcal{B} : \corps^n
\mapsto \corps^m$ respectivement représentées par les matrices $A,B
\in \matrice(\corps,m,n)$. On définit l’addition matricielle $A + B$
pour qu’elle représente la fonction $\mathcal{A} + \mathcal{B}$. On
a donc :

$$ (A + B) \cdot u
= (\mathcal{A} + \mathcal{B})(u)
= \mathcal{A}(u) + \mathcal{B}(u) $$

pour tout vecteur colonne $u \in \corps^p$. Par définition du produit
matrice-vecteur :

$$ (A + B) \cdot u
= \mathcal{A}(u) + \mathcal{B}(u)
= A \cdot u + B \cdot u $$

En terme de composantes, si :

$$ A = (a_{ij})_{i,j} $$

$$ B = (b_{ij})_{i,j} $$

$$ u = (u_i)_i $$

on a :

\begin{align*}
(A + B) \cdot u
&= A \cdot u + B \cdot u \\
&= \sum_{j=1}^n a_{ij} \cdot u_j + \sum_{j=1}^n b_{ij} \cdot u_j \\
&= \sum_{j=1}^n \left[ a_{ij} + b_{ij} \right] \cdot u_j
\end{align*}

La dernière ligne représente le produit d’une matrice
$C\in\matrice(\corps,m,p)$ de composantes :

$$ \composante_{ij} C = a_{ij} + b_{ij} $$

par $u$ :

$$ (A + B) \cdot u = C \cdot u $$

Ce résultat étant valable pour tout vecteur colonne $u \in \corps^p$, on
en déduit que $C$ est identique à $A + B$, ce qui nous donne :

$$A + B = \left[ a_{ij} + b_{ij} \right]_{i,j}$$

** Neutre

La matrice nulle $0$ est le neutre pour cette opération :

$$A + 0 = A$$

On en déduit que tous ses éléments doivent être nuls :

$$0 = ( 0 )_{i,j}$$

On note $0_{m,n}$ au lieu de $0$ lorsqu'on veut préciser que $0$ est
de taille $(m,n)$.

** Opposé

L'opposé de $A$, noté $-A$, est tel que :

$$A + (-A) = 0$$

On a donc clairement :

$$-A = (-a_{ij})_{i,j}$$

* Soustraction

Soit les fonctions linéaires $\mathcal{A},\mathcal{B} : \corps^n
\mapsto \corps^m$ respectivement représentées par les matrices $A,B
\in \matrice(\corps,m,n)$. On définit la soustraction matricielle $A -
B$ par :

$$ A - B = A + (-B) $$

On a clairement :

$$A - B = \left[ a_{ij} - b_{ij} \right]_{i,j}$$

* Multiplication mixte

Soit un scalaire $\alpha \in \corps$ et la fonction linéaire
$\mathcal{A} : \corps^n \mapsto \corps^m$ représentée par la matrice $A \in
\matrice(\corps,m,n)$. On définit la multiplication mixte $\alpha \cdot
A$ pour qu’elle représente la fonction $\alpha \cdot \mathcal{A}$. On
a donc :

$$ (\alpha \cdot A) \cdot u
= (\alpha \cdot \mathcal{A})(u)
= \alpha \cdot \mathcal{A}(u) $$

pour tout vecteur colonne $u \in \corps^p$. Par définition du produit
matrice-vecteur :

$$ (\alpha \cdot A) \cdot u
= \alpha \cdot \mathcal{A}(u)
= \alpha \cdot (A \cdot u) $$

En terme de composantes, si :

$$ A = (a_{ij})_{i,j} $$

$$ u = (u_i)_i $$

on a :

\begin{align*}
(\alpha \cdot A) \cdot u
&= \alpha \cdot (A \cdot u) \\
&= \alpha \sum_{j=1}^n a_{ij} \cdot u_j \\
&= \sum_{j=1}^n \left[ \alpha \cdot a_{ij} \right] \cdot u_j
\end{align*}

La dernière ligne représente le produit d’une matrice
$C\in\matrice(\corps,m,p)$ de composantes :

$$ \composante_{ij} C = \alpha \cdot a_{ij} $$

par $u$ :

$$ (\alpha \cdot A) \cdot u = C \cdot u $$

Ce résultat étant valable pour tout vecteur colonne $u \in \corps^p$, on
en déduit que $C$ est identique à $\alpha \cdot A$, ce qui nous donne :

$$\alpha \cdot A = \left[ \alpha \cdot a_{ij} \right]_{i,j}$$

Choissons également $\gamma \in \corps$. On note comme d'habitude :

#+BEGIN_CENTER
\(
A \cdot \beta = \beta \cdot A \)

\(
\gamma \cdot \beta \cdot A = (\gamma \cdot \beta) \cdot A \)

\(
\beta A = \beta \cdot A
\)
#+END_CENTER

* Produit matriciel

#+TOC: headlines 1 local

** Introduction

Soit les fonctions linéaires $\mathcal{A} : \corps^n \mapsto
\corps^m$ et $\mathcal{B} : \corps^p \mapsto \corps^n$ respectivement
représentées par les matrices $A \in \matrice(\corps,m,n)$ et $B \in
\matrice(\corps,n,p)$. On définit le produit matriciel $A \cdot B$
pour qu’il représente la fonction $\mathcal{A} \circ \mathcal{B}$. On
a donc :

$$ (A \cdot B) \cdot u
= (\mathcal{A} \circ \mathcal{B})(u)
= \mathcal{A}\left(\mathcal{B}(u)\right) $$

pour tout vecteur colonne $u \in \corps^p$. Par définition du produit
matrice-vecteur :

$$ (A \cdot B) \cdot u
= \mathcal{A}\left(\mathcal{B}(u)\right)
= A \cdot (B \cdot u) $$

En terme de composantes, si :

$$ A = (a_{ij})_{i,j} $$

$$ B = (b_{ij})_{i,j} $$

$$ u = (u_i)_i $$

on a :

\begin{align*}
(A \cdot B) \cdot u
&= A \cdot (B \cdot u) \\
&= \sum_{k=1}^n a_{ik} \cdot (B \cdot u)_k = \sum_{k=1}^n a_{ik} \sum_{j=1}^p b_{kj} \cdot u_j \\
&= \sum_{k=1}^n \sum_{j=1}^p a_{ik} \cdot b_{kj} \cdot u_j \\
&= \sum_{j=1}^p \left[ \sum_{k=1}^n a_{ik} \cdot b_{kj} \right] \cdot u_j
\end{align*}

La dernière ligne représente le produit d’une matrice
$C\in\matrice(\corps,m,p)$ de composantes :

$$ \composante_{ij} C = \sum_{k=1}^n a_{ik} \cdot b_{kj} $$

par $u$ :

$$ (A \cdot B) \cdot u = C \cdot u $$

Ce résultat étant valable pour tout vecteur colonne $u \in \corps^p$, on
en déduit que $C$ est identique à $A \cdot B$, ce qui nous donne :

$$A \cdot B = \left[\sum_{k=1}^n a_{ik} \cdot b_{kj}\right]_{i,j}$$

Le produit d'une matrice de taille $(m,n)$ par une matrice de taille
$(n,p)$ est une matrice de taille $(m,p)$. Pour que ce produit soit
bien défini, il est nécessaire que le nombre de colonnes $n$ de $A$
et le nombre de lignes de $B$ soient identiques.

** Notation

En pratique, on laisse souvent tomber le ``$\cdot$'' et on note $A B$
au lieu de $A \cdot B$ lorsqu'il est évident que $A$ et $B$ sont deux
matrices différentes.

** Propriétés

*** Associativité

Soit les matrices $A \in \matrice(\corps,m,n)$, $B \in
\matrice(\corps,n,p)$ et $C \in \matrice(\corps,p,q)$ définies par :

$$ A = ( a_{ij} )_{i,j} $$

$$ B = ( b_{ij} )_{i,j} $$

$$ C = ( c_{ij} )_{i,j} $$

La relation :

$$A \cdot (B \cdot C)
= \left[ \sum_{k,l} a_{ik} \cdot b_{kl} \cdot c_{lj} \right]_{i,j} = (A \cdot B) \cdot C$$

nous montre que la multiplication entre matrices est associative. On définit :

$$A \cdot B \cdot C = A \cdot (B \cdot C) = (A \cdot B) \cdot C$$

*** Non commutativité

Par contre, on peut trouver des matrices $A$ et $B$ telles que :

$$A \cdot B \ne B \cdot A$$

La multiplication matricielle n'est donc en général pas
commutative. D'ailleurs, pour que ces deux produits existent
simultanément, il faut que $A$ et $B$ soient toutes deux carrées,
ce qui n'est pas forcément le cas.

** Commutateur

La matrice associée au commutateur des fonctions linéaires :

$$[\mathcal{A},\mathcal{B}] = \mathcal{A} \circ \mathcal{B} - \mathcal{B} \circ \mathcal{A}$$

est donnée par le commutateur matriciel équivalent :

$$[A,B] = A \cdot B - B \cdot A$$

** Transposée d’un produit

Soit les matrices $A \in \matrice(\corps,m,n)$ et $B \in
\matrice(\corps,n,p)$ définies par :

$$ A = ( a_{ij} )_{i,j} $$

$$ B = ( b_{ij} )_{i,j} $$

On a :

$$ A \cdot B = \left[\sum_{k=1}^n a_{ik} \cdot b_{kj}\right]_{i,j} $$

La transposée du produit s’écrit :

$$ (A \cdot B)^T = \left[\sum_{k=1}^n a_{jk} \cdot b_{ki}\right]_{i,j} $$

On a aussi :

$$ B^T \cdot A^T
= \left[\sum_{k=1}^n b_{ki} \cdot a_{jk}\right]_{i,j}
= \left[\sum_{k=1}^n a_{jk} \cdot b_{ki}\right]_{i,j} $$

On en conclut que :

$$(A \cdot B)^T = B^T \cdot A^T$$

** Produit entre vecteurs

#+TOC: headlines 1 local

*** Produit scalaire usuel

Soit les vecteurs colonne $u,v \in \corps^n$ définis par :

$$ u = (u_i)_i $$

$$ v = (v_i)_i $$

On peut considérer que $u$ et $v$ sont aussi des matrices de taille
$(n,1)$ et calculer le produit du vecteur ligne $u^T$ par le vecteur
colonne $v$. On obtient une matrice de taille $(1,1)$, c’est-à-dire
un scalaire :

$$ u^T \cdot v = \sum_{i=1}^n u_i \cdot v_i $$

Un produit scalaire générique est souvent noté :

$$ \scalaire{u}{v} $$

**** Réel

Lorsque $\corps = \setR$, ce produit est appalé produit scalaire usuel
sur $\setR^n$. On le note :

$$ \scalaire{u}{v} = u^T \cdot v $$

On remarque le produit scalaire usuel d’un vecteur colonne réel avec
lui-même est positif ou nul :

$$ \scalaire{u}{u} = u^T \cdot u = \sum_{i=1}^n u_i^2 \ge 0 $$

**** Complexe

Lorsque $\corps = \setC$, on utilise le produit scalaire usuel sur
$\setC$ :

$$ \scalaire{u}{v} = \conjaccent{u^T} \cdot v $$

L’utilisation du complexe conjugué permet de garantir
que le produit d’un vecteur colonne avec lui-même est un réel positif ou
nul. En effet :

$$ \scalaire{u}{u}
= \conjaccent{u^T} \cdot u
= \sum_i \conjaccent{u_i} \cdot u_i
= \sum_i \abs{u_i}^2 $$

et donc :

$$ \scalaire{u}{u} \in \setR $$

$$ \scalaire{u}{u} \ge 0 $$

*** Produit tensoriel

Soit les vecteurs colonne $u \in \corps^m$ et $v \in \corps^n$ définis
par :

$$ u = (u_i)_i $$

$$ v = (v_i)_i $$

On peut considérer que $u$ et $v$ sont respectivement des matrices de
taille $(m,1)$ et $(n,1)$, et calculer le produit du vecteur colonne $u$
par le vecteur ligne $v^T$. On obtient une matrice de taille $(m,n)$ :

$$ u \cdot v^T = (u_i \cdot v_j)_{i,j} $$

Un produit tensoriel générique est souvent noté :

$$ u \otimes v $$

**** Réel

Lorsque $\corps = \setR$, ce produit est appalé produit tensoriel sur
$\setR^m \times \setR^n$. On le note :

$$ u \otimes v = u \cdot v^T $$

Si $w \in \setR^n$ est un troisième vecteur colonne, on a :

$$ (u \otimes v) \cdot w
= u \cdot v^T \cdot w
= u \cdot (v^T \cdot w) $$

et finalement :

$$ (u \otimes v) \cdot w = u \cdot \scalaire{v}{w} $$

**** Complexe

Lorsque $\corps = \setC$, on utilise une variante avec complexe conjugué :

$$ u \otimes v = u \cdot \conjaccent{v^T} $$

afin de conserver la propriété :

$$ (u \otimes v) \cdot w = u \cdot \scalaire{v}{w} $$

En effet, si $w \in \setC^n$ est un troisième vecteur colonne, on a :

$$ (u \otimes v) \cdot w
= u \cdot \conjaccent{v^T} \cdot w
= u \cdot (\conjaccent{v^T} \cdot w) $$

et finalement :

$$ (u \otimes v) \cdot w = u \cdot \scalaire{v}{w} $$

** Lignes et colonnes

Soit les matrices :

$$ A = (a_{ij})_{i,j} $$

$$ B = (b_{ij})_{i,j} $$

On a :

$$ \composante_{ij} (A \cdot B) = \sum_{k=1}^n a_{ik} \cdot b_{kj} $$

Si :

$$ l_i = \ligne_i(A) = (a_{ij})_j $$

$$ c_j = \colonne_j(B) = (a_{ij})_i $$

L’expression de la composante $i,j$ de $A \cdot B$ peut se réécrire :

$$\composante_{ij} (A \cdot B) = l_i \cdot c_j$$

** Produit matriciel et blocs

En utilisant l'associativité de l'addition, on peut facilement vérifier
que la formule de multiplication reste valable lorsqu'on considère
des blocs de matrices au lieu des éléments, à condition de respecter
l'ordre de multiplication.

Par exemple, si :

$$ A = \begin{Matrix}{cc}
A_{11} & A_{12} \\
A_{21} & A_{22}
\end{Matrix} $$

$$ B = \begin{Matrix}{cc}
B_{11} & B_{12} \\
B_{21} & B_{22}
\end{Matrix} $$

on a :

$$ A \cdot B
= \begin{Matrix}{cc}
A_{11} \cdot B_{11} +  A_{12} \cdot B_{21} & A_{11} \cdot B_{12} +  A_{12} \cdot B_{22} \\
A_{21} \cdot B_{11} +  A_{22} \cdot B_{21} & A_{21} \cdot B_{12} +  A_{22} \cdot B_{22}
\end{Matrix} $$

*** Bloc-diagonale

Un cas particulier important, si :

$$ A = \begin{Matrix}{cc}
A_1 & 0 \\
0 & A_2
\end{Matrix} $$

$$ B = \begin{Matrix}{cc}
B_1 & 0 \\
0 & B_2
\end{Matrix} $$

on a :

$$ A \cdot B = \begin{Matrix}{cc}
A_1 \cdot B_1 & 0 \\
0 & A_2 \cdot B_2
\end{Matrix} $$

* Distributivité

Soit les matrices $A \in \matrice(\corps,m,n)$, $B \in
\matrice(\corps,n,p)$ et $C \in \matrice(\corps,n,p)$ et
$D \in \matrice(\corps,p,q)$ définies par :

$$ A = ( a_{ij} )_{i,j} $$

$$ B = ( b_{ij} )_{i,j} $$

$$ C = ( c_{ij} )_{i,j} $$

$$ D = ( d_{ij} )_{i,j} $$

La relation :

\begin{align*}
A \cdot (B + C)
&= \left[ \sum_k a_{ik} \cdot (b_{kj} + c_{kj}) \right]_{i,j} \\
&= \left[ \sum_k a_{ik} \cdot b_{kj} \right]_{i,j} + \left[ \sum_k a_{ik} \cdot c_{kj} \right]_{i,j} \\
&= A \cdot B + A \cdot C
\end{align*}

nous montre que :

$$ A \cdot (B + C) = A \cdot B + A \cdot C $$

La relation :

\begin{align*}
(B + C) \cdot D
&= \left[ \sum_k (b_{ik} + c_{ik}) \cdot d_{kj} \right]_{i,j} \\
&= \left[ \sum_k b_{ik} \cdot d_{kj} \right]_{i,j} + \left[ \sum_k c_{ik} \cdot d_{kj} \right]_{i,j} \\
&= B \cdot D + C \cdot D
\end{align*}

nous montre que :

$$ (B + C) \cdot D = B \cdot D + C \cdot D $$

On en déduit que la multiplication matricielle se distribue sur
l’addition matricielle.

* Matrice identité

La matrice identité $I \in \matrice(\corps,n,n)$ correspond à la
fonction $\identite$. On a donc :

$$I \cdot u = u$$

pour tout $u \in \corps^n$. Si $(\canonique_1,...\canonique_n)$ est
la base canonique de $\corps^n$ sous la forme de vecteurs colonnes,
on a donc :

$$I \cdot \canonique_i = \canonique_i$$

ce qui entraîne directement :

$$I = ( \indicatrice_{ij} )_{i,j}$$

On remarque que :

$$I = [\canonique_1 \ \ \ldots \ \ \canonique_n]$$

** Neutre

Comme la fonction identité est neutre pour la composition, la matrice
unité correspondante doit être neutre pour la multiplication avec toutes
les matrices de dimensions compatibles. Soit $A \in \matrice(\corps,m,n)$
définie par :

$$ A = (a_{ij})_{i,j} $$

Si $I_n \in \matrice(\corps,n,n)$, on vérifie que l'on a bien :

$$ A \cdot I_n
= \left[ \sum_{k=1}^n a_{ik} \cdot \indicatrice_{kj} \right]_{i,j}
= \left[ a_{ij} \right]_{i,j} $$

c’est-à-dire :

$$ A \cdot I_n = A $$

Si $I_m \in \matrice(\corps,m,m)$, on vérifie que l'on a bien :

$$ I_m \cdot A
= \left[ \sum_{k=1}^m \indicatrice_{ik} \cdot a_{kj} \right]_{i,j}
= \left[ a_{ij} \right]_{i,j} $$

c’est-à-dire :

$$ I_m \cdot A = A $$

** Notation

On note aussi $I_n$ pour préciser que $I$ est de taille $(n,n)$.

* Inverse

** Inverse à gauche et à droite

On dit que $L$ est un inverse à gauche de $A$ si :

$$ L \cdot A = I $$

On dit que $R$ est un inverse à droite de $A$ si :

$$ A \cdot R = I $$

** Simultané

Si $A \in \matrice(\corps,n,n)$ possède à la fois un inverse $L$
à gauche et in un inverse $R$ à droite, on a :

$$ L = L \cdot I = L \cdot (A \cdot R) = (L \cdot A) \cdot R = I \cdot R = R $$

On a donc :

$$ L = R $$

La matrice inverse est donc unique et on la note $A^{-1}$. Elle est donc l'unique
matrice telle que :

$$A^{-1} \cdot A = A \cdot A^{-1} = I$$

** Fonction inverse

Soit la fonction linéaire $\mathcal{A} : \corps^n \mapsto \corps^m$
représentée par la matrice $A \in \matrice(\corps,n,n)$, qui possède un
inverse $A^{-1}$. Si $\mathcal{B}$ est la fonction linéaire représentée
par $A^{-1}$, on a :

$$ \mathcal{B}(\mathcal{A}(u)) = A^{-1} \cdot A \cdot u = I \cdot u = u $$

$$ \mathcal{A}(\mathcal{B}(u)) = A \cdot A^{-1} \cdot u = I \cdot u = u $$

pour tout vecteur colonne $u \in \corps^n$, ce qui implique :

$$ \mathcal{B} = \mathcal{A}^{-1} $$

La matrice inverse $A^{-1}$ représente la fonction linéaire inverse
$\mathcal{A}^{-1}$.

** Inverse d'un produit

Soit $A$ et $B$ deux matrices inversibles. Supposons que leur produit
$A \cdot B$ possède un inverse à gauche $L$ et un inverse à droite
$R$. La relation :

$$ L \cdot (A \cdot B) = L \cdot A \cdot B = I $$

nous donne :

$$ L \cdot A = I \cdot B^{-1} = B^{-1} $$

$$ L = B^{-1} \cdot A^{-1} $$

La relation :

$$ (A \cdot B) \cdot R = A \cdot B \cdot R = I $$

nous donne :

$$ B \cdot R = A^{-1} \cdot I = A^{-1} $$

$$ R = B^{-1} \cdot A^{-1} $$

On a donc :

$$ L = R = B^{-1} \cdot A^{-1} $$

L’inverse de $A \cdot B$ existe et est donné par :

$$(A \cdot B)^{-1} = B^{-1} \cdot A^{-1}$$

* Puissance

Il est possible de multiplier une matrice carrée $A$ avec elle-même.
On peut donc définir la puissance par :

\begin{align*}
A^0 &= I \\
A^k &= A \cdot A^{k-1}
\end{align*}

** Puissance négative

Si l'inverse $A^{-1}$ existe, on définit également :

$$A^{-k} = (A^{-1})^k$$
