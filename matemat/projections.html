<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN"
"http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" lang="fr" xml:lang="fr">
<head>
<!-- 2025-10-22 mer 11:25 -->
<meta http-equiv="Content-Type" content="text/html;charset=utf-8" />
<meta name="viewport" content="width=device-width, initial-scale=1" />
<title>Eclats de vers : Matemat : Projections</title>
<meta name="author" content="chimay" />
<meta name="generator" content="Org Mode" />
<style type="text/css">
  .title  { text-align: center;
             margin-bottom: .2em; }
  .subtitle { text-align: center;
              font-size: medium;
              font-weight: bold;
              margin-top:0; }
  .todo   { font-family: monospace; color: red; }
  .done   { font-family: monospace; color: green; }
  .priority { font-family: monospace; color: orange; }
  .tag    { background-color: #eee; font-family: monospace;
            padding: 2px; font-size: 80%; font-weight: normal; }
  .timestamp { color: #bebebe; }
  .timestamp-kwd { color: #5f9ea0; }
  .org-right  { margin-left: auto; margin-right: 0px;  text-align: right; }
  .org-left   { margin-left: 0px;  margin-right: auto; text-align: left; }
  .org-center { margin-left: auto; margin-right: auto; text-align: center; }
  .underline { text-decoration: underline; }
  #postamble p, #preamble p { font-size: 90%; margin: .2em; }
  p.verse { margin-left: 3%; }
  pre {
    border: 1px solid #e6e6e6;
    border-radius: 3px;
    background-color: #f2f2f2;
    padding: 8pt;
    font-family: monospace;
    overflow: auto;
    margin: 1.2em;
  }
  pre.src {
    position: relative;
    overflow: auto;
  }
  pre.src:before {
    display: none;
    position: absolute;
    top: -8px;
    right: 12px;
    padding: 3px;
    color: #555;
    background-color: #f2f2f299;
  }
  pre.src:hover:before { display: inline; margin-top: 14px;}
  /* Languages per Org manual */
  pre.src-asymptote:before { content: 'Asymptote'; }
  pre.src-awk:before { content: 'Awk'; }
  pre.src-authinfo::before { content: 'Authinfo'; }
  pre.src-C:before { content: 'C'; }
  /* pre.src-C++ doesn't work in CSS */
  pre.src-clojure:before { content: 'Clojure'; }
  pre.src-css:before { content: 'CSS'; }
  pre.src-D:before { content: 'D'; }
  pre.src-ditaa:before { content: 'ditaa'; }
  pre.src-dot:before { content: 'Graphviz'; }
  pre.src-calc:before { content: 'Emacs Calc'; }
  pre.src-emacs-lisp:before { content: 'Emacs Lisp'; }
  pre.src-fortran:before { content: 'Fortran'; }
  pre.src-gnuplot:before { content: 'gnuplot'; }
  pre.src-haskell:before { content: 'Haskell'; }
  pre.src-hledger:before { content: 'hledger'; }
  pre.src-java:before { content: 'Java'; }
  pre.src-js:before { content: 'Javascript'; }
  pre.src-latex:before { content: 'LaTeX'; }
  pre.src-ledger:before { content: 'Ledger'; }
  pre.src-lisp:before { content: 'Lisp'; }
  pre.src-lilypond:before { content: 'Lilypond'; }
  pre.src-lua:before { content: 'Lua'; }
  pre.src-matlab:before { content: 'MATLAB'; }
  pre.src-mscgen:before { content: 'Mscgen'; }
  pre.src-ocaml:before { content: 'Objective Caml'; }
  pre.src-octave:before { content: 'Octave'; }
  pre.src-org:before { content: 'Org mode'; }
  pre.src-oz:before { content: 'OZ'; }
  pre.src-plantuml:before { content: 'Plantuml'; }
  pre.src-processing:before { content: 'Processing.js'; }
  pre.src-python:before { content: 'Python'; }
  pre.src-R:before { content: 'R'; }
  pre.src-ruby:before { content: 'Ruby'; }
  pre.src-sass:before { content: 'Sass'; }
  pre.src-scheme:before { content: 'Scheme'; }
  pre.src-screen:before { content: 'Gnu Screen'; }
  pre.src-sed:before { content: 'Sed'; }
  pre.src-sh:before { content: 'shell'; }
  pre.src-sql:before { content: 'SQL'; }
  pre.src-sqlite:before { content: 'SQLite'; }
  /* additional languages in org.el's org-babel-load-languages alist */
  pre.src-forth:before { content: 'Forth'; }
  pre.src-io:before { content: 'IO'; }
  pre.src-J:before { content: 'J'; }
  pre.src-makefile:before { content: 'Makefile'; }
  pre.src-maxima:before { content: 'Maxima'; }
  pre.src-perl:before { content: 'Perl'; }
  pre.src-picolisp:before { content: 'Pico Lisp'; }
  pre.src-scala:before { content: 'Scala'; }
  pre.src-shell:before { content: 'Shell Script'; }
  pre.src-ebnf2ps:before { content: 'ebfn2ps'; }
  /* additional language identifiers per "defun org-babel-execute"
       in ob-*.el */
  pre.src-cpp:before  { content: 'C++'; }
  pre.src-abc:before  { content: 'ABC'; }
  pre.src-coq:before  { content: 'Coq'; }
  pre.src-groovy:before  { content: 'Groovy'; }
  /* additional language identifiers from org-babel-shell-names in
     ob-shell.el: ob-shell is the only babel language using a lambda to put
     the execution function name together. */
  pre.src-bash:before  { content: 'bash'; }
  pre.src-csh:before  { content: 'csh'; }
  pre.src-ash:before  { content: 'ash'; }
  pre.src-dash:before  { content: 'dash'; }
  pre.src-ksh:before  { content: 'ksh'; }
  pre.src-mksh:before  { content: 'mksh'; }
  pre.src-posh:before  { content: 'posh'; }
  /* Additional Emacs modes also supported by the LaTeX listings package */
  pre.src-ada:before { content: 'Ada'; }
  pre.src-asm:before { content: 'Assembler'; }
  pre.src-caml:before { content: 'Caml'; }
  pre.src-delphi:before { content: 'Delphi'; }
  pre.src-html:before { content: 'HTML'; }
  pre.src-idl:before { content: 'IDL'; }
  pre.src-mercury:before { content: 'Mercury'; }
  pre.src-metapost:before { content: 'MetaPost'; }
  pre.src-modula-2:before { content: 'Modula-2'; }
  pre.src-pascal:before { content: 'Pascal'; }
  pre.src-ps:before { content: 'PostScript'; }
  pre.src-prolog:before { content: 'Prolog'; }
  pre.src-simula:before { content: 'Simula'; }
  pre.src-tcl:before { content: 'tcl'; }
  pre.src-tex:before { content: 'TeX'; }
  pre.src-plain-tex:before { content: 'Plain TeX'; }
  pre.src-verilog:before { content: 'Verilog'; }
  pre.src-vhdl:before { content: 'VHDL'; }
  pre.src-xml:before { content: 'XML'; }
  pre.src-nxml:before { content: 'XML'; }
  /* add a generic configuration mode; LaTeX export needs an additional
     (add-to-list 'org-latex-listings-langs '(conf " ")) in .emacs */
  pre.src-conf:before { content: 'Configuration File'; }

  table { border-collapse:collapse; }
  caption.t-above { caption-side: top; }
  caption.t-bottom { caption-side: bottom; }
  td, th { vertical-align:top;  }
  th.org-right  { text-align: center;  }
  th.org-left   { text-align: center;   }
  th.org-center { text-align: center; }
  td.org-right  { text-align: right;  }
  td.org-left   { text-align: left;   }
  td.org-center { text-align: center; }
  dt { font-weight: bold; }
  .footpara { display: inline; }
  .footdef  { margin-bottom: 1em; }
  .figure { padding: 1em; }
  .figure p { text-align: center; }
  .equation-container {
    display: table;
    text-align: center;
    width: 100%;
  }
  .equation {
    vertical-align: middle;
  }
  .equation-label {
    display: table-cell;
    text-align: right;
    vertical-align: middle;
  }
  .inlinetask {
    padding: 10px;
    border: 2px solid gray;
    margin: 10px;
    background: #ffffcc;
  }
  #org-div-home-and-up
   { text-align: right; font-size: 70%; white-space: nowrap; }
  textarea { overflow-x: auto; }
  .linenr { font-size: smaller }
  .code-highlighted { background-color: #ffff00; }
  .org-info-js_info-navigation { border-style: none; }
  #org-info-js_console-label
    { font-size: 10px; font-weight: bold; white-space: nowrap; }
  .org-info-js_search-highlight
    { background-color: #ffff00; color: #000000; font-weight: bold; }
  .org-svg { }
</style>
<link rel="stylesheet" type="text/css" href="../style/defaut.css" />
<script>
  window.MathJax = {
    tex: {
      ams: {
        multlineWidth: '85%'
      },
      tags: 'ams',
      tagSide: 'right',
      tagIndent: '.8em'
    },
    chtml: {
      scale: 1.0,
      displayAlign: 'center',
      displayIndent: '0em'
    },
    svg: {
      scale: 1.0,
      displayAlign: 'center',
      displayIndent: '0em'
    },
    output: {
      font: 'mathjax-modern',
      displayOverflow: 'overflow'
    }
  };
</script>

<script
  id="MathJax-script"
  async
  src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js">
</script>
</head>
<body>
<div id="content" class="content">
<h1 class="title">Eclats de vers : Matemat : Projections</h1>
<p>
<a href="index.html">Index des Grimoires</a>
</p>

<p>
<a href="../index.html">Retour à l’accueil</a>
</p>

<div id="table-of-contents" role="doc-toc">
<h2>Table des matières</h2>
<div id="text-table-of-contents" role="doc-toc">
<ul>
<li><a href="#org6a7e489">1. Minimisation de l'écart</a></li>
<li><a href="#org5bea347">2. Théorème de Pythagore</a></li>
<li><a href="#orgfdd4466">3. Carré de la distance</a></li>
<li><a href="#org30c6313">4. Cauchy-Schwartz</a></li>
<li><a href="#orgac600e7">5. Propriétés extrémales</a></li>
<li><a href="#org5303575">6. Sous-espace vectoriel</a></li>
<li><a href="#orgc8e8ee4">7. Tenseur de projection</a></li>
<li><a href="#orgccc7360">8. Gram-Schmidt</a></li>
<li><a href="#org60c120a">9. Représentation matricielle</a></li>
<li><a href="#org13501bf">10. Factorisation</a></li>
<li><a href="#org7aed524">11. Vecteurs $A$-orthogonaux</a></li>
</ul>
</div>
</div>

<p>
\(
\newcommand{\parentheses}[1]{\left(#1\right)}
\newcommand{\crochets}[1]{\left[#1\right]}
\newcommand{\accolades}[1]{\left\{#1\right\}}
\newcommand{\ensemble}[1]{\left\{#1\right\}}
\newcommand{\identite}{\mathrm{Id}}
\newcommand{\indicatrice}{\boldsymbol{\delta}}
\newcommand{\dirac}{\delta}
\newcommand{\moinsun}{{-1}}
\newcommand{\inverse}{\ddagger}
\newcommand{\pinverse}{\dagger}
\newcommand{\topologie}{\mathfrak{T}}
\newcommand{\ferme}{\mathfrak{F}}
\newcommand{\img}{\mathbf{i}}
\newcommand{\binome}[2]{
\left\{ \begin{array}{c}
#1 \\
#2 \\
\end{array} \right\}
}
\newcommand{\canonique}{\mathfrak{c}}
\newcommand{\tenseuridentite}{\boldsymbol{\mathcal{I}}}
\newcommand{\permutation}{\boldsymbol{\epsilon}}
\newcommand{\matriceZero}{\mathfrak{0}}
\newcommand{\matriceUn}{\mathfrak{1}}
\newcommand{\christoffel}[2]{
\left\{ \begin{array}{c}
#1 \\
#2 \\
\end{array} \right\}
}
\newcommand{\lagrangien}{\mathfrak{L}}
\newcommand{\sousens}{\mathfrak{P}}
\newcommand{\partition}{\mathrm{Partition}}
\newcommand{\tribu}{\mathrm{Tribu}}
\newcommand{\topologies}{\mathrm{Topo}}
\newcommand{\setB}{\mathbb{B}}
\newcommand{\setN}{\mathbb{N}}
\newcommand{\setZ}{\mathbb{Z}}
\newcommand{\setQ}{\mathbb{Q}}
\newcommand{\setR}{\mathbb{R}}
\newcommand{\setC}{\mathbb{C}}
\newcommand{\corps}{\mathbb{K}}
\newcommand{\boule}{\mathfrak{B}}
\newcommand{\intervalleouvert}[2]{\left] #1 , #2 \right[}
\newcommand{\intervallesemiouvertgauche}[2]{ \left] #1 , #2 \right]}
\newcommand{\intervallesemiouvertdroite}[2]{\left[ #1 , #2 \right[ }
\newcommand{\fonction}{\mathbb{F}}
\newcommand{\bijection}{\mathrm{Bij}}
\newcommand{\polynome}{\mathrm{Poly}}
\newcommand{\lineaire}{\mathrm{Lin}}
\newcommand{\continue}{\mathrm{Cont}}
\newcommand{\homeomorphisme}{\mathrm{Hom}}
\newcommand{\etagee}{\mathrm{Etagee}}
\newcommand{\lebesgue}{\mathrm{Leb}}
\newcommand{\lipschitz}{\mathrm{Lip}}
\newcommand{\suitek}{\mathrm{Suite}}
\newcommand{\matrice}{\mathbb{M}}
\newcommand{\krylov}{\mathrm{Krylov}}
\newcommand{\tenseur}{\mathbb{T}}
\newcommand{\essentiel}{\mathfrak{E}}
\newcommand{\relation}{\mathrm{Rel}}
\DeclareMathOperator*{\strictinferieur}{\ < \ }
\DeclareMathOperator*{\strictsuperieur}{\ > \ }
\DeclareMathOperator*{\ensinferieur}{\eqslantless}
\DeclareMathOperator*{\enssuperieur}{\eqslantgtr}
\DeclareMathOperator*{\esssuperieur}{\gtrsim}
\DeclareMathOperator*{\essinferieur}{\lesssim}
\newcommand{\essegal}{\eqsim}
\newcommand{\union}{\ \cup \ }
\newcommand{\intersection}{\ \cap \ }
\newcommand{\opera}{\divideontimes}
\newcommand{\autreaddition}{\boxplus}
\newcommand{\autremultiplication}{\circledast}
\newcommand{\commutateur}[2]{\left[ #1 , #2 \right]}
\newcommand{\convolution}{\circledcirc}
\newcommand{\correlation}{\ \natural \ }
\newcommand{\diventiere}{\div}
\newcommand{\modulo}{\bmod}
\DeclareMathOperator*{\pgcd}{pgcd}
\DeclareMathOperator*{\ppcm}{ppcm}
\newcommand{\produitscalaire}[2]{\left\langle #1 \vert #2 \right\rangle}
\newcommand{\scalaire}[2]{\left\langle #1 \| #2 \right\rangle}
\newcommand{\braket}[3]{\left\langle #1 \vert #2 \vert #3 \right\rangle}
\newcommand{\orthogonal}{\bot}
\newcommand{\forme}[2]{\left\langle #1 , #2 \right\rangle}
\newcommand{\biforme}[3]{\left\langle #1 , #2 , #3 \right\rangle}
\newcommand{\contraction}[3]{\left\langle #1 \odot #3 \right\rangle_{#2}}
\newcommand{\dblecont}[5]{\left\langle #1 \vert #3 \vert #5 \right\rangle_{#2,#4}}
\DeclareMathOperator*{\major}{major}
\DeclareMathOperator*{\minor}{minor}
\DeclareMathOperator*{\maxim}{maxim}
\DeclareMathOperator*{\minim}{minim}
\DeclareMathOperator*{\argument}{arg}
\DeclareMathOperator*{\argmin}{arg\ min}
\DeclareMathOperator*{\argmax}{arg\ max}
\DeclareMathOperator*{\supessentiel}{ess\ sup}
\DeclareMathOperator*{\infessentiel}{ess\ inf}
\newcommand{\dual}{\star}
\newcommand{\distance}{\mathfrak{dist}}
\newcommand{\norme}[1]{\left\| #1 \right\|}
\newcommand{\normetrois}[1]{\left|\left\| #1 \right\|\right|}
\DeclareMathOperator*{\adh}{adh}
\DeclareMathOperator*{\interieur}{int}
\newcommand{\frontiere}{\partial}
\DeclareMathOperator*{\image}{im}
\DeclareMathOperator*{\domaine}{dom}
\DeclareMathOperator*{\noyau}{ker}
\DeclareMathOperator*{\support}{supp}
\DeclareMathOperator*{\signe}{sign}
\newcommand{\abs}[1]{\left| #1 \right|}
\newcommand{\unsur}[1]{\frac{1}{#1}}
\newcommand{\arrondisup}[1]{\lceil #1 \rceil}
\newcommand{\arrondiinf}[1]{\lfloor #1 \rfloor}
\DeclareMathOperator*{\conjugue}{conj}
\newcommand{\conjaccent}[1]{\overline{#1}}
\DeclareMathOperator*{\division}{division}
\newcommand{\difference}{\boldsymbol{\Delta}}
\newcommand{\differentielle}[2]{\mathfrak{D}^{#1}_{#2}}
\newcommand{\OD}[2]{\frac{d #1}{d #2}}
\newcommand{\OOD}[2]{\frac{d^2 #1}{d #2^2}}
\newcommand{\NOD}[3]{\frac{d^{#3} #1}{d #2^{#3}}}
\newcommand{\deriveepartielle}[2]{\frac{\partial #1}{\partial #2}}
\newcommand{\PD}[2]{\frac{\partial #1}{\partial #2}}
\newcommand{\dblederiveepartielle}[2]{\frac{\partial^2 #1}{\partial #2 \partial #2}}
\newcommand{\dfdxdy}[3]{\frac{\partial^2 #1}{\partial #2 \partial #3}}
\newcommand{\dfdxdx}[2]{\frac{\partial^2 #1}{\partial #2^2}}
\newcommand{\gradient}{\mathbf{\nabla}}
\newcommand{\combilin}[1]{\mathrm{span}\{ #1 \}}
\DeclareMathOperator*{\trace}{tr}
\newcommand{\proba}{\mathbb{P}}
\newcommand{\probaof}[1]{\mathbb{P}\left[#1\right]}
\newcommand{\esperof}[1]{\mathbb{E}\left[#1\right]}
\newcommand{\cov}[2]{\mathrm{cov} \left( #1 , #2 \right) }
\newcommand{\var}[1]{\mathrm{var} \left( #1 \right) }
\newcommand{\rand}{\mathrm{rand}}
\newcommand{\variation}[1]{\left\langle #1 \right\rangle}
\DeclareMathOperator*{\composante}{comp}
\DeclareMathOperator*{\bloc}{bloc}
\DeclareMathOperator*{\ligne}{ligne}
\DeclareMathOperator*{\colonne}{colonne}
\DeclareMathOperator*{\diagonale}{diag}
\newcommand{\matelementaire}{\mathrm{Elem}}
\DeclareMathOperator*{\matpermutation}{permut}
\newcommand{\matunitaire}{\mathrm{Unitaire}}
\newcommand{\gaussjordan}{\mathrm{GaussJordan}}
\newcommand{\householder}{\mathrm{Householder}}
\DeclareMathOperator*{\rang}{rang}
\newcommand{\schur}{\mathrm{Schur}}
\newcommand{\singuliere}{\mathrm{DVS}}
\newcommand{\convexe}{\mathrm{Convexe}}
\newcommand{\petito}[1]{o\left(#1\right)}
\newcommand{\grando}[1]{O\left(#1\right)}
\)
</p>

<p>
\(
\newenvironment{Eqts}
{ \begin{equation*} \begin{gathered} }
{ \end{gathered} \end{equation*} }
\newenvironment{Matrix}
{\left[ \begin{array}}
{\end{array} \right]}
\)
</p>

<p>
\label{chap:project}
</p>
<div id="outline-container-org6a7e489" class="outline-2">
<h2 id="org6a7e489"><span class="section-number-2">1.</span> Minimisation de l'écart</h2>
<div class="outline-text-2" id="text-1">
<p>
Soit un espace vectoriel \(E\) sur \(\setR\) et des vecteurs \(x,y \in E\), où \(x \ne 0\). La projection de \(y\) sur \(x\) est le vecteur :
</p>

<p>
\[p \in \combilin{x} = \{ \gamma \cdot x : \gamma \in \setR \}\]
</p>

<p>
qui minimise sur \(\combilin{x}\) la norme usuelle \(\norme{e} = \sqrt{ \scalaire{e}{e} }\) de l'écart \(e\) séparant \(y\) de \(p\). Afin de trouver le \(\lambda \in \setR\) minimisant cet écart, on utilise la fonction \(\mathcal{E} : \setR \mapsto \setR\) définie par :
</p>

<p>
\[\mathcal{E}(\gamma) = \scalaire{y - \gamma \cdot x}{y - \gamma \cdot x} = \scalaire{y}{y} - 2 \cdot \gamma \cdot \scalaire{x}{y} + \gamma^2 \cdot \scalaire{x}{x}\]
</p>

<p>
pour tout \(\gamma \in \setR\). Nous imposons l'annulation de la dérivée en un certain \(\gamma = \lambda\) qui minimise potentiellement l'écart :
</p>

<p>
\[\partial \mathcal{E}(\lambda) = - 2 \cdot \scalaire{x}{y} + 2 \cdot \lambda \cdot \scalaire{x}{x} = 0\]
</p>

<p>
ce qui nous donne :
</p>

<p>
\[\lambda = \frac{\scalaire{x}{y}}{\scalaire{x}{x}}\]
</p>
</div>
<div id="outline-container-org75247da" class="outline-3">
<h3 id="org75247da"><span class="section-number-3">1.1.</span> Produit scalaire complexe</h3>
<div class="outline-text-3" id="text-1-1">
<p>
Nous nous plaçons à présent dans le cas plus général où \(E\) est un espace vectoriel sur \(\setC\). Le \(\lambda\) défini plus haut devient donc un complexe. Nous allons déterminer s'il minimise la fonction \(\mathcal{E} : \setC \mapsto \setR\) définie par :
</p>

<p>
\[\mathcal{E}(\gamma) = \norme{y - \gamma \cdot x}^2\]
</p>

<p>
pour tout \(\gamma \in \setC\). Considérons le vecteur d'écart :
</p>

<p>
\[e = y - \lambda \cdot x\]
</p>

<p>
On voit qu'il vérifie la propriété :
</p>

<p>
\[\scalaire{x}{e} = \scalaire{x}{y} - \lambda \cdot \scalaire{x}{x} = 0\]
</p>

<p>
On a donc aussi :
</p>

<p>
\[\scalaire{e}{x} = \conjugue \scalaire{x}{e} = 0\]
</p>

<p>
On dit que le vecteur d'écart est orthogonal à \(x\). Soit l'écart \(\delta = \gamma - \lambda\). On a alors \(\gamma = \lambda + \delta\) et :
</p>

<p>
\[y - \gamma \cdot x = y - \lambda \cdot x - \delta \cdot x = e - \delta \cdot x\]
</p>

<p>
En utilisant les propriétés du produit scalaire, on en déduit :
</p>

\begin{align}
\mathcal{E}(\gamma) &= \scalaire{e - \delta \cdot x}{e - \delta \cdot x} \)

\(
&= \scalaire{e}{e} - \delta \cdot \scalaire{e}{x} - \conjaccent{\delta} \cdot \scalaire{x}{e} + \abs{\delta}^2 \cdot \scalaire{x}{x} \)

\(
&= \scalaire{e}{e} + \abs{\delta}^2 \cdot \scalaire{x}{x}
\end{align}

<p>
Comme \(\abs{\delta}^2 \cdot \scalaire{x}{x}\) est un réel \(\ge 0\), on a finalement :
</p>

<p>
\[\mathcal{E}(\gamma) \ge \scalaire{e}{e} = \mathcal{E}(\lambda)\]
</p>

<p>
Notre paramètre \(\lambda\) ainsi défini minimise donc bien \(\mathcal{E}\) sur \(\setC\). De plus, si \(\delta \ne 0\) on a \(\abs{\delta}^2 \strictsuperieur 0\) et
\(\mathcal{E}(\gamma) \strictsuperieur \scalaire{e}{e}\), ce qui prouve que :
</p>

<p>
\[\lambda = \arg\min_{\gamma \in \setC} \mathcal{E}(\gamma)\]
</p>

<p>
est l'unique complexe à minimiser \(\mathcal{E}\). La racine carrée étant une fonction monotone strictement croissante sur \(\setR\), la norme \(\norme{e} = \sqrt{\mathcal{E}(\lambda)}\) est donc également minimisée et la projection s'écrit :
</p>

<p>
\[p = \lambda \cdot x = \frac{ \scalaire{x}{y} }{ \scalaire{x}{x} } \cdot x\]
</p>
</div>
</div>
</div>
<div id="outline-container-org5bea347" class="outline-2">
<h2 id="org5bea347"><span class="section-number-2">2.</span> Théorème de Pythagore</h2>
<div class="outline-text-2" id="text-2">
<p>
Calculons à présent la norme de \(y\) en partant de la relation :
</p>

<p>
\[y = p + e\]
</p>

<p>
On déduit de la propriété d'orthogonalité que :
</p>

<p>
\[\scalaire{p}{e} = \lambda \cdot \scalaire{x}{e} = 0\]
</p>

<p>
On peut donc appliquer le théorème de pythagore, qui nous dit que :
</p>

<p>
\[\norme{y}^2 = \norme{p + e}^2 = \norme{p}^2 + \norme{e}^2\]
</p>
</div>
</div>
<div id="outline-container-orgfdd4466" class="outline-2">
<h2 id="orgfdd4466"><span class="section-number-2">3.</span> Carré de la distance</h2>
<div class="outline-text-2" id="text-3">
<p>
Si :
</p>

<p>
\[D = \min_{\gamma \in \setC} \norme{y - \gamma \cdot x}\]
</p>

<p>
on a donc :
</p>

<p>
\[D^2 = \norme{y - \lambda \cdot x}^2 = \norme{e}^2 = \norme{y}^2 - \norme{p}^2\]
</p>

<p>
et finalement :
</p>

<p>
\[D^2 = \norme{y}^2 - \abs{\lambda}^2 \cdot \norme{x}^2\]
</p>
</div>
</div>
<div id="outline-container-org30c6313" class="outline-2">
<h2 id="org30c6313"><span class="section-number-2">4.</span> Cauchy-Schwartz</h2>
<div class="outline-text-2" id="text-4">
<p>
Par positivité du produit scalaire, on a bien évidemment :
</p>

<p>
\[\norme{e}^2 = \scalaire{e}{e} \ge 0\]
</p>

<p>
Le théorème de Pythagore implique donc que :
</p>

<p>
\[\scalaire{p}{p} = \norme{p}^2 \le \norme{y}^2 = \scalaire{y}{y}\]
</p>

<p>
En substituant \(p = \lambda \cdot x\), on obtient :
</p>

<p>
\[\conjaccent \lambda \cdot \lambda \cdot \scalaire{x}{x} \le \scalaire{y}{y}\]
</p>

<p>
Mais comme :
</p>

<p>
\[\conjaccent{\lambda} = \conjugue \frac{ \scalaire{x}{y} }{ \scalaire{x}{x} } = \frac{ \scalaire{y}{x} }{ \scalaire{x}{x} }\]
</p>

<p>
on a :
</p>

<p>
\[\frac{ \scalaire{y}{x} }{ \scalaire{x}{x} } \cdot \frac{ \scalaire{x}{y} }{ \scalaire{x}{x} } \cdot \scalaire{x}{x} \le \scalaire{y}{y}\]
</p>

<p>
En simplifiant et en multipliant par la norme carrée de \(x\), on a finalement :
</p>

<p>
\[\scalaire{y}{x} \cdot \scalaire{x}{y} \le \scalaire{x}{x} \cdot \scalaire{y}{y}\]
</p>

<p>
relation dont la racine nous donne l'inégalité de Cauchy-Schwartz :
</p>

<p>
\[\abs{ \scalaire{x}{y} } \le \norme{x} \cdot \norme{y}\]
</p>
</div>
</div>
<div id="outline-container-orgac600e7" class="outline-2">
<h2 id="orgac600e7"><span class="section-number-2">5.</span> Propriétés extrémales</h2>
<div class="outline-text-2" id="text-5">
<p>
L'égalité :
</p>

<p>
\[\norme{p}^2 = \norme{y}^2\]
</p>

<p>
n'est atteinte que lorsque \(\norme{e}^2 = 0\), c'est-à-dire \(e = 0\) et \(y = \lambda \cdot x\) pour un certain \(\lambda \in \setC\). Ce constat nous amène aux problèmes d'optimisations suivants. Soit un vecteur \(c \ne 0\) fixé et :
</p>

<p>
\[d = \norme{c} = \sqrt{ \scalaire{c}{c} }\]
</p>

<p>
On cherche à maximiser ou à minimiser :
</p>

<p>
\[\varphi(x) = \scalaire{c}{x}\]
</p>

<p>
sur l'ensemble \(B = \boule(0,r) = \{ x : \norme{x} \le r \}\). L'inégalité de Cauchy-Schwartz nous dit que :
</p>

<p>
\[- d \cdot r \le - \norme{c} \cdot \norme{x} \le \scalaire{c}{x} \le \norme{c} \cdot \norme{x} \le d \cdot r\]
</p>

<p>
Nous allons chercher nos solutions sous la forme \(x = \alpha \cdot c\), pour un certain \(\alpha \in \setC\). On a alors :
</p>

<p>
\[\norme{\alpha \cdot c} = \abs{\alpha} \cdot \norme{c} = \abs{\alpha} \cdot d \le r\]
</p>

<p>
et donc \(\abs{\alpha} \le r / d\). Si on choisit \(\alpha = r / d\), on a :
</p>

<p>
\[\scalaire{c}{x} = \frac{r}{d} \cdot \scalaire{c}{c} = \frac{r}{d} \cdot d^2 = d \cdot r\]
</p>

<p>
La borne supérieure est atteinte. La fonction \(\varphi\) est donc maximisée :
</p>

<p>
\[\eta = \frac{r}{d} \cdot c \in \arg\max_{x \in B} \scalaire{c}{x}\]
</p>

<p>
Si on choisit \(\alpha = - r / d\), on a :
</p>

<p>
\[\scalaire{c}{x} = - \frac{r}{d} \cdot \scalaire{c}{c} = - d \cdot r\]
</p>

<p>
La borne inférieure est atteinte. La fonction \(\varphi\) est donc minimisée :
</p>

<p>
\[\theta = - \frac{r}{d} \cdot c \in \arg\min_{x \in B} \scalaire{c}{x}\]
</p>
</div>
</div>
<div id="outline-container-org5303575" class="outline-2">
<h2 id="org5303575"><span class="section-number-2">6.</span> Sous-espace vectoriel</h2>
<div class="outline-text-2" id="text-6">
<p>
Soit l'espace vectoriel \(E\) sur \(\setR\) et un sous-espace vectoriel \(U \subseteq E\) possédant une base orthonormée \((u_1,...,u_n)\). La projection d'un vecteur quelconque \(z \in E\) sur \(U\) est le vecteur \(p \in U\) qui minimise la norme de l'écart entre \(z\) et \(p\). On cherche donc le \(\lambda = (\lambda_1,...,\lambda_n) \in \setR^n\) qui minimise la fonction \(\mathcal{E} : \setR^n \mapsto \setR\) définie par :
</p>

<p>
\[\mathcal{E}(\gamma) = \norme{z - \sum_{i = 1}^n \gamma_i \cdot u_i}^2\]
</p>

<p>
pour tout \(\gamma = (\gamma_1,...,\gamma_n) \in \setR^n\). En utilisant \(\scalaire{u_i}{u_j} = \indicatrice_{ij}\), on obtient :
</p>

\begin{align}
\mathcal{E}(\gamma) &= \scalaire{z}{z} - 2 \sum_i \gamma_i \cdot \scalaire{u_i}{z} + \sum_{i,j} \gamma_i \cdot \gamma_j \cdot \scalaire{u_i}{u_j} \)

\(
&= \scalaire{z}{z} - 2 \sum_i \gamma_i \cdot \scalaire{u_i}{z} + \sum_i \gamma_i^2
\end{align}

<p>
Imposant \(\partial_k \mathcal{E}(\lambda_1,...,\lambda_n) = 0\), on obtient les \(n\) équations :
</p>

<p>
\[-2 \lambda_k \cdot \scalaire{u_k}{z} + 2 \lambda_k = 0\]
</p>

<p>
On en déduit que le choix :
</p>

<p>
\[\lambda = (\lambda_1,...,\lambda_n) = (\scalaire{u_1}{z},...,\scalaire{u_n}{z})\]
</p>

<p>
minimise potentiellement \(\mathcal{E}\). Notre projection potentielle de \(z\) sur \(U\) est donc donnée par :
</p>

<p>
\[p = \sum_i \scalaire{u_i}{z} \cdot u_i\]
</p>
</div>
<div id="outline-container-orgf90c14f" class="outline-3">
<h3 id="orgf90c14f"><span class="section-number-3">6.1.</span> Produit scalaire complexe</h3>
<div class="outline-text-3" id="text-6-1">
<p>
Nous nous plaçons à présent dans le cas plus général où \(E\) est un espace vectoriel sur \(\setC\). Le \(\lambda\) défini plus haut devient donc un élément de \(\setC^n\). Nous allons déterminer s'il minimise la fonction \(\mathcal{E} : \setC^n \mapsto \setR\) définie par :
</p>

<p>
\[\mathcal{E}(\gamma) = \norme{z - \sum_{i = 1}^n \gamma_i \cdot u_i}^2\]
</p>

<p>
pour tout \(\gamma = (\gamma_1,...,\gamma_n) \in \setC^n\). Choisissons \(x \in U\). On a alors :
</p>

<p>
\[x = \sum_i x_i \cdot u_i\]
</p>

<p>
où \(x_i = \scalaire{u_i}{z}\). Considérons le vecteur d'écart :
</p>

<p>
\[e = z - p\]
</p>

<p>
On a alors :
</p>

\begin{align}
\scalaire{x}{e} &= \sum_i \conjaccent{x_i} \cdot \scalaire{u_i}{z - p} \)

\(
&= \sum_i \conjaccent{x_i} \cdot \scalaire{u_i}{z} - \sum_{i,j} \conjaccent{x_i} \cdot \scalaire{u_i}{u_j} \cdot \lambda_j \)

\(
&= \sum_i \scalaire{x}{u_i} \cdot \scalaire{u_i}{z} - \sum_i \scalaire{x}{u_i} \cdot \scalaire{u_i}{z} = 0
\end{align}

<p>
Le vecteur d'écart est donc orthogonal à tous les \(x \in U\). On a également :
</p>

<p>
\[\scalaire{e}{x} = \conjugue \scalaire{x}{e} = 0\]
</p>

<p>
Soit l'écart \(\delta = \gamma - \lambda\). On a alors \(\gamma = \lambda + \delta\). Posons :
</p>

<div class="org-center">
<p>
\(
g = \sum_i \gamma_i \cdot u_i \)
</p>

<p>
\(
p = \sum_i \lambda_i \cdot u_i \)
</p>

<p>
\(
\Delta = \sum_i \delta_i \cdot u_i
\)
</p>
</div>

<p>
On a alors \(z - g = z - p - \Delta = e - \Delta\) et :
</p>

\begin{align}
\mathcal{E}(\gamma) &= \scalaire{e - \Delta}{e - \Delta} \)

\(
&= \scalaire{e}{e} - \scalaire{e}{\Delta} - \scalaire{\Delta}{e} + \scalaire{\Delta}{\Delta}
\end{align}

<p>
Comme \(\Delta \in U\), ses produits scalaires avec \(e\) s'annulent par orthogonalité et on a finalement :
</p>

<p>
\[\mathcal{E}(\gamma) = \scalaire{e}{e} + \scalaire{\Delta}{\Delta} \ge \scalaire{e}{e} = \mathcal{E}(\lambda)\]
</p>

<p>
Les scalaires \(\lambda_k\) minimisent donc bien la norme de l'écart sur \(U\).
</p>
</div>
</div>
<div id="outline-container-org963eaa9" class="outline-3">
<h3 id="org963eaa9"><span class="section-number-3">6.2.</span> Théorème de Pythagore</h3>
<div class="outline-text-3" id="text-6-2">
<p>
On a \(z = p + e\). Comme \(p\) est un vecteur de \(U\), on a \(\scalaire{p}{e} = \scalaire{e}{p} = 0\). Le théorème de Pythagore est donc applicable :
</p>

<p>
\[\norme{z}^2 = \norme{p}^2 + \norme{e}^2\]
</p>
</div>
</div>
<div id="outline-container-org0323c61" class="outline-3">
<h3 id="org0323c61"><span class="section-number-3">6.3.</span> Carré de la distance</h3>
<div class="outline-text-3" id="text-6-3">
<p>
Soit :
</p>

<p>
\[D = \min \{ \norme{z - v} : v \in U \}\]
</p>

<p>
Par orthonormalité, on a :
</p>

<p>
\[\norme{p}^2 = \sum_{i,j} \conjugue(\scalaire{u_i}{z}) \cdot \scalaire{u_j}{z} \cdot \scalaire{u_i}{u_j} = \sum_i \abs{\scalaire{u_i}{z}}^2\]
</p>

<p>
et donc :
</p>

<p>
\[D^2 = \norme{z}^2 - \norme{p}^2 = \norme{z}^2 - \sum_i \abs{\scalaire{u_i}{z}}^2\]
</p>
</div>
</div>
</div>
<div id="outline-container-orgc8e8ee4" class="outline-2">
<h2 id="orgc8e8ee4"><span class="section-number-2">7.</span> Tenseur de projection</h2>
<div class="outline-text-2" id="text-7">
<p>
On peut réécrire la projection de \(z\) sur \(U\) sous la forme :
</p>

<p>
\[p = \sum_i u_i \cdot \scalaire{u_i}{z}\]
</p>

<p>
Cette expression ressemble à la contraction d'ordre \(1\) d'un tenseur d'ordre \(2\). Effectivement, si on pose :
</p>

<p>
\[\mathcal{P} = \sum_i u_i \otimes u_i\]
</p>

<p>
on a alors :
</p>

<p>
\[p = \mathcal{P} \cdot z = \contraction{\mathcal{P} }{1}{z}\]
</p>
</div>
<div id="outline-container-org32cc0ed" class="outline-3">
<h3 id="org32cc0ed"><span class="section-number-3">7.1.</span> Identité locale</h3>
<div class="outline-text-3" id="text-7-1">
<p>
Pour tout \(y \in U\), on a :
</p>

<p>
\[y = \sum_i \scalaire{u_i}{y} \cdot u_i\]
</p>

<p>
et :
</p>

<p>
\[\mathcal{P} \cdot y = \sum_i u_i \cdot \scalaire{u_i}{y}\]
</p>

<p>
Ces deux expressions étant identiques, on a \(\mathcal{P} \cdot y = y\). Le tenseur de projection correspond donc localement (sur \(U\)) au tenseur identité.
</p>
</div>
</div>
<div id="outline-container-org75a16a1" class="outline-3">
<h3 id="org75a16a1"><span class="section-number-3">7.2.</span> Invariance</h3>
<div class="outline-text-3" id="text-7-2">
<p>
Pour tout \(z \in E\), on a \(y = \mathcal{P} \cdot z \in U\). On en conclut que :
</p>

<p>
\[\mathcal{P} \cdot z = y = \mathcal{P} \cdot y = \mathcal{P} \cdot (\mathcal{P} \cdot z)\]
</p>

<p>
Donc \(\mathcal{P} = \mathcal{P} \cdot \mathcal{P}\).
</p>
</div>
</div>
<div id="outline-container-org84ea325" class="outline-3">
<h3 id="org84ea325"><span class="section-number-3">7.3.</span> Complémentaire</h3>
<div class="outline-text-3" id="text-7-3">
<p>
Soit \((u_1,...,u_n)\) une base orthonormée de \(E\). On voit que le tenseur identité :
</p>

<p>
\[\tenseuridentite = \sum_{i = 1}^n u_i \otimes u_i\]
</p>

<p>
est le tenseur de projection de \(E\) dans lui-même. On considère le tenseur de projection sur \(\combilin{u_1,...,u_m}\), où \(m \le n\) :
</p>

<p>
\[\mathcal{P} = \sum_{i = 1}^m u_i \otimes u_i\]
</p>

<p>
Le tenseur complémentaire est défini par :
</p>

<p>
\[\mathcal{Q} = \tenseuridentite - \mathcal{P} = \sum_{i = 1}^n u_i \otimes u_i - \sum_{i = 1}^m u_i \otimes u_i = \sum_{i = m + 1}^n u_i \otimes u_i\]
</p>

<p>
Il s'agit donc d'un tenseur de projection sur l'espace complémentaire \(\combilin{u_{m + 1},...,u_n}\). Il est lié à l'écart de projection car \(e = z - p = \mathcal{Q} \cdot z\). On remarque l'orthogonalité :
</p>

<p>
\[\mathcal{Q} \cdot \mathcal{P} = (\tenseuridentite - \mathcal{P}) \cdot \mathcal{P} = \mathcal{P} - \mathcal{P} \cdot \mathcal{P} = 0\]
</p>

<p>
De même :
</p>

<p>
\[\mathcal{P} \cdot \mathcal{Q} = \mathcal{P} \cdot \mathcal{P} - \mathcal{P} = 0\]
</p>

<p>
On a aussi sans surprise :
</p>

<p>
\[\mathcal{Q} \cdot \mathcal{Q} = \mathcal{Q} - \mathcal{Q} \cdot \mathcal{P} =  \mathcal{Q}\]
</p>
</div>
</div>
</div>
<div id="outline-container-orgccc7360" class="outline-2">
<h2 id="orgccc7360"><span class="section-number-2">8.</span> Gram-Schmidt</h2>
<div class="outline-text-2" id="text-8">
<p>
Nous allons construire une suite de vecteurs orthonormaux \((u_1,u_2,...u_n)\) à partir d'une suite \((a_1,a_2,...,a_n)\) de vecteurs linéairement indépendants de \(E\). L'indépendance linéaire nous garantit que \(a_1 \ne 0\). On peut donc normaliser pour obtenir le premier vecteur de la série :
</p>

<p>
\[u_1 = \frac{a_1}{ \norme{a_1} }\]
</p>

<p>
On a alors :
</p>

<p>
\[\scalaire{u_1}{u_1} = \frac{ \scalaire{a_1}{a_1} }{ \norme{a_1}^2 } = \frac{ \scalaire{a_1}{a_1} }{ \scalaire{a_1}{a_1} } = 1\]
</p>

<p>
Nous projetons \(a_2\) sur \(u_1\) en utilisant le tenseur \(\mathcal{P}_1 = u_1 \otimes u_1\) et nous évaluons l'écart :
</p>

<p>
\[e_2 = (\tenseuridentite - \mathcal{P}_1) \cdot a_2 = a_2 - u_1 \cdot \scalaire{u_1}{a_2}\]
</p>

<p>
Les propriétés d'orthogonalité de l'écart nous assurent alors que \(\scalaire{u_1}{e_2} = 0\). L'indépendance linéaire nous garantit que \(e_2 \ne 0\). On peut donc normaliser en utilisant \(u_2 = e_2 / \norme{e_2}\). On a alors clairement \(\scalaire{u_2}{u_2} = 1\) et \(\scalaire{u_1}{u_2} = 0\).
</p>

<p>
Supposons à présent avoir trouvé la suite orthonormée \((u_1,u_2,...,u_k)\), où \(k \le n - 1\). Nous projetons \(a_{k + 1}\) sur l'espace vectoriel \(U_k = \combilin{u_1,u_2,...,u_k}\) en utilisant le tenseur :
</p>

<p>
\[\mathcal{P}_k = \sum_{i = 1}^k u_i \otimes u_i\]
</p>

<p>
et nous évaluons l'écart :
</p>

<p>
\[e_{k + 1} = (\tenseuridentite - \mathcal{P}_k) \cdot a_{k + 1} = a_{k + 1} - \sum_{i = 1}^k u_i \cdot \scalaire{u_i}{a_{k + 1} }\]
</p>

<p>
Les propriétés d'orthogonalité de l'écart nous assurent alors que \(\scalaire{u_j}{e_{k + 1} } = 0\) pour tout \(j \in \{1,2,...,k\}\). L'indépendance linéaire nous garantit que \(e_{k + 1} \ne 0\). On peut donc normaliser en utilisant :
</p>

<p>
\[u_{k + 1} = \frac{e_{k + 1} }{ \norme{ e_{k + 1} } }\]
</p>

<p>
On a alors clairement \(\scalaire{u_j }{u_{k + 1} } = \indicatrice_{j, k + 1}\).
</p>

<p>
Nous disposons donc à la fin du processus d'une suite de vecteurs \((u_1,u_2,...,u_n)\) orthonormée :
</p>

<p>
\[\scalaire{u_i}{u_j} = \indicatrice_{ij}\]
</p>

<p>
Ce procédé est appelé processus d'orthogonalisation de Gram-Schmidt.
</p>
</div>
<div id="outline-container-org18090a4" class="outline-3">
<h3 id="org18090a4"><span class="section-number-3">8.1.</span> Remarque</h3>
<div class="outline-text-3" id="text-8-1">
<p>
Dans le cas où l'indépendance linéaire n'est pas garantie, il est toujours possible d'adapter l'algorithme en enlevant dynamiquement de la liste des \(a_k\) les vecteurs donnant un écart de projection nul. On se retrouve alors à la fin du processus avec une suite orthonormée \((u_1,...,u_m)\), où \(m \le n\).
</p>
</div>
</div>
</div>
<div id="outline-container-org60c120a" class="outline-2">
<h2 id="org60c120a"><span class="section-number-2">9.</span> Représentation matricielle</h2>
<div class="outline-text-2" id="text-9">
<p>
Soient \(u_1,u_2,...,u_m \in \matrice(\corps,n,1)\) des vecteurs matriciels orthonormés pour le produit scalaire usuel :
</p>

<p>
\[u_i^\dual \cdot u_j = \indicatrice_{ij}\]
</p>

<p>
La matrice de projection associée au tenseur de projection sur \(U = \combilin{u_1,...,u_m}\) s'écrit :
</p>

<p>
\[P = \sum_{k = 1}^m u_k \otimes u_k = \sum_{k = 1}^m u_k \cdot u_k^\dual\]
</p>

<p>
Il s'agit donc d'une matrice de taille \((n,n)\). Elle permet de projeter tout vecteur matriciel \(z \in \matrice(\corps,n,1)\) sur \(U\) :
</p>

<p>
\[P \cdot z = \sum_{k = 1}^m u_k \cdot u_k^\dual \cdot z = \sum_{k = 1}^m u_k \cdot \scalaire{u_k}{z}\]
</p>
</div>
</div>
<div id="outline-container-org13501bf" class="outline-2">
<h2 id="org13501bf"><span class="section-number-2">10.</span> Factorisation</h2>
<div class="outline-text-2" id="text-10">
<p>
En terme de composantes, si \(u_k = ( u_{kj} )_j\) et si \(P = ( p_{ij} )_{i,j}\), on a :
</p>

<p>
\[p_{ij} = \sum_{k = 1}^m u_{ki} \cdot \conjaccent{u}_{kj}\]
</p>

<p>
expression qui ressemble furieusement à un produit matriciel. Soit la matrice \(U\) de taille \((n,m)\) rassemblant les \(m\) vecteurs \(u_k\) :
</p>

<p>
\[U = [u_1 \ u_2 \ \hdots \ u_m]\]
</p>

<p>
On a alors :
</p>

\begin{align}
\composante_{ik} U &= u_{ki} \)

\(
\composante_{kj} U^\dual &= \conjugue \composante_{jk} U = \conjaccent{u}_{kj}
\end{align}

<p>
Le produit ci-dessus peut donc s'écrire :
</p>

<p>
\[p_{ij} = \sum_{k = 1}^m b_{ik} \cdot c_{kj}\]
</p>

<p>
où \(b_{ik} = \composante_{ik} U\) et \(c_{kj} = \composante_{kj} U^\dual\). On a donc finalement :
</p>

<p>
\[P = U \cdot U^\dual\]
</p>
</div>
<div id="outline-container-orge901fac" class="outline-3">
<h3 id="orge901fac"><span class="section-number-3">10.1.</span> Propriétés</h3>
<div class="outline-text-3" id="text-10-1">
<p>
Comme les \(u_k\) sont orthonormaux, on a :
</p>

<p>
\[\composante_{ij} (U^\dual \cdot U) = u_i^\dual \cdot u_j = \scalaire{u_i}{u_j} = \indicatrice_{ij}\]
</p>

<p>
On a donc \(U^\dual \cdot U = I\), la matrice identité de taille \((m,m)\). On a également, comme attendu :
</p>

<p>
\[P^2 = U \cdot U^\dual \cdot U \cdot U^\dual = U \cdot I \cdot U^\dual = U \cdot U^\dual = P\]
</p>

<p>
Si on pose \(Q = I - P\), on a aussi :
</p>

<p>
\[P \cdot Q = Q \cdot P = P - P^2 = 0\]
</p>
</div>
</div>
<div id="outline-container-org9d2ba87" class="outline-3">
<h3 id="org9d2ba87"><span class="section-number-3">10.2.</span> Cas particulier</h3>
<div class="outline-text-3" id="text-10-2">
<p>
Les \(m\) vecteurs \(u_i\) étant linéairement indépendants dans \(\corps^n\) qui est de dimension \(n\), on doit forcément avoir \(m \le n\). Dans le cas où \(m = n\), on a de plus \(U^\dual = U^{-1}\) et :
</p>

<p>
\[\sum_{i = 1}^n u_i \otimes u_i = P = U \cdot U^{-1} = I\]
</p>
</div>
</div>
</div>
<div id="outline-container-org7aed524" class="outline-2">
<h2 id="org7aed524"><span class="section-number-2">11.</span> Vecteurs $A$-orthogonaux</h2>
<div class="outline-text-2" id="text-11">
<p>
Soit une matrice de produit scalaire \(A \in \matrice(\corps,n,n)\). On peut utiliser le procédé de Gram-Schmidt pour construire une base de vecteurs orthogonaux pour le produit scalaire :
</p>

<p>
\[\scalaire{x}{y} = x^\dual \cdot A \cdot y\]
</p>

<p>
On part d'une suite \((a_1,a_2,...,a_n)\) de vecteurs matriciels linéairement indépendants de \(\corps^n\) (typiquement la base canonique). On commence par normaliser \(a_1\) :
</p>

<p>
\[u_1 = \frac{a_1}{ \sqrt{a_1^\dual \cdot A \cdot a_1} }\]
</p>

<p>
et on construit étape après étape la suite des \(u_i\). Supposons être arrivé à la suite \((u_1,u_2,...,u_k)\) vérifiant \(\scalaire{u_i}{u_j} = u_i^\dual \cdot A \cdot u_j = \indicatrice_{ij}\), où \(k \le n - 1\). Nous projetons \(a_{k + 1}\) sur l'espace vectoriel \(\combilin{u_1,u_2,...,u_k}\) et nous évaluons l'écart :
</p>

\begin{align}
e_{k + 1} &= a_{k + 1} - \sum_{i = 1}^k u_i \cdot \scalaire{u_i}{ a_{k + 1} } \)

\(
&= a_{k + 1} - \sum_{i = 1}^k u_i \cdot u_i^\dual \cdot A \cdot a_{k + 1}
\end{align}

<p>
Ensuite, nous normalisons le résultat :
</p>

<p>
\[u_{k + 1} = \frac{e_{k + 1} }{ \sqrt{e_{k + 1}^\dual \cdot A \cdot e_{k + 1} } }\]
</p>

<p>
Nous disposons donc à la fin du processus d'une suite de vecteurs \((u_1,u_2,...,u_n)\) orthonormée :
</p>

<p>
\[\scalaire{u_i}{u_j} = u_i^\dual \cdot A \cdot u_j = \indicatrice_{ij}\]
</p>

<p>
On dit que la suite des \(u_i\) est $A$-orthonormée. Si on définit la famille de matrices :
</p>

<p>
\[U_m = [u_1 \ u_2 \ ... \ u_m]\]
</p>

<p>
pour tout \(m \le n\), on peut réecrire l'orthogonalité comme suit :
</p>

<p>
\[U_m^\dual \cdot A \cdot U_m = I_m\]
</p>

<p>
On note aussi \(U = U_n\).
</p>
</div>
<div id="outline-container-org1dd7c70" class="outline-3">
<h3 id="org1dd7c70"><span class="section-number-3">11.1.</span> Complément orthogonal</h3>
<div class="outline-text-3" id="text-11-1">
<p>
Si les vecteurs orthonormaux \((u_1,...,u_p)\), où \(p \le n\), sont donnés, on peut simplement commencer le processus à \(k = p\) pour complèter la suite de vecteurs jusqu'à \(k = n\). On obtient alors la suite orthonormée \((u_1,...,u_p,u_{p + 1},...,u_n)\). On dit que \((u_{p + 1},...,u_n)\) est le complément orthogonal de \((u_1,...,u_p)\).
</p>
</div>
</div>
<div id="outline-container-org45d965f" class="outline-3">
<h3 id="org45d965f"><span class="section-number-3">11.2.</span> Orthogonalité usuelle</h3>
<div class="outline-text-3" id="text-11-2">
<p>
Dans le cas où l'on choisit \(A = I\), cette méthode offre un moyen d'obtenir (ou de compléter) une suite de vecteurs \(u_i\) tels que \(u_i^\dual \cdot u_j = \indicatrice_{ij}\) et des matrices \(U_m\) correspondantes telles que \(U_m^\dual \cdot U_m = I_m\). Comme \(U = U_n\) est carrée, on a de plus :
</p>

<p>
\[U^{-1} = U^\dual\]
</p>
</div>
</div>
<div id="outline-container-orga70751b" class="outline-3">
<h3 id="orga70751b"><span class="section-number-3">11.3.</span> Systèmes linéaires</h3>
<div class="outline-text-3" id="text-11-3">
<p>
Lorsqu'on dispose de \(n\) vecteurs $A$-orthonormés, il est facile de résoudre le système linéaire \(A \cdot x = y\). Les \(u_i\) forment alors une base de \(\corps^n\) et on peut trouver des scalaires \(x_i \in \corps\) tels que :
</p>

<p>
\[x = \sum_{i = 1}^n x_i \cdot u_i\]
</p>

<p>
En prenant le produit scalaire de \(x\) avec \(u_k\), on obtient :
</p>

<p>
\[u_k^\dual \cdot A \cdot x = \sum_{i = 1}^n (u_k^\dual \cdot A \cdot u_i) \cdot x_i = x_k\]
</p>

<p>
On voit donc apparaître une expression analogue à celle d'une projection usuelle :
</p>

<p>
\[x = \sum_{i = 1}^n u_i \cdot (u_i^\dual \cdot A \cdot x) = \sum_{i = 1}^n u_i \cdot u_i^\dual \cdot y\]
</p>

<p>
Attention, analogue n'est pas identique, les \(u_i\) ne sont en général pas orthonormés pour le produit scalaire usuel.
</p>
</div>
</div>
</div>
</div>
<div id="postamble" class="status">
<p class="author">Auteur: chimay</p>
<p class="date">Created: 2025-10-22 mer 11:25</p>
<p class="validation"><a href="https://validator.w3.org/check?uri=referer">Validate</a></p>
</div>
</body>
</html>
